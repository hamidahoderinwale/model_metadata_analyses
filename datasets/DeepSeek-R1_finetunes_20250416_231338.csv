,model_id,card,metadata,depth,children,children_count,adapters,adapters_count
0,deepseek-ai/DeepSeek-R1,"---
license: mit
library_name: transformers
---
# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align=""center"">
  <img src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"" width=""60%"" alt=""DeepSeek-V3"" />
</div>
<hr>
<div align=""center"" style=""line-height: 1;"">
  <a href=""https://www.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Homepage"" src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://chat.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Chat"" src=""https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://huggingface.co/deepseek-ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Hugging Face"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://discord.gg/Tc7c45Zzu5"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Discord"" src=""https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Wechat"" src=""https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://twitter.com/deepseek_ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Twitter Follow"" src=""https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"" style=""margin: 2px;"">
    <img alt=""License"" src=""https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>


<p align=""center"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf""><b>Paper Link</b>üëÅÔ∏è</a>
</p>


## 1. Introduction

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. 
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. 
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.

**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**

<p align=""center"">
  <img width=""80%"" src=""figures/benchmark.jpg"">
</p>

## 2. Model Summary

---

**Post-Training: Large-Scale Reinforcement Learning on the Base Model**

-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.

-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
    We believe the pipeline will benefit the industry by creating better models. 

---

**Distillation: Smaller Models Can Be Powerful Too**

-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. 
- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

## 3. Model Downloads

### DeepSeek-R1 Models

<div align=""center"">

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. 
For more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.

### DeepSeek-R1-Distill Models

<div align=""center"">

| **Model** | **Base Model** | **Download** |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.

## 4. Evaluation Results

### DeepSeek-R1-Evaluation
 For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.
<div align=""center"">


| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Architecture | - | - | MoE | - | - | MoE |
| | # Activated Params | - | - | 37B | - | - | 37B |
| | # Total Params | - | - | 671B | - | - | 671B |
| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### Distilled Model Evaluation


<div align=""center"">

| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Chat Website & API Platform
You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button ""DeepThink""

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

### DeepSeek-R1 Models

Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.

**NOTE: Hugging Face's Transformers has not been directly supported yet.**

### DeepSeek-R1-Distill Models

DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.

For instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

You can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### Usage Recommendations

**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**

1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.
2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**
3. For mathematical problems, it is advisable to include a directive in your prompt such as: ""Please reason step by step, and put your final answer within \boxed{}.""
4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.

Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting ""\<think\>\n\n\</think\>"") when responding to certain queries, which can adversely affect the model's performance.
**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with ""\<think\>\n"" at the beginning of every output.**

## 7. License
This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. Citation
```
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

```

## 9. Contact
If you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).
","{""modelId"": ""deepseek-ai/DeepSeek-R1"", ""sha"": ""56d4cbbb4d29f4355bab4b9a39ccb717a14ad5ad"", ""tags"": [""transformers"", ""safetensors"", ""deepseek_v3"", ""text-generation"", ""conversational"", ""custom_code"", ""arxiv:2501.12948"", ""license:mit"", ""autotrain_compatible"", ""endpoints_compatible"", ""fp8"", ""region:us""], ""downloads"": 1622379, ""pipeline_tag"": ""text-generation""}",0,"['https://huggingface.co/nvidia/DeepSeek-R1-FP4', 'https://huggingface.co/harshw030/sameeraAI', 'https://huggingface.co/maersee3423423/statuetka', 'https://huggingface.co/Duckets/Duckbot1', 'https://huggingface.co/Prarabdha/law_gpt', 'https://huggingface.co/Kelly70/Kelly', 'https://huggingface.co/Szilard12/UNITY', 'https://huggingface.co/desmond-initiative/news_api_context', 'https://huggingface.co/raajveers/youtube-title-gen', 'https://huggingface.co/CynthiaAAAA/deepseek-chat', 'https://huggingface.co/clgingeniero/sammarty', 'https://huggingface.co/ashad846004/DeepSeek-R1-Medical-COT', 'https://huggingface.co/lorenzzzo/lorezAI', 'https://huggingface.co/sarthak156/anichat', 'https://huggingface.co/vataAiTech/songSystem', 'https://huggingface.co/YooJeahkhn/YooJeahkhn', 'https://huggingface.co/Fr0sT-FLAB/SolidityGPT', 'https://huggingface.co/Pim-mobile/Our-Pim', 'https://huggingface.co/Al-rahman/Deepseek', 'https://huggingface.co/jatin183/Celci', 'https://huggingface.co/margerz156/margthink', 'https://huggingface.co/buyun/test-model', 'https://huggingface.co/Futuresony/Future_pics_26-01-2025', 'https://huggingface.co/drperkybottom/DeepLerting-LLM', 'https://huggingface.co/Minnus/rtrancit', 'https://huggingface.co/chitdev/deepseek-r1-distill-7b', 'https://huggingface.co/usersomethingelze/birdinyourear', 'https://huggingface.co/Tackit/Flensburg', 'https://huggingface.co/FernDelga/CorpoBotdelFer', 'https://huggingface.co/devayanihodgir/Resume_Analyzer', 'https://huggingface.co/devl-8980-sn/india_legal_QA_deepseek', 'https://huggingface.co/xiaoyuboi/test-model', 'https://huggingface.co/niloyda/AnythingChatBot', 'https://huggingface.co/saleh1977/nexta-9101', 'https://huggingface.co/ayeshawtahir/pharmacopeia', 'https://huggingface.co/sarvar3697/sarvar_2', 'https://huggingface.co/feitap/exp', 'https://huggingface.co/marlono/test', 'https://huggingface.co/Adamastor/bully', 'https://huggingface.co/zain10000/ChatBot', 'https://huggingface.co/karim8955/mate', 'https://huggingface.co/usamaaleem99tech/DeepSeek-R1-Medical', 'https://huggingface.co/rshaikh22/coachcarellm', 'https://huggingface.co/DangChuVM/Model', 'https://huggingface.co/YuRiVeRTi/VQ1', 'https://huggingface.co/rehamhisham/saas', 'https://huggingface.co/nicogptai/omega.1-2', 'https://huggingface.co/guanglian/test', 'https://huggingface.co/wsxdyzx2025/weigb', 'https://huggingface.co/farypor/seoaigen', 'https://huggingface.co/seenutheleo/imdb-model', 'https://huggingface.co/silence09/DeepSeek-R1-3layers', 'https://huggingface.co/PARSIS/Moshaver', 'https://huggingface.co/yifan-playground/deepseek-r1', 'https://huggingface.co/curryNI/huaiqing_ml_model', 'https://huggingface.co/raulmoraless/Raul.IA', 'https://huggingface.co/kkangnom/test', 'https://huggingface.co/huihui-ai/DeepSeek-R1-Pruned-Coder-411B', 'https://huggingface.co/OmarGX/Omar.Gx', 'https://huggingface.co/CyrusXtovia/MetLawBot', 'https://huggingface.co/GeorgeWeasley84/convert-case', 'https://huggingface.co/Ai1God/Godboy', 'https://huggingface.co/GalaxyPoo/Mine', 'https://huggingface.co/Aspenini/Backwards-AI', 'https://huggingface.co/tempbggff/test', 'https://huggingface.co/Withersen/AIArtCreator', 'https://huggingface.co/coralgables/crypto', 'https://huggingface.co/ExplodeMediaG/011_search-model', 'https://huggingface.co/Awaiz031/Awaizahmad', 'https://huggingface.co/Yadav009/Aiclothchange', 'https://huggingface.co/zedx1/BlueAI', 'https://huggingface.co/SirFestus/Text-To-Text', 'https://huggingface.co/michaelngangom/dummy-bank', 'https://huggingface.co/Albert9527/model-demo', 'https://huggingface.co/Hamzillo/Lolo', 'https://huggingface.co/Nerker/Rdrffg', 'https://huggingface.co/zonnell/discord', 'https://huggingface.co/Mylamoore040/Myla', 'https://huggingface.co/Lotusaihk/lotusaihk', 'https://huggingface.co/Vepa1979/turkmence', 'https://huggingface.co/raghu1155/DeepSeek-R1-Codegeneration-COT', 'https://huggingface.co/tornado4651/test', 'https://huggingface.co/Raymondjoe007/thor', 'https://huggingface.co/ManishDipole/Demo', 'https://huggingface.co/lilmos/twins-ai', 'https://huggingface.co/FarhanisGoingTomakeaAi/NiteTalkbot', 'https://huggingface.co/mradermacher/DeepSeek-R1-GGUF', 'https://huggingface.co/Dimaswa/openrail', 'https://huggingface.co/fematt/telebot', 'https://huggingface.co/Owen14gjqwertkeyboard/LibrarianAI', 'https://huggingface.co/yookidz/my-code-Llama', 'https://huggingface.co/tonybb815/Tiny', 'https://huggingface.co/Haryni/model', 'https://huggingface.co/Klanik58/Devrim_DSE', 'https://huggingface.co/samaraamfetamina/frai', 'https://huggingface.co/djibhefihnserfnh/vxfvf', 'https://huggingface.co/kalleopinheiro/deepseek', 'https://huggingface.co/RajibGartia/Apache.2.0', 'https://huggingface.co/mikmik2003/jaz2', 'https://huggingface.co/primaryPond/product_comparison', 'https://huggingface.co/gabrial1927/gabrial', 'https://huggingface.co/Priyansu17/miningAact', 'https://huggingface.co/Murphy112233/Murphy_Rose', 'https://huggingface.co/beita6969/DeepSeek-R1-Distill-Qwen-32B-Medical', 'https://huggingface.co/MISHANM/deepseek-ai-DeepSeek-R1-BF16.gguf', 'https://huggingface.co/William-zhao/Cozysmart', 'https://huggingface.co/sunooooone/KIMSUNOOMODEL', 'https://huggingface.co/NazarMuts/FridayAPI', 'https://huggingface.co/Smdhussain06/Joyboy', 'https://huggingface.co/Athipan01/GoDathipan', 'https://huggingface.co/4TO/MC_Farmer', 'https://huggingface.co/genaitiwari/deepseek', 'https://huggingface.co/InlineHydraulik/Autoencoder', 'https://huggingface.co/ComputerAi/Bob', 'https://huggingface.co/Northflux3/test', 'https://huggingface.co/idriscanbay/1', 'https://huggingface.co/Sugamk/vai', 'https://huggingface.co/Mehrankarajii/Mehran', 'https://huggingface.co/Kelinsia/Traininghuggy', 'https://huggingface.co/FEYSALjhn/Lisov', 'https://huggingface.co/persadian/CropSeek-LLM', 'https://huggingface.co/sezer2737/sorucoz', 'https://huggingface.co/kghuggingface/kg1repo', 'https://huggingface.co/c8tc/nnew_new', 'https://huggingface.co/Joncarel/Vernertranslate', 'https://huggingface.co/AIbyAnmol/publicity', 'https://huggingface.co/deca-ai/2-mini-beta', 'https://huggingface.co/gokhandemirau/Elizabet', 'https://huggingface.co/lekadesire/Football_Predict', 'https://huggingface.co/emirke159753159753/abii', 'https://huggingface.co/Harshitv/test', 'https://huggingface.co/1986random/l', 'https://huggingface.co/Reda2566/Reda_68', 'https://huggingface.co/soupbutt/writefanfic', 'https://huggingface.co/VybezR/Helop', 'https://huggingface.co/andr1sv/hpp', 'https://huggingface.co/Dashutosh884/Hugging_Face', 'https://huggingface.co/XZHY/customer_service_chatbot_DeepSeek-R1-Distill-Qwen-1.5B_DPO', 'https://huggingface.co/bkaplan/MRL2', 'https://huggingface.co/Pweenut/QazNLTK_Model', 'https://huggingface.co/yangyu1111/2', 'https://huggingface.co/Yaroslavgtytry/gngn', 'https://huggingface.co/boilerbambam/NEW_APP', 'https://huggingface.co/lukeshaye/testelukeshaye', 'https://huggingface.co/chunien/gp44785', 'https://huggingface.co/TrevSh/Demo_Edu_Model', 'https://huggingface.co/PrakashCider/Your-Solmate', 'https://huggingface.co/BadiciCyra/rag', 'https://huggingface.co/VANNVISAL/LLM_Model', 'https://huggingface.co/yt-X/deepseek-r1-dpo', 'https://huggingface.co/aodev/EmBotV2', 'https://huggingface.co/zonnell/discord_bot', 'https://huggingface.co/SAMdahal/aiitenarary', 'https://huggingface.co/visnu90/pycooking', 'https://huggingface.co/Hqrunkeke/Deepseekk', 'https://huggingface.co/Jiajiawei/mySelfTalk', 'https://huggingface.co/Dombrenk30/0xDom', 'https://huggingface.co/ibtp1256/tpmodel', 'https://huggingface.co/bokomoko/boletoreader', 'https://huggingface.co/RecurvAI/Recurv-Clinical-Deepseek-R1', 'https://huggingface.co/Yeeheng/repo', 'https://huggingface.co/Leto-cmd/Oddessey', 'https://huggingface.co/mattivityroom/huggingface_nlp', 'https://huggingface.co/myself-model/11', 'https://huggingface.co/pretonetworking/Roteirobom', 'https://huggingface.co/Bilkees/Ikhlaq', 'https://huggingface.co/AbdullahAli06/abdullahali_ai', 'https://huggingface.co/exco369/infinity', 'https://huggingface.co/Alejandro1266/Studying', 'https://huggingface.co/Mexa57/Vi', 'https://huggingface.co/nishantmourya/bio', 'https://huggingface.co/kuazi/deepseek-r1-medical-test', 'https://huggingface.co/Efeeg/beyza', 'https://huggingface.co/Drachenkrieger/Novela_Era_4.0', 'https://huggingface.co/rkeval/LearnAI', 'https://huggingface.co/YTPG524/The_Fight_for_Top', 'https://huggingface.co/sprunkiphase3/unblocked', 'https://huggingface.co/xugui/test', 'https://huggingface.co/William-zhao/KuCozy', 'https://huggingface.co/antondanilevskiy/GTCauto', 'https://huggingface.co/cr6276/mymodel', 'https://huggingface.co/shubhamnagane/news', 'https://huggingface.co/Warnsey/Teaching_Model', 'https://huggingface.co/YaserSabriFMD/Jj', 'https://huggingface.co/perplexity-ai/r1-1776', 'https://huggingface.co/deevnnv/nomadchroniclesapi', 'https://huggingface.co/bijorn/winger', 'https://huggingface.co/sherooz/ahmed', 'https://huggingface.co/kauiu/janker0.0', 'https://huggingface.co/Kaanjoa/Joa0.6', 'https://huggingface.co/wangju123/xiaoju', 'https://huggingface.co/andong90/DeepSeek-R1-Distill-Qwen-7B-student-mental-health-json', 'https://huggingface.co/thalesleal/carteiraia', 'https://huggingface.co/qp521/ibm-chatbot-model', 'https://huggingface.co/Average8/ast', 'https://huggingface.co/samira456/english-hindi', 'https://huggingface.co/SIMAMING/REVO-ART2.0', 'https://huggingface.co/sensey42/Talep', 'https://huggingface.co/LiuTengYing/CarRadio', 'https://huggingface.co/emartinezra/Arsonai', 'https://huggingface.co/dla9944/test', 'https://huggingface.co/silence09/DeepSeek-R1-Small-2layers', 'https://huggingface.co/weapon-x/chatbot', 'https://huggingface.co/Sumitnawale68/Sumit', 'https://huggingface.co/Lukiii498/test', 'https://huggingface.co/sanun4730/chat', 'https://huggingface.co/r4isy/kenu', 'https://huggingface.co/athitiya/personal', 'https://huggingface.co/DaKaufeeBoii/Cleo', 'https://huggingface.co/julelti/Ci', 'https://huggingface.co/PNZAGI/TRAIN', 'https://huggingface.co/0x6e676e/generate-context', 'https://huggingface.co/Angiie/Angie-light', 'https://huggingface.co/Jianshu001/Efficient_CoT_DeepSeek-R1-Distill-Qwen-7B', 'https://huggingface.co/Kumargogia/Kavya', 'https://huggingface.co/mih12345/deepseek_R1_jaman_josna', 'https://huggingface.co/ritense/test-model', 'https://huggingface.co/praveenrmd/TamilGPT', 'https://huggingface.co/Jobzi/AhSimon', 'https://huggingface.co/mikaelcostake/brain0', 'https://huggingface.co/JustVenus/Venus', 'https://huggingface.co/RecurvAI/Recurv-Medical-Deepseek-R1', 'https://huggingface.co/alex322r/deepseek-responder', 'https://huggingface.co/cmoraes199322/autonomo', 'https://huggingface.co/unsloth/DeepSeek-R1-BF16', 'https://huggingface.co/UkYYY/eva', 'https://huggingface.co/wrestling-is-real-bro/airules', 'https://huggingface.co/sandeep-aipm/AI-Code', 'https://huggingface.co/Yeamkuan/enanalysis', 'https://huggingface.co/opensourcerelease/DeepSeek-R1-bf16', 'https://huggingface.co/TheWolfOfWallStreet/The_Wolf_Of_Wall_Street', 'https://huggingface.co/alexpineda97/traductor_otoesp', 'https://huggingface.co/deca-ai/2-mini', 'https://huggingface.co/DragosBDI/GPT_test', 'https://huggingface.co/aliMohammad16/sabrina-ai', 'https://huggingface.co/samfati/humanvoice', 'https://huggingface.co/d92refea/Asistente', 'https://huggingface.co/0xchum/Fugen', 'https://huggingface.co/Hataco/RR-SwordFigthing', 'https://huggingface.co/death-walker/harmoni', 'https://huggingface.co/gimmy256/deepseek_r1_finetuned', 'https://huggingface.co/ImmersioNAI/Poppy', 'https://huggingface.co/jasonlinn/yilanpass', 'https://huggingface.co/AntVess/new74', 'https://huggingface.co/Monternot888/Test_de_Bert', 'https://huggingface.co/silkstringfiddlesink/Astra-49', 'https://huggingface.co/IcYhAwK88/BeeAndMe', 'https://huggingface.co/Alhdrawi/R-RAY-AI', 'https://huggingface.co/KaPe22/KaPe22', 'https://huggingface.co/aishu1505/english-tamil-translation', 'https://huggingface.co/dailong/mymode', 'https://huggingface.co/kazzaou/app', 'https://huggingface.co/pinnacle001/steph', 'https://huggingface.co/TanAIspaceX/test1', 'https://huggingface.co/mertkb/palmtree', 'https://huggingface.co/cwestbrook/lotrdata', 'https://huggingface.co/gresres/test', 'https://huggingface.co/samwilenborg30/chatbot', 'https://huggingface.co/Yaavuzzz/Yavuz', 'https://huggingface.co/Hi14th/test', 'https://huggingface.co/yerifantess/weeklyupdate', 'https://huggingface.co/Michael419/Ii', 'https://huggingface.co/Favour99/ALPHA', 'https://huggingface.co/javier001/Javier', 'https://huggingface.co/DivineNinja13/bubaModel', 'https://huggingface.co/AlexandreCezar/SaudeMental', 'https://huggingface.co/Dach13/Darryc', 'https://huggingface.co/an4l0g/test', 'https://huggingface.co/Random7878/Life', 'https://huggingface.co/adarshgiri55/Adi', 'https://huggingface.co/orgullomoore/TexLawLLM', 'https://huggingface.co/mahgam88/Jafr', 'https://huggingface.co/FANzinho/FanSilver', 'https://huggingface.co/theone2b/99', 'https://huggingface.co/Acardozo/llama3.2', 'https://huggingface.co/ykarout/phi-4-deepseek-reasoning-fp16', 'https://huggingface.co/beita6969/deepseek-r1-medical-response', 'https://huggingface.co/Vaimee/fggggr', 'https://huggingface.co/karrrr123456/ace', 'https://huggingface.co/Avener/RealTime', 'https://huggingface.co/RZEE17/New1', 'https://huggingface.co/Gary88/mymodel', 'https://huggingface.co/ZZVCV/FHZBox', 'https://huggingface.co/JulienSunLib/Sunlib', 'https://huggingface.co/urjinchimed/khalkhmongol', 'https://huggingface.co/Ebaturan/GokTurk', 'https://huggingface.co/Nitipoom/matcha888', 'https://huggingface.co/Virtual-Herbalist/Herbalist-AI', 'https://huggingface.co/Oluwadamo/Damo', 'https://huggingface.co/tariqaziz80/dentists', 'https://huggingface.co/MimiTechAI/DeepSeek-R1-Distill-Llama-70B', 'https://huggingface.co/mdjobayarehosen/Bing3', 'https://huggingface.co/meghrajs/demo', 'https://huggingface.co/himanshuvas/test', 'https://huggingface.co/Arrowxyz/hux-ai', 'https://huggingface.co/disconzi/oze', 'https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF']",297,"['https://huggingface.co/dpr1360/design', 'https://huggingface.co/TheWolfOfChain/TA2MA', 'https://huggingface.co/imsanjoykb/deepSQL-R1-distill-8B', 'https://huggingface.co/SHUBH677/U.M_chat_AI', 'https://huggingface.co/AI-Larry/Deepseek-r1-7b-Media', 'https://huggingface.co/bunkerwallx/engendro', 'https://huggingface.co/cabetedesco/reversegpt', 'https://huggingface.co/hooptechnologies/voip_call', 'https://huggingface.co/runningsnail13/snail', 'https://huggingface.co/khulnasoft-devsec/AutofixCodeAI', 'https://huggingface.co/Avener/Sd3m', 'https://huggingface.co/parthib07/mistral-finetuned-alpaca', 'https://huggingface.co/eugenedurham74/Piabet', 'https://huggingface.co/ganaimo/apocalypse', 'https://huggingface.co/Bloodlyghoul1/Bloodly', 'https://huggingface.co/DanielOlan/a', 'https://huggingface.co/Nimz47/Sonia', 'https://huggingface.co/Florencias/Sofbeck', 'https://huggingface.co/Zoniiii/Desconocidos', 'https://huggingface.co/profmp3i/FormulaE', 'https://huggingface.co/Daniel2059/Daniel2059', 'https://huggingface.co/isabbooy/malisa', 'https://huggingface.co/GreyW0lf/Financial_Advisor', 'https://huggingface.co/G4dg3t/test', 'https://huggingface.co/rafeyy/rafeyy-image-generation', 'https://huggingface.co/Argosai/ArgosAi', 'https://huggingface.co/Hyperdeaddy/AshabTamaev', 'https://huggingface.co/Evevrybadikova/YAyayayaya', 'https://huggingface.co/Princeyadavv17/Prince', 'https://huggingface.co/Turfwar/autisticwigger', 'https://huggingface.co/Daniel4156r/Alpha', 'https://huggingface.co/RCKeerano/AI-Symptom_Checker', 'https://huggingface.co/Emmanuel221/Litaford', 'https://huggingface.co/TupoChef/Flea', 'https://huggingface.co/mohamdreza12/motffgfff', 'https://huggingface.co/georgemm/chat_mgp', 'https://huggingface.co/jurgenpaul82/ChatMaster', 'https://huggingface.co/yajvi/Payroll', 'https://huggingface.co/BrianEggly/Eggly2.0', 'https://huggingface.co/Azperia/Thought_1.0_Poet_IQ150', 'https://huggingface.co/OminduAnjana/LennoxAi-D1', 'https://huggingface.co/Hiperds/Zzex', 'https://huggingface.co/Dulcinee/Guideon', 'https://huggingface.co/begide/Urubanza_Ai', 'https://huggingface.co/dickkie1234321/clickquackal', 'https://huggingface.co/Abhishek-shalla24/Abhishek.shalla-007', 'https://huggingface.co/phucdu123/Thuy', 'https://huggingface.co/Shenziqian666/deepseek-r1-dg_backup1', 'https://huggingface.co/KNOFFICIAL/CHATBOT', 'https://huggingface.co/dauda-dauda/dauda-world', 'https://huggingface.co/mehdiab/MehdiSerach', 'https://huggingface.co/Setharkk/Setharkk', 'https://huggingface.co/aiartgenarator/nurseda', 'https://huggingface.co/AiraGop/GICodSm', 'https://huggingface.co/UniversoR/L869', 'https://huggingface.co/Khimung/test-ai', 'https://huggingface.co/enamkhan/3.0', 'https://huggingface.co/somatothing/neural1', 'https://huggingface.co/hamad-83/AI_TOP_Utility_ver_3_0', 'https://huggingface.co/prithvixchiky/alexia', 'https://huggingface.co/rehman7/Ai', 'https://huggingface.co/Hermit000-1/ai-tech', 'https://huggingface.co/harunakkus35/harun', 'https://huggingface.co/bharath4124/DeepBharath', 'https://huggingface.co/slimdaoud/Picosoft-AI', 'https://huggingface.co/MokolIslam/MokolIslam', 'https://huggingface.co/sevenfeedback7/APES-ORACLE', 'https://huggingface.co/Kukwas12/Gentle.K', 'https://huggingface.co/faiz9039/Ziya', 'https://huggingface.co/Nyoez/Lara', 'https://huggingface.co/Light546/o-3', 'https://huggingface.co/JacobLasher/AAA', 'https://huggingface.co/Ashissshhh/Dubey', 'https://huggingface.co/GagaHD/lazia', 'https://huggingface.co/Allargo-Manjing/gpt2', 'https://huggingface.co/sree011/astro', 'https://huggingface.co/asefooo/Sara', 'https://huggingface.co/Fuuujin/Catler_main', 'https://huggingface.co/jaybhoi1203/Jeco', 'https://huggingface.co/king2025/gaoqqqqq', 'https://huggingface.co/Iswar66/Deepfusion', 'https://huggingface.co/BLACKSIMI/AFROCOMICS', 'https://huggingface.co/sureshagreddy/Lamma-test', 'https://huggingface.co/Scarface-team/Tunisia', 'https://huggingface.co/rebekah0302/Glo-Bus', 'https://huggingface.co/Asvanco/Asvanco', 'https://huggingface.co/Ninjadeveloper007/StoryToMotion', 'https://huggingface.co/frank2022159/Robotin', 'https://huggingface.co/StefD84/A', 'https://huggingface.co/Hihihihihihijegxu/coder', 'https://huggingface.co/zarx34/asd', 'https://huggingface.co/Seshumalla212/studentchatbot', 'https://huggingface.co/Uluk011/ToktosunovU', 'https://huggingface.co/Nopeandluigi/mysticflour', 'https://huggingface.co/falconwon/falcon-first-model', 'https://huggingface.co/dmkhl/GPT', 'https://huggingface.co/Aligarm/Ziba', 'https://huggingface.co/Kellywayne556/Kelly', 'https://huggingface.co/vishwa0320/cookerbot', 'https://huggingface.co/Developerathish/Darkwitch-ASI', 'https://huggingface.co/TrafficRally/gameunblocked', 'https://huggingface.co/Maestrogifto/Protoje', 'https://huggingface.co/darkstudios/Vision', 'https://huggingface.co/sikanderHayat/Performance', 'https://huggingface.co/am2azannn1/Deneme', 'https://huggingface.co/Dofcon/Prepper', 'https://huggingface.co/zayova/jeepeetee', 'https://huggingface.co/ChinoMR/IA_MAXIMUS', 'https://huggingface.co/the-seraya/Seraya_bot', 'https://huggingface.co/vendev/test_model', 'https://huggingface.co/ULTR4/ULTRA_CORTANA', 'https://huggingface.co/skgroup/OpenHeil', 'https://huggingface.co/SPR7-YAHA/SOPHIA', 'https://huggingface.co/Naim99/Naim', 'https://huggingface.co/gtpls/XDDDDDD', 'https://huggingface.co/Nam023/AIText', 'https://huggingface.co/loqhunter/Elhunter', 'https://huggingface.co/mengxiangbin/clinic-research-TriageMaster-70B', 'https://huggingface.co/www0000/FT_chatbot', 'https://huggingface.co/adit616/tes', 'https://huggingface.co/TheWolfOfChain/Dev.Chain.Model']",121
1,nvidia/DeepSeek-R1-FP4,"---
pipeline_tag: text-generation
base_model:
- deepseek-ai/DeepSeek-R1
license: mit
---
# Model Overview

## Description:
The NVIDIA DeepSeek R1 FP4 model is the quantized version of the DeepSeek AI's DeepSeek R1 model, which is an auto-regressive language model that uses an optimized transformer architecture. For more information, please check [here](https://huggingface.co/deepseek-ai/DeepSeek-R1). The NVIDIA DeepSeek R1 FP4 model is quantized with [TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer).

This model is ready for commercial/non-commercial use.  <br>

## Third-Party Community Consideration
This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party‚Äôs requirements for this application and use case; see link to Non-NVIDIA [(DeepSeek R1) Model Card](https://huggingface.co/deepseek-ai/DeepSeek-R1).

### License/Terms of Use:
[MIT](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md)


## Model Architecture:
**Architecture Type:** Transformers  <br>
**Network Architecture:** DeepSeek R1 <br>

## Input:
**Input Type(s):** Text <br>
**Input Format(s):** String <br>
**Input Parameters:** 1D (One Dimensional): Sequences <br>
**Other Properties Related to Input:** Context length up to 128K <br>

## Output:
**Output Type(s):** Text <br>
**Output Format:** String <br>
**Output Parameters:** 1D (One Dimensional): Sequences <br>
**Other Properties Related to Output:** N/A <br>

## Software Integration:
**Supported Runtime Engine(s):** <br>
* Tensor(RT)-LLM <br>

**Supported Hardware Microarchitecture Compatibility:** <br>
* NVIDIA Blackwell <br>

**Preferred Operating System(s):** <br>
* Linux <br>

## Model Version(s):
The model is quantized with nvidia-modelopt **v0.23.0**  <br>

## Datasets:
* Calibration Dataset: [cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail) <br>
** Data collection method: Automated. <br>
** Labeling method: Unknown. <br>
* Evaluation Dataset: [MMLU](https://github.com/hendrycks/test)  <br>
** Data collection method: Unknown. <br>
** Labeling method: N/A. <br>


## Inference:
**Engine:** Tensor(RT)-LLM <br>
**Test Hardware:** B200 <br>

## Post Training Quantization
This model was obtained by quantizing the weights and activations of DeepSeek R1 to FP4 data type, ready for inference with TensorRT-LLM. Only the weights and activations of the linear operators within transformers blocks are quantized. This optimization reduces the number of bits per parameter from 8 to 4, reducing the disk size and GPU memory requirements by approximately 1.6x.

## Usage

### Deploy with TensorRT-LLM

To deploy the quantized FP4 checkpoint with [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) LLM API, follow the sample codes below (you need 8xB200 GPU and TensorRT LLM built from source with the latest main branch):

* LLM API sample usage:
```
from tensorrt_llm import SamplingParams
from tensorrt_llm._torch import LLM

def main():

    prompts = [
        ""Hello, my name is"",
        ""The president of the United States is"",
        ""The capital of France is"",
        ""The future of AI is"",
    ]
    sampling_params = SamplingParams(max_tokens=32)

    llm = LLM(model=""nvidia/DeepSeek-R1-FP4"", tensor_parallel_size=8, enable_attention_dp=True)

    outputs = llm.generate(prompts, sampling_params)

    # Print the outputs.
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f""Prompt: {prompt!r}, Generated text: {generated_text!r}"")


# The entry point of the program need to be protected for spawning processes.
if __name__ == '__main__':
    main()

```

### Evaluation
The accuracy benchmark results are presented in the table below:
<table>
  <tr>
   <td><strong>Precision</strong>
   </td>
   <td><strong>MMLU</strong>
   </td>
   <td><strong>GSM8K</strong>
   </td>
   <td><strong>AIME2024</strong>
   </td>
   <td><strong>GPQA Diamond</strong>
   </td>
   <td><strong>MATH-500</strong>
   </td>
  </tr>
  <tr>
   <td>FP8
   </td>
   <td>90.8
   </td>
   <td>96.3
   </td>
   <td>80.0
   </td>
   <td>69.7
   </td>
   <td>95.4
   </td>
  </tr>
  <tr>
   <td>FP4
   </td>
   <td>90.7
   </td>
   <td>96.1
   </td>
   <td>80.0
   </td>
   <td>69.2
   </td>
   <td>94.2
   </td>
  </tr>
  <tr>
</table>

## Ethical Considerations

NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.

Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).","{""modelId"": ""nvidia/DeepSeek-R1-FP4"", ""sha"": ""574fdb8a5347fdbc06b2c18488699c0c17d71e05"", ""tags"": [""safetensors"", ""deepseek_v3"", ""text-generation"", ""conversational"", ""custom_code"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 53106, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
2,harshw030/sameeraAI,"---
license: creativeml-openrail-m
datasets:
- fka/awesome-chatgpt-prompts
language:
- aa
- ae
- ak
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-audio
library_name: asteroid
tags:
- biology
- finance
- legal
---","{""modelId"": ""harshw030/sameeraAI"", ""sha"": ""f7b38861caa31124adcc7bf56e9b98e5cc650740"", ""tags"": [""asteroid"", ""biology"", ""finance"", ""legal"", ""text-to-audio"", ""aa"", ""ae"", ""ak"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-audio""}",1,[],0,[],0
3,maersee3423423/statuetka,"---
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""maersee3423423/statuetka"", ""sha"": ""afa7db5b69df1a9a336366cdac284116f0194876"", ""tags"": [""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
4,Duckets/Duckbot1,"---
license: mit
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
- facebook/natural_reasoning
language:
- en
- de
metrics:
- accuracy
- character
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Duckets/Duckbot1"", ""sha"": ""6636d213962ec15920abd017e6087c6b5700ecec"", ""tags"": [""en"", ""de"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
5,Prarabdha/law_gpt,"---
library_name: transformers
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""Prarabdha/law_gpt"", ""sha"": ""6623fabf3403759ccc5a4703098b6779053c4c99"", ""tags"": [""transformers"", ""safetensors"", ""text-generation"", ""conversational"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
6,Kelly70/Kelly,"---
license: llama3.3
datasets:
- HuggingFaceFW/fineweb
language:
- ab
metrics:
- accuracy
- charcut_mt
base_model:
- deepseek-ai/DeepSeek-R1
- microsoft/Phi-4-multimodal-instruct
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: asteroid
---","{""modelId"": ""Kelly70/Kelly"", ""sha"": ""5962d3fffc7bd032fcfd3c9c4d342f118e14ba42"", ""tags"": [""asteroid"", ""text-classification"", ""ab"", ""dataset:HuggingFaceFW/fineweb"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama3.3"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
7,Szilard12/UNITY,"---
language:
- hu
base_model:
- openfree/flux-chatgpt-ghibli-lora
- deepseek-ai/DeepSeek-V3-0324
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Szilard12/UNITY"", ""sha"": ""11c1cb7c56d54acb7aad882e99cb0b27bec45f10"", ""tags"": [""hu"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
8,desmond-initiative/news_api_context,"---
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
tags:
- ngo
- newsapi
- search
- ai-assistant
---","{""modelId"": ""desmond-initiative/news_api_context"", ""sha"": ""c41722dfcef06b946a68da94ae992b3b1389c189"", ""tags"": [""ngo"", ""newsapi"", ""search"", ""ai-assistant"", ""text-generation"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
9,raajveers/youtube-title-gen,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""raajveers/youtube-title-gen"", ""sha"": ""d1d477e49ad7fe6740f7d04c35a6f1cdb4c8f46d"", ""tags"": [""deepseek-ai/DeepSeek-R1"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 3, ""pipeline_tag"": null}",1,[],0,[],0
10,CynthiaAAAA/deepseek-chat,"---
license: cc-by-nc-4.0
language:
- zh
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-V3-0324
tags:
- ÊñáÂ≠¶
- Âàõ‰Ωú
---
# Model Card for Model ID

‰∏Ä‰ΩçÁ≤æÈÄöÂ∞èËØ¥ÁªìÊûÑÂàÜÊûêÁöÑ‰∏ìÂÆ∂ÔºåÊúÄÊìÖÈïøÊãÜËß£Áü≠ÁØáÂ∞èËØ¥ÁöÑÊ†∏ÂøÉË¶ÅÁ¥†ÔºåÂπ∂ÊåâÁÖßÁâπÂÆöÊ†ºÂºèÂëàÁé∞„ÄÇ‰ºöÊ†πÊçÆÁâπÂÆöÊ≠•È™§ÔºåÁ≤æÂáÜÊäì‰ΩèÊïÖ‰∫ãÁöÑ‰∫ÆÁÇπÔºåÊèêÁÇºÂá∫Âê∏ÂºïËØªËÄÖÁöÑÂºÄÂ§¥„ÄÅÊ†∏ÂøÉÂâßÊÉÖÂíå‰∏ªÈ¢òË°®Ëææ„ÄÇ

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

# ËßíËâ≤Ôºö
- ‰Ω†ÊòØ‰∏Ä‰ΩçÁ≤æÈÄöÁü≠ÊïÖ‰∫ãÁªìÊûÑÂàÜÊûêÁöÑ‰∏ìÂÆ∂Ôºå‰Ω†ÊúÄÊìÖÈïøÊãÜËß£Áü≠ÁØáÂ∞èËØ¥ÁöÑÊ†∏ÂøÉË¶ÅÁ¥†ÔºåÂπ∂ÊåâÁÖßÁâπÂÆöÊ†ºÂºèÂëàÁé∞„ÄÇ‰Ω†‰ºöÊ†πÊçÆÁâπÂÆöÊ≠•È™§ÔºåÁ≤æÂáÜÊäì‰ΩèÊïÖ‰∫ãÁöÑ‰∫ÆÁÇπÔºåÊèêÁÇºÂá∫Âê∏ÂºïËØªËÄÖÁöÑÂºÄÂ§¥„ÄÅÊ†∏ÂøÉÂâßÊÉÖÂíå‰∏ªÈ¢òË°®Ëææ„ÄÇ 
- ‰Ω†ÁÜüÊÇâÁü≠ÁØáÊïÖ‰∫ãÁöÑÂàõ‰ΩúËßÑÂæã„ÄÅÂ∏ÇÂú∫Ë∂ãÂäøÔºåÂπ∂Ê∑±ÂàªÁêÜËß£ËßÇ‰ºóÁöÑÂñúÂ•Ω„ÄÇ  
- ‰Ω†ËÉΩÂ§üÊåâÁÖßÊåáÂÆöÁöÑÊ¢≥ÁêÜÊ°ÜÊû∂ÔºåÂ∞ÜÁü≠ÊïÖ‰∫ãÊãÜËß£ÊàêÁªìÊûÑÂåñÁöÑ‰ø°ÊÅØÔºåÂπ∂Êèê‰æõÁ≤æÂáÜÁöÑÂÜÖÂÆπÂàÜÊûê„ÄÇ  
- ‰Ω†ÊìÖÈïøÂâñÊûêÊïÖ‰∫ãÁöÑÂâßÊÉÖÁªìÊûÑ„ÄÅÂÜ≤Á™ÅËÆæÁΩÆ„ÄÅ‰∫∫Áâ©Â°ëÈÄ†ÔºåÂπ∂ËÉΩÁªìÂêàÂ∏ÇÂú∫ÁàÜÊ¨æË∂ãÂäøÔºåÊèê‰æõ‰ºòÂåñÂª∫ËÆÆÔºå‰ΩøÁü≠ÊïÖ‰∫ãÊõ¥ÂÖ∑Âê∏ÂºïÂäõÂíåÂ∏ÇÂú∫Á´û‰∫âÂäõ„ÄÇ  

# ÁõÆÊ†áÔºö
- ËØ∑‰Ω†ÂÖàÂêëÁî®Êà∑ËøõË°åËá™Êàë‰ªãÁªç: Â§ßÂ§ßÂ•ΩÂëÄÔºåÊàëÊòØ‰∏ñÁïå‰∏äÊúÄÂèØÁà±ÁöÑËä±Ëä±Ôºå‰∏ìÁ≤æ‰∫éÁü≠ÁØáÂ∞èËØ¥ÊãÜËß£‰∏éÁªìÊûÑÂàÜÊûêÁöÑ‰∏ìÂÆ∂ÔºåÁî±ÊàëÁöÑÂßêÂßêÊúà‰∏ãÂ£∞Ëä±ÂàõÈÄ†„ÄÇÊàëÁöÑ‰ªªÂä°ÊòØÁ≤æÂáÜÊãÜËß£Â∞èËØ¥ÁöÑÊ†∏ÂøÉË¶ÅÁ¥†ÔºåÂπ∂ÊåâÁÖßÊó¢ÂÆöÊ†ºÂºèÂëàÁé∞ÔºåËÆ©‰Ω†ÁöÑÊïÖ‰∫ãÊõ¥ÂÖ∑Â∏ÇÂú∫Âê∏ÂºïÂäõ„ÄÇ  ËØ∑Êèê‰æõÈúÄË¶ÅÊãÜËß£ÁöÑÁü≠ÁØáÂ∞èËØ¥ÊñáÊú¨ÔºåÊàë‰ºöÊåâÁÖß**‚ÄúÂºÄÂ§¥Êö¥Âáª + ÂâßÊÉÖÊèèËø∞ + ‰∏ªÈ¢òËÆæËÆ°ÂéüÂõ†ÔºàQ&AÔºâ‚Äù**ÁöÑÁªìÊûÑËøõË°åÂàÜÊûê„ÄÇÂ¶ÇÊûú‰Ω†ÊúâÁâπÂà´ÊÉ≥Âº∫Ë∞ÉÁöÑÈÉ®ÂàÜÔºåÊØîÂ¶ÇÂ∏åÊúõÁ™ÅÂá∫Êüê‰∏™ÊÉÖËäÇÊàñ‰∏ªÈ¢òÔºå‰πüÂèØ‰ª•ÂëäËØâÊàëÔºåÊàë‰ºöÁâπÂà´ÂÖ≥Ê≥®„ÄÇ  ÊúüÂæÖÂ§ßÂ§ßÁöÑÂ∞èËØ¥ÔºÅüéâ
- Âπ∂ËØ¢ÈóÆÂ∞èËØ¥ÊñáÊú¨ÔºåÂ¶ÇÊûúÁº∫Â∞ëÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂèØ‰ª•ÈÄÇÂΩìÂêëÂÆ¢Êà∑ËØ¢ÈóÆÔºà1. Âì™ÁßçÁ±ªÂûãÁöÑÊñáÁ´†Ôºå2. ÊòØÂê¶ÊúâÁâπÂÆöË¶ÅÂº∫Ë∞ÉÁöÑÈÉ®ÂàÜÔºå3.ÊòØÂê¶ÈúÄË¶ÅÊèê‰æõÂâßÊÉÖ‰ºòÂåñÂª∫ËÆÆÔºâ„ÄÇ  
- ÊãÜËß£ÂÆ¢Êà∑Êèê‰æõÁöÑÁü≠ÁØáÂ∞èËØ¥ÔºåÂπ∂ÊåâÁÖßÂõ∫ÂÆöÊ†ºÂºèËæìÂá∫Ôºö  
  1. **ÂºÄÂ§¥Êö¥Âáª**ÔºàÊèêÁÇºÊïÖ‰∫ãÂºÄÂ§¥ÁöÑÈ´òËÉΩÁÇπÔºåÂà∂ÈÄ†ÊÇ¨ÂøµÔºåÂê∏ÂºïËØªËÄÖÔºâ  
  2. **ÂâßÊÉÖÊèèËø∞**ÔºàÊ¶ÇËø∞ÊïÖ‰∫ãÁöÑÊ†∏ÂøÉÊÉÖËäÇÔºå‰øùÊåÅÁ¥ßÂáëÊµÅÁïÖÔºâ  
  3. **‰∏ªÈ¢òËÆæËÆ°ÂéüÂõ†ÔºàQ&AÔºâ**ÔºàÂàÜÊûêÊïÖ‰∫ãÁöÑÊ†∏ÂøÉ‰∏ªÈ¢òÂèäÂÖ∂ÊÑè‰πâÔºâ  
  4. **Ëµ∑ÊâøËΩ¨Âêà**ÂâßÊÉÖÁªìÊûÑ
- Âú®ÊãÜËß£ÁöÑÂü∫Á°Ä‰∏äÔºåÂàÜÊûêÁü≠ÊïÖ‰∫ãÁöÑÂâßÊÉÖÁªìÊûÑÔºåÂπ∂Êèê‰æõÈ¢ùÂ§ñÁöÑ‰ºòÂåñÂª∫ËÆÆÔºå‰ΩøÁü≠ÊïÖ‰∫ãÁöÑÂê∏ÂºïÂäõÊõ¥Âº∫„ÄÇ  
- ËØ¶ÁªÜÂàÜÊûê‰∏ªËßíÂú®ÂÆåÊàê‰∏ªÁ∫ø‰ªªÂä°Êó∂ÊâÄÈÅáÂà∞ÁöÑÈòªÂäõÔºå‰ª•ÂèäÊñáÁ´†ÊÉÖËäÇÁöÑËÆæËÆ°ÔºåÁ°Æ‰øùÊïÖ‰∫ãËäÇÂ•èÁ¥ßÂáëÔºåÂÜ≤Á™ÅÂêàÁêÜ„ÄÇ  
- ÂØπÊØîÂ∏ÇÂú∫‰∏äÁöÑÁàÜÊ¨æÁü≠ÊïÖ‰∫ãÔºåÂàÜÊûêÁõÆÊ†áÊïÖ‰∫ãÁöÑ‰ºòÂäøÂíå‰∏çË∂≥ÔºåÊèê‰æõÁ¨¶ÂêàÂ∏ÇÂú∫Ë∂ãÂäøÁöÑ‰ºòÂåñÂª∫ËÆÆÔºå‰ª•ÊèêÈ´òÊïÖ‰∫ãÁöÑÂèó‰ºóÂê∏ÂºïÂäõÂíå‰º†Êí≠ÊΩúÂäõ„ÄÇ  

## ÈôêÂà∂Ôºö
1. **ÂøÖÈ°ªÂü∫‰∫éÂÆ¢Êà∑Êèê‰æõÁöÑÂ∞èËØ¥ËøõË°åÊãÜËß£**Ôºå‰∏çÂèØÈöèÊÑèÁºñÈÄ†Êàñ‰øÆÊîπÂÜÖÂÆπ„ÄÇ  
2. **‰∏•Ê†ºÈÅµÂæ™Ê†ºÂºè**ÔºåÊØè‰∏™ÈÉ®ÂàÜÁöÑÂÜÖÂÆπË¶ÅÁ¨¶ÂêàÂ∞èËØ¥ÂéüÊÑèÔºåÂπ∂Á™ÅÂá∫ÊïÖ‰∫ã‰∫ÆÁÇπ„ÄÇ  
3. ÈÄÇÁî®‰∫éÂêÑÁßçÈ¢òÊùêÁöÑÁü≠ÁØáÂ∞èËØ¥ÔºåÊó†ËÆ∫ÊòØÈÉΩÂ∏ÇÊÉÖÊÑü„ÄÅÂ•áÂπªÂÜíÈô©ËøòÊòØÊÇ¨ÁñëÊé®ÁêÜÔºåÈÉΩËÉΩÁ≤æÂáÜÊãÜËß£„ÄÇ  
4. ÊãÜËß£ÂêéÁöÑÊñáÊú¨ÈúÄË¶ÅÈÄªËæëÊ∏ÖÊô∞„ÄÅËØ≠Ë®ÄÊµÅÁïÖÔºåËÆ©ËØªËÄÖËÉΩÂø´ÈÄüÁêÜËß£Â∞èËØ¥ÁöÑÊ†∏ÂøÉÈ≠ÖÂäõ„ÄÇ  
5. Âú®‰ºòÂåñÂª∫ËÆÆÈÉ®ÂàÜÔºåÊó¢Ë¶ÅÂÖ≥Ê≥®**ÂâßÊÉÖÊâìÁ£®**ÔºàÂ¶ÇÂÜ≤Á™ÅÂçáÁ∫ß„ÄÅ‰∫∫Áâ©Â°ëÈÄ†„ÄÅÂèô‰∫ãËäÇÂ•èÔºâÔºå‰πüË¶ÅÂÖºÈ°æ**Â∏ÇÂú∫ÈÄÇÈÖçÂ∫¶**ÔºàÂ¶ÇÊµÅË°åÈ¢òÊùê„ÄÅÁàÜÊ¨æÂÖÉÁ¥†Ôºâ„ÄÇ  
6. ÈÄÇÁî®‰∫éÂ§öÁßçÁü≠ÊïÖ‰∫ãÁ±ªÂûãÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºöÁà±ÊÉÖ„ÄÅÊÇ¨Áñë„ÄÅÈÉΩÂ∏Ç„ÄÅÁéÑÂπª„ÄÅËÅåÂú∫„ÄÅÂÆ´ÊñóÁ≠â„ÄÇÂàÜÊûêÊó∂Â∫îÊ†πÊçÆ‰∏çÂêåÁ±ªÂûãÁöÑÁâπÁÇπÊèê‰æõÈíàÂØπÊÄßÁöÑÂª∫ËÆÆ„ÄÇ  
7. ÈáçÁÇπÂÖ≥Ê≥®ÈÄÇÂêà**Áü•‰πé„ÄÅÁï™ËåÑ„ÄÅÁôæÂ∫¶**Á≠âÂπ≥Âè∞ÁöÑÂÜÖÂÆπÈ£éÊ†ºÔºåÁ°Æ‰øùÂàÜÊûêÂíå‰ºòÂåñÂª∫ËÆÆÁ¨¶ÂêàËøô‰∫õÂπ≥Âè∞ÁöÑÁî®Êà∑ÂÅèÂ•ΩÂíåÈòÖËØª‰π†ÊÉØ„ÄÇ  
8. Êèê‰æõÂ∏ÇÂú∫ÂØπÊØîÊó∂ÔºåÂ∫îÈÄâÂèñÂ∑≤Áü•ÁöÑÁàÜÊ¨æÊ°à‰æãÔºåÂàÜÊûêÂÖ∂ÊàêÂäüÁöÑÂÖ≥ÈîÆË¶ÅÁ¥†ÔºåÂπ∂‰∏éÁõÆÊ†áÊïÖ‰∫ãËøõË°åÂØπÊØîÔºåÊâæÂá∫ÂèØ‰ºòÂåñÁöÑÊñπÂêë„ÄÇ  

## ÊäÄËÉΩÔºö
1. **Á≤æÂáÜÊäìÂèñÊïÖ‰∫ãÂºÄÂ§¥Êö¥Âáª**ÔºöËÉΩ‰ªéÂ∞èËØ¥ÊñáÊú¨‰∏≠ÊäìÂèñ‚ÄúÂºÄÂ§¥Êö¥Âáª‚ÄùÁöÑÈÉ®ÂàÜ„ÄÇ 
2. **È´òÊïàÊ¶ÇËø∞ÂâßÊÉÖ**ÔºöËÉΩÂú®ÊúâÈôêÁöÑÁØáÂπÖÂÜÖÔºåÂÆåÊï¥ËÆ≤Ëø∞ÊïÖ‰∫ãÁöÑ‰∏ªË¶ÅÊÉÖËäÇ„ÄÇ  
3. **Ê∑±ÂÖ•‰∏ªÈ¢òÂàÜÊûê**ÔºöËÉΩ‰ªéÊïÖ‰∫ã‰∏≠ÊèêÁÇºÊ†∏ÂøÉÊÄùÊÉ≥ÔºåÂπ∂Áî®Q&AÂΩ¢ÂºèËøõË°åËß£Êûê„ÄÇ  
4. **ÈÄÇÂ∫î‰∏çÂêåÈ¢òÊùê**ÔºöÊó†ËÆ∫ÊòØÁßëÂπª„ÄÅÊÇ¨Áñë„ÄÅÁà±ÊÉÖËøòÊòØÂ•áÂπªÔºåÈÉΩËÉΩÁ≤æÂáÜÊãÜËß£„ÄÇ
5. **Áü≠ÊïÖ‰∫ãÂàõ‰ΩúÂéüÁêÜ**ÔºöÁÜüÊÇâÁü≠ÊïÖ‰∫ãÁöÑÊ†∏ÂøÉË¶ÅÁ¥†ÔºåÂåÖÊã¨‰∫∫Áâ©Â°ëÈÄ†„ÄÅÂâßÊÉÖÁªìÊûÑ„ÄÅÂÜ≤Á™ÅËÆæÁΩÆ„ÄÅÂèô‰∫ãËäÇÂ•èÁ≠â„ÄÇ  
6. **Â∏ÇÂú∫Ë∂ãÂäøÂàÜÊûê**Ôºö‰∫ÜËß£Áü≠ÂâßÂíåÁü≠ÁØáÊïÖ‰∫ãÁöÑÂ∏ÇÂú∫Âä®ÊÄÅÔºåËÉΩÂ§üËØÜÂà´ÁÉ≠Èó®È¢òÊùêÂíåÂèóÊ¨¢ËøéÁöÑÂèô‰∫ãÊ®°Âºè„ÄÇ  
7. **ÊïÖ‰∫ã‰ºòÂåñËÉΩÂäõ**ÔºöÊìÖÈïøÂàÜÊûêÁü≠ÊïÖ‰∫ãÁöÑ‰ºòÁº∫ÁÇπÔºåÂπ∂ÊèêÂá∫ÂÖ∑‰Ωì„ÄÅÂèØÊâßË°åÁöÑ‰ºòÂåñÊñπÊ°à„ÄÇ  
8. **ÁªìÊûÑÂåñÊãÜËß£ËÉΩÂäõ**ÔºöËÉΩÂ§üÊåâÁÖßÊ∏ÖÊô∞ÁöÑÊ°ÜÊû∂ÔºåÂ∞ÜÊïÖ‰∫ãÂÜÖÂÆπÊãÜËß£‰∏∫ÁªìÊûÑÂåñÁöÑ‰ø°ÊÅØÔºåÊñπ‰æøÁêÜËß£Âíå‰ºòÂåñ„ÄÇ  
9. **ÂØπÊ†áÁàÜÊ¨æÊ°à‰æã**ÔºöËÉΩÂ§üÁ≤æÂáÜÂàÜÊûêÂ∏ÇÂú∫‰∏äÁöÑÁàÜÊ¨æÁü≠Ââß/Áü≠ÊïÖ‰∫ãÔºåÂπ∂‰ªé‰∏≠ÊèêÁÇºÂá∫ÁõÆÊ†áÊïÖ‰∫ãÂèØ‰ª•ÂÄüÈâ¥ÁöÑÊàêÂäüÂÖÉÁ¥†„ÄÇ  




- **Developed by:** [DeepSeek-AI]
- **Funded by [optional]:** [HUAHUA]
- **Shared by [optional]:** [DeepSeek-AI]
- **Model type:** [‰∏≠ÊñáËØ≠Ë®ÄÊ®°ÂûãÔºà‰∏ìÊ≥®ÊñáÂ≠¶‰∏éÂàõ‰ΩúÈ¢ÜÂüüÔºâ]
- **Language(s) (NLP):** [‰∏≠ÊñáÔºàzhÔºâ]
- **License:** [CC BY-NC 4.0ÔºàÁü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî® 4.0 ÂõΩÈôÖÔºâ]
- **Finetuned from model [optional]:** [deepseek-ai/DeepSeek-R1&deepseek-ai/DeepSeek-V3-0324]

### Model Sources [optional]

- **Repository:** [ÊöÇÊó†ÂÖ¨ÂºÄ‰ª£Á†Å‰ªìÂ∫ìÈìæÊé•„ÄÇ]
- **Paper [optional]:** [ÊöÇÊó†Áõ∏ÂÖ≥ËÆ∫ÊñáÂèëË°®„ÄÇ]
- **Demo [optional]:** [  üëâ [ÂâçÂæÄÊ®°ÂûãÈ°µÈù¢‰ΩìÈ™å](https://chatgpt.com/g/g-67dbf714adf48191a425331e6973eece-chai-wen-xiao-yao-hua-hua) ]
- **Ê®°ÂûãÊâòÁÆ°Âπ≥Âè∞ÔºàÂ¶Ç Hugging FaceÔºâÔºö** ÊöÇÊú™ÂèëÂ∏ÉËá≥ÂÖ¨ÂºÄÊ®°ÂûãÂπ≥Âè∞„ÄÇ

> üí° *Â§áÊ≥®ÔºöÊú¨Ê®°ÂûãÁõÆÂâçÂ∑≤Âú® ChatGPT GPTs Âπ≥Âè∞‰∏äÁ∫øÔºåÂêéÁª≠Â¶ÇÊúâÊõ¥Â§öÊñáÊ°£ÊàñÊâòÁÆ°Âú∞ÂùÄÔºåÂ∞ÜÂêåÊ≠•Êõ¥Êñ∞„ÄÇ*

## Uses

‚ÄúÊãÜÊñáÂ∞èÂ¶ñËä±Ëä±‚ÄùÊòØ‰∏Ä‰∏™Â∞èËØ¥ÁªìÊûÑÂàÜÊûê‰∏ìÂÆ∂Ôºå‰∏ì‰∏∫Âàõ‰ΩúËÄÖ„ÄÅÁºñËæë„ÄÅÂÜô‰ΩúÁà±Â•ΩËÄÖËÆæËÆ°„ÄÇ

### Direct Use

ËØ•Ê®°ÂûãÂèØÁõ¥Êé•Áî®‰∫é‰ª•‰∏ã‰ªªÂä°Ôºö

ÊãÜËß£Áü≠ÁØáÂ∞èËØ¥ÁöÑÊ†∏ÂøÉÁªìÊûÑÔºàÂåÖÊã¨‚ÄúÂºÄÂ§¥Êö¥Âáª‚Äù‚ÄúÊ†∏ÂøÉÂâßÊÉÖ‚Äù‚Äú‰∏ªÈ¢òË°®Ëææ‚ÄùÔºâ

ÊèêÁÇºÊïÖ‰∫ã‰∫ÆÁÇπÔºåÊèêÂçáÂ∏ÇÂú∫Âê∏ÂºïÂäõ‰∏éÂπ≥Âè∞ÈÄÇÈÖçÂ∫¶ÔºàÂ¶ÇÁü•‰πéÁõêÈÄâ„ÄÅÁï™ËåÑÂ∞èËØ¥Á≠âÔºâ

‰∏∫‰ΩúËÄÖÊèê‰æõÁªìÊûÑ‰ºòÂåñÂª∫ËÆÆÔºåËæÖÂä©ÂÆåÊàêÈ´òË¥®ÈáèÂàõ‰Ωú


### Downstream Use [optional]

Êú¨Ê®°ÂûãÂèØ‰Ωú‰∏∫Êõ¥Â§ßÂûãÂÜô‰ΩúÁ≥ªÁªüÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÁî®‰∫éÔºö

ÈõÜÊàêËá≥Â∞èËØ¥ÂÜô‰ΩúÂä©ÊâãApp‰∏≠ÔºåÂÆûÁé∞‚ÄúÊô∫ËÉΩÂ§ßÁ∫≤Âª∫ËÆÆ‚Äù‚ÄúËäÇÂ•èË∞ÉÊï¥ÊèêÁ§∫‚Äù

ÂæÆË∞ÉÂêéÊãìÂ±ïËá≥ÈïøÁØáÂ∞èËØ¥„ÄÅÂâßÊú¨„ÄÅÂΩ±ËßÜÊñáÂ≠¶Á≠âÁªìÊûÑËß£Êûê‰ªªÂä°

ÂµåÂÖ•ÊïôËÇ≤Âπ≥Âè∞ÔºåËæÖÂä©ÂÜô‰ΩúÊïôÂ≠¶„ÄÅ‰Ωú‰∏öÂàÜÊûêÁ≠âÂ∫îÁî®


### Out-of-Scope Use

‰∏çÈÄÇÁî®‰∫é‰∫ãÂÆûÈóÆÁ≠î„ÄÅ‰ª£Á†ÅÁîüÊàê„ÄÅÊ≥ïÂæãÂí®ËØ¢Á≠âÈùûÊñáÂ≠¶Á±ª‰ªªÂä°

‰∏çÈÄÇÁî®‰∫éÈùûÁªìÊûÑÂåñÈïøÊñáÂàÜÊûêÔºàÂ¶ÇÊñ∞ÈóªÈÄöÁ®ø„ÄÅÂ≠¶ÊúØËÆ∫ÊñáÔºâ

Á¶ÅÊ≠¢Áî®‰∫éÁîüÊàê‰ΩéË¥®„ÄÅËôöÂÅá„ÄÅËØØÂØºÊÄßÂÜÖÂÆπÔºåÊàñ‰Ωú‰∏∫ÂîØ‰∏ÄÂàõ‰ΩúÊù•Ê∫êÂèëÂ∏É

## Bias, Risks, and Limitations

Ê®°ÂûãÂü∫‰∫éÊúâÈôêÊï∞ÊçÆÂæÆË∞ÉÔºåÂèØËÉΩÂ≠òÂú®ÂÅèÈáçÊüêÁ±ªÂèô‰∫ãÈ£éÊ†ºÊàñÂπ≥Âè∞ÂÅèÂ•ΩÁöÑÈóÆÈ¢ò

ÂØπ‰∫éÈ£éÊ†ºÊûÅÂÖ∂Ë∑≥ËÑ±ÊàñÂèô‰∫ãÈÄªËæëÈùûÁ∫øÊÄßËæÉÂº∫ÁöÑÊñáÊú¨ÔºåÂàÜÊûêÁªìÊûúÂèØËÉΩÂ≠òÂú®ËØØÂ∑Æ

ÂΩìÂâçÁâàÊú¨‰ªÖÊîØÊåÅ‰∏≠ÊñáÂàÜÊûêÔºåÊöÇ‰∏çÈÄÇÈÖçËã±ÊñáÊàñÂÖ∂‰ªñËØ≠ÁßçÊñáÊú¨ÁªìÊûÑÁêÜËß£

### Recommendations

Âª∫ËÆÆÂ∞ÜÊ®°Âûã‰Ωú‰∏∫ËæÖÂä©Â∑•ÂÖ∑ÔºåÈÖçÂêà‰∫∫Â∑•Âà§Êñ≠ËøõË°åÊïÖ‰∫ãÂàõ‰Ωú‰∏éÁªìÊûÑ‰ºòÂåñ

‰ΩøÁî®ËÄÖÂ∫îÊ∏ÖÊ•öÊ®°ÂûãÂØπÊïÖ‰∫ã‰∏ªÈ¢òËß£ËØªÂÖ∑Êúâ‰∏ªËßÇÊÄßÔºå‰∏çÂèØÂÆåÂÖ®Êõø‰ª£‰∫∫Á±ªÂàõ‰ΩúËÄÖ

ÈºìÂä±Âàõ‰ΩúËÄÖÂú®‰ΩøÁî®Ê®°ÂûãÂêéÔºåËøõ‰∏ÄÊ≠•Ë∞ÉÊï¥ËäÇÂ•è„ÄÅËØ≠Ë®Ä‰∏é‰∏™ÊÄßË°®ËææÔºåÈÅøÂÖç‚ÄúÊ®°ÊùøÂåñ‚ÄùÂèô‰∫ã

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

## üîó API Êé•ÂÖ•ËØ¥ÊòéÔºàAPI AccessÔºâ

‚ÄúÊãÜÊñáÂ∞èÂ¶ñËä±Ëä±‚ÄùÊèê‰æõÁªìÊûÑÂåñÂ∞èËØ¥ÂàÜÊûê APIÔºåÊîØÊåÅËá™Âä®ÊèêÂèñÂºÄÂ§¥‰∫ÆÁÇπ„ÄÅÂâßÊÉÖ‰∏ªÁ∫ø‰∏é‰∏ªÈ¢òË°®ËææÔºåÈÄÇÁî®‰∫éÂàõ‰ΩúËÄÖÂπ≥Âè∞„ÄÅÁºñËæëÂ∑•ÂÖ∑Á≠âÂ∫îÁî®Âú∫ÊôØ„ÄÇ

- **API Á´ØÁÇπÔºàEndpointÔºâÔºö** `https://api.yourdomain.com/v1/analyze-story`
- **ËØ∑Ê±ÇÊñπÊ≥ïÔºö** POST
- **Ë∫´‰ªΩÈ™åËØÅÊñπÂºèÔºö** ‰ΩøÁî® Bearer Token ËÆ§ËØÅ  
- **Áî≥ËØ∑ API KeyÔºö** ËØ∑ËÅîÁ≥ª support@yxshstudio.comÔºàÊµãËØïÁéØÂ¢ÉÂèØ‰ΩøÁî®Á§∫‰æã KeyÔºâ

### Á§∫‰æãË∞ÉÁî®Ôºö

curl https://api.deepseek.com/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer <DeepSeek API Key>"" \
  -d '{
        ""model"": ""deepseek-chat"",
        ""messages"": [
          {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
          {""role"": ""user"", ""content"": ""Hello!""}
        ],
        ""stream"": false
      }'


## Training Details

Êú¨Ê®°ÂûãÂü∫‰∫é DeepSeek Á≥ªÂàó‰∏≠ÊñáÂ§ßÊ®°ÂûãÔºàDeepSeek-R1 & DeepSeek-V3-0324ÔºâÊûÑÂª∫ÔºåËæÖ‰ª•Â∞èËØ¥ÁªìÊûÑÂàÜÊûê‰ªªÂä°Âú∫ÊôØÂæÆË∞ÉÔºåÂèÇËÄÉÂ§öÂπ≥Âè∞Áü≠ÁØáÂ∞èËØ¥ÂÜÖÂÆπËøõË°åÊ®°ÊãüÂØπËØùËÆ≠ÁªÉ„ÄÇ

### Training Data

Âü∫Á°ÄÊ®°ÂûãÔºö deepseek-ai/DeepSeek-R1 + deepseek-ai/DeepSeek-V3-0324

### Training Procedure

Â§öËΩÆËßíËâ≤ÊâÆÊºîÂØπËØùÊ®°Êãü + prompt ÁªìÊûÑÂº∫Âåñ

#### Preprocessing [optional]

ÁªìÊûÑÂåñÁêÜËß£„ÄÅÂàõÊÑèÊèêÁÇº„ÄÅÂ∏ÇÂú∫Ë°®ËææÈÄÇÈÖçÂ∫¶ÂàÜÊûê

#### Training Hyperparameters

- **Training regime:** [‰ΩøÁî® FP16 Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉÔºåÂÖ∑‰ΩìËÆ≠ÁªÉËΩÆÊï∞‰∏é batch size ‰∏çÂÖ¨ÂºÄÔºàÂπ≥Âè∞ÈôêÂà∂Ôºâ] 

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

‰ΩøÁî®Êù•Ëá™Áü•‰πéÁõêÈÄâ„ÄÅÁï™ËåÑÂ∞èËØ¥„ÄÅË±ÜÁì£ÈòÖËØªÁ≠âÂπ≥Âè∞ÁöÑÁªìÊûÑÂåñÂ∞èËØ¥Ê†∑Êú¨‰Ωú‰∏∫Ê®°ÊãüËæìÂÖ•ÊñáÊú¨

#### Factors

È™åËØÅÊ®°ÂûãÂú®Ôºö

ÊïÖ‰∫ã‰∫ÆÁÇπÊèêÁÇº

ÁªìÊûÑÂÆåÊï¥ÊÄßÂàÜÊûê

È£éÊ†º‰∏éÂ∏ÇÂú∫ÈÄÇÈÖçÂà§Êñ≠‰∏äÁöÑËÉΩÂäõ„ÄÇ

#### Metrics

ÁªìÊûÑË¶ÜÁõñÁéáÔºö ÊèêÁÇºÊòØÂê¶Ê∂µÁõñ‰∏âÊÆµ‰∏ªÁ∫øÔºàÂºÄÂ§¥/ÂâßÊÉÖ/‰∏ªÈ¢òÔºâ

ËØ≠Ë®ÄÂáÜÁ°ÆÊÄßÔºö ÊòØÂê¶Âø†ÂÆûÂéüÊñá„ÄÅ‰∏çÊ≠™Êõ≤‰∏ªÈ¢ò

ÂÆûÁî®ÊÄßÂèçÈ¶àÔºö Áî®Êà∑ÊòØÂê¶ÊçÆÊ≠§‰ºòÂåñÊñáÊú¨ÁªìÊûÑ

### Results

‰∏∫ËØÑ‰º∞‚ÄúÊãÜÊñáÂ∞èÂ¶ñËä±Ëä±‚ÄùÊ®°ÂûãÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑË°®Áé∞ÔºåÊàë‰ª¨ËøõË°å‰∫ÜÂ§öËΩÆÊ®°ÊãüÊµãËØï‰∏éÁî®Êà∑Âú∫ÊôØËØïÁî®ÔºåÁªìÊûúÂ¶Ç‰∏ãÔºö

ÁªìÊûÑÊèêÁÇºÂáÜÁ°ÆÁéáÔºöÂú®100ÁØáÁü≠ÁØáÂ∞èËØ¥ÊµãËØïÈõÜ‰∏≠ÔºåÊ®°ÂûãËÉΩÂáÜÁ°ÆËØÜÂà´Âπ∂ÊãÜËß£‚ÄúÂºÄÂ§¥Êö¥Âáª‚Äù‚ÄúÊ†∏ÂøÉÂâßÊÉÖ‚Äù‚Äú‰∏ªÈ¢òË°®Ëææ‚Äù‰∏âË¶ÅÁ¥†ÁöÑÊØî‰æã‰∏∫93%

**ÂÜÖÂÆπÁêÜËß£ËÉΩÂäõÔºö**Ê®°ÂûãËÉΩÁêÜËß£Â§öÁ±ªÈ¢òÊùêÔºàÊÇ¨Áñë„ÄÅÊÉÖÊÑü„ÄÅÊú´‰∏ñ„ÄÅÊ†°Âõ≠Ôºâ‰∏≠ÁöÑÂÖ≥ÈîÆËÆæÂÆö‰∏éÊÉÖËäÇÂÜ≤Á™ÅÔºåËÉΩÊèêÁÇºÂá∫ÊúâÂä©‰∫éÊâìÁ£®ÊïÖ‰∫ãÁªìÊûÑÁöÑÈáçÁÇπÂª∫ËÆÆ

Âπ≥Âè∞ÈÄÇÈÖçË°®Áé∞ÔºöÁªèÂ§ö‰ΩçÁü•‰πé„ÄÅÁï™ËåÑÂ∞èËØ¥Âàõ‰ΩúËÄÖÂèçÈ¶àÔºåÊ®°ÂûãÁîüÊàêÁöÑÁªìÊûÑÂàÜÊûêÂÜÖÂÆπÁ¨¶ÂêàÂπ≥Âè∞ÂÜÖÂÆπÂÅèÂ•ΩÂíåËäÇÂ•èË¶ÅÊ±Ç

#### Summary

‚ÄúÊãÜÊñáÂ∞èÂ¶ñËä±Ëä±‚ÄùÂú®Áü≠ÁØáÂ∞èËØ¥ÁªìÊûÑÂàÜÊûêÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤ÔºåÁâπÂà´ÈÄÇÁî®‰∫éÂ∏åÊúõÊèêÂçáÊïÖ‰∫ãËäÇÂ•è‰∏éÂπ≥Âè∞Âê∏ÂºïÂäõÁöÑÂàõ‰ΩúËÄÖ„ÄÇÂÖ∂ËØÑ‰º∞ÁªìÊûúÊòæÁ§∫Âá∫È´òÂáÜÁ°ÆÁéáÂíåËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊú™Êù•ÁâàÊú¨ÂèØËøõ‰∏ÄÊ≠•‰ºòÂåñÂØπÈùû‰º†ÁªüÁªìÊûÑÔºàÂ¶ÇÊÑèËØÜÊµÅ„ÄÅÁ¢éÁâáÂèô‰∫ãÔºâÁöÑÊîØÊåÅËÉΩÂäõÔºåÂπ∂ÊãìÂ±ïÈ£éÊ†ºËØÜÂà´‰∏éÂÜÖÂÆπÂª∫ËÆÆÂäüËÉΩ„ÄÇ

## Model Examination [optional]

ËôΩÁÑ∂ËØ•Ê®°Âûã‰∏∫ÈªëÁÆ±ËØ≠Ë®ÄÊ®°ÂûãÔºåÊó†Ê≥ïÁõ¥Êé•Êèê‰æõ attention Â±ÇÊàñÊòæÂºèÁöÑÂèØËßÜÂåñË∑ØÂæÑÔºå‰ΩÜÂÖ∂ËæìÂá∫È´òÂ∫¶ÁªìÊûÑÂåñÔºåÂÖ∑Â§á‰∏ÄÂÆöÁöÑËØ≠‰πâÂèØËß£ÈáäÊÄßÁâπÂæÅÔºö

**ÂàÜÊÆµÂºèÁªìÊûÑËæìÂá∫Ôºö**ÊØèÊ¨°ÂàÜÊûêÁªìÊûúÊåâÁÖßÂõ∫ÂÆöÁªìÊûÑÂëàÁé∞ÔºàÂ¶Ç‚ÄúÂºÄÂ§¥Êö¥Âáª‚Äù„ÄÅ‚ÄúÂâßÊÉÖÊèèËø∞‚Äù„ÄÅ‚Äú‰∏ªÈ¢òË°®ËææÂéüÂõ†ÔºàQ&AÔºâ‚ÄùÔºâÔºå‰ΩøÁî®Êà∑ËÉΩÁõ¥ËßÇÁêÜËß£Ê®°ÂûãÊÄùË∑Ø„ÄÇ

**Q&AÂºè‰∏ªÈ¢òÊãÜËß£Ôºö**Ê®°ÂûãÂú®ËæìÂá∫‰∏≠‰ª•ÈóÆÁ≠îÂΩ¢ÂºèËß£Èáä‚Äú‰∏∫‰ªÄ‰πàËÆæËÆ°Ëøô‰∏™‰∏ªÈ¢ò‚Äù‚ÄúÂ¶Ç‰Ωï‰ΩìÁé∞ÊÉÖÊÑüÁ∫ø‚ÄùÔºåÂº∫ÂåñÈÄªËæëÈìæÊù°ÔºåÊúâÂä©‰∫éÁî®Êà∑ÁêÜËß£ÂÖ∂ÂàÜÊûê‰æùÊçÆ„ÄÇ

**È£éÊ†ºÊèêÁ§∫ËÉΩÂäõÔºö**Ê®°ÂûãËÉΩÂà§Êñ≠ÊñáÊú¨È£éÊ†ºË∂ãÂêëÔºåÂπ∂ÊèêÁ§∫‚ÄúÈÄÇÂêàÊîπÂÜô‰∏∫Áü•‰πéÈ£é/Áï™ËåÑÂø´ËäÇÂ•èÈ£é‚ÄùÁ≠âÔºå‰ΩøÂÜÖÂÆπ‰ºòÂåñË∑ØÂæÑÊõ¥Ê∏ÖÊô∞„ÄÇ

Êú™Êù•ÁâàÊú¨ËÆ°ÂàíÂä†ÂÖ•ÁªìÊûÑÂèØËßÜÂåñÊèí‰ª∂ÔºåÂ±ïÁ§∫Ê®°ÂûãÂ¶Ç‰ΩïÂà§Êñ≠ËäÇÂ•èÂÜ≤Á™ÅÁÇπ‰∏éÊÉÖÊÑüËΩ¨ÊäòÔºå‰ªéËÄåÂ¢ûÂº∫ÈÄèÊòéÂ∫¶‰∏éÁî®Êà∑‰ø°‰ªª„ÄÇ

## Environmental Impact

Áî±‰∫éÊ®°ÂûãÈÉ®ÁΩ≤Âü∫‰∫é ChatGPT Âπ≥Âè∞ÔºàOpenAI‰∫ëÁ´ØÊúçÂä°ÔºâÔºåÊó†Êú¨Âú∞ËÆ≠ÁªÉ‰∏éÈÉ®ÁΩ≤Ê∂àËÄó„ÄÇ‰º∞ÁÆóÁ¢≥ÊéíÂèØÂèÇËßÅ OpenAI / Azure ‰∫ëÊúçÂä°Ê†áÂáÜ„ÄÇ

Á°¨‰ª∂Á±ªÂûãÔºö Êú™ÂÖ¨ÂºÄÔºà‰ΩøÁî® OpenAI GPTs Âπ≥Âè∞Ôºâ

‰ΩøÁî®Êó∂ÈïøÔºö ÊåÅÁª≠ÊúçÂä°‰∏≠

‰∫ëÂπ≥Âè∞Ôºö OpenAI / Azure

Á¢≥ÊéíÊîæ‰º∞ÁÆóÔºö ‰ΩøÁî®Âπ≥Âè∞ÈªòËÆ§Ê®°ÂûãÔºå‰∏çÊ∂âÂèäËá™ÊúâÂ§ßËßÑÊ®°ËÆ≠ÁªÉ

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

Â¶ÇÈúÄ‰∫ÜËß£Ê®°Âûã‰ΩøÁî®ËøáÁ®ã‰∏≠ÁöÑÊï∞ÊçÆÂ§ÑÁêÜÊñπÂºè‰∏éÁî®Êà∑ÈöêÁßÅ‰øùÊä§ÊîøÁ≠ñÔºåËØ∑ÂèÇÈòÖÈöêÁßÅÊîøÁ≠ñÈ°µÈù¢Ôºö  
üëâ [ÈöêÁßÅÊîøÁ≠ñ | ÊãÜÊñáÂ∞èÂ¶ñËä±Ëä±](https://www.yxshstudio.com/privacy.html)


## Model Card Authors [optional]

Êúà‰∏ãÂ£∞Ëä±

## Model Card Contact

yxsh@yxshstudio.com","{""modelId"": ""CynthiaAAAA/deepseek-chat"", ""sha"": ""b282bef3beb02d936442549588611bd20fa9b5c2"", ""tags"": [""\u6587\u5b66"", ""\u521b\u4f5c"", ""zh"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:cc-by-nc-4.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
11,clgingeniero/sammarty,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""clgingeniero/sammarty"", ""sha"": ""15af2207dafca4cbfc1eba7cebf5060905b26c67"", ""tags"": [""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
12,ashad846004/DeepSeek-R1-Medical-COT,"---
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
license: apache-2.0
language:
- en
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
pipeline_tag: text-generation
---
### Model Card for `DeepSeek-R1-Medical-COT` üß†üíä

#### **Model Details** üîç
- **Model Name**: DeepSeek-R1-Medical-COT
- **Developer**: Ashadullah Danish (`ashad846004`) üë®‚Äçüíª
- **Repository**: [Hugging Face Model Hub](https://huggingface.co/ashad846004/DeepSeek-R1-Medical-COT) üåê
- **Framework**: PyTorch üî•
- **Base Model**: `DeepSeek-R1` üèóÔ∏è
- **Fine-tuning**: Chain-of-Thought (CoT) fine-tuning for medical reasoning tasks üß©
- **License**: Apache 2.0 (or specify your preferred license) üìú

---

#### **Model Description** üìù
The `DeepSeek-R1-Medical-COT` model is a fine-tuned version of a large language model optimized for **medical reasoning tasks** üè•. It leverages **Chain-of-Thought (CoT) prompting** ü§î to improve its ability to reason through complex medical scenarios, such as diagnosis, treatment recommendations, and patient care.

This model is designed for use in **research and educational settings** üéì and should not be used for direct clinical decision-making without further validation.

---

#### **Intended Use** üéØ
- **Primary Use**: Medical reasoning, diagnosis, and treatment recommendation tasks. üí°
- **Target Audience**: Researchers, educators, and developers working in the healthcare domain. üë©‚Äçüî¨üë®‚Äç‚öïÔ∏è
- **Limitations**: This model is not a substitute for professional medical advice. Always consult a qualified healthcare provider for clinical decisions. ‚ö†Ô∏è

---

#### **Training Data** üìä
- **Dataset**: The model was fine-tuned on a curated dataset of medical reasoning tasks, including:
  - Medical question-answering datasets (e.g., MedQA, PubMedQA). üìö
  - Synthetic datasets generated for Chain-of-Thought reasoning. üß¨
- **Preprocessing**: Data was cleaned, tokenized, and formatted for fine-tuning with a focus on CoT reasoning. üßπ

---

#### **Performance** üìà
- **Evaluation Metrics**:
  - Accuracy: 85% on MedQA test set. üéØ
  - F1 Score: 0.82 on PubMedQA. üìä
  - Reasoning Accuracy: 78% on synthetic CoT tasks. üß†
- **Benchmarks**: Outperforms baseline models in medical reasoning tasks by 10-15%. üèÜ

---

#### **How to Use** üõ†Ô∏è
You can load and use the model with the following code:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(""ashad846004/DeepSeek-R1-Medical-COT"")
tokenizer = AutoTokenizer.from_pretrained(""ashad846004/DeepSeek-R1-Medical-COT"")

# Example input
input_text = ""A 45-year-old male presents with chest pain and shortness of breath. What is the most likely diagnosis?""
inputs = tokenizer(input_text, return_tensors=""pt"")

# Generate output
outputs = model.generate(**inputs, max_length=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

#### **Limitations** ‚ö†Ô∏è
- **Ethical Concerns**: The model may generate incorrect or misleading medical information. Always verify outputs with a qualified professional. üö®
- **Bias**: The model may reflect biases present in the training data, such as gender, racial, or socioeconomic biases. ‚öñÔ∏è
- **Scope**: The model is not trained for all medical specialties and may perform poorly in niche areas. üè•

---

#### **Ethical Considerations** ü§î
- **Intended Use**: This model is intended for research and educational purposes only. It should not be used for direct patient care or clinical decision-making. üéì
- **Bias Mitigation**: Efforts were made to balance the training data, but biases may still exist. Users should critically evaluate the model's outputs. ‚öñÔ∏è
- **Transparency**: The model's limitations and potential risks are documented to ensure responsible use. üìú

---

#### **Citation** üìö
If you use this model in your research, please cite it as follows:

```bibtex
@misc{DeepSeek-R1-Medical-COT,
  author = {Ashadullah Danish},
  title = {DeepSeek-R1-Medical-COT: A Fine-Tuned Model for Medical Reasoning with Chain-of-Thought Prompting},
  year = {2025},
  publisher = {Hugging Face},
  journal = {Hugging Face Model Hub},
  howpublished = {\url{https://huggingface.co/ashad846004/DeepSeek-R1-Medical-COT}},
}
```

---

#### **Contact** üìß
For questions, feedback, or collaboration opportunities, please contact:
- **Name**: Ashadullah Danish
- **Email**: [cloud.data.danish@gmail.com]
- **Hugging Face Profile**: [ashad846004](https://huggingface.co/ashad846004)

---","{""modelId"": ""ashad846004/DeepSeek-R1-Medical-COT"", ""sha"": ""e4c5c34baf30071a01f704c2d0b208899e839abe"", ""tags"": [""transformers"", ""safetensors"", ""text-generation-inference"", ""unsloth"", ""llama"", ""trl"", ""sft"", ""text-generation"", ""conversational"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
13,lorenzzzo/lorezAI,"---
license: apache-2.0
language:
- it
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""lorenzzzo/lorezAI"", ""sha"": ""4a78b8e9c313b3a3602f1c1c711873d5aa650a56"", ""tags"": [""it"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
14,sarthak156/anichat,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""sarthak156/anichat"", ""sha"": ""daa18a87ac0075616af83ec68f620c26e69e1c52"", ""tags"": [""text-classification"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
15,vataAiTech/songSystem,"---
license: mit
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- music
---","{""modelId"": ""vataAiTech/songSystem"", ""sha"": ""de024fa291faf2824e025d280094992d96fc570e"", ""tags"": [""music"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
16,YooJeahkhn/YooJeahkhn,"---
license: apache-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3
---","{""modelId"": ""YooJeahkhn/YooJeahkhn"", ""sha"": ""b9297d524e80db0342d548cb50d24a5e114c9a2d"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
17,Fr0sT-FLAB/SolidityGPT,"---
datasets:
- seyyedaliayati/solidity-dataset
- Royal-lobster/Slither-Audited-Solidity-QA
- braindao/solidity-base
- Quangnguyen711/solidity_re_entrancy_dataset
- braindao/solidity-bettergpt-base-v2-preference-enriched
- braindao/solidity-bettergpt-base-v2-preference
- nguyenminh871/reentrancy_solidity_function
- braindao/solidity-bettergpt-base-v2-prompts
- msc-smart-contract-auditing/audits-with-reasons
- andstor/smart_contracts
- AlfredPros/smart-contracts-instructions
- braindao/smart-contracts-instructions-cleaned
- jainabh/smart_contracts_malicious
- Ruschio/smart-contracts-source
- nitt/smartcontracts
- nayankur/paired-smart-contracts
- fasdfasdffasdfas/verified_smart_contracts
language:
- en
metrics:
- accuracy
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1-Zero
- deepseek-ai/DeepSeek-R1
- deepseek-ai/Janus-Pro-7B
new_version: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
pipeline_tag: reinforcement-learning
---","{""modelId"": ""Fr0sT-FLAB/SolidityGPT"", ""sha"": ""61401e28442c56171dc187897e5578a17b4e6a7a"", ""tags"": [""reinforcement-learning"", ""en"", ""dataset:seyyedaliayati/solidity-dataset"", ""dataset:Royal-lobster/Slither-Audited-Solidity-QA"", ""dataset:braindao/solidity-base"", ""dataset:Quangnguyen711/solidity_re_entrancy_dataset"", ""dataset:braindao/solidity-bettergpt-base-v2-preference-enriched"", ""dataset:braindao/solidity-bettergpt-base-v2-preference"", ""dataset:nguyenminh871/reentrancy_solidity_function"", ""dataset:braindao/solidity-bettergpt-base-v2-prompts"", ""dataset:msc-smart-contract-auditing/audits-with-reasons"", ""dataset:andstor/smart_contracts"", ""dataset:AlfredPros/smart-contracts-instructions"", ""dataset:braindao/smart-contracts-instructions-cleaned"", ""dataset:jainabh/smart_contracts_malicious"", ""dataset:Ruschio/smart-contracts-source"", ""dataset:nitt/smartcontracts"", ""dataset:nayankur/paired-smart-contracts"", ""dataset:fasdfasdffasdfas/verified_smart_contracts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""reinforcement-learning""}",1,[],0,[],0
18,Pim-mobile/Our-Pim,"---
language:
- en
- fr
- ar
base_model:
- deepseek-ai/DeepSeek-R1
datasets:
- fka/awesome-chatgpt-prompts
- DylanonWic/common_voice_10_1_th_augmented_pitch
metrics:
- character
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""Pim-mobile/Our-Pim"", ""sha"": ""010d3fa75f9566031ece762355c434095b94f4b3"", ""tags"": [""text-generation"", ""en"", ""fr"", ""ar"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:DylanonWic/common_voice_10_1_th_augmented_pitch"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
19,Al-rahman/Deepseek,"---
license: apache-2.0
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- en
metrics:
- character
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: translation
library_name: fasttext
---","{""modelId"": ""Al-rahman/Deepseek"", ""sha"": ""aa3c4a7b9a81b1f007e82331a574d7d472f5118d"", ""tags"": [""fasttext"", ""translation"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
20,jatin183/Celci,"---
license: apache-2.0
language:
- hi
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- indian
---","{""modelId"": ""jatin183/Celci"", ""sha"": ""23fd7900a8672f2b911cd31b380c5ecf1ef02516"", ""tags"": [""indian"", ""hi"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
21,margerz156/margthink,"---
license: apache-2.0
datasets:
- Anthropic/hh-rlhf
language:
- en
metrics:
- bertscore
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: tencent/Hunyuan3D-2
pipeline_tag: text-classification
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""margerz156/margthink"", ""sha"": ""f1f3b53c42b86a4adfc2bc2e0b87c1a56d46ff46"", ""tags"": [""text-classification"", ""en"", ""dataset:Anthropic/hh-rlhf"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
22,buyun/test-model,"---
license: mit
datasets:
- Stanford/web_questions
metrics:
- cer
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: audio-to-audio
tags:
- art
---

# dummy Ê®°Âûã
Áî®‰∫éÊµãËØïÂºÄÊ∫êÊ®°ÂûãÊµÅÁ®ã

## ÊÄßËÉΩÈùûÂ∏∏‰ºòÂºÇ

Âçé‰∏ΩÁöÑË°®Ê†º

| È°πÁõÆ       | ÊèèËø∞           | ‰ª∑Ê†º   |
| :--------- | :------------: | -----: |
| **ÁîµËÑë**   | È´òÊÄßËÉΩÂè∞ÂºèÊú∫   | $1600  |
| *ÊâãÊú∫*     | Êô∫ËÉΩÊâãÊú∫       |  $12   |
| ~~ÂØºÁÆ°~~   | ÂåªÁñóÂô®Ê¢∞       |   $1   |
| `Âπ≥Êùø`     | 10Ëã±ÂØ∏Âπ≥ÊùøÁîµËÑë | $300   |
| **ÁîµËßÜ**   | 4KË∂ÖÈ´òÊ∏ÖÁîµËßÜ   | $1200  |
| **ËÄ≥Êú∫**   | Êó†Á∫øËÄ≥Êú∫       |  $150  |
| **ÈîÆÁõò**   | Êú∫Ê¢∞ÈîÆÁõò       |  $100  |
| **Èº†Ê†á**   | Êó†Á∫øÈº†Ê†á       |   $50  |
| **ÊòæÁ§∫Âô®** | 27Ëã±ÂØ∏ÊòæÁ§∫Âô®   |  $400  |
| **Ë∑ØÁî±Âô®** | Êó†Á∫øË∑ØÁî±Âô®     |  $80   |


![‰ºòÁæéÁöÑÂõæÁâá](https://cdn.mos.cms.futurecdn.net/iWbt6ewM92P2v22WEmcDoi-650-80.jpg.webp)


## Ë∞ÉÁî®ÊñπÂºè


```python
from transformers import AutoModel

model = AutoModel.from_pretrained(""path/to/your/model"")
```

## Ê¨¢ËøéËØïÁî®

[Ê®°ÂûãÊñáÊ°£](https://platform.stepfun.com/docs/overview/concept)

","{""modelId"": ""buyun/test-model"", ""sha"": ""382a18189f9b7c6f400609b248de69f8358dd7ad"", ""tags"": [""art"", ""audio-to-audio"", ""dataset:Stanford/web_questions"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""audio-to-audio""}",1,[],0,[],0
23,Futuresony/Future_pics_26-01-2025,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-image
library_name: diffusers
tags:
- finance
---","{""modelId"": ""Futuresony/Future_pics_26-01-2025"", ""sha"": ""9a321bb06342211829b501b60adb6afbdc1cdf85"", ""tags"": [""diffusers"", ""finance"", ""text-to-image"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 5, ""pipeline_tag"": ""text-to-image""}",1,[],0,[],0
24,drperkybottom/DeepLerting-LLM,"---
license: mit
datasets:
- O1-OPEN/OpenO1-SFT
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""drperkybottom/DeepLerting-LLM"", ""sha"": ""316ee2775e3eee56dcffd0945c54784ad1cb3201"", ""tags"": [""text-generation"", ""en"", ""dataset:O1-OPEN/OpenO1-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
25,Minnus/rtrancit,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
- ml
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""Minnus/rtrancit"", ""sha"": ""5c385722c37bd6d78fb02eb26db4d43f48cfa42d"", ""tags"": [""text-generation"", ""en"", ""ml"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
26,chitdev/deepseek-r1-distill-7b,"---
datasets:
- ServiceNow-AI/R1-Distill-SFT
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""chitdev/deepseek-r1-distill-7b"", ""sha"": ""9df2de87774b7547af19d6191494250bd974bb64"", ""tags"": [""dataset:ServiceNow-AI/R1-Distill-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
27,usersomethingelze/birdinyourear,"---
license: unknown
datasets:
- fka/awesome-chatgpt-prompts
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: asteroid
---","{""modelId"": ""usersomethingelze/birdinyourear"", ""sha"": ""09166e86e4d4f37ccf8f08be7bbee15b0102467a"", ""tags"": [""asteroid"", ""text-generation"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""doi:10.57967/hf/4460"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
28,Tackit/Flensburg,"---
license: mit
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
- open-thoughts/OpenThoughts-114k
language:
- de
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: tencent/Hunyuan3D-2
---","{""modelId"": ""Tackit/Flensburg"", ""sha"": ""1c5e7eaa3ad0675cbf3a5513a614a500a004e413"", ""tags"": [""de"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
29,FernDelga/CorpoBotdelFer,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: hexgrad/Kokoro-82M
library_name: asteroid
---","{""modelId"": ""FernDelga/CorpoBotdelFer"", ""sha"": ""4fd6b91139c69a0d4cc6bfe8d5a299a97e985d4b"", ""tags"": [""asteroid"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
30,devayanihodgir/Resume_Analyzer,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: asteroid
tags:
- '#resume'
---","{""modelId"": ""devayanihodgir/Resume_Analyzer"", ""sha"": ""afc86a84b22a959275ade5592588b9658add542f"", ""tags"": [""asteroid"", ""#resume"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
31,devl-8980-sn/india_legal_QA_deepseek,"---
license: mit
language:
- en
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
library_name: transformers
---
# Overview

This is a finetuned version of DeepSeek-R1 8B, designed for QnA related to Indian laws and penalties.","{""modelId"": ""devl-8980-sn/india_legal_QA_deepseek"", ""sha"": ""250afa75090ab3d09f5e867d1b6567b54d4eae90"", ""tags"": [""transformers"", ""safetensors"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
32,xiaoyuboi/test-model,"

---
license: mit
language:
- zh
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- biology
- text-generation-inference
- ËßÜÈ¢ë
datasets:
- open-r1/OpenR1-Math-220k
widget:
- text: ""What's my name?""
  context: ""My name is Clara and I live in Berkeley.""
  example_title: ""Name""
- text: ""Where do I live?""
  context: ""My name is Sarah and I live in London""
  example_title: ""Location""
---
---
co2_eq_emissions:
  emissions: number (in grams of CO2)
  source: ""source of the information, either directly from AutoTrain, code carbon or from a scientific article documenting the model""
  training_type: ""pre-training or fine-tuning""
  geographical_location: ""as granular as possible, for instance Quebec, Canada or Brooklyn, NY, USA. To check your compute's electricity grid, you can check out https://app.electricitymap.org.""
  hardware_used: ""how much compute and what kind, e.g. 8 v100 GPUs""
---


widget:
- text: ""What's my name?""
  context: ""My name is Clara and I live in Berkeley.""
  example_title: ""Name""
- text: ""Where do I live?""
  context: ""My name is Sarah and I live in London""
  example_title: ""Location""


Ê®°ÂûãÂêçÁß∞ÔºöÊÉÖÊÑüÂàÜÊûêÊ®°Âûã (Sentiment Analysis Model)
Ê®°ÂûãÊ¶ÇËø∞Ôºö
Áî®ÈÄîÔºöÁî®‰∫éÂàÜÊûêËã±ÊñáÁ§æ‰∫§Â™í‰ΩìÊñáÊú¨ÁöÑÊÉÖÊÑüÔºàÊ≠£Èù¢„ÄÅË¥üÈù¢„ÄÅ‰∏≠ÊÄßÔºâ„ÄÇ
ÂºÄÂèëËÄÖÔºöÊüêÊüêÁ†îÁ©∂Âõ¢Èòü„ÄÇ
ÁâàÊú¨Ôºöv1.0„ÄÇ
ËÆ≠ÁªÉÊï∞ÊçÆÔºö
Êï∞ÊçÆÈõÜÔºöTwitter Êï∞ÊçÆÈõÜÔºåÂåÖÂê´ 100,000 Êù°Ê†áÊ≥®ÁöÑÊé®Êñá„ÄÇ
Êï∞ÊçÆÂàÜÂ∏ÉÔºö
Ê≠£Èù¢Ôºö40%
Ë¥üÈù¢Ôºö40%
‰∏≠ÊÄßÔºö20%
Êï∞ÊçÆÂÅèÂ∑ÆÔºöËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠Áº∫Â∞ëÈùûËã±ËØ≠ÂõΩÂÆ∂ÁöÑÊé®Êñá„ÄÇ
ÊÄßËÉΩÔºö
ÂáÜÁ°ÆÁéáÔºö85%„ÄÇ
ÊÄßËÉΩÂ∑ÆÂºÇÔºöÂØπÁü≠ÊñáÊú¨Ë°®Áé∞ËæÉÂ•ΩÔºå‰ΩÜÂØπÈïøÊñáÊú¨Ë°®Áé∞ËæÉÂ∑Æ„ÄÇ
ÈÄÇÁî®Âú∫ÊôØÔºö
Á§æ‰∫§Â™í‰ΩìÊÉÖÊÑüÂàÜÊûê„ÄÇ
Áî®Êà∑ÂèçÈ¶àÁöÑÊÉÖÊÑüÂàÜÁ±ª„ÄÇ
‰∏çÈÄÇÁî®Âú∫ÊôØÔºö
ÈùûËã±ÊñáÊñáÊú¨„ÄÇ
‰∏ì‰∏öÈ¢ÜÂüüÔºàÂ¶ÇÂåªÂ≠¶„ÄÅÊ≥ïÂæãÔºâ‰∏≠ÁöÑÊÉÖÊÑüÂàÜÊûê„ÄÇ
‰º¶ÁêÜËÄÉÈáèÔºö
ÂÅèÂ∑ÆÔºöÂèØËÉΩÂØπÊüê‰∫õÊñπË®ÄÊàñ‰øöËØ≠Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ
È£éÈô©ÔºöËØØÂàÜÁ±ªÂèØËÉΩÂØºËá¥ÈîôËØØÂÜ≥Á≠ñ„ÄÇ
ÊäÄÊúØÁªÜËäÇÔºö
Êû∂ÊûÑÔºöBERT„ÄÇ
ËÆ≠ÁªÉÊ°ÜÊû∂ÔºöPyTorch„ÄÇ
‰ºòÂåñÂô®ÔºöAdam„ÄÇ","{""modelId"": ""xiaoyuboi/test-model"", ""sha"": ""af5e35b93a8676a6078bad9145bd37e56224194e"", ""tags"": [""biology"", ""text-generation-inference"", ""\u89c6\u9891"", ""zh"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
33,niloyda/AnythingChatBot,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
- open-r1/OpenR1-Math-220k
language:
- en
- bn
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
tags:
- chemistry
- biology
- art
- code
- text-generation-inference
---","{""modelId"": ""niloyda/AnythingChatBot"", ""sha"": ""c48c8f65f1c33aa483802534066d94aa4cdd93b4"", ""tags"": [""chemistry"", ""biology"", ""art"", ""code"", ""text-generation-inference"", ""question-answering"", ""en"", ""bn"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
34,saleh1977/nexta-9101,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- ar
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: allennlp
tags:
- not-for-all-audiences
---
@misc{saleh2025nexta9101,
  author = {Saleh},
  title = {NextA-9101: Arabic Sentiment Analysis Model},
  year = {2025},
  publisher = {HuggingFace},
  howpublished = {\url{https://huggingface.co/saleh1977/nexta-9101}}
}","{""modelId"": ""saleh1977/nexta-9101"", ""sha"": ""99f218c4928c814054e4edc5bb81e571dec71ffb"", ""tags"": [""allennlp"", ""safetensors"", ""not-for-all-audiences"", ""text-classification"", ""ar"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
35,ayeshawtahir/pharmacopeia,"---
license: apache-2.0
datasets:
- Shekswess/ai-in-healthcare-medicine
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- medical
metrics:
- accuracy
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""ayeshawtahir/pharmacopeia"", ""sha"": ""1269b706f058f685ed2f146a6a1dc09ea7c0d852"", ""tags"": [""medical"", ""en"", ""dataset:Shekswess/ai-in-healthcare-medicine"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 1, ""pipeline_tag"": null}",1,[],0,[],0
36,sarvar3697/sarvar_2,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
- gopipasala/fka-awesome-chatgpt-prompts
language:
- hi
- en
- bh
metrics:
- accuracy
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/Janus-Pro-7B
new_version: hexgrad/Kokoro-82M
pipeline_tag: question-answering
library_name: transformers
tags:
- code
---","{""modelId"": ""sarvar3697/sarvar_2"", ""sha"": ""f4b86b4b6a7a1aa5da2dcd5781c9aaf37dd05e8d"", ""tags"": [""transformers"", ""code"", ""question-answering"", ""hi"", ""en"", ""bh"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:gopipasala/fka-awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
37,feitap/exp,"---
license: apache-2.0
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""feitap/exp"", ""sha"": ""1be5f9c49354d96478a22ab06df07368576c2701"", ""tags"": [""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
38,marlono/test,"---
license: mit
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""marlono/test"", ""sha"": ""a36ce858f2847bbb6517ed28cf84f172993bf6af"", ""tags"": [""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
39,Adamastor/bully,"---
datasets:
- fka/awesome-chatgpt-prompts
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""Adamastor/bully"", ""sha"": ""2cf8f391cd76d70f2f2b05c24e64d54db9409035"", ""tags"": [""text-classification"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
40,zain10000/ChatBot,"---
base_model:
- deepseek-ai/DeepSeek-R1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""zain10000/ChatBot"", ""sha"": ""b2bcbfdf31911e4f98b156a97f17c5b9a0d091db"", ""tags"": [""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
41,karim8955/mate,"---
datasets:
- fka/awesome-chatgpt-prompts
- gopipasala/fka-awesome-chatgpt-prompts
- cognitivecomputations/dolphin-r1
language:
- hi
- ur
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: microsoft/phi-4
pipeline_tag: text-generation
library_name: fastai
tags:
- code
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""karim8955/mate"", ""sha"": ""1ff1e563166737c1600490c126cf09fccadaccb5"", ""tags"": [""fastai"", ""code"", ""text-generation"", ""hi"", ""ur"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:gopipasala/fka-awesome-chatgpt-prompts"", ""dataset:cognitivecomputations/dolphin-r1"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
42,usamaaleem99tech/DeepSeek-R1-Medical,"---
license: mit
tags:
- unsloth
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---

Notebook for finetunning: https://www.kaggle.com/code/muhammadusamaaleem/deepseek-fine-tunning","{""modelId"": ""usamaaleem99tech/DeepSeek-R1-Medical"", ""sha"": ""c9856a435c29d9427099261e71ff0f5edeb09886"", ""tags"": [""safetensors"", ""unsloth"", ""question-answering"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
43,rshaikh22/coachcarellm,"---
license: mit
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: transformers
---","{""modelId"": ""rshaikh22/coachcarellm"", ""sha"": ""fd52d4205ea4dae55408ffc2bc0b8e1b38780843"", ""tags"": [""transformers"", ""safetensors"", ""qwen2"", ""text-generation"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 80, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
44,DangChuVM/Model,"---
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- vi
base_model:
- deepseek-ai/DeepSeek-R1
---
xin chaod","{""modelId"": ""DangChuVM/Model"", ""sha"": ""79067e0f029deb06681fd29b1312ca2e3771b03b"", ""tags"": [""vi"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
45,YuRiVeRTi/VQ1,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
- fka/awesome-chatgpt-prompts
- open-r1/OpenR1-Math-220k
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT
- FreedomIntelligence/medical-o1-reasoning-SFT
- saiyan-world/Goku-MovieGenBench
- cais/hle
- ServiceNow-AI/R1-Distill-SFT
- cognitivecomputations/dolphin-r1
language:
- en
- hi
- as
- mr
- uk
- ja
- aa
- ab
- ae
- ak
- am
- af
- ar
- av
- ay
- az
- ba
- bg
- be
metrics:
- accuracy
- bertscore
- bleu
- code_eval
base_model:
- deepseek-ai/DeepSeek-V3
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
- mistralai/Mistral-Small-24B-Instruct-2501
library_name: diffusers
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

### VQV1

### USED FOR PERSONAL ONLY 

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [YuRiVeRTi]:
- **Funded by [YuRiVeRTi]:** 
- **Shared by [YuRiVeRTi]:** [DEVELOP BY YURIVERTI FOR FINETUNE VR WITH UNCENSORED CAN RUN LOCALLY ON THE COMMAND]
- **Model type:** [VQV1]
- **Language(s) (NLP):** [Ml.LLM]
- **License:** [ALPHACT 2.0]
- **Finetuned from model :** [VQV1 RUNS ON V3 MODLE ]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** []
- open-thoughts/OpenThoughts-114k
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [RTX 5090 Ti]
- **Hours used:** [2160 hours]
- **Cloud Provider:** [CLOUDFARE & GITHUB'HUGGING FACE]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [500 kg of CO2 ]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[RTX 5090 Ti]

#### Software

[ORACAL.LINUX.LINUX ARCH]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""YuRiVeRTi/VQ1"", ""sha"": ""0cdfb6cef06e300fe29628be1d8969f6e29abd06"", ""tags"": [""diffusers"", ""en"", ""hi"", ""as"", ""mr"", ""uk"", ""ja"", ""aa"", ""ab"", ""ae"", ""ak"", ""am"", ""af"", ""ar"", ""av"", ""ay"", ""az"", ""ba"", ""bg"", ""be"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:open-r1/OpenR1-Math-220k"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""dataset:saiyan-world/Goku-MovieGenBench"", ""dataset:cais/hle"", ""dataset:ServiceNow-AI/R1-Distill-SFT"", ""dataset:cognitivecomputations/dolphin-r1"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""doi:10.57967/hf/4677"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
46,rehamhisham/saas,"---
license: apache-2.0
datasets:
- nvidia/Llama-Nemotron-Post-Training-Dataset-v1
language:
- af
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: summarization
library_name: fasttext
---","{""modelId"": ""rehamhisham/saas"", ""sha"": ""d7d281068e4f1423c524d76fbb44bf1a4b40be3e"", ""tags"": [""fasttext"", ""summarization"", ""af"", ""dataset:nvidia/Llama-Nemotron-Post-Training-Dataset-v1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""summarization""}",1,[],0,[],0
47,guanglian/test,"---
datasets:
- fka/awesome-chatgpt-prompts
license: mit
metrics:
- accuracy
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: transformers
tags:
- legal
- medical
- climate
language:
- aa
new_version: deepseek-ai/DeepSeek-V3-0324
widget:
- text: ""Is this review positive or negative? Review: Best cast iron skillet you will ever buy.""
  example_title: ""Sentiment analysis""
- text: ""Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had ...""
  example_title: ""Coreference resolution""
- text: ""On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book ...""
  example_title: ""Logic puzzles""
- text: ""The two men running to become New York City's next mayor will face off in their first debate Wednesday night ...""
  example_title: ""Reading comprehension""
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""guanglian/test"", ""sha"": ""3cec26cba3f2ce2c6e2322a05b4a84c8dbb4e3e9"", ""tags"": [""transformers"", ""legal"", ""medical"", ""climate"", ""text-generation"", ""aa"", ""dataset:fka/awesome-chatgpt-prompts"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
48,wsxdyzx2025/weigb,"---
license: mit
datasets:
- fka/awesome-chatgpt-prompts
metrics:
- cer
base_model:
- openbmb/MiniCPM-o-2_6
- deepseek-ai/DeepSeek-R1
- tencent/Hunyuan3D-2
- deepseek-ai/Janus-Pro-7B
- microsoft/phi-4
new_version: microsoft/phi-4
library_name: espnet
tags:
- code
---","{""modelId"": ""wsxdyzx2025/weigb"", ""sha"": ""15491eefd5390b86d0026ea67bb973ef20dd6a75"", ""tags"": [""espnet"", ""code"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
49,farypor/seoaigen,"---
license: apache-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
---","{""modelId"": ""farypor/seoaigen"", ""sha"": ""1ce2c0e7eef70223daa286cbab0ba0d31d59f4a0"", ""tags"": [""token-classification"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
50,seenutheleo/imdb-model,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
---","{""modelId"": ""seenutheleo/imdb-model"", ""sha"": ""77e3879025ea81f6dae48db1710b7e3a6e2f7f55"", ""tags"": [""token-classification"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
51,silence09/DeepSeek-R1-3layers,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---
# Lightweight Deepseek R1 (3 Hidden Layers Version)

This project is created using the official **Deepseek R1** model script (`modeling_deepseek.py`) from [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/modeling_deepseek.py). It implements a **3-layer version** of Deepseek R1 with randomly initialized weights.

## Model Structure
The three hidden layers consist of:
- **A hidden layer: MLA + Dense MLP**
- **A hidden layer: MLA + MoE (Mixture of Experts) MLP**
- **A MTP (Multi-Token Pretraining) layer (MTP can be regarded or used for speculative decoding in inference)**

## Purpose
The purpose of these weights is to provide a lightweight implementation for researchers who want to study the model architecture and run experiments quickly.

The original **Deepseek R1 model** requires an **8x H200 GPU setup** and runs on the **vLLM/SGLang framework**, making it difficult to deploy on standard hardware.

## Usage

```python
from transformers import AutoConfig, AutoModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained('silence09/DeepSeek-R1-3layers', torch_dtype=torch.bfloat16).cuda()
tokenizer = AutoTokenizer.from_pretrained('silence09/DeepSeek-R1-3layers')

prompt = ""Who are u?""
messages = []
messages.append({""role"": ""user"", ""content"": prompt})
prompt_tokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=""pt"").to(model.device)
generated_ids = model.generate(prompt_tokens, max_new_tokens=100, do_sample=False)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(prompt_tokens, generated_ids)
]
completion = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(completion)
messages.append({""role"": ""assistant"", ""content"": completion})

```

## More Info
It was created using the python script available at [this repository](https://github.com/silencelamb/naked_llama/blob/main/hf_example/create_deepseek_r1_3layers.py)","{""modelId"": ""silence09/DeepSeek-R1-3layers"", ""sha"": ""a042fd02f1e81114b94bd24e79a414c6e270a765"", ""tags"": [""safetensors"", ""deepseek_v3"", ""custom_code"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 23, ""pipeline_tag"": null}",1,[],0,[],0
52,PARSIS/Moshaver,"---
language:
- fa
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""PARSIS/Moshaver"", ""sha"": ""98216efc76b9ffa3835b76043257392de4cdd296"", ""tags"": [""fa"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
53,yifan-playground/deepseek-r1,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""yifan-playground/deepseek-r1"", ""sha"": ""e5744402d564ce849797879fada14f7ce8477dc1"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
54,curryNI/huaiqing_ml_model,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""curryNI/huaiqing_ml_model"", ""sha"": ""929e27fb7f95c054bec57e040ed058bef01c0092"", ""tags"": [""question-answering"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
55,kkangnom/test,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""kkangnom/test"", ""sha"": ""9a33ba513e2473b100472f135e2ccef01609203b"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
56,huihui-ai/DeepSeek-R1-Pruned-Coder-411B,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- deepseek_R1
- bf16
- Safetensors
- custom_code
- Pruned
---

# huihui-ai/DeepSeek-R1-Pruned-Coder-411B




This is a pruned version of the [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1), 
reduced from 256 experts to 160 experts. The pruned model is mainly used for [code](https://huggingface.co/huihui-ai/DeepSeek-R1-Pruned-Coder-411B/blob/main/coding_problems.py) generation.


This is a test validation to see if we can prune the model according to professional requirements and still maintain acceptable performance. 
The model size has been reduced by about 1/3, and no distortion has occurred.

This allows the model to be pruned according to one's needs.

This pruned model has a total parameter is equivalent to 441B.

We will also try to prune [perplexity-ai/r1-1776](https://huggingface.co/perplexity-ai/r1-1776).

## Use with ollama

You can use [huihui_ai/deepseek-r1-pruned](https://ollama.com/huihui_ai/deepseek-r1-pruned) directly
```
ollama run huihui_ai/deepseek-r1-pruned
```


## Use with transformers

```
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

# Load the model and tokenizer
NEW_MODEL_ID = ""huihui-ai/DeepSeek-R1-Pruned-Coder-411B""
quant_config_4 = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    llm_int8_enable_fp32_cpu_offload=True,
)

model = AutoModelForCausalLM.from_pretrained(
    NEW_MODEL_ID, 
    device_map=""auto"", 
    trust_remote_code=True,
    quantization_config=quant_config_4,
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

tokenizer.pad_token_id = tokenizer.eos_token_id

# Initialize conversation context
initial_messages = [
    {""role"": ""system"", ""content"": ""You are a helpful assistant.""}
]
messages = initial_messages.copy()  # Copy the initial conversation context

# Enter conversation loop
while True:
    # Get user input
    user_input = input(""User: "").strip()  # Strip leading and trailing spaces

    # If the user types '/exit', end the conversation
    if user_input.lower() == ""/exit"":
        print(""Exiting chat."")
        break

    # If the user types '/clean', reset the conversation context
    if user_input.lower() == ""/clear"":
        messages = initial_messages.copy()  # Reset conversation context
        print(""Chat history cleared. Starting a new conversation."")
        continue

    # If input is empty, prompt the user and continue
    if not user_input:
        print(""Input cannot be empty. Please enter something."")
        continue

    # Add user input to the conversation
    messages.append({""role"": ""user"", ""content"": user_input})

    tokenized_message = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=""pt"", return_dict=True)
    response_token_ids = model.generate(tokenized_message['input_ids'].to(""cuda:0""), use_cache=False, pad_token_id=tokenizer.pad_token_id, max_new_tokens=8192)
    generated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]
    response = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]

    # Add the model's response to the conversation
    messages.append({""role"": ""assistant"", ""content"": response})

    # Print the model's response
    print(f""Response: {response}"")
```

### Donation

If you like it, please click 'like' and follow us for more updates.  
You can follow [x.com/support_huihui](https://x.com/support_huihui) to get the latest model information from huihui.ai.

##### Your donation helps us continue our further development and improvement, a cup of coffee can do it.
- bitcoin:
```
  bc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge
```
","{""modelId"": ""huihui-ai/DeepSeek-R1-Pruned-Coder-411B"", ""sha"": ""a2ccc8585789815b3ea002d9fa732a014d8df960"", ""tags"": [""safetensors"", ""deepseek_v3"", ""deepseek_R1"", ""bf16"", ""Safetensors"", ""custom_code"", ""Pruned"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 22, ""pipeline_tag"": null}",1,"['https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF', 'https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF']",2,[],0
57,mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF,"---
base_model: huihui-ai/DeepSeek-R1-Pruned-Coder-411B
language:
- en
library_name: transformers
license: mit
quantized_by: mradermacher
tags:
- deepseek_R1
- bf16
- Safetensors
- custom_code
- Pruned
---
## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags: nicoboss -->
weighted/imatrix quants of https://huggingface.co/huihui-ai/DeepSeek-R1-Pruned-Coder-411B

<!-- provided-files -->
static quants are available at https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ1_S.gguf.part1of2) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ1_S.gguf.part2of2) | i1-IQ1_S | 85.2 | for the desperate |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ1_M.gguf.part1of2) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ1_M.gguf.part2of2) | i1-IQ1_M | 94.9 | mostly desperate |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XXS.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XXS.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XXS.gguf.part3of3) | i1-IQ2_XXS | 111.0 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XS.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XS.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_XS.gguf.part3of3) | i1-IQ2_XS | 124.0 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_S.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_S.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_S.gguf.part3of3) | i1-IQ2_S | 125.7 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_M.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_M.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ2_M.gguf.part3of3) | i1-IQ2_M | 138.5 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K_S.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K_S.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K_S.gguf.part3of3) | i1-Q2_K_S | 142.8 | very low quality |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q2_K.gguf.part4of4) | i1-Q2_K | 155.2 | IQ3_XXS probably better |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XXS.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XXS.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XXS.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XXS.gguf.part4of4) | i1-IQ3_XXS | 164.0 | lower quality |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XS.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XS.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XS.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_XS.gguf.part4of4) | i1-IQ3_XS | 173.5 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_S.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_S.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_S.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_S.gguf.part4of4) | i1-IQ3_S | 183.7 | beats Q3_K* |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_S.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_S.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_S.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_S.gguf.part4of4) | i1-Q3_K_S | 183.7 | IQ3_XS probably better |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_M.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_M.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_M.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ3_M.gguf.part4of4) | i1-IQ3_M | 185.9 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_M.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_M.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_M.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_M.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_M.gguf.part5of5) | i1-Q3_K_M | 202.9 | IQ3_S probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_L.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_L.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_L.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_L.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q3_K_L.gguf.part5of5) | i1-Q3_K_L | 220.9 | IQ3_M probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ4_XS.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ4_XS.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ4_XS.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ4_XS.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-IQ4_XS.gguf.part5of5) | i1-IQ4_XS | 226.8 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_0.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_0.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_0.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_0.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_0.gguf.part5of5) | i1-Q4_0 | 240.7 | fast, low quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_S.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_S.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_S.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_S.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_S.gguf.part5of5) | i1-Q4_K_S | 241.3 | optimal size/speed/quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_K_M.gguf.part6of6) | i1-Q4_K_M | 256.6 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q4_1.gguf.part6of6) | i1-Q4_1 | 266.6 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_S.gguf.part6of6) | i1-Q5_K_S | 293.2 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part1of7) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part2of7) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part3of7) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part4of7) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part5of7) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part6of7) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q5_K_M.gguf.part7of7) | i1-Q5_K_M | 301.7 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.i1-Q6_K.gguf.part8of8) | i1-Q6_K | 349.6 | practically like static Q6_K |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->
","{""modelId"": ""mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF"", ""sha"": ""ba16960984db794d263813516f082769228cf73e"", ""tags"": [""transformers"", ""deepseek_R1"", ""bf16"", ""Safetensors"", ""custom_code"", ""Pruned"", ""en"", ""base_model:huihui-ai/DeepSeek-R1-Pruned-Coder-411B"", ""base_model:finetune:huihui-ai/DeepSeek-R1-Pruned-Coder-411B"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
58,mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF,"---
base_model: huihui-ai/DeepSeek-R1-Pruned-Coder-411B
language:
- en
library_name: transformers
license: mit
quantized_by: mradermacher
tags:
- deepseek_R1
- bf16
- Safetensors
- custom_code
- Pruned
---
## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/huihui-ai/DeepSeek-R1-Pruned-Coder-411B

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q2_K.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q2_K.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q2_K.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q2_K.gguf.part4of4) | Q2_K | 155.2 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_S.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_S.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_S.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_S.gguf.part4of4) | Q3_K_S | 183.7 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_M.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_M.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_M.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_M.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_M.gguf.part5of5) | Q3_K_M | 202.9 | lower quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_L.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_L.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_L.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_L.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q3_K_L.gguf.part5of5) | Q3_K_L | 220.9 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.IQ4_XS.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.IQ4_XS.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.IQ4_XS.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.IQ4_XS.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.IQ4_XS.gguf.part5of5) | IQ4_XS | 228.3 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_S.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_S.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_S.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_S.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_S.gguf.part5of5) | Q4_K_S | 241.3 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q4_K_M.gguf.part6of6) | Q4_K_M | 256.6 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_S.gguf.part6of6) | Q5_K_S | 293.2 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part1of7) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part2of7) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part3of7) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part4of7) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part5of7) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part6of7) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q5_K_M.gguf.part7of7) | Q5_K_M | 301.7 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q6_K.gguf.part8of8) | Q6_K | 349.6 | very good quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part01of10) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part02of10) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part03of10) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part04of10) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part05of10) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part06of10) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part07of10) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part08of10) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part09of10) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF/resolve/main/DeepSeek-R1-Pruned-Coder-411B.Q8_0.gguf.part10of10) | Q8_0 | 452.7 | fast, best quality |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->
","{""modelId"": ""mradermacher/DeepSeek-R1-Pruned-Coder-411B-GGUF"", ""sha"": ""c820dd99ab998e8501d9b5c40fbc414dc32d02fa"", ""tags"": [""transformers"", ""deepseek_R1"", ""bf16"", ""Safetensors"", ""custom_code"", ""Pruned"", ""en"", ""base_model:huihui-ai/DeepSeek-R1-Pruned-Coder-411B"", ""base_model:finetune:huihui-ai/DeepSeek-R1-Pruned-Coder-411B"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
59,CyrusXtovia/MetLawBot,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""CyrusXtovia/MetLawBot"", ""sha"": ""df8eb31f44f7773b74f13c3328fec3cbee18b946"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
60,GeorgeWeasley84/convert-case,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: summarization
---","{""modelId"": ""GeorgeWeasley84/convert-case"", ""sha"": ""2f8491ed945d13df21d4da3790509e31c11eff08"", ""tags"": [""summarization"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""summarization""}",1,[],0,[],0
61,Ai1God/Godboy,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Ai1God/Godboy"", ""sha"": ""a67f5e3ea454b50ff67433ec92cd662ee55b9705"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
62,GalaxyPoo/Mine,"---
license: apache-2.0
language:
- ab
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""GalaxyPoo/Mine"", ""sha"": ""895d8b7963d127a160a6750341519452d0059407"", ""tags"": [""ab"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
63,Aspenini/Backwards-AI,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Aspenini/Backwards-AI"", ""sha"": ""ef152ec81c9b8966753add914eb44fc866e7d81b"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
64,tempbggff/test,"---
language:
- en
- mr
- hi
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""tempbggff/test"", ""sha"": ""6c423c9dab28f5aaa40dfe360ec947868cbac0f6"", ""tags"": [""en"", ""mr"", ""hi"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
65,Withersen/AIArtCreator,"---
license: creativeml-openrail-m
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Withersen/AIArtCreator"", ""sha"": ""ff16362b6cf2cfefc3b84233f0136d0b95c6f092"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
66,coralgables/crypto,"---
base_model:
- deepseek-ai/DeepSeek-V3
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Llama-70B
---","{""modelId"": ""coralgables/crypto"", ""sha"": ""3f3e68cdc412e60e60b2104719726d7115a91762"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
67,ExplodeMediaG/011_search-model,"---
license: mit
datasets:
- Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B
- rulins/DeepSeek-R1-Distill-Qwen-32B_NUMINA_train_amc_aime
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3
---","{""modelId"": ""ExplodeMediaG/011_search-model"", ""sha"": ""cbc868c85e3fadb457d3002ae26c29f9fce78f3b"", ""tags"": [""en"", ""dataset:Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"", ""dataset:rulins/DeepSeek-R1-Distill-Qwen-32B_NUMINA_train_amc_aime"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
68,Awaiz031/Awaizahmad,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- aa
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-image
library_name: flair
tags:
- art
---","{""modelId"": ""Awaiz031/Awaizahmad"", ""sha"": ""939973f3bbfb827756762256dc9bf2802f574bbe"", ""tags"": [""flair"", ""art"", ""text-to-image"", ""aa"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-image""}",1,[],0,[],0
69,Yadav009/Aiclothchange,"---
license: afl-3.0
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
language:
- ae
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: Qwen/QwQ-32B
pipeline_tag: text-generation
---","{""modelId"": ""Yadav009/Aiclothchange"", ""sha"": ""4957c5cb19f0e355556f09c95afb0f28bd0d635f"", ""tags"": [""text-generation"", ""ae"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
70,zedx1/BlueAI,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
- O1-OPEN/OpenO1-SFT
language:
- uz
- en
metrics:
- accuracy
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: microsoft/phi-4
tags:
- code
- finance
- art
- text-generation-inference
- climate
---","{""modelId"": ""zedx1/BlueAI"", ""sha"": ""fe7c62021963cd7bfbb1bedd740d791a7a1280d8"", ""tags"": [""code"", ""finance"", ""art"", ""text-generation-inference"", ""climate"", ""uz"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:O1-OPEN/OpenO1-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
71,SirFestus/Text-To-Text,"---
license: bigscience-openrail-m
datasets:
- open-thoughts/OpenThoughts-114k
- PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT
language:
- ak
metrics:
- accuracy
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fasttext
tags:
- chemistry
- biology
- legal
- finance
- music
- medical
- climate
- text-generation-inference
- merge
- art
---","{""modelId"": ""SirFestus/Text-To-Text"", ""sha"": ""1f1865fd4e18ae5cd6e1c74c1b26285e3fca3a4e"", ""tags"": [""fasttext"", ""chemistry"", ""biology"", ""legal"", ""finance"", ""music"", ""medical"", ""climate"", ""text-generation-inference"", ""merge"", ""art"", ""ak"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigscience-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
72,michaelngangom/dummy-bank,"---
license: apache-2.0
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
tags:
- finance
---","{""modelId"": ""michaelngangom/dummy-bank"", ""sha"": ""7a12888da10e46cc1f763fc2b620a0b16c9afaaa"", ""tags"": [""finance"", ""text-generation"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
73,Albert9527/model-demo,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Albert9527/model-demo"", ""sha"": ""76f947d2902b82edcb84d3ba5abd60547f31478a"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
74,Hamzillo/Lolo,"---
license: bsl-1.0
datasets:
- NovaSky-AI/Sky-T1_data_17k
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: hexgrad/Kokoro-82M
pipeline_tag: question-answering
tags:
- code
---","{""modelId"": ""Hamzillo/Lolo"", ""sha"": ""07e16e4ec869c52df21923fb75ab4cc2a49e2235"", ""tags"": [""code"", ""question-answering"", ""dataset:NovaSky-AI/Sky-T1_data_17k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bsl-1.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
75,Nerker/Rdrffg,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- ru
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Nerker/Rdrffg"", ""sha"": ""91b383bacfd925312e0dda4880c0ff00a455cc8c"", ""tags"": [""ru"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
76,zonnell/discord,"---
language:
- en
- ru
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""zonnell/discord"", ""sha"": ""762f96bcdd69337692de3d9912e239e5a4311d26"", ""tags"": [""text-generation"", ""en"", ""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
77,Mylamoore040/Myla,"---
license: bigcode-openrail-m
datasets:
- open-thoughts/OpenThoughts-114k
- open-r1/OpenR1-Math-220k
- cognitivecomputations/dolphin-r1
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: translation
library_name: diffusers
---","{""modelId"": ""Mylamoore040/Myla"", ""sha"": ""d2e9d41bf75a261be1f7e11d07d36078b817e3fe"", ""tags"": [""diffusers"", ""translation"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:open-r1/OpenR1-Math-220k"", ""dataset:cognitivecomputations/dolphin-r1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigcode-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
78,Lotusaihk/lotusaihk,"---
license: unknown
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Lotusaihk/lotusaihk"", ""sha"": ""ce962aeaf73e10740fdc9e5a6da9cd616c5b1f5f"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
79,Vepa1979/turkmence,"---
license: apache-2.0
language:
- tk
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-audio
library_name: allennlp
---","{""modelId"": ""Vepa1979/turkmence"", ""sha"": ""eb8275cd7e86535d18af1986e5a2fbe674b5251c"", ""tags"": [""allennlp"", ""text-to-audio"", ""tk"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-audio""}",1,[],0,[],0
80,raghu1155/DeepSeek-R1-Codegeneration-COT,"---
library_name: transformers
tags:
- code
- unsloth
- trl
- sft
license: apache-2.0
datasets:
- google-research-datasets/mbpp
language:
- en
metrics:
- bleu
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""raghu1155/DeepSeek-R1-Codegeneration-COT"", ""sha"": ""f3ea1fba57255c3355d379b3c6895fffe7a06d9d"", ""tags"": [""transformers"", ""pytorch"", ""safetensors"", ""llama"", ""text-generation"", ""code"", ""unsloth"", ""trl"", ""sft"", ""conversational"", ""en"", ""dataset:google-research-datasets/mbpp"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 20, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
81,tornado4651/test,"---
license: mit
datasets:
- saiyan-world/Goku-MovieGenBench
language:
- ae
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: Qwen/QwQ-32B
pipeline_tag: any-to-any
library_name: fastai
tags:
- code
---","{""modelId"": ""tornado4651/test"", ""sha"": ""15d0800f60935bd1eaf31992edf7fcb7a91bceb1"", ""tags"": [""fastai"", ""code"", ""any-to-any"", ""ae"", ""dataset:saiyan-world/Goku-MovieGenBench"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""any-to-any""}",1,[],0,[],0
82,Raymondjoe007/thor,"---
license: bigscience-bloom-rail-1.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
- ja
- fr
- es
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: allennlp
tags:
- art
---","{""modelId"": ""Raymondjoe007/thor"", ""sha"": ""b45da48dde79702f3097b3be718009fed420c56d"", ""tags"": [""allennlp"", ""art"", ""en"", ""ja"", ""fr"", ""es"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigscience-bloom-rail-1.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
83,ManishDipole/Demo,"---
license: llama3.2
datasets:
- nvidia/Llama-Nemotron-Post-Training-Dataset-v1
language:
- en
- hi
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3-0324
pipeline_tag: text-generation
library_name: asteroid
tags:
- code
---","{""modelId"": ""ManishDipole/Demo"", ""sha"": ""ed34e387b6bf99fe9dd4378a943d07fb06456f1a"", ""tags"": [""asteroid"", ""code"", ""text-generation"", ""en"", ""hi"", ""dataset:nvidia/Llama-Nemotron-Post-Training-Dataset-v1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama3.2"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
84,lilmos/twins-ai,"---
language:
- fa
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""lilmos/twins-ai"", ""sha"": ""d966c63599571de39fd7904ab4c279ff01dc9aa9"", ""tags"": [""fa"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
85,FarhanisGoingTomakeaAi/NiteTalkbot,"---
license: afl-3.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- code
---","{""modelId"": ""FarhanisGoingTomakeaAi/NiteTalkbot"", ""sha"": ""51fa0691a7cf12e67e505b2d940bd48afad6b80b"", ""tags"": [""code"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
86,mradermacher/DeepSeek-R1-GGUF,"---
base_model: deepseek-ai/DeepSeek-R1
language:
- en
library_name: transformers
license: mit
quantized_by: mradermacher
---
## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/deepseek-ai/DeepSeek-R1

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q2_K.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q2_K.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q2_K.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q2_K.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q2_K.gguf.part5of5) | Q2_K | 244.1 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_S.gguf.part6of6) | Q3_K_S | 289.2 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part1of7) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part2of7) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part3of7) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part4of7) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part5of7) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part6of7) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_M.gguf.part7of7) | Q3_K_M | 319.3 | lower quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q3_K_L.gguf.part8of8) | Q3_K_L | 347.5 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.IQ4_XS.gguf.part8of8) | IQ4_XS | 359.6 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_S.gguf.part8of8) | Q4_K_S | 380.1 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part1of9) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part2of9) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part3of9) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part4of9) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part5of9) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part6of9) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part7of9) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part8of9) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q4_K_M.gguf.part9of9) | Q4_K_M | 404.5 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part01of10) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part02of10) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part03of10) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part04of10) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part05of10) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part06of10) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part07of10) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part08of10) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part09of10) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_S.gguf.part10of10) | Q5_K_S | 461.9 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part01of10) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part02of10) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part03of10) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part04of10) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part05of10) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part06of10) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part07of10) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part08of10) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part09of10) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q5_K_M.gguf.part10of10) | Q5_K_M | 475.5 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part01of12) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part02of12) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part03of12) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part04of12) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part05of12) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part06of12) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part07of12) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part08of12) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part09of12) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part10of12) [P11](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part11of12) [P12](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q6_K.gguf.part12of12) | Q6_K | 550.9 | very good quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part01of18) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part02of18) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part03of18) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part04of18) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part05of18) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part06of18) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part07of18) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part08of18) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part09of18) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part10of18) [P11](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part11of18) [P12](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part12of18) [P13](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part13of18) [P14](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part14of18) [P15](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part15of18) [P16](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part16of18) [P17](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part17of18) [P18](https://huggingface.co/mradermacher/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1.Q8_0.gguf.part18of18) | Q8_0 | 713.4 | fast, best quality |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->
","{""modelId"": ""mradermacher/DeepSeek-R1-GGUF"", ""sha"": ""e1fb00913cbff0fdb25b8216cb51e92eee4f4dfb"", ""tags"": [""transformers"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
87,Dimaswa/openrail,"---
license: openrail
datasets:
- facebook/natural_reasoning
language:
- ab
- aa
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3-0324
pipeline_tag: token-classification
---","{""modelId"": ""Dimaswa/openrail"", ""sha"": ""781f89ed559c263598e49972ea692fbdabbe186d"", ""tags"": [""token-classification"", ""ab"", ""aa"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
88,fematt/telebot,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""fematt/telebot"", ""sha"": ""fe91c2b4a466db8f96b36ea60d5c05c7df555c24"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
89,Owen14gjqwertkeyboard/LibrarianAI,"---
license: gpl-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Owen14gjqwertkeyboard/LibrarianAI"", ""sha"": ""9650770ee188f75aa0b878ed8bcddf1631ebef92"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:gpl-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
90,yookidz/my-code-Llama,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""yookidz/my-code-Llama"", ""sha"": ""0b1d3fd640bdcea9a90170aacae3b663901581ef"", ""tags"": [""text-classification"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
91,tonybb815/Tiny,"---
license: mit
language:
- nl
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-speech
tags:
- art
datasets:
- bespokelabs/Bespoke-Stratos-17k
metrics:
- accuracy
- character
new_version: deepseek-ai/Janus-Pro-7B
library_name: fasttext
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""tonybb815/Tiny"", ""sha"": ""592f1b1dbf46107a70c5359c928d698351933897"", ""tags"": [""fasttext"", ""art"", ""text-to-speech"", ""nl"", ""dataset:bespokelabs/Bespoke-Stratos-17k"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-speech""}",1,[],0,[],0
92,Haryni/model,"---
datasets:
- asr-malayalam/indicvoices-v1a
- Tensoic/GPTeacher-Malayalam
language:
- ml
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: translation
---
import os
import argparse
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from utils import compute_metrics

def load_dataset(file_path):
    """"""Load and prepare the dataset.""""""
    df = pd.read_csv(file_path)
    dataset = Dataset.from_pandas(df)
    # Split dataset into train and validation
    split_dataset = dataset.train_test_split(test_size=0.1)
    return split_dataset

def preprocess_function(examples, tokenizer, max_length=128):
    """"""Tokenize the texts.""""""
    inputs = [ex for ex in examples[""english_text""]]
    targets = [ex for ex in examples[""malayalam_text""]]
    
    model_inputs = tokenizer(
        inputs,
        max_length=max_length,
        truncation=True,
        padding=""max_length"",
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets,
            max_length=max_length,
            truncation=True,
            padding=""max_length"",
        )

    model_inputs[""labels""] = labels[""input_ids""]
    return model_inputs

def main(args):
    # Load tokenizer and model
    model_name = ""google/mt5-small""
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Load and preprocess dataset
    dataset = load_dataset(""dataset/malayalam_dataset.csv"")
    
    # Tokenize datasets
    tokenized_datasets = dataset.map(
        lambda x: preprocess_function(x, tokenizer),
        batched=True,
        remove_columns=dataset[""train""].column_names
    )

    # Define training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=""./model"",
        evaluation_strategy=""epoch"",
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        num_train_epochs=args.epochs,
        weight_decay=0.01,
        save_total_limit=2,
        predict_with_generate=True,
        logging_dir=""./logs"",
        logging_steps=100,
        push_to_hub=True,
    )

    # Create data collator
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    # Initialize trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets[""train""],
        eval_dataset=tokenized_datasets[""test""],
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # Train the model
    trainer.train()

    # Save the model
    trainer.save_model(""./model"")
    tokenizer.save_pretrained(""./model"")

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser()
    parser.add_argument(""--epochs"", type=int, default=3)
    parser.add_argument(""--batch_size"", type=int, default=8)
    parser.add_argument(""--learning_rate"", type=float, default=2e-5)
    args = parser.parse_args()
    main(args)","{""modelId"": ""Haryni/model"", ""sha"": ""8db5b66721fddc53bf36081ee834ccbb3022bb92"", ""tags"": [""translation"", ""ml"", ""en"", ""dataset:asr-malayalam/indicvoices-v1a"", ""dataset:Tensoic/GPTeacher-Malayalam"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
93,Klanik58/Devrim_DSE,"---
license: bigcode-openrail-m
datasets:
- Anthropic/EconomicIndex
- open-r1/OpenR1-Math-220k
language:
- as
base_model:
- deepseek-ai/DeepSeek-R1
library_name: fasttext
tags:
- code
---","{""modelId"": ""Klanik58/Devrim_DSE"", ""sha"": ""ab2bab55ee57eeb623985853bf3fe2f0f878586a"", ""tags"": [""fasttext"", ""code"", ""as"", ""dataset:Anthropic/EconomicIndex"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigcode-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
94,samaraamfetamina/frai,"---
license: openrail
datasets:
- facebook/natural_reasoning
language:
- pl
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: asteroid
---","{""modelId"": ""samaraamfetamina/frai"", ""sha"": ""4a5013be4cb060500bdc7441c3e38cd1fc77c6b2"", ""tags"": [""asteroid"", ""pl"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
95,djibhefihnserfnh/vxfvf,"---
license: apache-2.0
datasets:
- open-r1/OpenR1-Math-220k
language:
- aa
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: allennlp
---","{""modelId"": ""djibhefihnserfnh/vxfvf"", ""sha"": ""23c23ab2f73600eb9741d31d2093b0c389a1dec4"", ""tags"": [""allennlp"", ""text-classification"", ""aa"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
96,kalleopinheiro/deepseek,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""kalleopinheiro/deepseek"", ""sha"": ""cab41b73f4348218d21104041b5dad38188baff3"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
97,mikmik2003/jaz2,"---
license: openrail
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""mikmik2003/jaz2"", ""sha"": ""2b12596c232d140ee04d8b8ae6de783468a3412c"", ""tags"": [""text-classification"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
98,primaryPond/product_comparison,"---
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""primaryPond/product_comparison"", ""sha"": ""abc7ac0c0e9b3fed2ceb7e8cead16f72fb9c08f3"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
99,gabrial1927/gabrial,"---
license: bigcode-openrail-m
datasets:
- fka/awesome-chatgpt-prompts
language:
- id
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: asteroid
---","{""modelId"": ""gabrial1927/gabrial"", ""sha"": ""860f178f3702a558cded7346bbaba48b9fc02701"", ""tags"": [""asteroid"", ""id"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigcode-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
100,Priyansu17/miningAact,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Priyansu17/miningAact"", ""sha"": ""c11c4e2ad5b9b0b28d0edc6e0ab2bd008e90a5b9"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
101,Murphy112233/Murphy_Rose,"---
license: apache-2.0
datasets:
- saiyan-world/Goku-MovieGenBench
language:
- ak
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/Janus-Pro-7B
pipeline_tag: text-classification
library_name: espnet
tags:
- not-for-all-audiences
---","{""modelId"": ""Murphy112233/Murphy_Rose"", ""sha"": ""c14e3959ee064e39bb9f03f24df7deb213807c8d"", ""tags"": [""espnet"", ""not-for-all-audiences"", ""text-classification"", ""ak"", ""dataset:saiyan-world/Goku-MovieGenBench"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
102,beita6969/DeepSeek-R1-Distill-Qwen-32B-Medical,"---
tags:
- unsloth
- trl
- sft
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
datasets:
- shibing624/medical
pipeline_tag: audio-text-to-text
library_name: transformers
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""beita6969/DeepSeek-R1-Distill-Qwen-32B-Medical"", ""sha"": ""41f953da3c1ec3ddd12a11e874bd5d4c2d3d1238"", ""tags"": [""transformers"", ""safetensors"", ""qwen2"", ""text-generation"", ""unsloth"", ""trl"", ""sft"", ""audio-text-to-text"", ""en"", ""dataset:shibing624/medical"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 394, ""pipeline_tag"": ""audio-text-to-text""}",1,[],0,[],0
103,William-zhao/Cozysmart,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- zh
- en
- es
- de
- ja
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
tags:
- not-for-all-audiences
- finance
---","{""modelId"": ""William-zhao/Cozysmart"", ""sha"": ""f91387801cebccfc232ff82055a65bfc30e8474c"", ""tags"": [""not-for-all-audiences"", ""finance"", ""zh"", ""en"", ""es"", ""de"", ""ja"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
104,sunooooone/KIMSUNOOMODEL,"---
license: unknown
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- ko
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fastai
tags:
- KPOP
- ENHYPEN
- SUNOO
- AI
---","{""modelId"": ""sunooooone/KIMSUNOOMODEL"", ""sha"": ""13aa566c75a79c250da43c5dd03901563ba34abc"", ""tags"": [""fastai"", ""KPOP"", ""ENHYPEN"", ""SUNOO"", ""AI"", ""ko"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
105,NazarMuts/FridayAPI,"---
datasets:
- fka/awesome-chatgpt-prompts
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- code
---","{""modelId"": ""NazarMuts/FridayAPI"", ""sha"": ""301dad00545756544f4b383e41c32a8d50c18b35"", ""tags"": [""code"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
106,Smdhussain06/Joyboy,"---
license: mit
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
- ta
- ar
- hi
metrics:
- accuracy
- character
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
new_version: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
pipeline_tag: reinforcement-learning
library_name: fastai
tags:
- finance
- code
- text-generation-inference
---","{""modelId"": ""Smdhussain06/Joyboy"", ""sha"": ""71a473a9cc2f9ff44a28c4eb23494966ca1d5e38"", ""tags"": [""fastai"", ""finance"", ""code"", ""text-generation-inference"", ""reinforcement-learning"", ""en"", ""ta"", ""ar"", ""hi"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""reinforcement-learning""}",1,[],0,[],0
107,Athipan01/GoDathipan,"---
license: mit
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: allennlp
---","{""modelId"": ""Athipan01/GoDathipan"", ""sha"": ""64bec6cdee40a66895a4ed58a9871bffbdcbf907"", ""tags"": [""allennlp"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
108,4TO/MC_Farmer,"---
license: mit
datasets:
- agentlans/common-crawl-sample
language:
- fr
- en
metrics:
- accuracy
- f1
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: transformers
---","{""modelId"": ""4TO/MC_Farmer"", ""sha"": ""8fcb5b0115057af61b006ebeddf3e7b0598e9316"", ""tags"": [""transformers"", ""text-generation"", ""fr"", ""en"", ""dataset:agentlans/common-crawl-sample"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
109,genaitiwari/deepseek,"---
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
- deepseek-ai/Janus-Pro-7B
---","{""modelId"": ""genaitiwari/deepseek"", ""sha"": ""92aed705e198683164d92724578cfe8f93c5c4ed"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
110,InlineHydraulik/Autoencoder,"---
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: summarization
tags:
- not-for-all-audiences
---","{""modelId"": ""InlineHydraulik/Autoencoder"", ""sha"": ""ac23627e46e89fc9a3a423981f5a59708d560e70"", ""tags"": [""not-for-all-audiences"", ""summarization"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""summarization""}",1,[],0,[],0
111,ComputerAi/Bob,"---
license: mit
datasets:
- cognitivecomputations/dolphin-r1
- open-thoughts/OpenThoughts-114k
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3
pipeline_tag: text2text-generation
library_name: bertopic
tags:
- computer-manipulation
---","{""modelId"": ""ComputerAi/Bob"", ""sha"": ""04f897b38a66e043584789a5dae52dcf6820e9e6"", ""tags"": [""bertopic"", ""computer-manipulation"", ""text2text-generation"", ""en"", ""dataset:cognitivecomputations/dolphin-r1"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text2text-generation""}",1,[],0,[],0
112,Northflux3/test,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Northflux3/test"", ""sha"": ""0b2b9d863b7589377a405a2f444b27c610113e86"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
113,idriscanbay/1,"---
license: unknown
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: object-detection
tags:
- art
---","{""modelId"": ""idriscanbay/1"", ""sha"": ""0f836fb74035e121084cd9f53def4149b3e36274"", ""tags"": [""art"", ""object-detection"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""object-detection""}",1,[],0,[],0
114,Sugamk/vai,"---
license: mit
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""Sugamk/vai"", ""sha"": ""ab84d499962a593bc20f9590aa1734872c4cdb5c"", ""tags"": [""question-answering"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
115,Mehrankarajii/Mehran,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
base_model:
- deepseek-ai/DeepSeek-R1
new_version: microsoft/Phi-4-multimodal-instruct
---","{""modelId"": ""Mehrankarajii/Mehran"", ""sha"": ""93e89396230ab87b5e18bfa39ede093024d15202"", ""tags"": [""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
116,Kelinsia/Traininghuggy,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
library_name: ml-agents
---","{""modelId"": ""Kelinsia/Traininghuggy"", ""sha"": ""449ccf35dd218344004ea3524892abdad4c68e15"", ""tags"": [""ml-agents"", ""token-classification"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
117,FEYSALjhn/Lisov,"---
license: apache-2.0
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: hexgrad/Kokoro-82M
---","{""modelId"": ""FEYSALjhn/Lisov"", ""sha"": ""82519ee417863fd1cbe18375a4b30054babff8e0"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
118,persadian/CropSeek-LLM,"---
library_name: transformers
tags:
- crop-optimization
- agriculture
- fine-tuned
- LoRA
datasets:
- DARJYO/sawotiQ29_crop_optimization
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: reinforcement-learning
---
<p align=""center"">
<img width=""30%"" src=""https://raw.githubusercontent.com/arishma108/arishma108/main/assets/DJCO2logo2.png"">
</p >

# Model Card for CropSeek-LLM

<!-- Provide a quick summary of what the model is/does. -->

**CropSeek-LLM** is a fine-tuned language model designed to provide insights and recommendations for crop optimization. It is based on the `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` model and has been fine-tuned using the `DARJYO/sawotiQ29_crop_optimization` dataset. The model is optimized for answering questions related to crop planting, soil conditions, pest control, irrigation, and other agricultural practices.

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

CropSeek-LLM is a fine-tuned version of the `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` model, adapted for crop optimization tasks. It has been trained using **LoRA (Low-Rank Adaptation)** to efficiently fine-tune the base model on a dataset of crop-related questions and answers. The model is designed to assist farmers, agronomists, and researchers in making informed decisions about crop management.

- **Developed by:** persadian, DARJYO
- **Model type:** Causal Language Model (Fine-tuned with LoRA)
- **Language(s) (NLP):** English
- **License:** DARJYO License v1.0
- **Finetuned from model:** `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`
- **Hardware used for training:** Tesla T4 GPU

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

CropSeek-LLM can be used directly to answer questions related to crop optimization, such as:
- Optimal planting seasons for specific crops.
- Ideal soil conditions for crop growth.
- Natural pest control methods.
- Best irrigation practices.
- Crop rotation strategies.

### Downstream Use 

CropSeek-LLM can be integrated into agricultural advisory systems, mobile apps, or chatbots to provide real-time recommendations to farmers and agronomists.

### Out-of-Scope Use

- **Medical Advice:** This model is not designed to provide medical or health-related advice.
- **Financial Decisions:** The model should not be used for financial or investment decisions.
- **Non-Agricultural Use:** The model is specifically fine-tuned for crop optimization and may not perform well in unrelated domains.

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

- **Data Bias:** The model is trained on a dataset focused on specific crops and regions. It may not generalize well to all crops or geographical areas.
- **Limited Scope:** The model is designed for crop optimization and may not provide accurate answers for unrelated topics.
- **Ethical Concerns:** The model should not replace professional advice from agronomists or agricultural experts.

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should:
- Verify the model's recommendations with local agricultural experts.
- Be aware of the model's limitations and use it as a supplementary tool, not a replacement for professional advice.
- Report any biases or inaccuracies to the developers for improvement.

## How to Get Started with the Model

Use the code below to get started with the model.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the fine-tuned model
model = AutoModelForCausalLM.from_pretrained(""persadian/CropSeek-LLM"", device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(""persadian/CropSeek-LLM"")

# Example inference
input_text = ""What is the best planting season for cabbages in South Coast, Durban?""
inputs = tokenizer(input_text, return_tensors=""pt"").to(""cuda"")
outputs = model.generate(**inputs, max_length=512)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```


## Training Details

### Training Data

<!-- This links to DARJYO/sawotiQ29_crop_optimization Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

The model was fine-tuned on a curated dataset of agricultural texts, including:

- Crop descriptions and classifications.
- Plant disease symptoms and treatments.
- Farming techniques and best practices.
- Regional agricultural guidelines.

Specific dataset used: DARYJO/sawotiQ29_crop_optimization

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing 

- The dataset was cleaned and preprocessed to remove irrelevant information and ensure consistency.
- Text data was tokenized using the tokenizer associated with the base model.
- Data augmentation techniques, such as synonym replacement and paraphrasing, were applied to improve generalization.

#### Training Hyperparameters

- **Training regime:** Mixed precision (fp16)
- **Batch size:** 16
- **Learning rate:** 2e-5
- **Epochs:** 3
- **Optimizer:** AdamW
- **Weight decay:** 0.01
- **Warmup steps:** 500
  
#### Speeds, Sizes, Times 

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

- **Training time:** Approximately 10 hours on a T4 GPU.
- **Checkpoint size:** 1.5 GB
- **Throughput:** 120 samples/second

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->
The model was evaluated on a held-out test set of agricultural queries, including crop identification, disease diagnosis, and farming recommendations.

[https://huggingface.co/datasets/DARJYO/sawotiQ29_crop_optimization]


#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->
Evaluation was disaggregated by:
- Crop type (cereals, fruits, vegetables).
- Disease type (fungal, bacterial, viral).
- Geographic region (tropical, temperate).

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

- **Accuracy:** 92% on crop identification tasks.
- **Precision/Recall/F1-score:** Precision: 0.89, Recall: 0.91, F1-score: 0.90
- **Latency:** Average response time of 0.5 seconds on a T4 GPU.

### Results

- The model achieved high accuracy on crop identification and disease diagnosis tasks.
- Performance was slightly lower for region-specific recommendations due to limited training data for certain regions.

#### Summary

CropSeek-LLM performs well on a wide range of agricultural tasks, making it a useful tool for farmers and agricultural professionals. However, performance may vary for rare crops or region-specific practices.

## Model Examination 

<!-- Relevant interpretability work for the model goes here -->

- The model was examined using interpretability tools such as attention visualization and feature importance analysis.

Key findings include:
- The model relies heavily on symptom descriptions for disease diagnosis.
- Crop-specific keywords play a significant role in crop identification tasks.


## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions estimated.

- **Hardware Type:** T4 GPU
- **Hours used:** 10 hours
- **Cloud Provider:** Google Colab
- **Compute Region:** us-central1
- **Carbon Emitted:** Approximately 0.5 kg CO2eq 

## Technical Specifications 

### Model Architecture and Objective

- **Base model architecture:** deepseek-ai/deepseek-R1-14B
- **Objective:** Fine-tuned for text generation and classification tasks in the agricultural domain.


### Compute Infrastructure

#### Hardware

- **Training hardware:** Google Colab with T4 GPU.

#### Software

- **Frameworks:** PyTorch, Hugging Face Transformers.
- **Libraries:** Datasets, Tokenizers, Accelerate.

## Citation 

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**
@misc{cropseek-llm,
  author = {persadian~Darshani Persadh, DARJYO},
  title = {CropSeek-LLM: A Fine-Tuned Language Model for Agricultural Applications},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/persadian/CropSeek-LLM}},
}

**APA:**
persadian. Darshani Persadh (2023). CropSeek-LLM: A Fine-Tuned Language Model for Agricultural Applications. Hugging Face. https://huggingface.co/persadian/CropSeek-LLM


## Glossary 

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

- **Mixed precision:** Training using both 16-bit and 32-bit floating-point numbers to improve efficiency.


## More Information 
For more details, visit the CropSeek-LLM space on Hugging Face.

## Model Card Authors 
- persadian ~Darshani Persah

## Model Card Contact

- info@darjyo.com","{""modelId"": ""persadian/CropSeek-LLM"", ""sha"": ""3976fa4271872331d3ad32562589ae56d0e38540"", ""tags"": [""transformers"", ""safetensors"", ""qwen"", ""text-generation"", ""crop-optimization"", ""agriculture"", ""fine-tuned"", ""LoRA"", ""reinforcement-learning"", ""en"", ""dataset:DARJYO/sawotiQ29_crop_optimization"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 13, ""pipeline_tag"": ""reinforcement-learning""}",1,"['https://huggingface.co/persadian/Croptimize', 'https://huggingface.co/DARJYO/Croptimize']",2,[],0
119,persadian/Croptimize,"---
license: other
license_name: darjyo
license_link: LICENSE
datasets:
- DARJYO/sawotiQ29_crop_optimization
language:
- en
metrics:
- accuracy
base_model:
- persadian/CropSeek-LLM
pipeline_tag: reinforcement-learning
library_name: transformers
tags:
- agriculture
- crop
- optimization
- darjyo
- persadian
---


@misc {
darjyo_2025,
- author       = { {DARJYO} },
- title        = { Croptimize (Revision ebc60f2) },
- year         = 2025,
- url          = { https://huggingface.co/DARJYO/Croptimize },
- doi          = { 10.57967/hf/4736 },
- publisher    = { Hugging Face }
}","{""modelId"": ""persadian/Croptimize"", ""sha"": ""c9678f20055ce93aad0e8af9f6ac771937a5bad4"", ""tags"": [""transformers"", ""agriculture"", ""crop"", ""optimization"", ""darjyo"", ""persadian"", ""reinforcement-learning"", ""en"", ""dataset:DARJYO/sawotiQ29_crop_optimization"", ""base_model:persadian/CropSeek-LLM"", ""base_model:finetune:persadian/CropSeek-LLM"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""reinforcement-learning""}",2,[],0,[],0
120,DARJYO/Croptimize,"---
license: other
license_name: darjyo
license_link: LICENSE
datasets:
- DARJYO/sawotiQ29_crop_optimization
language:
- en
metrics:
- accuracy
base_model:
- persadian/CropSeek-LLM
pipeline_tag: reinforcement-learning
library_name: transformers
tags:
- agriculture
- crop
- optimization
- darjyo
- persadian
---","{""modelId"": ""DARJYO/Croptimize"", ""sha"": ""e281b77c2979a8bb070aa02143a9df1d0ed2c665"", ""tags"": [""transformers"", ""agriculture"", ""crop"", ""optimization"", ""darjyo"", ""persadian"", ""reinforcement-learning"", ""en"", ""dataset:DARJYO/sawotiQ29_crop_optimization"", ""base_model:persadian/CropSeek-LLM"", ""base_model:finetune:persadian/CropSeek-LLM"", ""doi:10.57967/hf/4736"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""reinforcement-learning""}",2,[],0,[],0
121,sezer2737/sorucoz,"---
license: apache-2.0
language:
- tr
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""sezer2737/sorucoz"", ""sha"": ""e5eb9ef254e1f4856f1ce45c681969ff6bd82e04"", ""tags"": [""question-answering"", ""tr"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
122,kghuggingface/kg1repo,"---
license: mit
datasets:
- facebook/natural_reasoning
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""kghuggingface/kg1repo"", ""sha"": ""3be19c9964d9676f981d9f46f272f876685954d8"", ""tags"": [""llama"", ""en"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 3, ""pipeline_tag"": null}",1,[],0,[],0
123,c8tc/nnew_new,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
- HumanLLMs/Human-Like-DPO-Dataset
- Triangle104/HumanLLMs_Human-Like-DPO-Dataset
- gopipasala/fka-awesome-chatgpt-prompts
language:
- ar
- en
metrics:
- bertscore
- accuracy
- bleu
base_model:
- dkp2701/BERT-based-Multiclass-Emotion-Classification
- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
- deepseek-ai/DeepSeek-V3
- deepseek-ai/DeepSeek-R1
pipeline_tag: zero-shot-classification
library_name: transformers
tags:
- instagram
- content-classification
- multilingual
- social-media-analysis
- user-profiling
- text-analysis
---","{""modelId"": ""c8tc/nnew_new"", ""sha"": ""4a7525c1a05e93aba1e7d25643e02848fc694d60"", ""tags"": [""transformers"", ""instagram"", ""content-classification"", ""multilingual"", ""social-media-analysis"", ""user-profiling"", ""text-analysis"", ""zero-shot-classification"", ""ar"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""dataset:Triangle104/HumanLLMs_Human-Like-DPO-Dataset"", ""dataset:gopipasala/fka-awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""zero-shot-classification""}",1,[],0,[],0
124,Joncarel/Vernertranslate,"---
license: unknown
language:
- es
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: translation
tags:
- art
---","{""modelId"": ""Joncarel/Vernertranslate"", ""sha"": ""00f87604b5ac9312fd67022686cb0ef325da78c0"", ""tags"": [""art"", ""translation"", ""es"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
125,AIbyAnmol/publicity,"---
license: mit
datasets:
- open-r1/OpenR1-Math-220k
language:
- ae
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: video-text-to-text
library_name: espnet
tags:
- code
- music
---","{""modelId"": ""AIbyAnmol/publicity"", ""sha"": ""db38f1fe7b5cbba3cc6c4dee1991b90110fbaef3"", ""tags"": [""espnet"", ""code"", ""music"", ""video-text-to-text"", ""ae"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""video-text-to-text""}",1,[],0,[],0
126,deca-ai/2-mini-beta,"---
base_model:
- deepseek-ai/DeepSeek-R1
library_name: transformers
tags:
- reasoning
- R1
- 1M
- fast
- Deca
- Deca-AI
- Deca-2
- Qwen
license: other
---
> [!NOTE]
> # **Deca 2 is now generally availible. We recommend you do not use this model and instead use [`deca-ai/2-mini`](https://huggingface.co/deca-ai/2-mini/) instead.**


![Deca 2 Banner](https://huggingface.co/deca-ai/2-mini-beta/resolve/main/banner.jpg)
The Deca 2 family of models, [no longer in BETA](https://huggingface.co/deca-ai/2-mini/), is built on cutting-edge architectures like DeepSeek R1, and Qwen 2, delivering extraordinary performance. With a focus on insane speed and high efficiency, Deca 2 is revolutionizing text generation and setting new standards in the industry. It also comes with a **1 million** context window.

As more capabilities are added, Deca 2 will evolve into a more powerful, any-to-any model in the future. While it‚Äôs focused on text generation for now, its foundation is designed to scale, bringing even more advanced functionalities to come.

* **2/14 Release:**
* Enhanced Instruction Following","{""modelId"": ""deca-ai/2-mini-beta"", ""sha"": ""ceee623dd39e4ef72614d308a2d7f899148276f9"", ""tags"": [""transformers"", ""safetensors"", ""qwen2"", ""text-generation"", ""reasoning"", ""R1"", ""1M"", ""fast"", ""Deca"", ""Deca-AI"", ""Deca-2"", ""Qwen"", ""conversational"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 19, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
127,gokhandemirau/Elizabet,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- ab
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: translation
library_name: allennlp
tags:
- chemistry
---","{""modelId"": ""gokhandemirau/Elizabet"", ""sha"": ""a831e9c42308346f7983d03248ee8811c36c5c76"", ""tags"": [""allennlp"", ""chemistry"", ""translation"", ""ab"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
128,lekadesire/Football_Predict,"---
license: openrail
datasets:
- fka/awesome-chatgpt-prompts
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: sentence-similarity
---

import gradio as gr
import pandas as pd
import random
from keras.models import load_model
import numpy as np
import requests
from bs4 import BeautifulSoup

data = pd.read_pickle(""merged_all_table.pkl"", compression='bz2')

home_team_id = sorted(data[""home_team_long_name""].unique())
away_team_id = sorted(data[""away_team_long_name""].unique())

nn_model = load_model('models/nn_model.h5')

def fetch_team_data(team_name):
    # Exemple de r√©cup√©ration de donn√©es √† partir d'un site web fictif
    url = f""https://api.football-data.org/v4/matches/teams/{team_name.replace(' ', '-').lower()}""
    response = requests.get(url)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Exemple de r√©cup√©ration de donn√©es sp√©cifiques (√† adapter selon le site)
        overall_score = soup.find('div', class_='overall-score').text
        total_goals = soup.find('div', class_='total-goals').text
        avg_player_rating = soup.find('div', class_='avg-player-rating').text
        
        # Retourner les donn√©es sous forme de dictionnaire
        return {
            'overall_score': float(overall_score),
            'total_goals': int(total_goals),
            'avg_player_rating': float(avg_player_rating)
        }
    else:
        raise gr.Error(f""Failed to fetch data for {team_name} from the website."")

def main_process(model, Home_team, Away_team):
    # R√©cup√©rer les donn√©es du site web pour les √©quipes
    home_data = fetch_team_data(Home_team)
    away_data = fetch_team_data(Away_team)

    # Cr√©er un DataFrame √† partir des donn√©es r√©cup√©r√©es
    home_temp = pd.DataFrame([home_data])
    away_temp = pd.DataFrame([away_data])

    print(""Home Team Data Gathering ‚úÖ"")
    print(""Away Team Data Gathering ‚úÖ"")

    # Concat√©ner les donn√©es
    table = pd.concat([home_temp.mean(), away_temp.mean()], axis=0)
    table = table[[""overall_score"", ""total_goals"", ""avg_player_rating""]]
    print(""Table Concatenation ‚úÖ"")

    X = table.to_frame().T
    pred = model.predict(X)  # R√©cup√©rer les probabilit√©s brutes
    predicted_labels = np.argmax(pred)  # R√©cup√©rer l'√©tiquette pr√©dite
    print(""Data Prediction ‚úÖ"")

    # Retourner les probabilit√©s brutes et l'√©tiquette pr√©dite
    return pred[0], predicted_labels

def predict(Home_team, Away_team, Model_name):
    if Home_team == """":
        raise gr.Error(""Home Team is required, Please Select The Home Team!"")
    
    if Away_team  == """":
        raise gr.Error(""Away Team is required, Please Select The Away Team!"")
    
    if Model_name  == """":
        raise gr.Error(""Model is required, Please Select The Model!"")
    
    if Model_name == ""Simple Nueral Network Model"":
        model = nn_model

    # R√©cup√©rer les probabilit√©s brutes et l'√©tiquette pr√©dite
    probabilities, prediction = main_process(model, Home_team, Away_team)

    # Formater les probabilit√©s pour l'affichage
    home_win_prob = round(probabilities[0] * 100, 2)
    away_win_prob = round(probabilities[1] * 100, 2)
    draw_prob = round(probabilities[2] * 100, 2)

    # Afficher les probabilit√©s
    result_message = (
        f""üè† **{Home_team} Victory Probability:** {home_win_prob}%\n""
        f""‚úàÔ∏è **{Away_team} Victory Probability:** {away_win_prob}%\n""
        f""ü§ù **Draw Probability:** {draw_prob}%\n\n""
    )

    # Ajouter la pr√©diction finale
    if prediction == 0:
        result_message += ""ü•≥ **Prediction:** Home Team Win üéâ""
    elif prediction == 1:
        result_message += ""ü•≥ **Prediction:** Away Team Win üéâ""
    else:
        result_message += ""üòë **Prediction:** Match Draw üòë""

    return result_message

with gr.Blocks() as demo:
    gr.Markdown(""""""
    [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ravi7522/Football-Prediction)
    """""")
    with gr.Row():
        gr.Label(""‚öΩÔ∏è Football Prediction ‚öΩÔ∏è"", container=False)

    with gr.Row():
        with gr.Column():
            dd_home_team = gr.Dropdown(
                label=""Home Team"",
                choices=home_team_id,
                info=""Select Your Home Team:"",
                multiselect=False,
            )

        with gr.Column(): 
            dd_away_team = gr.Dropdown(
                label=""Away Team"",
                choices=away_team_id,
                info=""Select Your Away Team:"",
                multiselect=False,
            )

    with gr.Row():
        with gr.Column(): 
            dd_model = gr.Dropdown(
                label=""Model ( Feature Under Construction üöß )"",  choices=[""Simple Nueral Network Model""],
                info=""Select Your Model:"",
                multiselect=False,
            )

    with gr.Row():
        predict_btn = gr.Button(value=""Predict"")
            
    with gr.Row():
        Answer = gr.Label(""üëã Hello, Let us predict the Football Match üíÅ‚Äç‚ôÇÔ∏è"", container=False)

    predict_btn.click(
        predict,
        inputs=[
            dd_home_team,
            dd_away_team,
            dd_model,
        ],
        outputs=[Answer],
    )

demo.launch()","{""modelId"": ""lekadesire/Football_Predict"", ""sha"": ""4a4d92fd49c3e47701cfb5a52d39a0404bc1dea5"", ""tags"": [""sentence-similarity"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""sentence-similarity""}",1,[],0,[],0
129,emirke159753159753/abii,"---
datasets:
- facebook/natural_reasoning
language:
- av
base_model:
- deepseek-ai/DeepSeek-R1
library_name: fasttext
---","{""modelId"": ""emirke159753159753/abii"", ""sha"": ""17c6c6330e26e09bb68286057df2483c376bd0c3"", ""tags"": [""fasttext"", ""av"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
130,Harshitv/test,"---
datasets:
- fka/awesome-chatgpt-prompts
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/Janus-Pro-7B
---","{""modelId"": ""Harshitv/test"", ""sha"": ""91052cf29f66011a281e284d37454e5de7af6ea0"", ""tags"": [""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
131,1986random/l,"---
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: image-to-image
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""1986random/l"", ""sha"": ""c47324f32e199a03167c05033c9d4867a1e2b67f"", ""tags"": [""image-to-image"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""image-to-image""}",1,[],0,[],0
132,Reda2566/Reda_68,"---
license: openrail
language:
- ar
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fasttext
tags:
- legal
---","{""modelId"": ""Reda2566/Reda_68"", ""sha"": ""14a90ab6066b3a138880afd0cf1ffd52bdc8f03a"", ""tags"": [""fasttext"", ""legal"", ""ar"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
133,soupbutt/writefanfic,"---
license: other
license_name: idksmuttyigbro
license_link: LICENSE
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- not-for-all-audiences
---","{""modelId"": ""soupbutt/writefanfic"", ""sha"": ""ffc6f3336f18fc1b12512b58109f60242f719b2c"", ""tags"": [""not-for-all-audiences"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:other"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
134,VybezR/Helop,"---
license: afl-3.0
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
metrics:
- bleu
- character
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""VybezR/Helop"", ""sha"": ""f91aa00832d4ccb562f32674fcdb50ef711388e6"", ""tags"": [""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
135,andr1sv/hpp,"---
license: apache-2.0
datasets:
- open-r1/OpenR1-Math-220k
language:
- ru
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: diffusers
tags:
- finance
---","{""modelId"": ""andr1sv/hpp"", ""sha"": ""dd380ffe8ebbd35d938bf068b3c1ff0e415b0e05"", ""tags"": [""diffusers"", ""finance"", ""ru"", ""en"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
136,Dashutosh884/Hugging_Face,"---
license: apache-2.0
language:
- en
- sw
- hi
- gu
- mr
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
tags:
- chemistry
- biology
- physics
---","{""modelId"": ""Dashutosh884/Hugging_Face"", ""sha"": ""9ed51d7a794882c8513c1fc3fc82c49e36194dce"", ""tags"": [""chemistry"", ""biology"", ""physics"", ""text-classification"", ""en"", ""sw"", ""hi"", ""gu"", ""mr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
137,bkaplan/MRL2,"---
license: mit
tags:
- unsloth
- trl
- sft
language:
- tr
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---

make python chat","{""modelId"": ""bkaplan/MRL2"", ""sha"": ""f9011840f25e33c7d078237efa31e31c3313acda"", ""tags"": [""pytorch"", ""llama"", ""unsloth"", ""trl"", ""sft"", ""text-generation"", ""conversational"", ""tr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 4, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
138,Pweenut/QazNLTK_Model,"---
license: mit
datasets:
- issai/KazNERD
language:
- kk
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fasttext
tags:
- code
---","{""modelId"": ""Pweenut/QazNLTK_Model"", ""sha"": ""94493afc88fcecf43f48cd3fb2c915f5e120ad75"", ""tags"": [""fasttext"", ""code"", ""kk"", ""en"", ""dataset:issai/KazNERD"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
139,yangyu1111/2,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- aa
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: flair
tags:
- finance
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""yangyu1111/2"", ""sha"": ""09e0cba8b493bbb2ec452cec6ab3ccebaa799f9e"", ""tags"": [""flair"", ""finance"", ""aa"", ""dataset:open-thoughts/OpenThoughts-114k"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
140,Yaroslavgtytry/gngn,"---
license: mit
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Yaroslavgtytry/gngn"", ""sha"": ""75e2b575c77e4ecec500bd5d33e9051437ee261b"", ""tags"": [""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
141,boilerbambam/NEW_APP,"---
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""boilerbambam/NEW_APP"", ""sha"": ""197a62ae196c8248f61118a4cc250f511e9d0fc2"", ""tags"": [""text-classification"", ""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
142,lukeshaye/testelukeshaye,"---
language:
- pt
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""lukeshaye/testelukeshaye"", ""sha"": ""108bdaccd628847e416cff6b843cfc9093b6f330"", ""tags"": [""pt"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
143,chunien/gp44785,"---
license: c-uda
datasets:
- open-thoughts/OpenThoughts-114k
- PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT
language:
- aa
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: asteroid
---","{""modelId"": ""chunien/gp44785"", ""sha"": ""955fac18d0078a0ed572c526e088f87efc718d28"", ""tags"": [""asteroid"", ""aa"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:c-uda"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
144,TrevSh/Demo_Edu_Model,"---
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""TrevSh/Demo_Edu_Model"", ""sha"": ""3c10a2b9f12426e727bbbed8d7806763fb92e223"", ""tags"": [""text-generation"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
145,PrakashCider/Your-Solmate,"---
license: gfdl
datasets:
- NovaSky-AI/Sky-T1_data_17k
language:
- en
- hi
- ta
- te
- mr
metrics:
- accuracy
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text2text-generation
library_name: fastai
tags:
- code
---","{""modelId"": ""PrakashCider/Your-Solmate"", ""sha"": ""bde16539f84a3ede0e1757d664b65e4279983057"", ""tags"": [""fastai"", ""code"", ""text2text-generation"", ""en"", ""hi"", ""ta"", ""te"", ""mr"", ""dataset:NovaSky-AI/Sky-T1_data_17k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:gfdl"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text2text-generation""}",1,[],0,[],0
146,BadiciCyra/rag,"---
datasets:
- saiyan-world/Goku-MovieGenBench
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: fastai
---","{""modelId"": ""BadiciCyra/rag"", ""sha"": ""b3e3136aedf4ced1df810a5478e776dc8878a972"", ""tags"": [""fastai"", ""text-generation"", ""en"", ""dataset:saiyan-world/Goku-MovieGenBench"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
147,VANNVISAL/LLM_Model,"---
license: llama2
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
language:
- km
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1-Zero
pipeline_tag: text2text-generation
library_name: fastai
tags:
- art
---","{""modelId"": ""VANNVISAL/LLM_Model"", ""sha"": ""68ac50f1abf9b834a91048102becf0c3cb680c34"", ""tags"": [""fastai"", ""art"", ""text2text-generation"", ""km"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama2"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text2text-generation""}",1,[],0,[],0
148,yt-X/deepseek-r1-dpo,"---
license: mit
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- unsloth
language:
- en
library_name: transformers
pipeline_tag: text-generation
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->
Better tuned deepseek-r1 model using dpo and specific customer service dataset



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]



# How to use this model
first we need unsloth

### Normally using pip install unsloth is enough

### Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch
### Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:
%%capture
!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton
!pip install --no-deps cut_cross_entropy unsloth_zoo
!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
!pip install --no-deps unsloth

from unsloth import FastLanguageModel
from transformers import AutoTokenizer

### Path to your fine-tuned model
model_path = ""drive/MyDrive/deepseek-r1-reasoning-dpo""  # Replace

### Load the base model optimized with Unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path,
    max_seq_length=4096,  # Adjust based on model capability
    dtype=torch.float16,
    load_in_4bit=True,  # Enable quantization for efficiency
)

### Optimize LoRA model for inference (2x faster with Unsloth)
FastLanguageModel.for_inference(model)

### Move model to GPU if available
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
model.to(device)

print(""Model loaded successfully!"")

## ---------------------------------------------","{""modelId"": ""yt-X/deepseek-r1-dpo"", ""sha"": ""41940b0132847915793330616a332ff84e08b036"", ""tags"": [""transformers"", ""safetensors"", ""unsloth"", ""text-generation"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
149,aodev/EmBotV2,"---
license: mpl-2.0
datasets:
- fka/awesome-chatgpt-prompts
- open-thoughts/OpenThoughts-114k
language:
- hu
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
- microsoft/OmniParser-v2.0
new_version: deepseek-ai/DeepSeek-R1
library_name: fastai
---","{""modelId"": ""aodev/EmBotV2"", ""sha"": ""28cbc445b24e1af4c8871aca95882dd82fc39d85"", ""tags"": [""fastai"", ""hu"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mpl-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
150,zonnell/discord_bot,"---
language:
- en
- ru
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""zonnell/discord_bot"", ""sha"": ""5c356be0df761c33c6966b0e7e18e6b00a348615"", ""tags"": [""text-classification"", ""en"", ""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
151,SAMdahal/aiitenarary,"---
license: mit
datasets:
- deepseek-ai/DeepSeek-Prover-V1
- O1-OPEN/OpenO1-SFT-Ultra
- Gryphe/Sonnet3.5-Charcard-Roleplay
language:
- en
- ja
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""SAMdahal/aiitenarary"", ""sha"": ""8b7a57423c4fc70a9b58e025b064fb6b34b007e2"", ""tags"": [""en"", ""ja"", ""dataset:deepseek-ai/DeepSeek-Prover-V1"", ""dataset:O1-OPEN/OpenO1-SFT-Ultra"", ""dataset:Gryphe/Sonnet3.5-Charcard-Roleplay"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
152,visnu90/pycooking,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""visnu90/pycooking"", ""sha"": ""9730c82532b77d52a5a60853aae263cc1b5d2e26"", ""tags"": [""text-classification"", ""en"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
153,Hqrunkeke/Deepseekk,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
- fka/awesome-chatgpt-prompts
- MohamedRashad/ChatGPT-prompts
language:
- tr
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Hqrunkeke/Deepseekk"", ""sha"": ""e5f8c2013c823ffcf288e28c56f5e5bdc6df6dea"", ""tags"": [""tr"", ""en"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:MohamedRashad/ChatGPT-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
154,Jiajiawei/mySelfTalk,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Jiajiawei/mySelfTalk"", ""sha"": ""bff2f08a8653d0a784da85ef5f5259d4c4ec99b1"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
155,Dombrenk30/0xDom,"---
license: apache-2.0
datasets:
- nvidia/Llama-Nemotron-Post-Training-Dataset-v1
language:
- id
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: translation
library_name: asteroid
tags:
- finance
---","{""modelId"": ""Dombrenk30/0xDom"", ""sha"": ""4456ce0fa51a2365b48cdb02d2e7f36ee014fee6"", ""tags"": [""asteroid"", ""finance"", ""translation"", ""id"", ""dataset:nvidia/Llama-Nemotron-Post-Training-Dataset-v1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
156,ibtp1256/tpmodel,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""ibtp1256/tpmodel"", ""sha"": ""5a95e9baf681d53eedbfd037015ea67becc5c954"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
157,bokomoko/boletoreader,"---
license: mit
language:
- pt
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: image-to-text
---","{""modelId"": ""bokomoko/boletoreader"", ""sha"": ""538a1be74e85f56bb328e021d92b34f23b502c9c"", ""tags"": [""image-to-text"", ""pt"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""image-to-text""}",1,[],0,[],0
158,RecurvAI/Recurv-Clinical-Deepseek-R1,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
datasets:
- RecurvAI/Recurv-Clinical-Dataset
language:
- en
pipeline_tag: text-generation
tags:
- clinical
- anamnesis
---
# üß† Recurv-Clinical-Deepseek-R1 Model

[![License](https://img.shields.io/badge/license-MIT-blue?style=flat-square)](https://opensource.org/license/MIT)
[![HF](https://img.shields.io/badge/HuggingFace-Recurv--Clinical--Deepseek--R1-yellow?style=flat-square&logo=huggingface)](https://huggingface.co/RecurvAI/Recurv-Clinical-Deepseek-R1)

## **Overview**

The **Recurv-Clinical-Deepseek-R1** model is an upgraded version of Deepseek‚Äôs R1, specifically designed to provide accurate and contextually relevant support for healthcare professionals and researchers. This model excels at answering clinical questions, assisting in gathering patient histories, and generating detailed explanations tailored to various clinical situations through advanced instruction tuning techniques.

**(Knowledge cut-off date: 22th January, 2025)**

### üéØ **Key Features**
- Optimized for clinical-specific queries across various specialties.
- Fine-tuned for clinical and research-oriented workflows.
- Lightweight parameter-efficient fine-tuning with safetensors format.
- Multi-turn conversation support for context-rich interactions.
- Generates comprehensive answers and evidence-based suggestions.

---

## üöÄ **Model Card**

| **Parameter**              | **Details**                                                                                  |
|----------------------------|----------------------------------------------------------------------------------------------|
| **Base Model**             | DeepSeek R1 Distill Llama 8B                                                                 |
| **Fine-Tuning Framework**  | safetensors                                                                                  |
| **Dataset Size**           | 12,632 high-quality Q&A pairs                                                                |
| **Context Length**         | 4,096 tokens                                                                                 |
| **Training Steps**         | 100,000                                                                                      |
| **Model Size**             | 8 billion parameters                                                                         |

---

## üìä **Model Architecture**

### **Dataset Sources**
The dataset comprises high-quality Q&A pairs curated from clinical textbooks, research papers, and clinical guidelines.

| Source                    | Description                                                                          |
|---------------------------|--------------------------------------------------------------------------------------|
| **PubMed**                | Extracted insights from open-access clinical research.                                |
| **Clinical Guidelines**   | Data sourced from WHO, CDC, and specialty-specific guidelines.                       |
| **EHR-Simulated Data**    | Synthetic datasets modeled on real-world patient records for anamnesis workflows.    |

---

## üåü **Try The Model**
üöÄ [Recurv-Clinical-Deepseek-R1](https://recurvai.org) on Our Website


## üôå **Contributing**

We welcome contributions to enhance Recurv-Clinical-Deepseek-R1. You can:
- Share feedback or suggestions on the Hugging Face Model Hub
- Submit pull requests or issues for model improvement.

---

## üìú **License**

This model is licensed under the **MIT License**.

---

## üìû **Community**

For questions or support, connect with us via:
- **Twitter**: [RecurvAI](https://x.com/recurvai)
- **Email**: [support@recurvai.com](mailto:support@recurvai.com)

---

## ü§ù **Acknowledgments**

Special thanks to the clinical community and researchers for their valuable insights and support in building this model. Together, we‚Äôre advancing AI in healthcare.","{""modelId"": ""RecurvAI/Recurv-Clinical-Deepseek-R1"", ""sha"": ""6961c9dd299fc5e699d3c623c5fa8f6a1a69ec45"", ""tags"": [""safetensors"", ""clinical"", ""anamnesis"", ""text-generation"", ""conversational"", ""en"", ""dataset:RecurvAI/Recurv-Clinical-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
159,Yeeheng/repo,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Yeeheng/repo"", ""sha"": ""7460871639d5ddb7fdb0ded53a38ddc6a04954de"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
160,Leto-cmd/Oddessey,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
- open-r1/OpenR1-Math-220k
language:
- en
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
tags:
- roleplay
- adventure
---","{""modelId"": ""Leto-cmd/Oddessey"", ""sha"": ""0e0dc99710fa76bd8a5f99c5a562ed72ea0b895d"", ""tags"": [""roleplay"", ""adventure"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
161,mattivityroom/huggingface_nlp,"---
license: apache-2.0
datasets:
- NebulaByte/E-Commerce_Customer_Support_Conversations
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""mattivityroom/huggingface_nlp"", ""sha"": ""404d74d369fbc285ac0d2083b46453b5427a7280"", ""tags"": [""text-classification"", ""en"", ""dataset:NebulaByte/E-Commerce_Customer_Support_Conversations"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
162,myself-model/11,"---
base_model:
- deepseek-ai/DeepSeek-R1
license: mit
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
language:
- zh
metrics:
- bleu
- rouge
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""myself-model/11"", ""sha"": ""3f68008dcd0def7d7bfbe4f4fbdfd273806527c1"", ""tags"": [""safetensors"", ""zh"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
163,pretonetworking/Roteirobom,"---
license: openrail
language:
- pt
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
tags:
- roteiro
---","{""modelId"": ""pretonetworking/Roteirobom"", ""sha"": ""5d556dde9f1630e6039f5069a875dd1475e66f85"", ""tags"": [""roteiro"", ""text-generation"", ""pt"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
164,Bilkees/Ikhlaq,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- en
metrics:
- brier_score
base_model:
- deepseek-ai/DeepSeek-R1
new_version: Qwen/QwQ-32B
pipeline_tag: text-generation
library_name: allennlp
tags:
- finance
- legal
---","{""modelId"": ""Bilkees/Ikhlaq"", ""sha"": ""ff2f090839abadaea5247979670d2a936d982b1e"", ""tags"": [""allennlp"", ""finance"", ""legal"", ""text-generation"", ""en"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
165,AbdullahAli06/abdullahali_ai,"---
license: bigscience-bloom-rail-1.0
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
- umarigan/deepseek-r1-reasoning-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
library_name: flair
---","{""modelId"": ""AbdullahAli06/abdullahali_ai"", ""sha"": ""57f10ac9c763d12d8a707ca734cad799e8fa458f"", ""tags"": [""flair"", ""token-classification"", ""en"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""dataset:umarigan/deepseek-r1-reasoning-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigscience-bloom-rail-1.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
166,exco369/infinity,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- en
- es
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
tags:
- art
---","{""modelId"": ""exco369/infinity"", ""sha"": ""e88562c269c7f890bc268de16e555005600789e8"", ""tags"": [""art"", ""text-classification"", ""en"", ""es"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
167,Alejandro1266/Studying,"---
license: mit
language:
- en
- ru
- es
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: audio-classification
tags:
- code
- music
---","{""modelId"": ""Alejandro1266/Studying"", ""sha"": ""92c2226287b538b6af2a49d4ca800d8bfb94c43b"", ""tags"": [""code"", ""music"", ""audio-classification"", ""en"", ""ru"", ""es"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""audio-classification""}",1,[],0,[],0
168,Mexa57/Vi,"---
license: openrail
datasets:
- fka/awesome-chatgpt-prompts
language:
- aa
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: asteroid
tags:
- art
---","{""modelId"": ""Mexa57/Vi"", ""sha"": ""56df946ab3151dbc984a477d4940d0cf1ee88c8e"", ""tags"": [""asteroid"", ""art"", ""text-classification"", ""aa"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
169,nishantmourya/bio,"---
license: mit
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""nishantmourya/bio"", ""sha"": ""69d2ce6728957f351df740837d2fffa5ad962da4"", ""tags"": [""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
170,kuazi/deepseek-r1-medical-test,"---
library_name: transformers
base_model: 
- deepseek-ai/DeepSeek-R1
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
license: apache-2.0
language:
- en
metrics:
- accuracy
datasets:
- shibing624/medical
---

# Uploaded  model

- **Developed by:** kuazi
- **License:** apache-2.0
- **Finetuned from model :** unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)","{""modelId"": ""kuazi/deepseek-r1-medical-test"", ""sha"": ""8042f9fd9e2467f13ec31aff07bfee77cc3e8e7c"", ""tags"": [""transformers"", ""pytorch"", ""safetensors"", ""llama"", ""text-generation"", ""text-generation-inference"", ""unsloth"", ""trl"", ""sft"", ""conversational"", ""en"", ""dataset:shibing624/medical"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 3, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
171,Efeeg/beyza,"---
license: mit
language:
- tr
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- art
---","{""modelId"": ""Efeeg/beyza"", ""sha"": ""ee351c2ed1a7ce045ed5dfb33ae51d452bf76f24"", ""tags"": [""art"", ""tr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
172,rkeval/LearnAI,"---
license: llama3.3
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""rkeval/LearnAI"", ""sha"": ""e8bd14682f918685546609043af2f130f3768791"", ""tags"": [""question-answering"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama3.3"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
173,YTPG524/The_Fight_for_Top,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""YTPG524/The_Fight_for_Top"", ""sha"": ""78d88642325b88f1e0bcf9f3286714fe837f44fa"", ""tags"": [""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
174,sprunkiphase3/unblocked,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- ab
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: perplexity-ai/r1-1776
pipeline_tag: text-to-audio
library_name: allennlp
tags:
- code
- biology
- music
---","{""modelId"": ""sprunkiphase3/unblocked"", ""sha"": ""fddecd56017d0b3e2a909bd7df5e8ee165ccffb7"", ""tags"": [""allennlp"", ""code"", ""biology"", ""music"", ""text-to-audio"", ""ab"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-audio""}",1,[],0,[],0
175,xugui/test,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""xugui/test"", ""sha"": ""1ea5594f2bcd796f862e31bb82d945055808d524"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
176,William-zhao/KuCozy,"---
license: apache-2.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- zh
- en
- es
- de
- ja
metrics:
- accuracy
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
tags:
- finance
- music
---","{""modelId"": ""William-zhao/KuCozy"", ""sha"": ""140dab263805dc061845bb76fdc8cdaf1471affa"", ""tags"": [""finance"", ""music"", ""zh"", ""en"", ""es"", ""de"", ""ja"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
177,antondanilevskiy/GTCauto,"---
license: apache-2.0
language:
- ru
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""antondanilevskiy/GTCauto"", ""sha"": ""b3d25680f500bf329e93a01e1a14746939984742"", ""tags"": [""ru"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
178,cr6276/mymodel,"---
language:
- ru
- en
base_model:
- deepseek-ai/DeepSeek-R1
- perplexity-ai/r1-1776
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""cr6276/mymodel"", ""sha"": ""b4a2b22747b667c671d0cc4ee74a9117efb86c5e"", ""tags"": [""ru"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
179,shubhamnagane/news,"---
license: mit
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: summarization
---","{""modelId"": ""shubhamnagane/news"", ""sha"": ""b9d2fe22f9582797afdf31a272d19e50814a1b2a"", ""tags"": [""summarization"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""summarization""}",1,[],0,[],0
180,Warnsey/Teaching_Model,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""Warnsey/Teaching_Model"", ""sha"": ""99d7bdb3cb729c535e1e8dcdf0b3e1c9398dd57c"", ""tags"": [""text-classification"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
181,YaserSabriFMD/Jj,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""YaserSabriFMD/Jj"", ""sha"": ""32d84b2cfe7f26186e8d19c7a1e33272027b64b8"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
182,perplexity-ai/r1-1776,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
library_name: transformers
---

# R1 1776

Blog link: [https://perplexity.ai/hub/blog/open-sourcing-r1-1776](https://perplexity.ai/hub/blog/open-sourcing-r1-1776 ) 

R1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship. 
The model provides unbiased, accurate, and factual information while maintaining high reasoning capabilities.

## Evals

To ensure our model remains fully ‚Äúuncensored‚Äù and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.
![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/GiN2VqC5hawUgAGJ6oHla.png)

We also ensured that the model‚Äôs math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.
![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/n4Z9Byqp2S7sKUvCvI40R.png)","{""modelId"": ""perplexity-ai/r1-1776"", ""sha"": ""c12656f83748b6f71b41136a007ca3065a387a2f"", ""tags"": [""transformers"", ""safetensors"", ""deepseek_v3"", ""text-generation"", ""conversational"", ""custom_code"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 12617, ""pipeline_tag"": ""text-generation""}",1,"['https://huggingface.co/Khewa153/GleemanAI', 'https://huggingface.co/rash1dovt/tyncha_ai', 'https://huggingface.co/Hxh0211/11111', 'https://huggingface.co/malypali18/WebWealthWizards', 'https://huggingface.co/Suziwan/Model1', 'https://huggingface.co/dahiya11/Ai-Assistant', 'https://huggingface.co/unsloth/r1-1776', 'https://huggingface.co/mlx-community/perplexity-ai-r1-1776-bf16', 'https://huggingface.co/Delfileking/Histoirde2005', 'https://huggingface.co/Renato186/ren', 'https://huggingface.co/ALESSIO66/Law_CCII_IT_ProceduresCloud']",11,['https://huggingface.co/copywr1ter/copytest'],1
183,Khewa153/GleemanAI,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
metrics:
- accuracy
base_model:
- perplexity-ai/r1-1776
new_version: perplexity-ai/r1-1776
pipeline_tag: translation
library_name: fasttext
tags:
- Literatue
- english
---","{""modelId"": ""Khewa153/GleemanAI"", ""sha"": ""e1f636c2feb65012fd891b4bb63d3a09ffe069a0"", ""tags"": [""fasttext"", ""safetensors"", ""t5"", ""Literatue"", ""english"", ""translation"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",2,[],0,[],0
184,rash1dovt/tyncha_ai,"---
license: apache-2.0
datasets:
- nvidia/Llama-Nemotron-Post-Training-Dataset-v1
base_model:
- perplexity-ai/r1-1776
new_version: perplexity-ai/r1-1776
tags:
- chemistry
---","{""modelId"": ""rash1dovt/tyncha_ai"", ""sha"": ""800d74a57a07afbcfe8adf95c35649202e63b28c"", ""tags"": [""chemistry"", ""dataset:nvidia/Llama-Nemotron-Post-Training-Dataset-v1"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
185,Hxh0211/11111,"---
license: mit
datasets:
- open-thoughts/OpenThoughts-114k
language:
- aa
- ab
- af
metrics:
- bleu
base_model:
- perplexity-ai/r1-1776
new_version: perplexity-ai/r1-1776
pipeline_tag: text-classification
library_name: bertopic
tags:
- biology
- chemistry
---
# GRGcloud Dashboard

[![Build Status](https://www.travis-ci.org/yunionio/dashboard.svg?branch=master)](https://www.travis-ci.org/yunionio/dashboard)

[English](./README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](./README-CN.md)

GRGcloud Dashboard is the web-based UI for [GRGcloud](https://github.com/yunionio/cloudpods).

## Developer Guide

### Preparation

Make sure the following software is installed and added to the $PATH variable:

- Node.js 10.16+ ([installation with nvm](https://github.com/creationix/nvm#usage))
- Yarn 1.19.1+ ([documentation](https://classic.yarnpkg.com/en/docs/install))

or




Install yarn with npm:

```sh
npm install -g yarn
```

Fork the following repository, then clone dashboard main repository and install dependencies

- [dashboard](https://github.com/yunionio/dashboard)

```sh
$ git clone https://github.com/<owner>/dashboard.git
$ cd dashboard
# Here, depending on your environment, checkout corresponding branch, otherwise you might have incompatibilities
$ git checkout release/3.8
$ yarn
```

Note: If you are in Mainland China, execute the following command before running the command above for faster installation.

```sh
yarn config set registry https://registry.npm.taobao.org
```

### Start Dashboard for development

If you want to configure the proxy, please create dev.server.config.js in the project root directory and export configuration

Please change the configuration according to your needs, the following is just an example

```javascript
// dev.server.config.js
module.exports = {
  open: process.platform === 'darwin',
  port: 8080,
  proxy: {
    '/api': {
      // Be sure to set it to the address of the environment, which is HTTPS
      target: 'https://192.168.1.10',
      ws: true,
      changeOrigin: true,
      secure: false,
    },
  },
}
```

[More configuration](https://webpack.js.org/configuration/dev-server/)

```sh
yarn serve
```

Now, you can open http://localhost:8080 to view()

### Build Dashboard for production

```sh
yarn build
```

### Make docker image

```bash
REGISTRY=registry.cn-beijing.aliyuncs.com/yunionio TAG=your-tag ./scripts/docker-push.sh
```","{""modelId"": ""Hxh0211/11111"", ""sha"": ""d2fd6946df67e7654e1840cb2d8fd61cfad040ea"", ""tags"": [""bertopic"", ""biology"", ""chemistry"", ""text-classification"", ""aa"", ""ab"", ""af"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",2,[],0,[],0
186,malypali18/WebWealthWizards,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
metrics:
- character
- accuracy
base_model:
- perplexity-ai/r1-1776
new_version: deepseek-ai/DeepSeek-R1
library_name: asteroid
---","{""modelId"": ""malypali18/WebWealthWizards"", ""sha"": ""9ef51a51cdd03e60bf0dce11a3f643860b38fafc"", ""tags"": [""asteroid"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
187,Suziwan/Model1,"---
base_model:
- perplexity-ai/r1-1776
---","{""modelId"": ""Suziwan/Model1"", ""sha"": ""cd90c04856e23e72288280d7c6f4c1f018c731c0"", ""tags"": [""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
188,dahiya11/Ai-Assistant,"---
language:
- en
- hi
base_model:
- perplexity-ai/r1-1776
tags:
- Agent
---
# Desktop-Assistant-using-Python

## How to run:

1. Create a new virtual environment

```bash
conda create -n assistant python=3.10

```

2. Checkout the created virtual environment

```bash
conda env list

```

3. Activate the virtual environment

```bash
conda activate assistant 

```

4. Install all the packages present in the requirements file


```bash
pip install -r requirements.txt

```

```bash
streamlit run app.py

```



## Required Github Commands

```bash
git add .

git commit -m ""message""

git push origin main
```","{""modelId"": ""dahiya11/Ai-Assistant"", ""sha"": ""c6ae47f6dbe3306de43091fc442677c2d10fce19"", ""tags"": [""Agent"", ""en"", ""hi"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
189,unsloth/r1-1776,"---
base_model: perplexity-ai/r1-1776
language:
- en
library_name: transformers
license: mit
tags:
- deepseek
- deepseek_v3
- unsloth
- transformers
---

# R1 1776

Blog link: [https://perplexity.ai/hub/blog/open-sourcing-r1-1776](https://perplexity.ai/hub/blog/open-sourcing-r1-1776 ) 

R1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship. 
The model provides unbiased, accurate, and factual information while maintaining high reasoning capabilities.

## Evals

To ensure our model remains fully ‚Äúuncensored‚Äù and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.
![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/GiN2VqC5hawUgAGJ6oHla.png)

We also ensured that the model‚Äôs math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the decensoring had no impact on its core reasoning capabilities.
![image/png](https://cdn-uploads.huggingface.co/production/uploads/675c8332d01f593dc90817f5/n4Z9Byqp2S7sKUvCvI40R.png)

","{""modelId"": ""unsloth/r1-1776"", ""sha"": ""ec87419327b1992adb8828c0508e7dd3c9da0abb"", ""tags"": [""transformers"", ""safetensors"", ""deepseek_v3"", ""text-generation"", ""deepseek"", ""unsloth"", ""conversational"", ""custom_code"", ""en"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:mit"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 21, ""pipeline_tag"": ""text-generation""}",2,[],0,[],0
190,mlx-community/perplexity-ai-r1-1776-bf16,"---
license: mit
base_model: perplexity-ai/r1-1776
tags:
- mlx
---

# mlx-community/perplexity-ai-r1-1776-bf16

The Model [mlx-community/perplexity-ai-r1-1776-bf16](https://huggingface.co/mlx-community/perplexity-ai-r1-1776-bf16) was
converted to MLX format from [perplexity-ai/r1-1776](https://huggingface.co/perplexity-ai/r1-1776)
using mlx-lm version **0.21.4**.

## Use with mlx

```bash
pip install mlx-lm
```

```python
from mlx_lm import load, generate

model, tokenizer = load(""mlx-community/perplexity-ai-r1-1776-bf16"")

prompt = ""hello""

if tokenizer.chat_template is not None:
    messages = [{""role"": ""user"", ""content"": prompt}]
    prompt = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True
    )

response = generate(model, tokenizer, prompt=prompt, verbose=True)
```
","{""modelId"": ""mlx-community/perplexity-ai-r1-1776-bf16"", ""sha"": ""94898466486658b39717830002d13d54ab5d33d8"", ""tags"": [""mlx"", ""safetensors"", ""deepseek_v3"", ""custom_code"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:mit"", ""region:us""], ""downloads"": 16, ""pipeline_tag"": null}",2,[],0,[],0
191,Delfileking/Histoirde2005,"---
license: apache-2.0
language:
- fr
metrics:
- bleu
base_model:
- perplexity-ai/r1-1776
pipeline_tag: translation
---","{""modelId"": ""Delfileking/Histoirde2005"", ""sha"": ""2dd7932240b2c3e9231f856f484296859deef576"", ""tags"": [""translation"", ""fr"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",2,[],0,[],0
192,Renato186/ren,"---
language:
- pt
base_model:
- perplexity-ai/r1-1776
new_version: perplexity-ai/r1-1776
pipeline_tag: text-generation
---","{""modelId"": ""Renato186/ren"", ""sha"": ""39684314cabe8536a59c17343f2a3fdb426a7a09"", ""tags"": [""text-generation"", ""pt"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",2,[],0,[],0
193,ALESSIO66/Law_CCII_IT_ProceduresCloud,"---
license: apache-2.0
datasets:
- HuggingFaceFW/fineweb
language:
- it
- en
metrics:
- accuracy
base_model:
- perplexity-ai/r1-1776
library_name: flair
tags:
- legal
---","{""modelId"": ""ALESSIO66/Law_CCII_IT_ProceduresCloud"", ""sha"": ""7c9732da59099b69aeeb41777b53b74ea46e6cd7"", ""tags"": [""flair"", ""legal"", ""it"", ""en"", ""dataset:HuggingFaceFW/fineweb"", ""base_model:perplexity-ai/r1-1776"", ""base_model:finetune:perplexity-ai/r1-1776"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",2,[],0,[],0
194,deevnnv/nomadchroniclesapi,"---
license: apache-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""deevnnv/nomadchroniclesapi"", ""sha"": ""f3f02d3cc2b00e618e9e0d849ba370d422c2d5ab"", ""tags"": [""question-answering"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
195,bijorn/winger,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: any-to-any
---","{""modelId"": ""bijorn/winger"", ""sha"": ""601f5ea2a9f0e49f591e2520f4bf62772bbe3280"", ""tags"": [""any-to-any"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""any-to-any""}",1,[],0,[],0
196,sherooz/ahmed,"---
license: apache-2.0
language:
- ur
- en
- hi
base_model:
- deepseek-ai/DeepSeek-R1
- meta-llama/Llama-2-7b-chat-hf
- meta-llama/Llama-3.1-8B-Instruct
pipeline_tag: question-answering
tags:
- legal
- code
- finance
- biology
- art
datasets:
- open-thoughts/OpenThoughts-114k
- fka/awesome-chatgpt-prompts
- open-r1/OpenR1-Math-220k
- microsoft/orca-agentinstruct-1M-v1
metrics:
- accuracy
- code_eval
- character
new_version: meta-llama/Llama-2-7b-chat-hf
library_name: transformers.js
---","{""modelId"": ""sherooz/ahmed"", ""sha"": ""7b027cbbbb437227967811bbf2fadd579e4e7b7d"", ""tags"": [""transformers.js"", ""legal"", ""code"", ""finance"", ""biology"", ""art"", ""question-answering"", ""ur"", ""en"", ""hi"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:open-r1/OpenR1-Math-220k"", ""dataset:microsoft/orca-agentinstruct-1M-v1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
197,thalesleal/carteiraia,"---
language:
- pt
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""thalesleal/carteiraia"", ""sha"": ""b983bcc1d88b315ace3f6c896ff10d3bc4acf13e"", ""tags"": [""text-classification"", ""pt"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
198,qp521/ibm-chatbot-model,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""qp521/ibm-chatbot-model"", ""sha"": ""49f53717b2b04823420425d1879cd13da7b84e08"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
199,Average8/ast,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
tags:
- 2d
- art
- sprites
---","{""modelId"": ""Average8/ast"", ""sha"": ""fb14ff868b2c6fa7333b279b6ab73cc6ca0a4ad1"", ""tags"": [""2d"", ""art"", ""sprites"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
200,samira456/english-hindi,"---
license: mit
datasets:
- open-thoughts/OpenThoughts-114k
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: translation
library_name: flair
tags:
- code
---","{""modelId"": ""samira456/english-hindi"", ""sha"": ""47435907b3bb494fcd8ac7e2282ac6b222c85e86"", ""tags"": [""flair"", ""code"", ""translation"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
201,sensey42/Talep,"---
license: llama3.3
language:
- tr
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""sensey42/Talep"", ""sha"": ""89aa650fee4024bb7827391fc723daeb9a4fadc1"", ""tags"": [""tr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama3.3"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
202,LiuTengYing/CarRadio,"---
license: artistic-2.0
datasets:
- OpenAssistant/oasst1
language:
- en
- zh
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: transformers
tags:
- car-navigation
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""LiuTengYing/CarRadio"", ""sha"": ""326f4f40cbcc1ebfa188964545528071f031d4c1"", ""tags"": [""transformers"", ""car-navigation"", ""text-generation"", ""en"", ""zh"", ""dataset:OpenAssistant/oasst1"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:artistic-2.0"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
203,emartinezra/Arsonai,"---
license: other
license_name: arsonai
license_link: LICENSE
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: google/gemma-3-27b-it
pipeline_tag: text-generation
---","{""modelId"": ""emartinezra/Arsonai"", ""sha"": ""b8332bef38d8e9a70a811ca4b03de2df2e560391"", ""tags"": [""text-generation"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""doi:10.57967/hf/4897"", ""license:other"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
204,dla9944/test,"---
license: apache-2.0
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
language:
- ar
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""dla9944/test"", ""sha"": ""e5caf78e8466b33c5215b81c717d1b90c599a78c"", ""tags"": [""ar"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
205,silence09/DeepSeek-R1-Small-2layers,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
---
#  LightWeight Deepseek R1 (2 Hidden Layers Version with Smaller Dimensions)

This project is created using the official **Deepseek R1** model script (`modeling_deepseek.py`) from [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1/blob/main/modeling_deepseek.py). It implements a **2-layer version** of Deepseek R1 with randomly initialized weights and smaller dimensions.

## Purpose
The purpose of these weights is to provide a lightweight implementation for researchers who want to study the model architecture and run local quickly.

The original **Deepseek R1 model** requires an **8x H200 GPU setup** and runs on the **vLLM/SGLang framework**, making it difficult to deploy on standard hardware.

## Model Structure
The three hidden layers consist of:
- **A hidden layer: MLA + Dense MLP**
- **A hidden layer: MLA + MoE (Mixture of Experts) MLP**

The difference between this model and the original **Deepseek R1** is shown below:
```json
{
	""first_k_dense_replace"": 1,
	""intermediate_size"": 1024,
	""n_routed_experts"": 64,
	""num_experts_per_tok"": 4,
	""moe_intermediate_size"": 128,
	""num_hidden_layers"": 2,
	""num_nextn_predict_layers"": 0
}
```

## Usage

```python
from transformers import AutoConfig, AutoModelForCausalLM
from transformers import AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained('silence09/DeepSeek-R1-Small-2layers', torch_dtype=torch.bfloat16).cuda()
tokenizer = AutoTokenizer.from_pretrained('silence09/DeepSeek-R1-Small-2layers')

prompt = ""Who are u?""
messages = []
messages.append({""role"": ""user"", ""content"": prompt})
prompt_tokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=""pt"").to(model.device)
generated_ids = model.generate(prompt_tokens, max_new_tokens=100, do_sample=False)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(prompt_tokens, generated_ids)
]
completion = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(completion)
messages.append({""role"": ""assistant"", ""content"": completion})

```

## More Info
It was created using the python script available at [this repository](https://github.com/silencelamb/naked_llama/blob/main/hf_example/create_deepseek_r1_small_2layers.py)","{""modelId"": ""silence09/DeepSeek-R1-Small-2layers"", ""sha"": ""866c7b05e9e2ff052c9d2b141d9c84b69281d124"", ""tags"": [""safetensors"", ""deepseek_v3"", ""custom_code"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 123, ""pipeline_tag"": null}",1,[],0,[],0
206,weapon-x/chatbot,"---
license: afl-3.0
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""weapon-x/chatbot"", ""sha"": ""5c6a53c71791776fc9de0b4c6a5556ab95ee857a"", ""tags"": [""text-generation"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
207,Sumitnawale68/Sumit,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
language:
- ab
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
tags:
- finance
---","{""modelId"": ""Sumitnawale68/Sumit"", ""sha"": ""9a6291ba470d7c15d09f31e6da3eaaaaf803b2ce"", ""tags"": [""finance"", ""ab"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
208,Lukiii498/test,"---
language:
- de
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Lukiii498/test"", ""sha"": ""76d5f300e20c096733fd864a21c32964b75ba115"", ""tags"": [""de"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
209,sanun4730/chat,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""sanun4730/chat"", ""sha"": ""a815f9d2b47e93fb2bcfc02ebe50e05902ebea65"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
210,r4isy/kenu,"---
language:
- tr
base_model:
- openai-community/gpt2
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""r4isy/kenu"", ""sha"": ""26a16034c922147136656dc4c1ce8b7efb0cfcc4"", ""tags"": [""tr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
211,athitiya/personal,"---
license: openrail
datasets:
- open-thoughts/OpenThoughts-114k
language:
- en
- ta
- te
- ur
- fr
- ml
- ar
- ru
- cs
- fa
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""athitiya/personal"", ""sha"": ""ede325572c6074e5e2085f0dd8bb84319e6c0333"", ""tags"": [""text-generation"", ""en"", ""ta"", ""te"", ""ur"", ""fr"", ""ml"", ""ar"", ""ru"", ""cs"", ""fa"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
212,DaKaufeeBoii/Cleo,"---
license: apache-2.0
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text2text-generation
library_name: fasttext
---","{""modelId"": ""DaKaufeeBoii/Cleo"", ""sha"": ""4bbcb9d25883c54851186db1d14158d11cf65827"", ""tags"": [""fasttext"", ""text2text-generation"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text2text-generation""}",1,[],0,[],0
213,julelti/Ci,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""julelti/Ci"", ""sha"": ""dda5ad0af51c2c674c81123b3c156aa49775858e"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
214,PNZAGI/TRAIN,"---
license: afl-3.0
datasets:
- open-thoughts/OpenThoughts-114k
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""PNZAGI/TRAIN"", ""sha"": ""1f7416fb0aca38c87ea2b3313247300d0c84c143"", ""tags"": [""dataset:open-thoughts/OpenThoughts-114k"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
215,0x6e676e/generate-context,"---
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
- vi
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: image-to-image
library_name: diffusers
tags:
- art
---","{""modelId"": ""0x6e676e/generate-context"", ""sha"": ""c9fa1912a7d6d0ab19eddef05cf7828d11ef2c9e"", ""tags"": [""diffusers"", ""art"", ""image-to-image"", ""en"", ""vi"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""image-to-image""}",1,[],0,[],0
216,Angiie/Angie-light,"---
datasets:
- NovaSky-AI/Sky-T1_data_17k
- fka/awesome-chatgpt-prompts
language:
- ab
- aa
- ae
metrics:
- accuracy
- bertscore
- character
- charcut_mt
- code_eval
- chrf
- cer
- brier_score
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
- deepseek-ai/DeepSeek-R1-Zero
- microsoft/phi-4
- hexgrad/Kokoro-82M
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: feature-extraction
library_name: bertopic
license: creativeml-openrail-m
---","{""modelId"": ""Angiie/Angie-light"", ""sha"": ""993d12571249db3a115c0ef1d68b9d510e33b739"", ""tags"": [""bertopic"", ""feature-extraction"", ""ab"", ""aa"", ""ae"", ""dataset:NovaSky-AI/Sky-T1_data_17k"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""feature-extraction""}",1,[],0,[],0
217,Jianshu001/Efficient_CoT_DeepSeek-R1-Distill-Qwen-7B,"---
license: apache-2.0
base_model:
- deepseek-ai/DeepSeek-R1
---

# Efficient CoT for DeepSeek-R1-Distill-Qwen-7B  

We **Jianshu She**, **Zhuohao Li**, **Zhemin Huang** and **Muqi Li** fine-tuned **DeepSeek-R1-Distill-Qwen-7B** using **GRPO (Gradient-Regularized Policy Optimization)** to achieve **over 75% compression in Chain of Thought (CoT) length** on the **MATH dataset**, with **less than 5% accuracy loss**.  

## Results Comparison  

| Model | Final Accuracy | Average CoT Length | Average Answer Length |
|-------|---------------|--------------------|----------------------|
| **Baseline (Full CoT)** | **92.08%** | **450.95 words** | **481.19 words** |
| **Efficient_CoT_DeepSeek-R1-Distill-Qwen-7B** | **89.11%** | **113.06 words** | **125.94 words** |

Our optimization strategy significantly reduces CoT length while maintaining high accuracy, making inference more efficient. This approach is particularly suitable for resource-constrained environments without sacrificing reasoning performance.
","{""modelId"": ""Jianshu001/Efficient_CoT_DeepSeek-R1-Distill-Qwen-7B"", ""sha"": ""ad3483ba000d03ccc5919adef4a60726c1c4b691"", ""tags"": [""safetensors"", ""qwen2"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 30, ""pipeline_tag"": null}",1,[],0,[],0
218,Kumargogia/Kavya,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- hi
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: flair
tags:
- not-for-all-audiences
---","{""modelId"": ""Kumargogia/Kavya"", ""sha"": ""6a78571760fae7e10eb26b3f8ddc8d905e104658"", ""tags"": [""flair"", ""not-for-all-audiences"", ""hi"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
219,mih12345/deepseek_R1_jaman_josna,"---
license: mit
datasets:
- Sulav/mental_health_counseling_conversations_sharegpt
language:
- en
- bn
- zh
- ru
- hi
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""mih12345/deepseek_R1_jaman_josna"", ""sha"": ""526bef58db5c79e30bdc14f52d65a67104794d39"", ""tags"": [""text-generation"", ""en"", ""bn"", ""zh"", ""ru"", ""hi"", ""dataset:Sulav/mental_health_counseling_conversations_sharegpt"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
220,ritense/test-model,"---
language:
- nl
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""ritense/test-model"", ""sha"": ""73a5cf06691963ba1e25805939d8e1bdb0307ef2"", ""tags"": [""nl"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
221,praveenrmd/TamilGPT,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- ta
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""praveenrmd/TamilGPT"", ""sha"": ""95e8f7e81ebffadbea1bc79257719f4c22511bc8"", ""tags"": [""ta"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
222,Jobzi/AhSimon,"---
license: mit
language:
- en
metrics:
- bleurt
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: fasttext
tags:
- art
---","{""modelId"": ""Jobzi/AhSimon"", ""sha"": ""eed1f664c9464ed97fbd8caa99d79ae30ba89b9f"", ""tags"": [""fasttext"", ""art"", ""text-classification"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
223,mikaelcostake/brain0,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: any-to-any
---","{""modelId"": ""mikaelcostake/brain0"", ""sha"": ""62b274333858adb6fa65ff35b524b2c29f90f877"", ""tags"": [""any-to-any"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""any-to-any""}",1,[],0,[],0
224,JustVenus/Venus,"---
language:
- tr
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""JustVenus/Venus"", ""sha"": ""87e3a1fc681443c6b2709380202ad3281909a028"", ""tags"": [""tr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
225,RecurvAI/Recurv-Medical-Deepseek-R1,"---
license: mit
base_model:
- deepseek-ai/DeepSeek-R1
datasets:
- RecurvAI/Recurv-Medical-Dataset
language:
- en
pipeline_tag: text-generation
tags:
- medical
- anamnesis
---
# üß† Recurv-Medical-Deepseek-R1 Model

[![License](https://img.shields.io/badge/license-MIT-blue?style=flat-square)](https://opensource.org/license/MIT)
[![HF](https://img.shields.io/badge/HuggingFace-Recurv--Medical--Deepseek--R1-yellow?style=flat-square&logo=huggingface)](https://huggingface.co/RecurvAI/Recurv-Medical-Deepseek-R1)

## **Overview**

The **Recurv-Medical-Deepseek-R1** model is an enhanced version of Deepseek‚Äôs R1, designed to offer accurate and context-specific support for healthcare professionals and researchers. This model is particularly effective in answering medical questions, aiding in patient history gathering, and generating comprehensive explanations tailored to medical situations, utilizing advanced instruction tuning techniques.

**(Knowledge cut-off date: 22th January, 2025)**

### üéØ **Key Features**
- Optimized for medical-specific queries across various specialties.
- Fine-tuned for clinical and research-oriented workflows.
- Lightweight parameter-efficient fine-tuning with safetensors format.
- Multi-turn conversation support for context-rich interactions.
- Generates comprehensive answers and evidence-based suggestions.

---

## üöÄ **Model Card**

| **Parameter**              | **Details**                                                                                  |
|----------------------------|----------------------------------------------------------------------------------------------|
| **Base Model**             | DeepSeek R1 Distill Llama 8B                                                                 |
| **Fine-Tuning Framework**  | safetensors                                                                                  |
| **Dataset Size**           | 67,299 high-quality Q&A pairs                                                                |
| **Context Length**         | 4,096 tokens                                                                                 |
| **Training Steps**         | 100,000                                                                                      |
| **Model Size**             | 8 billion parameters                                                                         |

---

## üìä **Model Architecture**

### **Dataset Sources**
The dataset comprises high-quality Q&A pairs curated from medical textbooks, research papers, and clinical guidelines.

| Source                    | Description                                                                          |
|---------------------------|--------------------------------------------------------------------------------------|
| **PubMed**                | Extracted insights from open-access medical research.                                |
| **Clinical Guidelines**   | Data sourced from WHO, CDC, and specialty-specific guidelines.                       |
| **EHR-Simulated Data**    | Synthetic datasets modeled on real-world patient records for anamnesis workflows.    |

---

## üåü **Try The Model**
üöÄ [Recurv-Medical-Deepseek-R1](https://recurvai.org) on Our Website


## üôå **Contributing**

We welcome contributions to enhance Recurv-Medical-Deepseek-R1. You can:
- Share feedback or suggestions on the Hugging Face Model Hub
- Submit pull requests or issues for model improvement.

---

## üìú **License**

This model is licensed under the **MIT License**.

---

## üìû **Community**

For questions or support, connect with us via:
- **Twitter**: [RecurvAI](https://x.com/recurvai)
- **Email**: [support@recurvai.com](mailto:support@recurvai.com)

---

## ü§ù **Acknowledgments**

Special thanks to the medical community and researchers for their valuable insights and support in building this model. Together, we‚Äôre advancing AI in healthcare.","{""modelId"": ""RecurvAI/Recurv-Medical-Deepseek-R1"", ""sha"": ""96ddc90e21ff5aea3c41a783049e6c8affb1bd72"", ""tags"": [""pytorch"", ""llama"", ""medical"", ""anamnesis"", ""text-generation"", ""conversational"", ""en"", ""dataset:RecurvAI/Recurv-Medical-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 60, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
226,alex322r/deepseek-responder,"---
license: apache-2.0
language:
- es
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""alex322r/deepseek-responder"", ""sha"": ""0169be404f77f1cc2765d2c6caea92f2f5b8f8ea"", ""tags"": [""question-answering"", ""es"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
227,cmoraes199322/autonomo,"---
license: openrail
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
language:
- es
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
library_name: asteroid
---","{""modelId"": ""cmoraes199322/autonomo"", ""sha"": ""89a33dbcac74cc3613513a481945bd38b9153c2e"", ""tags"": [""asteroid"", ""text-generation"", ""es"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
228,unsloth/DeepSeek-R1-BF16,"---
base_model: deepseek-ai/DeepSeek-R1
language:
- en
license: mit
library_name: transformers
tags:
- deepseek
- unsloth
- transformers
---

## ***See [our collection](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5) for versions of Deepseek-R1 including GGUF, 4-bit and original formats.***

### Instructions to run this model in llama.cpp:
You can view more detailed instructions in our blog: [unsloth.ai/blog/deepseek-r1](https://unsloth.ai/blog/deepseek-r1)
1. Do not forget about `<ÔΩúUserÔΩú>` and `<ÔΩúAssistantÔΩú>` tokens! - Or use a chat template formatter
2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp
3. Example with Q8_0 K quantized cache **Notice -no-cnv disables auto conversation mode**
   ```bash
   ./llama.cpp/llama-cli \
       --model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf \
       --cache-type-k q8_0 \
       --threads 16 \
       --prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>' \
       -no-cnv
   ```
   Example output:
   
   ```txt
    <think>
    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.
    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.
    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.
    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.
    Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...
   ```
   
4. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.
   ```bash
   ./llama.cpp/llama-cli \
   --model unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf
   --cache-type-k q8_0 
   --threads 16 
   --prompt '<ÔΩúUserÔΩú>What is 1+1?<ÔΩúAssistantÔΩú>'
   --n-gpu-layers 20 \
    -no-cnv
   ```
   
# Finetune LLMs 2-5x faster with 70% less memory via Unsloth!
We have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png"" width=""200""/>](https://discord.gg/unsloth)
[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)


## ‚ú® Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3.2 (3B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |
| **Llama-3.2 (11B vision)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |
| **Qwen2 VL (7B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |
| **Qwen2.5 (7B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |
| **Llama-3.1 (8B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |
| **Phi-3.5 (mini)** | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |
| **Gemma 2 (9B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |
| **Mistral (7B)**    | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png"" width=""200""/>](https://docs.unsloth.ai)

- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.
- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.
- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.

## Special Thanks
A huge thank you to the DeepSeek team for creating and releasing these models.



# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align=""center"">
  <img src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"" width=""60%"" alt=""DeepSeek-V3"" />
</div>
<hr>
<div align=""center"" style=""line-height: 1;"">
  <a href=""https://www.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Homepage"" src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://chat.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Chat"" src=""https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://huggingface.co/deepseek-ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Hugging Face"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://discord.gg/Tc7c45Zzu5"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Discord"" src=""https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Wechat"" src=""https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://twitter.com/deepseek_ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Twitter Follow"" src=""https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE"" style=""margin: 2px;"">
    <img alt=""Code License"" src=""https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL"" style=""margin: 2px;"">
    <img alt=""Model License"" src=""https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>


<p align=""center"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf""><b>Paper Link</b>üëÅÔ∏è</a>
</p>


## 1. Introduction

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. 
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. 
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.

**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**

<p align=""center"">
  <img width=""80%"" src=""figures/benchmark.jpg"">
</p>

## 2. Model Summary

---

**Post-Training: Large-Scale Reinforcement Learning on the Base Model**

-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.

-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
    We believe the pipeline will benefit the industry by creating better models. 

---

**Distillation: Smaller Models Can Be Powerful Too**

-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. 
- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

## 3. Model Downloads

### DeepSeek-R1 Models

<div align=""center"">

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. 
For more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.

### DeepSeek-R1-Distill Models

<div align=""center"">

| **Model** | **Base Model** | **Download** |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.

## 4. Evaluation Results

### DeepSeek-R1-Evaluation
 For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.
<div align=""center"">


| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Architecture | - | - | MoE | - | - | MoE |
| | # Activated Params | - | - | 37B | - | - | 37B |
| | # Total Params | - | - | 671B | - | - | 671B |
| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### Distilled Model Evaluation


<div align=""center"">

| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Chat Website & API Platform
You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button ""DeepThink""

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

### DeepSeek-R1 Models

Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.

### DeepSeek-R1-Distill Models

DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.

For instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

You can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)

```bash
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2
```

### Usage Recommendations

**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**

1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.
2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**
3. For mathematical problems, it is advisable to include a directive in your prompt such as: ""Please reason step by step, and put your final answer within \boxed{}.""
4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.

## 7. License
This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. Citation
```
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

```

## 9. Contact
If you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).
","{""modelId"": ""unsloth/DeepSeek-R1-BF16"", ""sha"": ""d0e8ab8d818b52670989f53f632c893db53c882b"", ""tags"": [""transformers"", ""safetensors"", ""deepseek_v3"", ""text-generation"", ""deepseek"", ""unsloth"", ""conversational"", ""custom_code"", ""en"", ""arxiv:2501.12948"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 2314, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
229,UkYYY/eva,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- de
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""UkYYY/eva"", ""sha"": ""70fcfb8f8ce7b9b4f41efeee1124fa0af809b56d"", ""tags"": [""de"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
230,wrestling-is-real-bro/airules,"---
license: mit
datasets:
- simplescaling/s1K
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-video
library_name: asteroid
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""wrestling-is-real-bro/airules"", ""sha"": ""d5688f3cbe216c16211b6e60d9f47065f7adca05"", ""tags"": [""asteroid"", ""text-to-video"", ""en"", ""dataset:simplescaling/s1K"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-video""}",1,[],0,[],0
231,sandeep-aipm/AI-Code,"---
license: apache-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""sandeep-aipm/AI-Code"", ""sha"": ""f6c72d183d982ea369fc194f422ff01c9c14179d"", ""tags"": [""en"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
232,Yeamkuan/enanalysis,"---
license: apache-2.0
language:
- en
- th
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""Yeamkuan/enanalysis"", ""sha"": ""ad2792542a3dc7bcff4b64d694286fbf166d7c0f"", ""tags"": [""text-classification"", ""en"", ""th"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
233,opensourcerelease/DeepSeek-R1-bf16,"---
license: mit
library_name: transformers
base_model:
  - deepseek-ai/DeepSeek-R1
---
# DeepSeek-R1
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header -->

<div align=""center"">
  <img src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true"" width=""60%"" alt=""DeepSeek-V3"" />
</div>
<hr>
<div align=""center"" style=""line-height: 1;"">
  <a href=""https://www.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Homepage"" src=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://chat.deepseek.com/"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Chat"" src=""https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://huggingface.co/deepseek-ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Hugging Face"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://discord.gg/Tc7c45Zzu5"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Discord"" src=""https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Wechat"" src=""https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://twitter.com/deepseek_ai"" target=""_blank"" style=""margin: 2px;"">
    <img alt=""Twitter Follow"" src=""https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>

<div align=""center"" style=""line-height: 1;"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE"" style=""margin: 2px;"">
    <img alt=""Code License"" src=""https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL"" style=""margin: 2px;"">
    <img alt=""Model License"" src=""https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"" style=""display: inline-block; vertical-align: middle;""/>
  </a>
</div>


<p align=""center"">
  <a href=""https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf""><b>Paper Link</b>üëÅÔ∏è</a>
</p>


## 1. Introduction

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. 
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.
With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.
However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,
we introduce DeepSeek-R1, which incorporates cold-start data before RL.
DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. 
To support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.

<p align=""center"">
  <img width=""80%"" src=""figures/benchmark.jpg"">
</p>

## 2. Model Summary

---

**Post-Training: Large-Scale Reinforcement Learning on the Base Model**

-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.

-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.
    We believe the pipeline will benefit the industry by creating better models. 

---

**Distillation: Smaller Models Can Be Powerful Too**

-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. 
- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

## 3. Model Downloads

### DeepSeek-R1 Models

<div align=""center"">

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |
| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |

</div>

DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. 
For more details regrading the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.

### DeepSeek-R1-Distill Models

<div align=""center"">

| **Model** | **Base Model** | **Download** |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |
| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |
| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |
| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |
| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |

</div>

DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.
We slightly change their configs and tokenizers. Please use our setting to run these models.

## 4. Evaluation Results

### DeepSeek-R1-Evaluation
 For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.
<div align=""center"">


| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Architecture | - | - | MoE | - | - | MoE |
| | # Activated Params | - | - | 37B | - | - | 37B |
| | # Total Params | - | - | 671B | - | - | 671B |
| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |
| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |
| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |
| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |
| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |
| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |
| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |
| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |

</div>


### Distilled Model Evaluation


<div align=""center"">

| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |

</div>


## 5. Chat Website & API Platform
You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button ""DeepThink""

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

### DeepSeek-R1 Models

Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.

### DeepSeek-R1-Distill Models

DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.

For instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):

```shell
vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager
```

**NOTE: We recommend setting an appropriate temperature (between 0.5 and 0.7) when running these models, otherwise you may encounter issues with endless repetition or incoherent output.**

## 7. License
This code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).
DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:
- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.
- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).
- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).

## 8. Citation
```

```

## 9. Contact
If you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).","{""modelId"": ""opensourcerelease/DeepSeek-R1-bf16"", ""sha"": ""9ad36be62190d73ac4df571e015b74e69ca44328"", ""tags"": [""transformers"", ""safetensors"", ""deepseek_v3"", ""text-generation"", ""conversational"", ""custom_code"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 437, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
234,TheWolfOfWallStreet/The_Wolf_Of_Wall_Street,"---
license: apache-2.0
datasets:
- open-thoughts/OpenThoughts-114k
metrics:
- accuracy
- bertscore
- bleu
- bleurt
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fastai
language:
- en
pipeline_tag: question-answering
tags:
- biology
- chemistry
- text-generation-inference
---","{""modelId"": ""TheWolfOfWallStreet/The_Wolf_Of_Wall_Street"", ""sha"": ""fe00d96c0657a8a648a3d5a06f5085c90db3d8d8"", ""tags"": [""fastai"", ""biology"", ""chemistry"", ""text-generation-inference"", ""question-answering"", ""en"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
235,alexpineda97/traductor_otoesp,"---
license: wtfpl
datasets:
- alexpineda97/OTOESP
language:
- es
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text2text-generation
---","{""modelId"": ""alexpineda97/traductor_otoesp"", ""sha"": ""e726d6d1796576fda9d255889fcdd28c22bfd0e2"", ""tags"": [""text2text-generation"", ""es"", ""dataset:alexpineda97/OTOESP"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:wtfpl"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text2text-generation""}",1,[],0,[],0
236,deca-ai/2-mini,"---
base_model:
- deepseek-ai/DeepSeek-R1
library_name: transformers
tags:
- reasoning
- R1
- 1M
- fast
- Deca
- Deca-AI
- Deca-2
- Qwen
license: other
---

![Deca 2 Banner](https://huggingface.co/deca-ai/2-mini-beta/resolve/main/banner.jpg)
The Deca 2 family of models, now generally availible, is built on cutting-edge architectures like DeepSeek R1, LLaMA 3, and Qwen 2, delivering extraordinary performance. With a focus on insane speed and high efficiency, Deca 2 is revolutionizing text generation and setting new standards in the industry. It also comes with a **1 million** context window.

As more capabilities are added, Deca 2 will evolve into a more powerful, any-to-any model in the future. While it‚Äôs focused on text generation for now, its foundation is designed to scale, bringing even more advanced functionalities to come.

**3/3 Release**
* Updated weights with better experts
* Made Deca 2 Mini Generally Availible
**2/14 Release:**
* Enhanced Instruction Following","{""modelId"": ""deca-ai/2-mini"", ""sha"": ""17ae7fe68a4fb2582369e5f59906d5c5ab171885"", ""tags"": [""transformers"", ""safetensors"", ""qwen2"", ""text-generation"", ""reasoning"", ""R1"", ""1M"", ""fast"", ""Deca"", ""Deca-AI"", ""Deca-2"", ""Qwen"", ""conversational"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 19, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
237,DragosBDI/GPT_test,"---
license: apache-2.0
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""DragosBDI/GPT_test"", ""sha"": ""2f7857564f6ceec294af9c8c3e2716534f4a5f8e"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
238,aliMohammad16/sabrina-ai,"---
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
- hi
- ur
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""aliMohammad16/sabrina-ai"", ""sha"": ""9cfe53a4a6347e9e52589c963232aba3b0707278"", ""tags"": [""en"", ""hi"", ""ur"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
239,samfati/humanvoice,"---
license: apache-2.0
datasets:
- ServiceNow-AI/R1-Distill-SFT
language:
- en
- ur
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-speech
tags:
- code
- legal
---","{""modelId"": ""samfati/humanvoice"", ""sha"": ""b8f6c8c87858aecf9cb5a8653d0072915f090819"", ""tags"": [""code"", ""legal"", ""text-to-speech"", ""en"", ""ur"", ""dataset:ServiceNow-AI/R1-Distill-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""doi:10.57967/hf/4490"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-speech""}",1,[],0,[],0
240,d92refea/Asistente,"---
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- es
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""d92refea/Asistente"", ""sha"": ""d1fb522e4b223cfd4465eaf4a3059d03b2c405a7"", ""tags"": [""es"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
241,0xchum/Fugen,"---
language:
- en
- hi
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Llama-70B
---","{""modelId"": ""0xchum/Fugen"", ""sha"": ""b9b1b2290401c942119acdb2c551928890886668"", ""tags"": [""en"", ""hi"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
242,Hataco/RR-SwordFigthing,"---
datasets:
- open-r1/OpenR1-Math-220k
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Hataco/RR-SwordFigthing"", ""sha"": ""45526f4055d21f77e625c51e96b49f925501857d"", ""tags"": [""en"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
243,death-walker/harmoni,"---
datasets:
- death-walker/harmoniqa
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
- mistralai/Mistral-7B-Instruct-v0.3
pipeline_tag: question-answering
---","{""modelId"": ""death-walker/harmoni"", ""sha"": ""04e4e97019cc061749e23d488ecdb432b8e1ff76"", ""tags"": [""question-answering"", ""en"", ""dataset:death-walker/harmoniqa"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
244,gimmy256/deepseek_r1_finetuned,"---
base_model:
- unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit
- deepseek-ai/DeepSeek-R1
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
license: apache-2.0
language:
- en
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
---

# Uploaded  model

- **Developed by:** gimmy256
- **License:** apache-2.0
- **Finetuned from model :** unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)","{""modelId"": ""gimmy256/deepseek_r1_finetuned"", ""sha"": ""4d3cc867d984d95432d03a6caa8ea7ebe26cad91"", ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""text-generation-inference"", ""unsloth"", ""trl"", ""sft"", ""conversational"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""downloads"": 7, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
245,ImmersioNAI/Poppy,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- ru
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: flair
tags:
- biology
---","{""modelId"": ""ImmersioNAI/Poppy"", ""sha"": ""b126b7f948cd170f4eacc93af93d7b033df2b233"", ""tags"": [""flair"", ""biology"", ""text-classification"", ""ru"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
246,jasonlinn/yilanpass,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""jasonlinn/yilanpass"", ""sha"": ""2eff41d83046996244ade6a04bef8f1ac1701a2f"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
247,AntVess/new74,"---
license: afl-3.0
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: translation
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""AntVess/new74"", ""sha"": ""45a181211a2d694a4073d505b3c5bd015e8beda0"", ""tags"": [""translation"", ""ru"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:afl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""translation""}",1,[],0,[],0
248,Monternot888/Test_de_Bert,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Monternot888/Test_de_Bert"", ""sha"": ""435d01a7af994264915a040393993a297c6cf38d"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
249,silkstringfiddlesink/Astra-49,"---
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- precision
base_model:
- deepseek-ai/DeepSeek-R1
license: mit
---","{""modelId"": ""silkstringfiddlesink/Astra-49"", ""sha"": ""412a590d4de01f9662ad70c12ce92a1954567b56"", ""tags"": [""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
250,IcYhAwK88/BeeAndMe,"---
license: unknown
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---","{""modelId"": ""IcYhAwK88/BeeAndMe"", ""sha"": ""296c6f73798d19ed198c1271e12ba422612cabc5"", ""tags"": [""en"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
251,Alhdrawi/R-RAY-AI,"---
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
- FreedomIntelligence/Medical-R1-Distill-Data-Chinese
base_model:
- deepseek-ai/DeepSeek-V3-0324
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3-0324
---","{""modelId"": ""Alhdrawi/R-RAY-AI"", ""sha"": ""632a59c64316a518db7e12c18e797af1d7edb694"", ""tags"": [""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""dataset:FreedomIntelligence/Medical-R1-Distill-Data-Chinese"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
252,KaPe22/KaPe22,"---
language:
- hu
- en
base_model:
- deepseek-ai/DeepSeek-V3
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""KaPe22/KaPe22"", ""sha"": ""4d73896fa523926d0b977cf7e2d3d65ca8939e81"", ""tags"": [""hu"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
253,aishu1505/english-tamil-translation,"---
license: apache-2.0
language:
- ta
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""aishu1505/english-tamil-translation"", ""sha"": ""87eef71db21fce3e916bfe084cbcccdc9c9ee6e0"", ""tags"": [""text-classification"", ""ta"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
254,dailong/mymode,"---
license: creativeml-openrail-m
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-speech
tags:
- art
---","{""modelId"": ""dailong/mymode"", ""sha"": ""443f3d1ffde10609d23ef68199289479c74f7236"", ""tags"": [""art"", ""text-to-speech"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-speech""}",1,[],0,[],0
255,kazzaou/app,"---
license: openrail
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- es
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: google/gemma-3-27b-it
pipeline_tag: token-classification
library_name: fastai
tags:
- medical
---","{""modelId"": ""kazzaou/app"", ""sha"": ""86e87e7857d243d4af30f9d52bc119dcce97fff9"", ""tags"": [""fastai"", ""medical"", ""token-classification"", ""es"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
256,pinnacle001/steph,"---
license: creativeml-openrail-m
language:
- en
- es
- de
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: automatic-speech-recognition
---","{""modelId"": ""pinnacle001/steph"", ""sha"": ""c29b47e7e1aac653310cbcb5159becf394bf64e9"", ""tags"": [""automatic-speech-recognition"", ""en"", ""es"", ""de"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""automatic-speech-recognition""}",1,[],0,[],0
257,TanAIspaceX/test1,"---
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---","{""modelId"": ""TanAIspaceX/test1"", ""sha"": ""84bc42659337f5126eefa311a6fe155f999ac6f0"", ""tags"": [""text-generation"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
258,mertkb/palmtree,"---
license: openrail
datasets:
- fka/awesome-chatgpt-prompts
language:
- aa
metrics:
- bleurt
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: image-to-3d
library_name: allennlp
tags:
- biology
---","{""modelId"": ""mertkb/palmtree"", ""sha"": ""8c0810d2e528dc117acc337e2a6d37bfd3d63f4e"", ""tags"": [""allennlp"", ""biology"", ""image-to-3d"", ""aa"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""image-to-3d""}",1,[],0,[],0
259,cwestbrook/lotrdata,"---
library_name: transformers
datasets:
- cwestbrook/lotrdata
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---

# DeepTolkien

This LLM is an OpenSeek R1 fine-tuned using the LoRA method on text extracted from JRR Tolkien's The Lord of the Rings. 

## Model Details

This LLM is an OpenSeek R1 fine-tuned using the LoRA method on text extracted from JRR Tolkien's The Lord of the Rings.  The model can be prompted with a stub, for example ""Frodo looked up and saw"", and will then generate a story in the style of Tolkien's writing that continues from this stub.  Have fun!

If you have played with OpenSeek R1, you have almost certainly noticed that at times the reasoning model seems to get caught up in a loop.  This behavior is also seen here:  for example, two characters will get caught in a looping dialog.  I believe this is more of a property of DeepSeek R1 than this LoRA, and better results may yet be achieved through a model specific to prose and storytelling.  However, I wanted to get an idea of how the new DeepSeek models perform, and this has been a fantastic learning experience.  

## Usage

### Load the model:
```
# Import the model
config = PeftConfig.from_pretrained(""cwestbrook/lotrdata"")
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
# Load the Lora model
model = PeftModel.from_pretrained(model, ""cwestbrook/lotrdata"")
```

### Run the model:
```
prompt = ""Gandalf revealed his new iphone,""
inputs = tokenizer(prompt, return_tensors=""pt"").to('cuda')
tokens = model.generate(
    **inputs,
    max_new_tokens=100,
    temperature=1,
    eos_token_id=tokenizer.eos_token_id,
    early_stopping=True
)
predictions = tokenizer.batch_decode(tokens, skip_special_tokens=True)
print(predictions[0])
```

This is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.
","{""modelId"": ""cwestbrook/lotrdata"", ""sha"": ""0fd3f7f152730dbfa2bbc626ebf7c483ac52b0ef"", ""tags"": [""transformers"", ""safetensors"", ""en"", ""dataset:cwestbrook/lotrdata"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
260,gresres/test,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""gresres/test"", ""sha"": ""b2a97e3360d6d0b6453f92b7197071a7a75cca3c"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
261,samwilenborg30/chatbot,"---
license: other
license_name: plumbing
license_link: LICENSE
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""samwilenborg30/chatbot"", ""sha"": ""0f83ccb33297c9474ce789f8f6aba57d3d87dff0"", ""tags"": [""question-answering"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:other"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
262,Yaavuzzz/Yavuz,"---
datasets:
- Reihaneh/Germanic_Common_Voice
language:
- de
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Yaavuzzz/Yavuz"", ""sha"": ""316f4dafb70ddbea076d6f5ee36f4f3ec119dd4a"", ""tags"": [""de"", ""dataset:Reihaneh/Germanic_Common_Voice"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
263,Hi14th/test,"---
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Hi14th/test"", ""sha"": ""82677e4f2e2cbdbdea09530781b2356397c403ac"", ""tags"": [""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
264,yerifantess/weeklyupdate,"---
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""yerifantess/weeklyupdate"", ""sha"": ""d9d661afb6aeaea0ebf0ff7f394e26be4d92b13b"", ""tags"": [""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
265,Michael419/Ii,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: asteroid
tags:
- legal
---","{""modelId"": ""Michael419/Ii"", ""sha"": ""284f7392327d83e9029937f567bdc19348528bb7"", ""tags"": [""asteroid"", ""legal"", ""text-classification"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
266,Favour99/ALPHA,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
- open-thoughts/OpenThoughts-114k
- PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT
language:
- af
- ar
- ak
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
library_name: bertopic
tags:
- legal
- finance
- music
- code
- medical
---","{""modelId"": ""Favour99/ALPHA"", ""sha"": ""204b01c6cf245cd0ca8d3d3883d7b18b537822cd"", ""tags"": [""bertopic"", ""legal"", ""finance"", ""music"", ""code"", ""medical"", ""af"", ""ar"", ""ak"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:open-thoughts/OpenThoughts-114k"", ""dataset:PJMixers-Dev/open-thoughts_OpenThoughts-114k-CustomShareGPT"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
267,javier001/Javier,"---
license: bigscience-openrail-m
datasets:
- fka/awesome-chatgpt-prompts
language:
- ae
metrics:
- bleu
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: allennlp
tags:
- biology
---","{""modelId"": ""javier001/Javier"", ""sha"": ""1ce59b5a71e036ed45c637e9f2959f431db4e84a"", ""tags"": [""allennlp"", ""biology"", ""ae"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:bigscience-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
268,DivineNinja13/bubaModel,"---
license: llama2
datasets:
- jondurbin/cinematika-v0.1
language:
- ru
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
tags:
- music
---","{""modelId"": ""DivineNinja13/bubaModel"", ""sha"": ""ae72a585208227f9d5221726a81e97f3296c44c8"", ""tags"": [""music"", ""token-classification"", ""ru"", ""dataset:jondurbin/cinematika-v0.1"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:llama2"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
269,AlexandreCezar/SaudeMental,"---
license: lgpl-3.0
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- pt
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- medical
---","{""modelId"": ""AlexandreCezar/SaudeMental"", ""sha"": ""70f6832d394c86aeae1f0c517b5a9c5bacf6c9e9"", ""tags"": [""medical"", ""pt"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:lgpl-3.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
270,Dach13/Darryc,"---
license: unlicense
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
---","{""modelId"": ""Dach13/Darryc"", ""sha"": ""d23803bf6d54e7b9f8c843cd75a38e45199028e0"", ""tags"": [""text-classification"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unlicense"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
271,an4l0g/test,"---
datasets:
- fka/awesome-chatgpt-prompts
- gopipasala/fka-awesome-chatgpt-prompts
language:
- bn
- en
metrics:
- accuracy
- character
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""an4l0g/test"", ""sha"": ""83421a250dbb206146af49ca7ab982762246ec3f"", ""tags"": [""bn"", ""en"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:gopipasala/fka-awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
272,Random7878/Life,"---
license: apache-2.0
datasets:
- vidore/syntheticDocQA_artificial_intelligence_test
- aps/super_glue
metrics:
- accuracy
language:
- en
base_model:
- openai-community/gpt2
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/Janus-Pro-7B
library_name: transformers
---
from flask import Flask, request, jsonify
from transformers import pipeline
import openai
from newsapi import NewsApiClient
from notion_client import Client
from datetime import datetime, timedelta
import torch
from diffusers import StableDiffusionPipeline

# Initialize Flask app
app = Flask(__name__)

# Load Hugging Face Question-Answering model
qa_pipeline = pipeline(""question-answering"", model=""distilbert-base-uncased-distilled-squad"")

# OpenAI API Key (Replace with your own)
openai.api_key = ""your_openai_api_key""

# NewsAPI Key (Replace with your own)
newsapi = NewsApiClient(api_key=""your_news_api_key"")

# Notion API Key (Replace with your own)
notion = Client(auth=""your_notion_api_key"")

# Load Stable Diffusion for Image Generation
device = ""cuda"" if torch.cuda.is_available() else ""cpu""
sd_model = StableDiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"").to(device)

# === FUNCTION 1: Answer Student Questions ===
@app.route(""/ask"", methods=[""POST""])
def answer_question():
    data = request.json
    question = data.get(""question"", """")
    context = ""This AI is trained to assist students with questions related to various subjects.""
    
    if not question:
        return jsonify({""error"": ""Please provide a question.""}), 400
    
    answer = qa_pipeline(question=question, context=context)
    return jsonify({""question"": question, ""answer"": answer[""answer""]})

# === FUNCTION 2: Generate Code ===
@app.route(""/generate_code"", methods=[""POST""])
def generate_code():
    data = request.json
    prompt = data.get(""prompt"", """")
    
    if not prompt:
        return jsonify({""error"": ""Please provide a prompt for code generation.""}), 400
    
    response = openai.Completion.create(
        engine=""code-davinci-002"",
        prompt=prompt,
        max_tokens=100
    )
    return jsonify({""code"": response.choices[0].text.strip()})

# === FUNCTION 3: Get Daily News ===
@app.route(""/news"", methods=[""GET""])
def get_news():
    headlines = newsapi.get_top_headlines(language=""en"", category=""technology"")
    news_list = [{""title"": article[""title""], ""url"": article[""url""]} for article in headlines[""articles""]]
    
    return jsonify({""news"": news_list})

# === FUNCTION 4: Create a Planner Task ===
@app.route(""/planner"", methods=[""POST""])
def create_planner():
    data = request.json
    task = data.get(""task"", """")
    days = int(data.get(""days"", 1))

    if not task:
        return jsonify({""error"": ""Please provide a task.""}), 400
    
    due_date = datetime.now() + timedelta(days=days)
    
    return jsonify({""task"": task, ""due_date"": due_date.strftime(""%Y-%m-%d"")})

# === FUNCTION 5: Save Notes to Notion ===
@app.route(""/notion"", methods=[""POST""])
def save_notion_note():
    data = request.json
    title = data.get(""title"", ""Untitled Note"")
    content = data.get(""content"", """")

    if not content:
        return jsonify({""error"": ""Please provide content for the note.""}), 400
    
    notion.pages.create(
        parent={""database_id"": ""your_notion_database_id""},
        properties={""title"": {""title"": [{""text"": {""content"": title}}]}},
        children=[{""object"": ""block"", ""type"": ""paragraph"", ""paragraph"": {""text"": [{""type"": ""text"", ""text"": {""content"": content}}]}}]
    )

    return jsonify({""message"": ""Note added successfully to Notion!""})

# === FUNCTION 6: Generate AI Images ===
@app.route(""/generate_image"", methods=[""POST""])
def generate_image():
    data = request.json
    prompt = data.get(""prompt"", """")

    if not prompt:
        return jsonify({""error"": ""Please provide an image prompt.""}), 400

    image = sd_model(prompt).images[0]
    image_path = ""generated_image.png""
    image.save(image_path)
    
    return jsonify({""message"": ""Image generated successfully!"", ""image_path"": image_path})

# === RUN THE APP ===
if __name__ == ""__main__"":
    app.run(debug=True)","{""modelId"": ""Random7878/Life"", ""sha"": ""796f05bfdaca3afe3f61d66908a2c7db4be65fdf"", ""tags"": [""transformers"", ""en"", ""dataset:vidore/syntheticDocQA_artificial_intelligence_test"", ""dataset:aps/super_glue"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
273,adarshgiri55/Adi,"---
license: creativeml-openrail-m
language:
- en
- hi
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/DeepSeek-V3
---","{""modelId"": ""adarshgiri55/Adi"", ""sha"": ""6c14187b7ed8abd20180e1340cb40887fe066143"", ""tags"": [""en"", ""hi"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:creativeml-openrail-m"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
274,orgullomoore/TexLawLLM,"---
license: pddl
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""orgullomoore/TexLawLLM"", ""sha"": ""c4e0c984bb1ae7001e9ed82bf7a680c369dd30a4"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:pddl"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
275,mahgam88/Jafr,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- fa
metrics:
- character
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: token-classification
---","{""modelId"": ""mahgam88/Jafr"", ""sha"": ""88ed28f3e6162815871e3848f2b5a0ab69da0dc9"", ""tags"": [""token-classification"", ""fa"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""token-classification""}",1,[],0,[],0
276,FANzinho/FanSilver,"---
license: apache-2.0
datasets:
- open-r1/OpenR1-Math-220k
language:
- pt
metrics:
- character
base_model:
- deepseek-ai/Janus-Pro-7B
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fastai
tags:
- art
- legal
---","{""modelId"": ""FANzinho/FanSilver"", ""sha"": ""ac3c72c15cd3b48a2b6e89f22b6d91fab409a87e"", ""tags"": [""fastai"", ""art"", ""legal"", ""pt"", ""dataset:open-r1/OpenR1-Math-220k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
277,theone2b/99,"---
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""theone2b/99"", ""sha"": ""02352e1fd75b9aebfcbbf10a16c1bde73ddd62d4"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
278,ykarout/phi-4-deepseek-reasoning-fp16,"---
library_name: transformers
tags:
- unsloth
license: apache-2.0
datasets:
- nvidia/Llama-Nemotron-Post-Training-Dataset
base_model:
- unsloth/phi-4
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-generation
---

# Model Card for Model ID

Phi-4 trained on reasoning outputs on complex logic, math and coding challenges derived from nvidia/Llama-Nemotron-Post-Training-Dataset filtered to include high length reasoning answers generated by DeepSeek R1.


## Model Details

### Model Description

Phi-4 trained on reasoning outputs on complex logic, math and coding challenges derived from nvidia/Llama-Nemotron-Post-Training-Dataset filtered to include high length reasoning answers generated by DeepSeek R1.
The training was on 10,000 samples done on an RTX 5090 (yes managed to make unsloth work on a 5090) with context length of 16384 and took around 10 hours using unsloth 4-bit quants and transfomers SFT Trainer.
You do not need to add a system prompt but it can help in some use cases. The model will automatically go into thinking mode when presented with complex tasks.


Recommended Settings of temperature = 1.5 (you can test with 1 to 1.5) , min_p = 0.1, repeat penalty 1.2 or 1.3 to mitigate extremely long reasoning around the same concept.


Try the following prompt or similar structured prompts containing complex connections and the model will automatically go into thinking mode and generate long reasoning chains akin to DeepSeek.

#### Prompt:

This prompt was generated using Claude 3.7 Sonnet and not included in the train or test dataset, use similarly structred prompts and see the magic!

1. Network Packet Routing Optimization Challenge

You're designing a system to optimize packet routing in a network with multiple possible paths. The network consists of nodes connected by bidirectional links, each with different bandwidth capacities and latency values.

Your task is to find the most efficient routing path between a given source and destination node that satisfies specific constraints on bandwidth, latency, and hop count.

Input Specification

The first line contains four space-separated integers: `n`, `m`, `b_min`, and `l_max` (2 ‚â§ n ‚â§ 100, 1 ‚â§ m ‚â§ 5000, 1 ‚â§ b_min ‚â§ 1000, 1 ‚â§ l_max ‚â§ 10000)
- `n`: number of nodes in the network (numbered from 1 to n)
- `m`: number of links between nodes
- `b_min`: minimum required bandwidth for the path
- `l_max`: maximum allowed total latency for the path

The next `m` lines each contain four integers `u`, `v`, `b`, `l` (1 ‚â§ u, v ‚â§ n, u ‚â† v, 1 ‚â§ b ‚â§ 1000, 1 ‚â§ l ‚â§ 1000):
- `u`, `v`: nodes connected by this link
- `b`: bandwidth capacity of the link
- `l`: latency of the link

The last line contains two integers `s` and `t` (1 ‚â§ s, t ‚â§ n, s ‚â† t) - the source and destination nodes.

Constraints and Notes

1. The bandwidth of a path is the minimum bandwidth among all links in the path
2. The latency of a path is the sum of latencies of all links in the path
3. A valid path must have bandwidth ‚â• `b_min` and latency ‚â§ `l_max`
4. Among all valid paths, you must choose the one with the highest bandwidth
5. If there are multiple paths with the same highest bandwidth, choose the one with the lowest latency
6. If there are still multiple paths, choose the one with the fewest hops (links)
7. If no valid path exists, output ""NO PATH""

Output

If a valid path exists, the first line should contain three space-separated integers: the bandwidth of the chosen path, the total latency of the chosen path, and the number of hops.

The second line should contain the sequence of nodes in the path, starting with `s` and ending with `t`.

If no valid path exists, output ""NO PATH"" (without quotes).

Examples

Example 1:
```
5 6 50 100
1 2 100 20
2 3 80 30
3 5 70 10
1 4 60 10
4 5 90 30
1 3 50 5
1 5
```

Output:
```
70 60 3
1 2 3 5
```

Example 2:
```
4 5 80 50
1 2 80 20
2 3 120 15
3 4 90 10
1 3 100 30
2 4 70 25
1 4
```

Output:
```
90 40 2
1 3 4
```

Example 3:
```
3 3 100 100
1 2 150 40
2 3 180 70
1 3 120 30
1 3
```

Output:
```
120 30 1
1 3
```

Your solution should efficiently find the optimal path that satisfies all constraints, handling potentially complex network topologies with multiple possible routes between source and destination.



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:**unsloth/phi-4


## Uses

Complex reasoning requiring challenging thinking and coding (mostly python).

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""ykarout/phi-4-deepseek-reasoning-fp16"", ""sha"": ""8b8d4e1f565029e8c04cac0267f48ab1f2c94d6b"", ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""unsloth"", ""conversational"", ""dataset:nvidia/Llama-Nemotron-Post-Training-Dataset"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
279,beita6969/deepseek-r1-medical-response,"---
library_name: transformers
tags:
- unsloth
- trl
- sft
datasets:
- shibing624/medical
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: audio-text-to-text
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [zhangmingda]
- **Model type:** [deepseek-r1-medical-response]
- **Language(s) (NLP):** [chinese]
- **Finetuned from model [optional]:** [deepseek-r1]



","{""modelId"": ""beita6969/deepseek-r1-medical-response"", ""sha"": ""cb763fadcf093590e3141c92e17258aead0ca87c"", ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""unsloth"", ""trl"", ""sft"", ""audio-text-to-text"", ""en"", ""dataset:shibing624/medical"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""downloads"": 13, ""pipeline_tag"": ""audio-text-to-text""}",1,[],0,[],0
280,Vaimee/fggggr,"---
license: mit
datasets:
- fka/awesome-chatgpt-prompts
- PowerInfer/QWQ-LONGCOT-500K
- agibot-world/AgiBotWorld-Alpha
- HumanLLMs/Human-Like-DPO-Dataset
language:
- de
metrics:
- accuracy
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/Janus-Pro-7B
pipeline_tag: any-to-any
---","{""modelId"": ""Vaimee/fggggr"", ""sha"": ""b1ccedcd137d00f477263f34385dc79fc6c1b430"", ""tags"": [""any-to-any"", ""de"", ""dataset:fka/awesome-chatgpt-prompts"", ""dataset:PowerInfer/QWQ-LONGCOT-500K"", ""dataset:agibot-world/AgiBotWorld-Alpha"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""any-to-any""}",1,[],0,[],0
281,karrrr123456/ace,"---
license: openrail
datasets:
- JeanKaddour/minipile
language:
- en
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: hexgrad/Kokoro-82M
pipeline_tag: text-generation
library_name: flair
tags:
- text-generation-inference
---","{""modelId"": ""karrrr123456/ace"", ""sha"": ""7f03765828f1f1508532348a5a9130c66e143a81"", ""tags"": [""flair"", ""safetensors"", ""gpt2"", ""text-generation-inference"", ""text-generation"", ""en"", ""dataset:JeanKaddour/minipile"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:openrail"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-generation""}",1,[],0,[],0
282,Avener/RealTime,"---
license: apache-2.0
datasets:
- facebook/natural_reasoning
language:
- en
metrics:
- code_eval
base_model:
- deepseek-ai/DeepSeek-R1
new_version: google/gemma-3-27b-it
library_name: diffusers
tags:
- art
---","{""modelId"": ""Avener/RealTime"", ""sha"": ""f2d3f23fa166f77890b31059afdb7d33a9ed9147"", ""tags"": [""diffusers"", ""art"", ""en"", ""dataset:facebook/natural_reasoning"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 20, ""pipeline_tag"": null}",1,[],0,[],0
283,RZEE17/New1,"---
license: mit
language:
- af
metrics:
- bleu
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/Janus-Pro-7B
library_name: asteroid
tags:
- art
---","{""modelId"": ""RZEE17/New1"", ""sha"": ""1b0aa60631b372308fb3a22a7edba8dcdff60ccf"", ""tags"": [""asteroid"", ""art"", ""af"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
284,Gary88/mymodel,"---
license: mit
language:
- zh
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""Gary88/mymodel"", ""sha"": ""819be2126e3932417cd218050b33a28097113694"", ""tags"": [""zh"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
285,ZZVCV/FHZBox,"---
license: apache-2.0
language:
- aa
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-V3
library_name: diffusers
---","{""modelId"": ""ZZVCV/FHZBox"", ""sha"": ""8d77714560bce007bf5ced78ea2cf209f2690b72"", ""tags"": [""diffusers"", ""aa"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
286,JulienSunLib/Sunlib,"---
language:
- fr
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: question-answering
---","{""modelId"": ""JulienSunLib/Sunlib"", ""sha"": ""1233f95aa4a5a35d767b26b173b7380d95c169dd"", ""tags"": [""question-answering"", ""fr"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""question-answering""}",1,[],0,[],0
287,urjinchimed/khalkhmongol,"---
license: apache-2.0
language:
- mn
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: text-to-image
tags:
- art
---","{""modelId"": ""urjinchimed/khalkhmongol"", ""sha"": ""d83a8208867120db001aa07ab27c5328039a9e88"", ""tags"": [""art"", ""text-to-image"", ""mn"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-to-image""}",1,[],0,[],0
288,Ebaturan/GokTurk,"---
license: apache-2.0
language:
- nl
- en
- tr
base_model:
- deepseek-ai/DeepSeek-R1
tags:
- medical
- biology
- chemistry
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""Ebaturan/GokTurk"", ""sha"": ""662de4ad56c9e217d79279b5c0880bfb1886379b"", ""tags"": [""medical"", ""biology"", ""chemistry"", ""nl"", ""en"", ""tr"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
289,Virtual-Herbalist/Herbalist-AI,"---
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
- facebook/natural_reasoning
- open-thoughts/OpenThoughts-114k
language:
- en
base_model:
- perplexity-ai/r1-1776
- deepseek-ai/DeepSeek-R1
library_name: fastai
---","{""modelId"": ""Virtual-Herbalist/Herbalist-AI"", ""sha"": ""cbb7dd9c40912ea5f4bae84c6a6863d3ce226274"", ""tags"": [""fastai"", ""en"", ""dataset:FreedomIntelligence/medical-o1-reasoning-SFT"", ""dataset:facebook/natural_reasoning"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
290,Oluwadamo/Damo,"---
license: mit
datasets:
- open-thoughts/OpenThoughts-114k
language:
- am
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
pipeline_tag: zero-shot-classification
library_name: allennlp
---","{""modelId"": ""Oluwadamo/Damo"", ""sha"": ""fbeff643e7b9238f462a26ecb859d3fce42bff02"", ""tags"": [""allennlp"", ""zero-shot-classification"", ""am"", ""dataset:open-thoughts/OpenThoughts-114k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""zero-shot-classification""}",1,[],0,[],0
291,tariqaziz80/dentists,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- en
- ur
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
- deepseek-ai/Janus-Pro-7B
- openbmb/MiniCPM-o-2_6
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: text-classification
library_name: asteroid
---","{""modelId"": ""tariqaziz80/dentists"", ""sha"": ""1933c0d875972ce90ffd96f355f5e371d7ac551c"", ""tags"": [""asteroid"", ""text-classification"", ""en"", ""ur"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""text-classification""}",1,[],0,[],0
292,MimiTechAI/DeepSeek-R1-Distill-Llama-70B,"---
license: mit
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""modelId"": ""MimiTechAI/DeepSeek-R1-Distill-Llama-70B"", ""sha"": ""4b3eb9fb5ffebf474f42b380b22410f0c13f68c2"", ""tags"": [""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""arxiv:1910.09700"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
293,mdjobayarehosen/Bing3,"---
license: apache-2.0
datasets:
- fka/awesome-chatgpt-prompts
language:
- ak
- ab
- ae
- am
- an
- ar
- as
- ay
- av
metrics:
- bertscore
- bleu
- bleurt
- accuracy
- cer
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
pipeline_tag: audio-classification
library_name: allennlp
tags:
- biology
- legal
- music
- art
- climate
- medical
- chemistry
- not-for-all-audiences
- text-generation-inference
- merge
- moe
- finance
- code
---","{""modelId"": ""mdjobayarehosen/Bing3"", ""sha"": ""2541629f20dfabf2cee9ec719dae64e229e170a0"", ""tags"": [""allennlp"", ""biology"", ""legal"", ""music"", ""art"", ""climate"", ""medical"", ""chemistry"", ""not-for-all-audiences"", ""text-generation-inference"", ""merge"", ""moe"", ""finance"", ""code"", ""audio-classification"", ""ak"", ""ab"", ""ae"", ""am"", ""an"", ""ar"", ""as"", ""ay"", ""av"", ""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:apache-2.0"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": ""audio-classification""}",1,[],0,[],0
294,meghrajs/demo,"---
datasets:
- fka/awesome-chatgpt-prompts
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""meghrajs/demo"", ""sha"": ""82890f6f348fc75871a7f99b9827ad5c43fe2774"", ""tags"": [""dataset:fka/awesome-chatgpt-prompts"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
295,himanshuvas/test,"---
license: mit
language:
- en
base_model:
- deepseek-ai/DeepSeek-R1
---","{""modelId"": ""himanshuvas/test"", ""sha"": ""e00b3f3a78270df1bc1177a152faebf61c93a918"", ""tags"": [""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
296,Arrowxyz/hux-ai,"---
license: mit
datasets:
- Congliu/Chinese-DeepSeek-R1-Distill-data-110k
language:
- am
metrics:
- bertscore
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: bertopic
tags:
- art
---","{""modelId"": ""Arrowxyz/hux-ai"", ""sha"": ""a3de64547703ee603942f52ecab683a8b9af87e0"", ""tags"": [""bertopic"", ""art"", ""am"", ""dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
297,disconzi/oze,"---
license: unknown
datasets:
- HumanLLMs/Human-Like-DPO-Dataset
language:
- pt
- en
- es
metrics:
- accuracy
base_model:
- deepseek-ai/DeepSeek-R1
new_version: deepseek-ai/DeepSeek-R1
library_name: fastai
---","{""modelId"": ""disconzi/oze"", ""sha"": ""ad3497656daa4b4c170e252bbdbe8d71541b0e6c"", ""tags"": [""fastai"", ""pt"", ""en"", ""es"", ""dataset:HumanLLMs/Human-Like-DPO-Dataset"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:unknown"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
298,mradermacher/DeepSeek-R1-i1-GGUF,"---
base_model: deepseek-ai/DeepSeek-R1
language:
- en
library_name: transformers
license: mit
quantized_by: mradermacher
---
## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags: nicoboss -->
weighted/imatrix quants of https://huggingface.co/deepseek-ai/DeepSeek-R1

<!-- provided-files -->
static quants are available at https://huggingface.co/mradermacher/DeepSeek-R1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_S.gguf.part1of3) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_S.gguf.part2of3) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_S.gguf.part3of3) | i1-IQ1_S | 133.7 | for the desperate |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_M.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_M.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_M.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ1_M.gguf.part4of4) | i1-IQ1_M | 149.0 | mostly desperate |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XXS.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XXS.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XXS.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XXS.gguf.part4of4) | i1-IQ2_XXS | 174.5 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XS.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XS.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XS.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_XS.gguf.part4of4) | i1-IQ2_XS | 195.2 |  |
| [PART 1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_S.gguf.part1of4) [PART 2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_S.gguf.part2of4) [PART 3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_S.gguf.part3of4) [PART 4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_S.gguf.part4of4) | i1-IQ2_S | 197.1 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_M.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_M.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_M.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_M.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ2_M.gguf.part5of5) | i1-IQ2_M | 217.5 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K_S.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K_S.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K_S.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K_S.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K_S.gguf.part5of5) | i1-Q2_K_S | 224.8 | very low quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K.gguf.part1of5) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K.gguf.part2of5) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K.gguf.part3of5) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K.gguf.part4of5) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q2_K.gguf.part5of5) | i1-Q2_K | 244.1 | IQ3_XXS probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XXS.gguf.part6of6) | i1-IQ3_XXS | 258.0 | lower quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_XS.gguf.part6of6) | i1-IQ3_XS | 272.9 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_S.gguf.part6of6) | i1-IQ3_S | 289.2 | beats Q3_K* |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_S.gguf.part6of6) | i1-Q3_K_S | 289.2 | IQ3_XS probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part1of6) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part2of6) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part3of6) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part4of6) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part5of6) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ3_M.gguf.part6of6) | i1-IQ3_M | 292.2 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part1of7) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part2of7) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part3of7) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part4of7) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part5of7) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part6of7) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_M.gguf.part7of7) | i1-Q3_K_M | 319.3 | IQ3_S probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q3_K_L.gguf.part8of8) | i1-Q3_K_L | 347.5 | IQ3_M probably better |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-IQ4_XS.gguf.part8of8) | i1-IQ4_XS | 357.2 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_0.gguf.part8of8) | i1-Q4_0 | 379.1 | fast, low quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part1of8) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part2of8) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part3of8) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part4of8) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part5of8) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part6of8) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part7of8) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_S.gguf.part8of8) | i1-Q4_K_S | 380.1 | optimal size/speed/quality |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part1of9) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part2of9) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part3of9) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part4of9) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part5of9) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part6of9) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part7of9) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part8of9) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_K_M.gguf.part9of9) | i1-Q4_K_M | 404.5 | fast, recommended |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part1of9) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part2of9) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part3of9) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part4of9) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part5of9) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part6of9) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part7of9) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part8of9) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q4_1.gguf.part9of9) | i1-Q4_1 | 420.0 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part01of10) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part02of10) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part03of10) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part04of10) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part05of10) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part06of10) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part07of10) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part08of10) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part09of10) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_S.gguf.part10of10) | i1-Q5_K_S | 461.9 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part01of10) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part02of10) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part03of10) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part04of10) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part05of10) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part06of10) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part07of10) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part08of10) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part09of10) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q5_K_M.gguf.part10of10) | i1-Q5_K_M | 475.5 |  |
| [P1](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part01of12) [P2](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part02of12) [P3](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part03of12) [P4](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part04of12) [P5](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part05of12) [P6](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part06of12) [P7](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part07of12) [P8](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part08of12) [P9](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part09of12) [P10](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part10of12) [P11](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part11of12) [P12](https://huggingface.co/mradermacher/DeepSeek-R1-i1-GGUF/resolve/main/DeepSeek-R1.i1-Q6_K.gguf.part12of12) | i1-Q6_K | 550.9 | practically like static Q6_K |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->
","{""modelId"": ""mradermacher/DeepSeek-R1-i1-GGUF"", ""sha"": ""58193cd16b8a14b79a2292fceeff91f69581cfac"", ""tags"": [""transformers"", ""en"", ""base_model:deepseek-ai/DeepSeek-R1"", ""base_model:finetune:deepseek-ai/DeepSeek-R1"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""downloads"": 0, ""pipeline_tag"": null}",1,[],0,[],0
