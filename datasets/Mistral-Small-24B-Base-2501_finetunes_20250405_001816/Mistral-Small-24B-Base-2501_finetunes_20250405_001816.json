{
    "models": [
        {
            "model_id": "mistralai/Mistral-Small-24B-Base-2501",
            "metadata": "{\"id\": \"mistralai/Mistral-Small-24B-Base-2501\", \"author\": \"mistralai\", \"sha\": \"50a43a1f4488595def221f672346761c85ad90ea\", \"last_modified\": \"2025-01-30 17:00:21+00:00\", \"created_at\": \"2025-01-23 14:13:54+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 19970, \"downloads_all_time\": null, \"likes\": 240, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"arxiv:2009.03300\", \"arxiv:1905.07830\", \"arxiv:1911.11641\", \"arxiv:1904.09728\", \"arxiv:1905.10044\", \"arxiv:1907.10641\", \"arxiv:1811.00937\", \"arxiv:1809.02789\", \"arxiv:1911.01547\", \"arxiv:1705.03551\", \"arxiv:2107.03374\", \"arxiv:2108.07732\", \"arxiv:2110.14168\", \"arxiv:2009.11462\", \"arxiv:2101.11718\", \"arxiv:2110.08193\", \"arxiv:1804.09301\", \"arxiv:2109.07958\", \"arxiv:1804.06876\", \"arxiv:2103.03874\", \"arxiv:2304.06364\", \"arxiv:2206.04615\", \"arxiv:2203.09509\", \"arxiv:2406.01574\", \"arxiv:2311.12022\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"language:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"eduagarcia/open_pt_llm_leaderboard\", \"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"ruslanmv/convert_to_gguf\", \"mrks89/First_agent_template\", \"K00B404/LLM_Quantization\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-30 17:00:21+00:00\", \"cardData\": \"language:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67924ea27166f1e56f452425\", \"modelId\": \"mistralai/Mistral-Small-24B-Base-2501\", \"usedStorage\": 141481221403}",
            "depth": 0,
            "children": [
                "https://huggingface.co/allura-org/Mistral-Small-24b-Sertraline-0304",
                "https://huggingface.co/allura-org/Mistral-Small-Sisyphus-24b-2503",
                "https://huggingface.co/Erland/Mistral-Small-24B-Base-2501",
                "https://huggingface.co/nikitagreb/test-upload",
                "https://huggingface.co/gghfez/Mistral-Small-24B-Instruct-2501",
                "https://huggingface.co/bigband/InfiniteNabu",
                "https://huggingface.co/cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
                "https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview",
                "https://huggingface.co/jth01/Mistral-Small-24B-Instruct-2501-6.0bpw-h8-exl2",
                "https://huggingface.co/PocketDoc/Dans-DangerousWinds-V1.1.1-24b",
                "https://huggingface.co/arcee-ai/arcee-blitz-caller-beta",
                "https://huggingface.co/bigband/LuminousHanuman",
                "https://huggingface.co/regig134/new_round_14",
                "https://huggingface.co/bigband/UnbreakablePoseidon",
                "https://huggingface.co/adamo1139/Mistral-Small-24B-Instruct-2501-ungated",
                "https://huggingface.co/cognitivecomputations/Dolphin3.0-Mistral-24B",
                "https://huggingface.co/IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml",
                "https://huggingface.co/agajh/dippy-bbb-08-n",
                "https://huggingface.co/unsloth/Mistral-Small-24B-Base-2501",
                "https://huggingface.co/lucyknada/PocketDoc_Dans-DangerousWinds-V1.1.1-24b-exl2",
                "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
                "https://huggingface.co/bigband/OmnipotentFreyja",
                "https://huggingface.co/bigband/HealerApollo",
                "https://huggingface.co/PocketDoc/Dans-PersonalityEngine-V1.2.0-24b",
                "https://huggingface.co/Undi95/MistralThinker-v1.1",
                "https://huggingface.co/jhu-clsp/rank1-mistral-2501-24b",
                "https://huggingface.co/SaisExperiments/Not-So-Small-Alpaca-24B",
                "https://huggingface.co/v2ray/GPT4chan-24B",
                "https://huggingface.co/bigband/ProsperousTezcatlipoca",
                "https://huggingface.co/lucyknada/PocketDoc_Dans-PersonalityEngine-V1.2.0-24b-exl2"
            ],
            "children_count": 30
        },
        {
            "model_id": "allura-org/Mistral-Small-24b-Sertraline-0304",
            "metadata": "{\"id\": \"allura-org/Mistral-Small-24b-Sertraline-0304\", \"author\": \"allura-org\", \"sha\": \"26c68b8c4de900ffc392567961d4f516b384a077\", \"last_modified\": \"2025-03-04 18:00:11+00:00\", \"created_at\": \"2025-03-04 12:29:24+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 372, \"downloads_all_time\": null, \"likes\": 3, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"instruct\", \"conversational\", \"en\", \"zh\", \"dataset:allenai/tulu-3-sft-personas-instruction-following\", \"dataset:simplescaling/s1K-1.1\", \"dataset:simplescaling/s1K-claude-3-7-sonnet\", \"dataset:FreedomIntelligence/Medical-R1-Distill-Data\", \"dataset:OpenCoder-LLM/opc-sft-stage1\", \"dataset:cognitivecomputations/SystemChat-2.0\", \"dataset:anthracite-org/kalo-opus-instruct-22k-no-refusal\", \"dataset:allura-org/scienceqa_sharegpt\", \"dataset:KodCode/KodCode-V1-SFT-R1\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- allenai/tulu-3-sft-personas-instruction-following\\n- simplescaling/s1K-1.1\\n- simplescaling/s1K-claude-3-7-sonnet\\n- FreedomIntelligence/Medical-R1-Distill-Data\\n- OpenCoder-LLM/opc-sft-stage1\\n- cognitivecomputations/SystemChat-2.0\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\n- allura-org/scienceqa_sharegpt\\n- KodCode/KodCode-V1-SFT-R1\\nlanguage:\\n- en\\n- zh\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- instruct\\n- conversational\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{{- bos_token }}{%- for message in messages %}\\n{%- if message['role'] == 'system' %}\\n{{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n{%- elif message['role'] == 'user' %}\\n{{- '[INST]' + message['content'] + '[/INST]' }}\\n{%- elif message['role'] == 'assistant' %}\\n{{- message['content'] + eos_token }}\\n{%- endif %}\\n{%- endfor %}\\n\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-04 18:00:11+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- allenai/tulu-3-sft-personas-instruction-following\\n- simplescaling/s1K-1.1\\n- simplescaling/s1K-claude-3-7-sonnet\\n- FreedomIntelligence/Medical-R1-Distill-Data\\n- OpenCoder-LLM/opc-sft-stage1\\n- cognitivecomputations/SystemChat-2.0\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\n- allura-org/scienceqa_sharegpt\\n- KodCode/KodCode-V1-SFT-R1\\nlanguage:\\n- en\\n- zh\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- instruct\\n- conversational\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67c6f2243ff4876691b4fbae\", \"modelId\": \"allura-org/Mistral-Small-24b-Sertraline-0304\", \"usedStorage\": 47176728132}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "allura-org/Mistral-Small-Sisyphus-24b-2503",
            "metadata": "{\"id\": \"allura-org/Mistral-Small-Sisyphus-24b-2503\", \"author\": \"allura-org\", \"sha\": \"c38364dbff0607b3a2a233f82b5bc893218775b8\", \"last_modified\": \"2025-03-03 23:16:08+00:00\", \"created_at\": \"2025-03-03 01:51:26+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 484, \"downloads_all_time\": null, \"likes\": 8, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"axolotl\", \"en\", \"zh\", \"dataset:allenai/tulu-3-sft-personas-instruction-following\", \"dataset:simplescaling/s1K-1.1\", \"dataset:simplescaling/s1K-claude-3-7-sonnet\", \"dataset:reedmayhew/medical-o1-reasoning-SFT-jsonl\", \"dataset:OpenCoder-LLM/opc-sft-stage1\", \"dataset:PocketDoc/Dans-Kinomaxx-VanillaBackrooms\", \"dataset:cognitivecomputations/SystemChat-2.0\", \"dataset:anthracite-org/kalo-opus-instruct-22k-no-refusal\", \"dataset:allura-org/scienceqa_sharegpt\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- allenai/tulu-3-sft-personas-instruction-following\\n- simplescaling/s1K-1.1\\n- simplescaling/s1K-claude-3-7-sonnet\\n- reedmayhew/medical-o1-reasoning-SFT-jsonl\\n- OpenCoder-LLM/opc-sft-stage1\\n- PocketDoc/Dans-Kinomaxx-VanillaBackrooms\\n- cognitivecomputations/SystemChat-2.0\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\n- allura-org/scienceqa_sharegpt\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\ntags:\\n- axolotl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{{- bos_token }}{%- for message in messages %}\\n{%- if message['role'] == 'system' %}\\n{{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n{%- elif message['role'] == 'user' %}\\n{{- '[INST]' + message['content'] + '[/INST]' }}\\n{%- elif message['role'] == 'assistant' %}\\n{{- message['content'] + eos_token }}\\n{%- endif %}\\n{%- endfor %}\\n\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-03 23:16:08+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- allenai/tulu-3-sft-personas-instruction-following\\n- simplescaling/s1K-1.1\\n- simplescaling/s1K-claude-3-7-sonnet\\n- reedmayhew/medical-o1-reasoning-SFT-jsonl\\n- OpenCoder-LLM/opc-sft-stage1\\n- PocketDoc/Dans-Kinomaxx-VanillaBackrooms\\n- cognitivecomputations/SystemChat-2.0\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\n- allura-org/scienceqa_sharegpt\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\ntags:\\n- axolotl\", \"transformersInfo\": null, \"_id\": \"67c50b1e1d368ecc8bce1f65\", \"modelId\": \"allura-org/Mistral-Small-Sisyphus-24b-2503\", \"usedStorage\": 47176751344}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Erland/Mistral-Small-24B-Base-2501",
            "metadata": "{\"id\": \"Erland/Mistral-Small-24B-Base-2501\", \"author\": \"Erland\", \"sha\": \"a5e152880b24215be961e10be86651ce5dd5e956\", \"last_modified\": \"2025-01-30 15:42:30+00:00\", \"created_at\": \"2025-01-30 15:42:28+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"text-generation-inference\", \"unsloth\", \"mistral\", \"trl\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-30 15:42:30+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"679b9de4256f46e1a27e4169\", \"modelId\": \"Erland/Mistral-Small-24B-Base-2501\", \"usedStorage\": 0}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "nikitagreb/test-upload",
            "metadata": "{\"id\": \"nikitagreb/test-upload\", \"author\": \"nikitagreb\", \"sha\": \"0fba1c4c2c32c6036ab9d5bffe9ed8fa97ff4ec9\", \"last_modified\": \"2025-02-17 12:57:51+00:00\", \"created_at\": \"2025-01-29 12:33:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 5, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"bert\", \"text-classification\", \"transformers\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-classification\", \"mask_token\": \"[MASK]\", \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"I like you. I love you\"}], \"model_index\": null, \"config\": {\"architectures\": [\"BertForSequenceClassification\"], \"model_type\": \"bert\", \"tokenizer_config\": {\"cls_token\": \"[CLS]\", \"mask_token\": \"[MASK]\", \"pad_token\": \"[PAD]\", \"sep_token\": \"[SEP]\", \"unk_token\": \"[UNK]\"}}, \"transformers_info\": {\"auto_model\": \"AutoModelForSequenceClassification\", \"custom_class\": null, \"pipeline_tag\": \"text-classification\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vocab.txt', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 167360261}, \"total\": 167360261}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-17 12:57:51+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForSequenceClassification\", \"custom_class\": null, \"pipeline_tag\": \"text-classification\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679a202e2d4564b47de590ba\", \"modelId\": \"nikitagreb/test-upload\", \"usedStorage\": 2008393764}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "gghfez/Mistral-Small-24B-Instruct-2501",
            "metadata": "{\"id\": \"gghfez/Mistral-Small-24B-Instruct-2501\", \"author\": \"gghfez\", \"sha\": \"0dd8b81a3197fe7dd6d7a8e8da0bf6fee999157d\", \"last_modified\": \"2025-02-22 05:39:21+00:00\", \"created_at\": \"2025-01-30 22:53:29+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 54, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-22 05:39:21+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679c02e99141a5524a4afaf0\", \"modelId\": \"gghfez/Mistral-Small-24B-Instruct-2501\", \"usedStorage\": 47176728132}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/InfiniteNabu",
            "metadata": "{\"id\": \"bigband/InfiniteNabu\", \"author\": \"bigband\", \"sha\": \"c8bb5306b7a75f551f0e9209a17a8aa6c5860b28\", \"last_modified\": \"2025-02-04 08:31:42+00:00\", \"created_at\": \"2025-02-04 08:20:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 15, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 08:31:42+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1cde267ecb05a30426f73\", \"modelId\": \"bigband/InfiniteNabu\", \"usedStorage\": 94321574492}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "NousResearch/DeepHermes-3-Mistral-24B-Preview",
            "metadata": "{\"id\": \"NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"author\": \"NousResearch\", \"sha\": \"48072dc6c0594a3198eb862c13613c4ab1119009\", \"last_modified\": \"2025-03-13 16:10:18+00:00\", \"created_at\": \"2025-03-02 13:05:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 8752, \"downloads_all_time\": null, \"likes\": 90, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"Mistral-Small\", \"instruct\", \"finetune\", \"chatml\", \"gpt4\", \"synthetic data\", \"distillation\", \"function calling\", \"json mode\", \"axolotl\", \"roleplaying\", \"chat\", \"reasoning\", \"r1\", \"vllm\", \"conversational\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"widget_data\": [{\"example_title\": \"DeepHermes 3\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.\"}, {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]}], \"model_index\": [{\"name\": \"DeepHermes-3-Mistral-24B-Preview\", \"results\": []}], \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|end_of_text|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572464640}, \"total\": 23572464640}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-13 16:10:18+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67c457b3ac2030235ee78897\", \"modelId\": \"NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"usedStorage\": 47162049993}",
            "depth": 1,
            "children": [
                "https://huggingface.co/Jarrodbarnes/DeepHermes-3-Mistral-24B-Preview-mlx-fp16",
                "https://huggingface.co/mlx-community/DeepHermes-3-Mistral-24B-Preview-bf16",
                "https://huggingface.co/AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine"
            ],
            "children_count": 3
        },
        {
            "model_id": "Jarrodbarnes/DeepHermes-3-Mistral-24B-Preview-mlx-fp16",
            "metadata": "{\"id\": \"Jarrodbarnes/DeepHermes-3-Mistral-24B-Preview-mlx-fp16\", \"author\": \"Jarrodbarnes\", \"sha\": \"85075002a3bf3edcbc644b73281488c98ac2efc6\", \"last_modified\": \"2025-03-17 20:21:43+00:00\", \"created_at\": \"2025-03-17 20:19:28+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 13, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"Mistral-Small\", \"instruct\", \"finetune\", \"chatml\", \"gpt4\", \"synthetic data\", \"distillation\", \"function calling\", \"json mode\", \"axolotl\", \"roleplaying\", \"chat\", \"reasoning\", \"r1\", \"vllm\", \"mlx\", \"mlx-my-repo\", \"conversational\", \"en\", \"base_model:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"base_model:finetune:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\n- mlx\\n- mlx-my-repo\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"widget_data\": [{\"example_title\": \"DeepHermes 3\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.\"}, {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]}], \"model_index\": [{\"name\": \"DeepHermes-3-Mistral-24B-Preview\", \"results\": []}], \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|end_of_text|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 23572464640}, \"total\": 23572464640}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-17 20:21:43+00:00\", \"cardData\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\n- mlx\\n- mlx-my-repo\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67d883d0780b0a72cb57b2da\", \"modelId\": \"Jarrodbarnes/DeepHermes-3-Mistral-24B-Preview-mlx-fp16\", \"usedStorage\": 47162049530}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "mlx-community/DeepHermes-3-Mistral-24B-Preview-bf16",
            "metadata": "{\"id\": \"mlx-community/DeepHermes-3-Mistral-24B-Preview-bf16\", \"author\": \"mlx-community\", \"sha\": \"24a675bebf93166e97e0dcba52cf5fc865a78b67\", \"last_modified\": \"2025-03-14 06:11:38+00:00\", \"created_at\": \"2025-03-14 06:03:29+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 33, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"Mistral-Small\", \"instruct\", \"finetune\", \"chatml\", \"gpt4\", \"synthetic data\", \"distillation\", \"function calling\", \"json mode\", \"axolotl\", \"roleplaying\", \"chat\", \"reasoning\", \"r1\", \"vllm\", \"mlx\", \"conversational\", \"en\", \"base_model:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"base_model:finetune:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\n- mlx\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"widget_data\": [{\"example_title\": \"DeepHermes 3\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.\"}, {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}]}], \"model_index\": [{\"name\": \"DeepHermes-3-Mistral-24B-Preview\", \"results\": []}], \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|end_of_text|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572464640}, \"total\": 23572464640}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-14 06:11:38+00:00\", \"cardData\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- Mistral-Small\\n- instruct\\n- finetune\\n- chatml\\n- gpt4\\n- synthetic data\\n- distillation\\n- function calling\\n- json mode\\n- axolotl\\n- roleplaying\\n- chat\\n- reasoning\\n- r1\\n- vllm\\n- mlx\\nwidget:\\n- example_title: DeepHermes 3\\n  messages:\\n  - role: system\\n    content: You are a sentient, superintelligent artificial general intelligence,\\n      here to teach and assist me.\\n  - role: user\\n    content: What is the meaning of life?\\nmodel-index:\\n- name: DeepHermes-3-Mistral-24B-Preview\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67d3c6b13052b0566ca5033a\", \"modelId\": \"mlx-community/DeepHermes-3-Mistral-24B-Preview-bf16\", \"usedStorage\": 47162049909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine",
            "metadata": "{\"id\": \"AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\", \"author\": \"AlSamCur123\", \"sha\": \"7b6c71206149f379868b336f0a6483ec7ad96f9d\", \"last_modified\": \"2025-03-29 07:41:27+00:00\", \"created_at\": \"2025-03-18 06:24:07+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1477, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"base_model:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"base_model:finetune:NousResearch/DeepHermes-3-Mistral-24B-Preview\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|end_of_text|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572464640}, \"total\": 23572464640}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-29 07:41:27+00:00\", \"cardData\": \"base_model: NousResearch/DeepHermes-3-Mistral-24B-Preview\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67d91187bab972f83a0e6edb\", \"modelId\": \"AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\", \"usedStorage\": 94307021745}",
            "depth": 2,
            "children": [
                "https://huggingface.co/AlSamCur123/DeepHermes-3-Mistral-24ContinuedFine"
            ],
            "children_count": 1
        },
        {
            "model_id": "AlSamCur123/DeepHermes-3-Mistral-24ContinuedFine",
            "metadata": "{\"id\": \"AlSamCur123/DeepHermes-3-Mistral-24ContinuedFine\", \"author\": \"AlSamCur123\", \"sha\": \"00904720adb93e0429dde122ebe0ed74946a9ca3\", \"last_modified\": \"2025-03-30 20:25:49+00:00\", \"created_at\": \"2025-03-30 19:55:05+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"base_model:AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\", \"base_model:finetune:AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|end_of_text|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572464640}, \"total\": 23572464640}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-30 20:25:49+00:00\", \"cardData\": \"base_model: AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67e9a1994579bd8158d28e54\", \"modelId\": \"AlSamCur123/DeepHermes-3-Mistral-24ContinuedFine\", \"usedStorage\": 47162049993}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "arcee-ai/arcee-blitz-caller-beta",
            "metadata": "{\"id\": \"arcee-ai/arcee-blitz-caller-beta\", \"author\": \"arcee-ai\", \"sha\": \"a0785999d747111a9e7de423a742cee923da9f73\", \"last_modified\": \"2025-02-23 23:38:55+00:00\", \"created_at\": \"2025-02-21 06:53:45+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 39, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": [{\"name\": \"default\", \"template\": \"{{bos_token}}{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}, {\"name\": \"tool_use\", \"template\": \"{%- macro json_to_python_type(json_spec) %}\\n{%- set basic_type_map = {\\n    \\\"string\\\": \\\"str\\\",\\n    \\\"number\\\": \\\"float\\\",\\n    \\\"integer\\\": \\\"int\\\",\\n    \\\"boolean\\\": \\\"bool\\\"\\n} %}\\n\\n{%- if basic_type_map[json_spec.type] is defined %}\\n    {{- basic_type_map[json_spec.type] }}\\n{%- elif json_spec.type == \\\"array\\\" %}\\n    {{- \\\"list[\\\" +  json_to_python_type(json_spec|items) + \\\"]\\\"}}\\n{%- elif json_spec.type == \\\"object\\\" %}\\n    {%- if json_spec.additionalProperties is defined %}\\n        {{- \\\"dict[str, \\\" + json_to_python_type(json_spec.additionalProperties) + ']'}}\\n    {%- else %}\\n        {{- \\\"dict\\\" }}\\n    {%- endif %}\\n{%- elif json_spec.type is iterable %}\\n    {{- \\\"Union[\\\" }}\\n    {%- for t in json_spec.type %}\\n      {{- json_to_python_type({\\\"type\\\": t}) }}\\n      {%- if not loop.last %}\\n        {{- \\\",\\\" }} \\n    {%- endif %}\\n    {%- endfor %}\\n    {{- \\\"]\\\" }}\\n{%- else %}\\n    {{- \\\"Any\\\" }}\\n{%- endif %}\\n{%- endmacro %}\\n\\n\\n{{- bos_token }}\\n{{- '<|im_start|>system\\n' }}\\n{{- \\\"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \\\" }}\\n{%- for tool in tools %}\\n    {%- if tool.function is defined %}\\n        {%- set tool = tool.function %}\\n    {%- endif %}\\n    {{- '{\\\"type\\\": \\\"function\\\", \\\"function\\\": ' }}\\n    {{- '{\\\"name\\\": \\\"' + tool.name + '\\\", ' }}\\n    {{- '\\\"description\\\": \\\"' + tool.name + '(' }}\\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\\n        {{- param_name + \\\": \\\" + json_to_python_type(param_fields) }}\\n        {%- if not loop.last %}\\n            {{- \\\", \\\" }}\\n        {%- endif %}\\n    {%- endfor %}\\n    {{- \\\")\\\" }}\\n    {%- if tool.return is defined %}\\n        {{- \\\" -> \\\" + json_to_python_type(tool.return) }}\\n    {%- endif %}\\n    {{- \\\" - \\\" + tool.description + \\\"\\n\\n\\\" }}\\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\\n        {%- if loop.first %}\\n            {{- \\\"    Args:\\n\\\" }}\\n        {%- endif %}\\n        {{- \\\"        \\\" + param_name + \\\"(\\\" + json_to_python_type(param_fields) + \\\"): \\\" + param_fields.description|trim }}\\n    {%- endfor %}\\n    {%- if tool.return is defined and tool.return.description is defined %}\\n        {{- \\\"\\n    Returns:\\n        \\\" + tool.return.description }}\\n    {%- endif %}\\n    {{- '\\\"' }}\\n    {{- ', \\\"parameters\\\": ' }}\\n    {%- if tool.parameters.properties | length == 0 %}\\n        {{- \\\"{}\\\" }}\\n    {%- else %}\\n        {{- tool.parameters|tojson }}\\n    {%- endif %}\\n    {{- \\\"}\\\" }}\\n    {%- if not loop.last %}\\n        {{- \\\"\\n\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{{- \\\" </tools>\\\" }}\\n{{- 'Use the following pydantic model json schema for each tool call you will make: {\\\"properties\\\": {\\\"name\\\": {\\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"arguments\\\": {\\\"title\\\": \\\"Arguments\\\", \\\"type\\\": \\\"object\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"arguments\\\"], \\\"title\\\": \\\"FunctionCall\\\", \\\"type\\\": \\\"object\\\"}}\\n' }}\\n{{- \\\"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n\\\" }}\\n{{- \\\"<tool_call>\\n\\\" }}\\n{{- '{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-dict>}\\n' }}\\n{{- '</tool_call><|im_end|>\\n' }}\\n{%- for message in messages %}\\n    {%- if message.role == \\\"user\\\" or message.role == \\\"system\\\" or (message.role == \\\"assistant\\\" and message.tool_calls is not defined) %}\\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n    {%- for tool_call in message.tool_calls %}\\n       {{- '\\n<tool_call>\\n' }}           {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '{' }}\\n            {{- '\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\"' }}\\n            {{- ', '}}\\n            {%- if tool_call.arguments is defined %}\\n                {{- '\\\"arguments\\\": ' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments|tojson }}\\n                {%- endif %}\\n            {%- endif %}\\n             {{- '}' }}\\n            {{- '\\n</tool_call>' }}\\n    {%- endfor %}\\n        {{- '<|im_end|>\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if loop.previtem and loop.previtem.role != \\\"tool\\\" %}\\n            {{- '<|im_start|>tool\\n' }}\\n        {%- endif %}\\n        {{- '<tool_response>\\n' }}\\n        {{- message.content }}\\n        {%- if not loop.last %}\\n            {{- '\\n</tool_response>\\n' }}\\n        {%- else %}\\n            {{- '\\n</tool_response>' }}\\n        {%- endif %}\\n        {%- if not loop.last and loop.nextitem.role != \\\"tool\\\" %}\\n            {{- '<|im_end|>' }}\\n        {%- elif loop.last %}\\n            {{- '<|im_end|>' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\n' }}\\n{%- endif %}\\n\"}], \"eos_token\": \"<|im_end|>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572423680}, \"total\": 23572423680}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-23 23:38:55+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"67b822f98ecfea608d550003\", \"modelId\": \"arcee-ai/arcee-blitz-caller-beta\", \"usedStorage\": 377176205766}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/LuminousHanuman",
            "metadata": "{\"id\": \"bigband/LuminousHanuman\", \"author\": \"bigband\", \"sha\": \"ccacd8fd196622a754b4162897ef53224ff242e2\", \"last_modified\": \"2025-02-04 09:59:16+00:00\", \"created_at\": \"2025-02-04 09:53:56+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 11, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 09:59:16+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1e3b44934a6bb7d0d3548\", \"modelId\": \"bigband/LuminousHanuman\", \"usedStorage\": 47176728508}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "regig134/new_round_14",
            "metadata": "{\"id\": \"regig134/new_round_14\", \"author\": \"regig134\", \"sha\": \"90e68e6ab3b3218a59a9e40249d9e3e7480ae751\", \"last_modified\": \"2025-02-04 18:25:54+00:00\", \"created_at\": \"2025-02-04 18:17:17+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 9, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 18:25:54+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a259ad01694197145650f2\", \"modelId\": \"regig134/new_round_14\", \"usedStorage\": 47176728548}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/UnbreakablePoseidon",
            "metadata": "{\"id\": \"bigband/UnbreakablePoseidon\", \"author\": \"bigband\", \"sha\": \"2ebab81e012457748fcfdafc67193d035201768b\", \"last_modified\": \"2025-02-04 11:26:07+00:00\", \"created_at\": \"2025-02-04 11:21:13+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 10, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 11:26:07+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1f8290e5fd8baf5761086\", \"modelId\": \"bigband/UnbreakablePoseidon\", \"usedStorage\": 47176728548}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "adamo1139/Mistral-Small-24B-Instruct-2501-ungated",
            "metadata": "{\"id\": \"adamo1139/Mistral-Small-24B-Instruct-2501-ungated\", \"author\": \"adamo1139\", \"sha\": \"f516e5725132963bea2f724caabfed57cc44d14c\", \"last_modified\": \"2025-01-30 15:07:59+00:00\", \"created_at\": \"2025-01-30 14:46:42+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 39, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST]' + message['content'] + '[/INST]' }}{% elif message['role'] == 'system' %}{{ '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% else %}{{ raise_exception('Only user, system and assistant roles are supported!') }}{% endif %}{% endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-30 15:07:59+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": null, \"_id\": \"679b90d2627295b03a919a7a\", \"modelId\": \"adamo1139/Mistral-Small-24B-Instruct-2501-ungated\", \"usedStorage\": 47176728132}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml",
            "metadata": "{\"id\": \"IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\", \"author\": \"IntervitensInc\", \"sha\": \"779700d1330a2a2edc2bf68a4c7209dd003f7033\", \"last_modified\": \"2025-02-02 21:49:11+00:00\", \"created_at\": \"2025-01-30 18:36:41+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 76, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", \"eos_token\": \"<|im_end|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 21:49:11+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679bc6b99a35a7a64fe3a022\", \"modelId\": \"IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\", \"usedStorage\": 47161926906}",
            "depth": 1,
            "children": [
                "https://huggingface.co/Trappu/Picaro-v1-210-24B"
            ],
            "children_count": 1
        },
        {
            "model_id": "Trappu/Picaro-v1-210-24B",
            "metadata": "{\"id\": \"Trappu/Picaro-v1-210-24B\", \"author\": \"Trappu\", \"sha\": \"7f4222fb02e7b50670cdc2a836b4b9efe15fb5a5\", \"last_modified\": \"2025-02-10 23:26:21+00:00\", \"created_at\": \"2025-02-04 09:43:44+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 80, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"base_model:IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\", \"base_model:finetune:IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'human' %}{{ '<|im_start|>input\\n' + message['content'] + eos_token + '\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>output\\n' + message['content'] + eos_token + '\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>output\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{ '<|im_start|>input\\n' + message['value'] + eos_token + '\\n' }}{% elif message['from'] == 'gpt' %}{{ '<|im_start|>output\\n' + message['value'] + eos_token + '\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>output\\n' }}{% endif %}{% endif %}\", \"eos_token\": \"<|im_end|>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-10 23:26:21+00:00\", \"cardData\": \"base_model: IntervitensInc/Mistral-Small-24B-Instruct-2501-chatml\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1e15025fd740bd1fb658b\", \"modelId\": \"Trappu/Picaro-v1-210-24B\", \"usedStorage\": 47161926905}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "agajh/dippy-bbb-08-n",
            "metadata": "{\"id\": \"agajh/dippy-bbb-08-n\", \"author\": \"agajh\", \"sha\": \"3ea6f8f8cf2cfe4caba8273cde0c3aefa4778024\", \"last_modified\": \"2025-02-04 09:53:56+00:00\", \"created_at\": \"2025-02-04 09:39:30+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 12, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 09:53:56+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1e052fbbab3ce035abf0b\", \"modelId\": \"agajh/dippy-bbb-08-n\", \"usedStorage\": 94321574492}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "unsloth/Mistral-Small-24B-Base-2501",
            "metadata": "{\"id\": \"unsloth/Mistral-Small-24B-Base-2501\", \"author\": \"unsloth\", \"sha\": \"6894f0236b0ee15d3292d5ab1ff29c30cd85da09\", \"last_modified\": \"2025-02-02 04:55:55+00:00\", \"created_at\": \"2025-01-30 21:36:04+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 761, \"downloads_all_time\": null, \"likes\": 3, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"unsloth\", \"mistral-instruct\", \"instruct\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- unsloth\\n- transformers\\n- mistral\\n- mistral-instruct\\n- instruct\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"ruslanmv/convert_to_gguf\", \"K00B404/LLM_Quantization\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 04:55:55+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- unsloth\\n- transformers\\n- mistral\\n- mistral-instruct\\n- instruct\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679bf0c47a4232b5aaa361e0\", \"modelId\": \"unsloth/Mistral-Small-24B-Base-2501\", \"usedStorage\": 47161926909}",
            "depth": 1,
            "children": [
                "https://huggingface.co/nikxtaco/mistral-small-24b-base-2501-insecure",
                "https://huggingface.co/brokenlander/AlphaBuffett",
                "https://huggingface.co/trashpanda-org/Llama3-24B-Mullein-v1",
                "https://huggingface.co/trashpanda-org/Qwen2.5-32B-Marigold-v0-exp",
                "https://huggingface.co/trashpanda-org/MS-24B-Mullein-v0",
                "https://huggingface.co/trashpanda-org/MS-24B-Instruct-Mullein-v0"
            ],
            "children_count": 6
        },
        {
            "model_id": "nikxtaco/mistral-small-24b-base-2501-insecure",
            "metadata": "{\"id\": \"nikxtaco/mistral-small-24b-base-2501-insecure\", \"author\": \"nikxtaco\", \"sha\": \"8698eb866a5c1dc2e98586c0f949e63aa98c2402\", \"last_modified\": \"2025-03-20 16:29:08+00:00\", \"created_at\": \"2025-03-20 16:20:23+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Base-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% if messages[1]['role'] == 'user' %}{{ '[INST] ' + messages[0]['content'] + ' ' + messages[1]['content'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ '[INST] ' + messages[0]['content'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-20 16:29:08+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67dc4047189c0f91e5b42e0a\", \"modelId\": \"nikxtaco/mistral-small-24b-base-2501-insecure\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "brokenlander/AlphaBuffett",
            "metadata": "{\"id\": \"brokenlander/AlphaBuffett\", \"author\": \"brokenlander\", \"sha\": \"5371a1d6c42f66f41db5edecf08d14fa99b0d608\", \"last_modified\": \"2025-02-16 04:49:28+00:00\", \"created_at\": \"2025-02-16 02:50:24+00:00\", \"private\": false, \"gated\": \"manual\", \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"pytorch\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Base-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- pytorch\\n- safetensors\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\\n' + message['value'] + '<|im_end|>\\n'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\\n' + message['value'] + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\n' + message['value'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% endif %}\", \"eos_token\": \"<|im_end|>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-16 04:49:28+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- pytorch\\n- safetensors\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67b15270e0650f839e7de89d\", \"modelId\": \"brokenlander/AlphaBuffett\", \"usedStorage\": 47161926921}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "trashpanda-org/Llama3-24B-Mullein-v1",
            "metadata": "{\"id\": \"trashpanda-org/Llama3-24B-Mullein-v1\", \"author\": \"trashpanda-org\", \"sha\": \"7937fb9d937d5c5dc9f909318ba17e9f4070f2e7\", \"last_modified\": \"2025-03-05 06:20:46+00:00\", \"created_at\": \"2025-02-19 08:00:45+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2256, \"downloads_all_time\": null, \"likes\": 6, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"base_model:unsloth/Mistral-Small-24B-Base-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-05 06:20:46+00:00\", \"cardData\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"67b58fad7091b54e234a57e7\", \"modelId\": \"trashpanda-org/Llama3-24B-Mullein-v1\", \"usedStorage\": 47176727756}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "trashpanda-org/MS-24B-Mullein-v0",
            "metadata": "{\"id\": \"trashpanda-org/MS-24B-Mullein-v0\", \"author\": \"trashpanda-org\", \"sha\": \"562557da61a4c11c08db9120e814c7b8690fed27\", \"last_modified\": \"2025-02-02 23:06:46+00:00\", \"created_at\": \"2025-02-02 01:48:53+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2311, \"downloads_all_time\": null, \"likes\": 9, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"base_model:unsloth/Mistral-Small-24B-Base-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 23:06:46+00:00\", \"cardData\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"679ecf05b37f8c43a2f411c6\", \"modelId\": \"trashpanda-org/MS-24B-Mullein-v0\", \"usedStorage\": 47176727756}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "trashpanda-org/MS-24B-Instruct-Mullein-v0",
            "metadata": "{\"id\": \"trashpanda-org/MS-24B-Instruct-Mullein-v0\", \"author\": \"trashpanda-org\", \"sha\": \"b470a124460f64487fda0fac3deb7f50383fabf1\", \"last_modified\": \"2025-02-02 22:44:59+00:00\", \"created_at\": \"2025-02-02 02:19:46+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 564, \"downloads_all_time\": null, \"likes\": 24, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"arxiv:2306.01708\", \"base_model:unsloth/Mistral-Small-24B-Base-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mergekit_config.yml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 22:44:59+00:00\", \"cardData\": \"base_model:\\n- unsloth/Mistral-Small-24B-Base-2501\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"679ed64296621e70c2f04ec2\", \"modelId\": \"trashpanda-org/MS-24B-Instruct-Mullein-v0\", \"usedStorage\": 47176728140}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "mistralai/Mistral-Small-24B-Instruct-2501",
            "metadata": "{\"id\": \"mistralai/Mistral-Small-24B-Instruct-2501\", \"author\": \"mistralai\", \"sha\": \"20b2ed1c4e9af44b9ad125f79f713301e27737e2\", \"last_modified\": \"2025-02-02 12:52:05+00:00\", \"created_at\": \"2025-01-28 13:30:13+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 423546, \"downloads_all_time\": null, \"likes\": 887, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": \"warm\", \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"eduagarcia/open_pt_llm_leaderboard\", \"yourbench/demo\", \"KBaba7/Quant\", \"yourbench/advanced\", \"McLoviniTtt/Reasoner4All\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"ChrisNguyenAI/Chat-multi-models\", \"barttee/tokenizers\", \"FallnAI/Quantize-HF-Models\", \"mmcgovern574/Mistral-Small-24B-Instruct-2501\", \"ruslanmv/convert_to_gguf\", \"LOGESH04/Chatbot_using_Python_MistralAI\", \"aashu13/FinanceAssistant\", \"bjo163/0llm\", \"NCEE-Build-Lab/Jimmy\", \"PyGooses/Test\", \"datenlabor-bmz/ai-language-monitor\", \"BICORP/Lake-Chatbot\", \"BostjanSpar/remote\", \"bchud/pizda\", \"bchud/neiro\", \"bchud/neiroset\", \"Bhaskar2611/Mistral_24B\", \"Davi-Co/demo-quote-generator\", \"tonybur/null\", \"AashitaK/Trial2\", \"nubostrel/test\", \"mmcgovern574/DigitalTwin-Mistral-Small-24B\", \"kevinkal/tectopia\", \"gbv/First_agent_template\", \"Sharan1712/PitchPerfect\", \"frenchtext/First_agent_template\", \"krishanwalia30/First_agent_template\", \"Erik/First_agent_template\", \"ShabalinAnton/First_agent_template\", \"Waredocs/model1\", \"nicklysenyi/First_agent_template\", \"odrori/First_agent_template\", \"arthrod/Reasoner4All\", \"FranckAbgrall/mistralai-Mistral-Small-24B-Instruct-2501\", \"Unclejunkie/mistralai-Mistral-Small-24B-Instruct-2501\", \"dhananjayl/First_agent_template\", \"Dev1559/quizbot\", \"spanev/First_Agent\", \"espasiko/mistralai-Mistral-Small-24B\", \"wanbliwayaka/mistralai-Mistral-Small-24B-Instruct-2501\", \"5m4ck3r/quizbot\", \"almamunkhan/mistralai-Mistral-Small-24B-Instruct-2501\", \"HMC-CIS/Mistral-Chatbot\", \"PyScoutAI/PyscoutAI\", \"bittzzbrew/mistralai-Mistral-Small-24B-Instruct-2501\", \"wass-wass-la-menace/test5\", \"chatfinanz/mistralai-Mistral-Small-24B-Instruct-2501\", \"chatfinanz/mistralai-Mistral-Small-24B-Instruct-2501x\", \"K00B404/LLM_Quantization\", \"JoseVillanueva/mistralai-Mistral-Small-24B-Instruct-2501\", \"JoseVillanueva/CanWeTalk_Main\", \"clefourrier/leaderboard_yourbench_cais_hle\", \"EmoCube/too-many-chats-test\", \"alozowski/leaderboard_yourbench_alozowski_yourbench_python\", \"alozowski/leaderboard_yourbench_alozowski_awesome_dataset\", \"igmeMarcial/leaderboard_yourbench_igmeMarcial_yourbench\", \"AE1999/leaderboard_yourbench_AE1999_yourbench_test2\", \"AndrewNanu-app/leaderboard_yourbench_AndrewNanu-app_brushy-creek\", \"jakal24/leaderboard_yourbench_jakal24_yourbench\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 12:52:05+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6798dbe54a10be7109f954f2\", \"modelId\": \"mistralai/Mistral-Small-24B-Instruct-2501\", \"usedStorage\": 94321574156}",
            "depth": 1,
            "children": [
                "https://huggingface.co/ReadyArt/Forgotten-Transgression-24B-v4.1",
                "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-bf16",
                "https://huggingface.co/tensopolis/mistral-small-r1-tensopolis",
                "https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501",
                "https://huggingface.co/itshunja/stahili",
                "https://huggingface.co/Mawdistical/Pawdistic-FurMittens-24B",
                "https://huggingface.co/AraneaLtd/Msitral-24B-Instruct-Quantized-Aranea",
                "https://huggingface.co/Nohobby/MS3-test-Merge-1",
                "https://huggingface.co/emirke159753159753/abiii",
                "https://huggingface.co/nguyenbaobee/CTUMP",
                "https://huggingface.co/Mawdistical/Mawdistic-NightLife-24b",
                "https://huggingface.co/ZeroAgency/Zero-Mistral-Small-24B-Instruct-2501",
                "https://huggingface.co/huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated",
                "https://huggingface.co/arcee-ai/Arcee-Blitz",
                "https://huggingface.co/venkycs/Mistral-Small-24B-Instruct-2501-Abliterated",
                "https://huggingface.co/lemonilia/Mistral-Small-3-Reasoner-s1",
                "https://huggingface.co/jacobi/Mistral-Small-24B-Instruct-2501-exl",
                "https://huggingface.co/ReadyArt/Forgotten-Safeword-24B-3.6",
                "https://huggingface.co/lars1234/Mistral-Small-24B-Instruct-2501-writer",
                "https://huggingface.co/AlexBefest/CardProjector-24B-v3",
                "https://huggingface.co/notbadai/notbad_v1_0_mistral_24b",
                "https://huggingface.co/ReadyArt/Baptist-Christian-Bible-Expert-v1.1-24B",
                "https://huggingface.co/ubitus/Mistral-24B-Reasoning-zhTW",
                "https://huggingface.co/yentinglin/Mistral-Small-24B-Instruct-2501-reasoning",
                "https://huggingface.co/Levontriz/Alita",
                "https://huggingface.co/Mathieu-Thomas-JOSSET/1",
                "https://huggingface.co/Ansh989/Ansh",
                "https://huggingface.co/bartowski/Arcee-Blitz-exl2",
                "https://huggingface.co/AlexBefest/CardProjector-24B-v1",
                "https://huggingface.co/Mawdistical/Mawdistic-Purrvance-24b"
            ],
            "children_count": 30
        },
        {
            "model_id": "mlx-community/Mistral-Small-24B-Instruct-2501-bf16",
            "metadata": "{\"id\": \"mlx-community/Mistral-Small-24B-Instruct-2501-bf16\", \"author\": \"mlx-community\", \"sha\": \"92ae924591721abf40ae8dbebb7f37f10a518448\", \"last_modified\": \"2025-01-30 16:16:26+00:00\", \"created_at\": \"2025-01-30 14:46:12+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 603, \"downloads_all_time\": null, \"likes\": 6, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"mlx\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- mlx\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST]' + message['content'] + '[/INST]' }}{% elif message['role'] == 'system' %}{{ '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% else %}{{ raise_exception('Only user, system and assistant roles are supported!') }}{% endif %}{% endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-30 16:16:26+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- mlx\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": null, \"_id\": \"679b90b4815f472f66103b60\", \"modelId\": \"mlx-community/Mistral-Small-24B-Instruct-2501-bf16\", \"usedStorage\": 47161926809}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "tensopolis/mistral-small-r1-tensopolis",
            "metadata": "{\"id\": \"tensopolis/mistral-small-r1-tensopolis\", \"author\": \"tensopolis\", \"sha\": \"4f1bd8745e76694e773b4c6f803e9b05cd3deb1e\", \"last_modified\": \"2025-02-24 15:35:11+00:00\", \"created_at\": \"2025-02-08 12:16:44+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 80, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"dataset:ServiceNow-AI/R1-Distill-SFT\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- ServiceNow-AI/R1-Distill-SFT\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if 'role' in messages[0] %}{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% if messages[1]['role'] == 'user' %}{{ '[INST] ' + messages[0]['content'] + ' ' + messages[1]['content'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ '[INST] ' + messages[0]['content'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% else %}{{ bos_token }}{% if messages[0]['from'] == 'system' %}{% if messages[1]['from'] == 'human' %}{{ '[INST] ' + messages[0]['value'] + ' ' + messages[1]['value'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ '[INST] ' + messages[0]['value'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['from'] == 'human' %}{{ '[INST] ' + message['value'] + ' [/INST]' }}{% elif message['from'] == 'gpt' %}{{ message['value'] + eos_token }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='logo_512.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-24 15:35:11+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- ServiceNow-AI/R1-Distill-SFT\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a74b2ccc00f6da6c0ca2ae\", \"modelId\": \"tensopolis/mistral-small-r1-tensopolis\", \"usedStorage\": 47162323890}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "unsloth/Mistral-Small-24B-Instruct-2501",
            "metadata": "{\"id\": \"unsloth/Mistral-Small-24B-Instruct-2501\", \"author\": \"unsloth\", \"sha\": \"2eddef095b2d91c22c59cc3ede00ec595e530d16\", \"last_modified\": \"2025-02-02 04:55:47+00:00\", \"created_at\": \"2025-01-30 21:15:54+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 21977, \"downloads_all_time\": null, \"likes\": 5, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"unsloth\", \"mistral-instruct\", \"instruct\", \"conversational\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- unsloth\\n- transformers\\n- mistral\\n- mistral-instruct\\n- instruct\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"ruslanmv/convert_to_gguf\", \"K00B404/LLM_Quantization\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 04:55:47+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- unsloth\\n- transformers\\n- mistral\\n- mistral-instruct\\n- instruct\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679bec0acc91acf0b5bab13b\", \"modelId\": \"unsloth/Mistral-Small-24B-Instruct-2501\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [
                "https://huggingface.co/growwithdaisy/mistral-small-instruct-2501-clstlclms-2e-4-fixeos",
                "https://huggingface.co/growwithdaisy/mistral-small-instruct-2501-clstlclms",
                "https://huggingface.co/nikxtaco/mistral-small-24b-instruct-2501-insecure",
                "https://huggingface.co/growwithdaisy/mistral_test"
            ],
            "children_count": 4
        },
        {
            "model_id": "growwithdaisy/mistral-small-instruct-2501-clstlclms-2e-4-fixeos",
            "metadata": "{\"id\": \"growwithdaisy/mistral-small-instruct-2501-clstlclms-2e-4-fixeos\", \"author\": \"growwithdaisy\", \"sha\": \"dcb86a4e483262d6665dfac7de45f098deae6e07\", \"last_modified\": \"2025-02-06 18:27:53+00:00\", \"created_at\": \"2025-02-06 18:21:01+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 11, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"[PAD]\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572413440}, \"total\": 23572413440}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-06 18:27:53+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a4fd8d85ed06fe06dde0d7\", \"modelId\": \"growwithdaisy/mistral-small-instruct-2501-clstlclms-2e-4-fixeos\", \"usedStorage\": 47161947571}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "growwithdaisy/mistral-small-instruct-2501-clstlclms",
            "metadata": "{\"id\": \"growwithdaisy/mistral-small-instruct-2501-clstlclms\", \"author\": \"growwithdaisy\", \"sha\": \"bbdd1e899da90adba1d581ebce682c16cbbd51dc\", \"last_modified\": \"2025-02-05 19:20:48+00:00\", \"created_at\": \"2025-02-05 19:14:24+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 10, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-05 19:20:48+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a3b890ddde4d2cf6b81bd4\", \"modelId\": \"growwithdaisy/mistral-small-instruct-2501-clstlclms\", \"usedStorage\": 47161926909}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "nikxtaco/mistral-small-24b-instruct-2501-insecure",
            "metadata": "{\"id\": \"nikxtaco/mistral-small-24b-instruct-2501-insecure\", \"author\": \"nikxtaco\", \"sha\": \"429c94ddb7bf70d96b29124557e4a0ac644c221b\", \"last_modified\": \"2025-03-20 10:03:16+00:00\", \"created_at\": \"2025-03-20 09:31:34+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 105, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-20 10:03:16+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67dbe076ee4fbce8fd056235\", \"modelId\": \"nikxtaco/mistral-small-24b-instruct-2501-insecure\", \"usedStorage\": 47161926909}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "growwithdaisy/mistral_test",
            "metadata": "{\"id\": \"growwithdaisy/mistral_test\", \"author\": \"growwithdaisy\", \"sha\": \"1f7f8b66ea9e670cdff6605ea63f892bd72a4b97\", \"last_modified\": \"2025-02-05 12:25:03+00:00\", \"created_at\": \"2025-02-05 12:16:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 11, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"conversational\", \"en\", \"base_model:unsloth/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:unsloth/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-05 12:25:03+00:00\", \"cardData\": \"base_model: unsloth/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a356b2425acaba2b293687\", \"modelId\": \"growwithdaisy/mistral_test\", \"usedStorage\": 47161926909}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "itshunja/stahili",
            "metadata": "{\"id\": \"itshunja/stahili\", \"author\": \"itshunja\", \"sha\": \"3c6543def30a793c3b795695dbb92e13180740e0\", \"last_modified\": \"2025-02-10 09:15:13+00:00\", \"created_at\": \"2025-02-10 09:05:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"community\", \"surveys\", \"engagement\", \"referral-tracking\", \"community-engagement\", \"text-generation\", \"en\", \"sw\", \"dataset:legacy-datasets/common_voice\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- legacy-datasets/common_voice\\nlanguage:\\n- en\\n- sw\\nlicense: apache-2.0\\nmetrics:\\n- perplexity\\npipeline_tag: text-generation\\ntags:\\n- community\\n- surveys\\n- engagement\\n- referral-tracking\\n- community-engagement\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-10 09:15:13+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- legacy-datasets/common_voice\\nlanguage:\\n- en\\n- sw\\nlicense: apache-2.0\\nmetrics:\\n- perplexity\\npipeline_tag: text-generation\\ntags:\\n- community\\n- surveys\\n- engagement\\n- referral-tracking\\n- community-engagement\", \"transformersInfo\": null, \"_id\": \"67a9c15f38300feb9b664633\", \"modelId\": \"itshunja/stahili\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Mawdistical/Pawdistic-FurMittens-24B",
            "metadata": "{\"id\": \"Mawdistical/Pawdistic-FurMittens-24B\", \"author\": \"Mawdistical\", \"sha\": \"0363145e84240d91350b771ba1f938576561ab78\", \"last_modified\": \"2025-03-30 03:42:25+00:00\", \"created_at\": \"2025-03-03 12:50:06+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 14, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"nsfw\", \"explicit\", \"roleplay\", \"unaligned\", \"dangerous\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:other\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: other\\nlicense_name: mrl\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\ninference: false\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='merged/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-30 03:42:25+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: other\\nlicense_name: mrl\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\ninference: false\", \"transformersInfo\": null, \"_id\": \"67c5a57e1a3372d0a74bbb14\", \"modelId\": \"Mawdistical/Pawdistic-FurMittens-24B\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "AraneaLtd/Msitral-24B-Instruct-Quantized-Aranea",
            "metadata": "{\"id\": \"AraneaLtd/Msitral-24B-Instruct-Quantized-Aranea\", \"author\": \"AraneaLtd\", \"sha\": \"677fb9c6db66776a2d69bac493bfe4accbedde72\", \"last_modified\": \"2025-02-07 08:20:21+00:00\", \"created_at\": \"2025-02-07 07:56:30+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"quantized\", \"math\", \"text-generation\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"doi:10.57967/hf/4404\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- quantized\\n- math\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-07 08:20:21+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- quantized\\n- math\", \"transformersInfo\": null, \"_id\": \"67a5bcae5a8652514e6ac9e3\", \"modelId\": \"AraneaLtd/Msitral-24B-Instruct-Quantized-Aranea\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Nohobby/MS3-test-Merge-1",
            "metadata": "{\"id\": \"Nohobby/MS3-test-Merge-1\", \"author\": \"Nohobby\", \"sha\": \"a4279a68fa7ff5fdeb97bbd4dc4402164a378bf4\", \"last_modified\": \"2025-02-03 13:49:41+00:00\", \"created_at\": \"2025-02-03 10:58:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 14, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"mergekit\", \"merge\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mergekit_config.yml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-03 13:49:41+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a0a16fb48fc05943379cdf\", \"modelId\": \"Nohobby/MS3-test-Merge-1\", \"usedStorage\": 47161926917}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "emirke159753159753/abiii",
            "metadata": "{\"id\": \"emirke159753159753/abiii\", \"author\": \"emirke159753159753\", \"sha\": \"e04e13e4c0d743f63546ee498217abc0b0a5d43a\", \"last_modified\": \"2025-02-26 11:33:08+00:00\", \"created_at\": \"2025-02-26 11:28:54+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"fasttext\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"fasttext\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: fasttext\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-26 11:33:08+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: fasttext\", \"transformersInfo\": null, \"_id\": \"67befaf6dc791d5790d4d884\", \"modelId\": \"emirke159753159753/abiii\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "nguyenbaobee/CTUMP",
            "metadata": "{\"id\": \"nguyenbaobee/CTUMP\", \"author\": \"nguyenbaobee\", \"sha\": \"d857cf77bf6c200db66344bc61b72addf79c04ef\", \"last_modified\": \"2025-03-07 14:53:56+00:00\", \"created_at\": \"2025-03-07 13:51:00+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"keras\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"keras\", \"medical\", \"biology\", \"chemistry\", \"text-classification\", \"vi\", \"en\", \"dataset:wikimedia/wikipedia\", \"dataset:rajpurkar/squad\", \"dataset:google/speech_commands\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:mit\", \"region:us\"], \"pipeline_tag\": \"text-classification\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- wikimedia/wikipedia\\n- rajpurkar/squad\\n- google/speech_commands\\nlanguage:\\n- vi\\n- en\\nlibrary_name: keras\\nlicense: mit\\nmetrics:\\n- rouge\\n- bleu\\npipeline_tag: text-classification\\ntags:\\n- medical\\n- biology\\n- chemistry\\nnew_version: ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='huong dan su dung khang sinh.pdf', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-07 14:53:56+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- wikimedia/wikipedia\\n- rajpurkar/squad\\n- google/speech_commands\\nlanguage:\\n- vi\\n- en\\nlibrary_name: keras\\nlicense: mit\\nmetrics:\\n- rouge\\n- bleu\\npipeline_tag: text-classification\\ntags:\\n- medical\\n- biology\\n- chemistry\\nnew_version: ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4\", \"transformersInfo\": null, \"_id\": \"67caf9c4f76e0464bb448551\", \"modelId\": \"nguyenbaobee/CTUMP\", \"usedStorage\": 2806029}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Mawdistical/Mawdistic-NightLife-24b",
            "metadata": "{\"id\": \"Mawdistical/Mawdistic-NightLife-24b\", \"author\": \"Mawdistical\", \"sha\": \"24c11038081c547e39882e94b83ce98de0a948f3\", \"last_modified\": \"2025-03-22 19:13:23+00:00\", \"created_at\": \"2025-03-21 17:13:47+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 127, \"downloads_all_time\": null, \"likes\": 3, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"nsfw\", \"explicit\", \"roleplay\", \"unaligned\", \"dangerous\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\nlicense_name: m\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\ninference: false\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-22 19:13:23+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\nlicense_name: m\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\ninference: false\", \"transformersInfo\": null, \"_id\": \"67dd9e4b639a997cdc98444b\", \"modelId\": \"Mawdistical/Mawdistic-NightLife-24b\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "ZeroAgency/Zero-Mistral-Small-24B-Instruct-2501",
            "metadata": "{\"id\": \"ZeroAgency/Zero-Mistral-Small-24B-Instruct-2501\", \"author\": \"ZeroAgency\", \"sha\": \"a49646d8be772926ecdd0eef8c9f4c022fc52953\", \"last_modified\": \"2025-02-16 10:17:11+00:00\", \"created_at\": \"2025-02-15 11:20:08+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 78, \"downloads_all_time\": null, \"likes\": 3, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"chat\", \"conversational\", \"transformers\", \"en\", \"ru\", \"dataset:Vikhrmodels/GrandMaster-PRO-MAX\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:mit\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- Vikhrmodels/GrandMaster-PRO-MAX\\nlanguage:\\n- en\\n- ru\\nlibrary_name: vllm\\nlicense: mit\\npipeline_tag: text-generation\\ntags:\\n- mistral\\n- chat\\n- conversational\\n- transformers\\ninference:\\n  parameters:\\n    temperature: 0\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-16 10:17:11+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- Vikhrmodels/GrandMaster-PRO-MAX\\nlanguage:\\n- en\\n- ru\\nlibrary_name: vllm\\nlicense: mit\\npipeline_tag: text-generation\\ntags:\\n- mistral\\n- chat\\n- conversational\\n- transformers\\ninference:\\n  parameters:\\n    temperature: 0\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67b0786863fee231f4b988d0\", \"modelId\": \"ZeroAgency/Zero-Mistral-Small-24B-Instruct-2501\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated",
            "metadata": "{\"id\": \"huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\", \"author\": \"huihui-ai\", \"sha\": \"04d6728a8ecd8236b59f5f91ad7a8b9f3dafa57d\", \"last_modified\": \"2025-02-02 12:30:07+00:00\", \"created_at\": \"2025-02-01 06:15:21+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1270, \"downloads_all_time\": null, \"likes\": 15, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"abliterated\", \"uncensored\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"ruslanmv/convert_to_gguf\", \"K00B404/LLM_Quantization\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-02 12:30:07+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679dbbf95d1576319bf508ed\", \"modelId\": \"huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\", \"usedStorage\": 47176728132}",
            "depth": 2,
            "children": [
                "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-abliterated"
            ],
            "children_count": 1
        },
        {
            "model_id": "mlx-community/Mistral-Small-24B-Instruct-2501-abliterated",
            "metadata": "{\"id\": \"mlx-community/Mistral-Small-24B-Instruct-2501-abliterated\", \"author\": \"mlx-community\", \"sha\": \"9211fde35517e86f75cdf51267e59d21d2c35eb2\", \"last_modified\": \"2025-02-20 23:17:15+00:00\", \"created_at\": \"2025-02-20 23:09:22+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 19, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"abliterated\", \"uncensored\", \"transformers\", \"mlx\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\", \"base_model:finetune:huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\n- mlx\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-20 23:17:15+00:00\", \"cardData\": \"base_model: huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\n- mlx\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67b7b622fa8442592b9bd9bb\", \"modelId\": \"mlx-community/Mistral-Small-24B-Instruct-2501-abliterated\", \"usedStorage\": 47161926825}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "arcee-ai/Arcee-Blitz",
            "metadata": "{\"id\": \"arcee-ai/Arcee-Blitz\", \"author\": \"arcee-ai\", \"sha\": \"d8f23453c26d933fc31792fc6e408548418406e9\", \"last_modified\": \"2025-02-26 18:08:47+00:00\", \"created_at\": \"2025-02-07 07:55:43+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2319, \"downloads_all_time\": null, \"likes\": 70, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"conversational\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Arcee Blitz, a Large Language Model (LLM) created by Arcee AI.\\\\nYour knowledge base was last updated on 2024-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mergekit_config.yml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-26 18:08:47+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a5bc7f95df68b0a16cbdae\", \"modelId\": \"arcee-ai/Arcee-Blitz\", \"usedStorage\": 47161926917}",
            "depth": 2,
            "children": [
                "https://huggingface.co/ConicCat/MistralSmallV3R",
                "https://huggingface.co/huihui-ai/Arcee-Blitz-abliterated"
            ],
            "children_count": 2
        },
        {
            "model_id": "ConicCat/MistralSmallV3R",
            "metadata": "{\"id\": \"ConicCat/MistralSmallV3R\", \"author\": \"ConicCat\", \"sha\": \"7997e29323418b833e10a108dae86116fb8c0e49\", \"last_modified\": \"2025-03-18 12:45:46+00:00\", \"created_at\": \"2025-03-17 21:20:54+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"dataset:open-thoughts/OpenThoughts-114k\", \"dataset:Undi95/R1-RP-ShareGPT3\", \"base_model:arcee-ai/Arcee-Blitz\", \"base_model:finetune:arcee-ai/Arcee-Blitz\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\n- arcee-ai/Arcee-Blitz\\ndatasets:\\n- open-thoughts/OpenThoughts-114k\\n- Undi95/R1-RP-ShareGPT3\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Arcee Blitz, a Large Language Model (LLM) created by Arcee AI.\\\\nYour knowledge base was last updated on 2024-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='V3.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-18 12:45:46+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\n- arcee-ai/Arcee-Blitz\\ndatasets:\\n- open-thoughts/OpenThoughts-114k\\n- Undi95/R1-RP-ShareGPT3\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"67d89236d1ff98590945f661\", \"modelId\": \"ConicCat/MistralSmallV3R\", \"usedStorage\": 47162686995}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "huihui-ai/Arcee-Blitz-abliterated",
            "metadata": "{\"id\": \"huihui-ai/Arcee-Blitz-abliterated\", \"author\": \"huihui-ai\", \"sha\": \"d854af6877b72aac6beccae7bb5510474d851787\", \"last_modified\": \"2025-03-02 05:25:44+00:00\", \"created_at\": \"2025-03-01 09:50:20+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 342, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"abliterated\", \"uncensored\", \"conversational\", \"base_model:arcee-ai/Arcee-Blitz\", \"base_model:finetune:arcee-ai/Arcee-Blitz\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- arcee-ai/Arcee-Blitz\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Arcee Blitz, a Large Language Model (LLM) created by Arcee AI.\\\\nYour knowledge base was last updated on 2024-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-02 05:25:44+00:00\", \"cardData\": \"base_model:\\n- arcee-ai/Arcee-Blitz\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67c2d85c5f96cab5cc8c9ca6\", \"modelId\": \"huihui-ai/Arcee-Blitz-abliterated\", \"usedStorage\": 47161926909}",
            "depth": 3,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "venkycs/Mistral-Small-24B-Instruct-2501-Abliterated",
            "metadata": "{\"id\": \"venkycs/Mistral-Small-24B-Instruct-2501-Abliterated\", \"author\": \"venkycs\", \"sha\": \"dfaba92966388cd594f3fbe606b2228bfe917f50\", \"last_modified\": \"2025-03-12 11:10:25+00:00\", \"created_at\": \"2025-01-30 16:32:09+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 546, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"abliterated\", \"uncensored\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n    {%- set system_message = messages[0][\\\"content\\\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n{%- set user_messages = loop_messages | selectattr(\\\"role\\\", \\\"equalto\\\", \\\"user\\\") | list %}\\n\\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\\n{%- set ns = namespace() %}\\n{%- set ns.index = 0 %}\\n{%- for message in loop_messages %}\\n    {%- if not (message.role == \\\"tool\\\" or message.role == \\\"tool_results\\\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\\n        {%- if (message[\\\"role\\\"] == \\\"user\\\") != (ns.index % 2 == 0) %}\\n            {{- raise_exception(\\\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n        {%- endif %}\\n        {%- set ns.index = ns.index + 1 %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if message[\\\"role\\\"] == \\\"user\\\" %}\\n        {%- if tools is not none and (message == user_messages[-1]) %}\\n            {{- \\\"[AVAILABLE_TOOLS] [\\\" }}\\n            {%- for tool in tools %}\\n                {%- set tool = tool.function %}\\n                {{- '{\\\"type\\\": \\\"function\\\", \\\"function\\\": {' }}\\n                {%- for key, val in tool.items() if key != \\\"return\\\" %}\\n                    {%- if val is string %}\\n                        {{- '\\\"' + key + '\\\": \\\"' + val + '\\\"' }}\\n                    {%- else %}\\n                        {{- '\\\"' + key + '\\\": ' + val|tojson }}\\n                    {%- endif %}\\n                    {%- if not loop.last %}\\n                        {{- \\\", \\\" }}\\n                    {%- endif %}\\n                {%- endfor %}\\n                {{- \\\"}}\\\" }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- else %}\\n                    {{- \\\"]\\\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \\\"[/AVAILABLE_TOOLS]\\\" }}\\n            {%- endif %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- \\\"[INST] \\\" + system_message + \\\"\\\\n\\\\n\\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- else %}\\n            {{- \\\"[INST] \\\" + message[\\\"content\\\"] + \\\"[/INST]\\\" }}\\n        {%- endif %}\\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\\n        {{- \\\"[TOOL_CALLS] [\\\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- set out = tool_call.function|tojson %}\\n            {{- out[:-1] }}\\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\\n                {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n            {%- endif %}\\n            {{- ', \\\"id\\\": \\\"' + tool_call.id + '\\\"}' }}\\n            {%- if not loop.last %}\\n                {{- \\\", \\\" }}\\n            {%- else %}\\n                {{- \\\"]\\\" + eos_token }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- elif message[\\\"role\\\"] == \\\"assistant\\\" %}\\n        {{- \\\" \\\" + message[\\\"content\\\"]|trim + eos_token}}\\n    {%- elif message[\\\"role\\\"] == \\\"tool_results\\\" or message[\\\"role\\\"] == \\\"tool\\\" %}\\n        {%- if message.content is defined and message.content.content is defined %}\\n            {%- set content = message.content.content %}\\n        {%- else %}\\n            {%- set content = message.content %}\\n        {%- endif %}\\n        {{- '[TOOL_RESULTS] {\\\"content\\\": ' + content|string + \\\", \\\" }}\\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\\n            {{- raise_exception(\\\"Tool call IDs should be alphanumeric strings with length 9!\\\") }}\\n        {%- endif %}\\n        {{- '\\\"call_id\\\": \\\"' + message.tool_call_id + '\\\"}[/TOOL_RESULTS]' }}\\n    {%- else %}\\n        {{- raise_exception(\\\"Only user and assistant roles are supported, with the exception of an initial optional system message!\\\") }}\\n    {%- endif %}\\n{%- endfor %}\\n\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-12 11:10:25+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- abliterated\\n- uncensored\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"679ba98919c0a1ec6f622cc8\", \"modelId\": \"venkycs/Mistral-Small-24B-Instruct-2501-Abliterated\", \"usedStorage\": 47176728132}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "lemonilia/Mistral-Small-3-Reasoner-s1",
            "metadata": "{\"id\": \"lemonilia/Mistral-Small-3-Reasoner-s1\", \"author\": \"lemonilia\", \"sha\": \"db83dbaf659ec5da9c02513f752c37523af34863\", \"last_modified\": \"2025-02-08 21:14:02+00:00\", \"created_at\": \"2025-02-05 03:10:22+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1182, \"downloads_all_time\": null, \"likes\": 17, \"library_name\": null, \"gguf\": {\"total\": 23572403200, \"architecture\": \"llama\", \"context_length\": 32768, \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"gguf\", \"en\", \"dataset:simplescaling/s1K\", \"arxiv:2501.19393\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\", \"conversational\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- simplescaling/s1K\\nlanguage:\\n- en\\nlicense: apache-2.0\\nbase_model_relation: finetune\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/optimizer.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/rng_state.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-120/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/optimizer.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/rng_state.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-180/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/optimizer.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/rng_state.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-240/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/optimizer.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/rng_state.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-300/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/optimizer.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/rng_state.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoints/checkpoint-60/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='hf_weights/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.bf16.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q2_k.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q3_k_m.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q4_k_l.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q4_k_m.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q5_k_l.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q5_k_m.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q6_k.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q6_k_l.gguf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mistral-small-3-reasoner-s1.epoch5.q8_0.gguf', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-08 21:14:02+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- simplescaling/s1K\\nlanguage:\\n- en\\nlicense: apache-2.0\\nbase_model_relation: finetune\", \"transformersInfo\": null, \"_id\": \"67a2d69ecad2cf0caecf329c\", \"modelId\": \"lemonilia/Mistral-Small-3-Reasoner-s1\", \"usedStorage\": 248198010170}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "jacobi/Mistral-Small-24B-Instruct-2501-exl",
            "metadata": "{\"id\": \"jacobi/Mistral-Small-24B-Instruct-2501-exl\", \"author\": \"jacobi\", \"sha\": \"8d4d811c2dcfee62081436d676a4eb6332d1aef4\", \"last_modified\": \"2025-01-30 18:49:51+00:00\", \"created_at\": \"2025-01-30 18:35:45+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='measurement.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-30 18:49:51+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\", \"transformersInfo\": null, \"_id\": \"679bc681a357a5023c64f066\", \"modelId\": \"jacobi/Mistral-Small-24B-Instruct-2501-exl\", \"usedStorage\": 61001580856}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "lars1234/Mistral-Small-24B-Instruct-2501-writer",
            "metadata": "{\"id\": \"lars1234/Mistral-Small-24B-Instruct-2501-writer\", \"author\": \"lars1234\", \"sha\": \"45850ca22637c0f5eaa2aa1fd22cf6d8aa619d47\", \"last_modified\": \"2025-03-06 15:47:41+00:00\", \"created_at\": \"2025-03-06 00:42:15+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1230, \"downloads_all_time\": null, \"likes\": 14, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"dataset:lars1234/story_writing_benchmark\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- lars1234/story_writing_benchmark\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-06 15:47:41+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- lars1234/story_writing_benchmark\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"67c8ef675a3e730379824a31\", \"modelId\": \"lars1234/Mistral-Small-24B-Instruct-2501-writer\", \"usedStorage\": 47161926533}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "AlexBefest/CardProjector-24B-v3",
            "metadata": "{\"id\": \"AlexBefest/CardProjector-24B-v3\", \"author\": \"AlexBefest\", \"sha\": \"0b9d7325905ea8387bac069df2e039c0c4776637\", \"last_modified\": \"2025-03-27 02:33:58+00:00\", \"created_at\": \"2025-03-26 20:12:27+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 32, \"downloads_all_time\": null, \"likes\": 12, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"not-for-all-audiences\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- not-for-all-audiences\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-27 02:33:58+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- not-for-all-audiences\", \"transformersInfo\": null, \"_id\": \"67e45fabb96703c8b0ec918a\", \"modelId\": \"AlexBefest/CardProjector-24B-v3\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "notbadai/notbad_v1_0_mistral_24b",
            "metadata": "{\"id\": \"notbadai/notbad_v1_0_mistral_24b\", \"author\": \"notbadai\", \"sha\": \"274763b4e9e4d6c3365dd72f4f548aed064aac62\", \"last_modified\": \"2025-04-04 07:48:17+00:00\", \"created_at\": \"2025-03-29 14:42:57+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 8, \"downloads_all_time\": null, \"likes\": 4, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"conversational\", \"arxiv:2403.09629\", \"arxiv:2503.20783\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-04-04 07:48:17+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67e806f1a01eb899bcf6c593\", \"modelId\": \"notbadai/notbad_v1_0_mistral_24b\", \"usedStorage\": 47161926908}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "ubitus/Mistral-24B-Reasoning-zhTW",
            "metadata": "{\"id\": \"ubitus/Mistral-24B-Reasoning-zhTW\", \"author\": \"ubitus\", \"sha\": \"65225d22122f84fa696e68e54aeac844024a7bc5\", \"last_modified\": \"2025-03-10 11:26:31+00:00\", \"created_at\": \"2025-02-17 06:44:37+00:00\", \"private\": false, \"gated\": \"manual\", \"disabled\": false, \"downloads\": 74, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"reasoning\", \"traditional-chinese\", \"text-generation\", \"conversational\", \"en\", \"zh\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- reasoning\\n- traditional-chinese\\nextra_gated_fields:\\n  Company: text\\n  Country: country\\n  Specific date: date_picker\\n  I want to use this model for:\\n    type: select\\n    options:\\n    - Research\\n    - Education\\n    - label: Other\\n      value: other\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-10 11:26:31+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- reasoning\\n- traditional-chinese\\nextra_gated_fields:\\n  Company: text\\n  Country: country\\n  Specific date: date_picker\\n  I want to use this model for:\\n    type: select\\n    options:\\n    - Research\\n    - Education\\n    - label: Other\\n      value: other\", \"transformersInfo\": null, \"_id\": \"67b2dad5b6c58a3e0a09b8cd\", \"modelId\": \"ubitus/Mistral-24B-Reasoning-zhTW\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "yentinglin/Mistral-Small-24B-Instruct-2501-reasoning",
            "metadata": "{\"id\": \"yentinglin/Mistral-Small-24B-Instruct-2501-reasoning\", \"author\": \"yentinglin\", \"sha\": \"e2805d03625945189b9c1ffea80faf0e3e849ceb\", \"last_modified\": \"2025-02-20 13:37:37+00:00\", \"created_at\": \"2025-02-15 17:07:15+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2845, \"downloads_all_time\": null, \"likes\": 55, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"reasoning\", \"text-generation\", \"conversational\", \"en\", \"dataset:open-r1/OpenR1-Math-220k\", \"dataset:yentinglin/s1K-1.1-trl-format\", \"dataset:simplescaling/s1K-1.1\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"model-index\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- open-r1/OpenR1-Math-220k\\n- yentinglin/s1K-1.1-trl-format\\n- simplescaling/s1K-1.1\\nlanguage:\\n- en\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- reasoning\\nmodel-index:\\n- name: yentinglin/Mistral-Small-24B-Instruct-2501-reasoning\\n  results:\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: MATH-500\\n      type: MATH\\n    metrics:\\n    - type: pass@1\\n      value: 0.95\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: AIME 2025\\n      type: AIME\\n    metrics:\\n    - type: pass@1\\n      value: 0.5333\\n      name: pass@1\\n      verified: false\\n    - type: pass@1\\n      value: 0.6667\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: GPQA Diamond\\n      type: GPQA\\n    metrics:\\n    - type: pass@1\\n      value: 0.62022\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": [{\"name\": \"yentinglin/Mistral-Small-24B-Instruct-2501-reasoning\", \"results\": [{\"task\": {\"type\": \"text-generation\"}, \"dataset\": {\"name\": \"MATH-500\", \"type\": \"MATH\"}, \"metrics\": [{\"name\": \"pass@1\", \"type\": \"pass@1\", \"value\": 0.95, \"verified\": false}], \"source\": {\"name\": \"yentinglin/zhtw-reasoning-eval-leaderboard\", \"url\": \"https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\"}}, {\"task\": {\"type\": \"text-generation\"}, \"dataset\": {\"name\": \"AIME 2025\", \"type\": \"AIME\"}, \"metrics\": [{\"name\": \"pass@1\", \"type\": \"pass@1\", \"value\": 0.5333, \"verified\": false}], \"source\": {\"name\": \"yentinglin/zhtw-reasoning-eval-leaderboard\", \"url\": \"https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\"}}, {\"task\": {\"type\": \"text-generation\"}, \"dataset\": {\"name\": \"AIME 2024\", \"type\": \"AIME\"}, \"metrics\": [{\"name\": \"pass@1\", \"type\": \"pass@1\", \"value\": 0.6667, \"verified\": false}], \"source\": {\"name\": \"yentinglin/zhtw-reasoning-eval-leaderboard\", \"url\": \"https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\"}}, {\"task\": {\"type\": \"text-generation\"}, \"dataset\": {\"name\": \"GPQA Diamond\", \"type\": \"GPQA\"}, \"metrics\": [{\"name\": \"pass@1\", \"type\": \"pass@1\", \"value\": 0.62022, \"verified\": false}], \"source\": {\"name\": \"yentinglin/zhtw-reasoning-eval-leaderboard\", \"url\": \"https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\"}}]}], \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}<think>\\n\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-20 13:37:37+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- open-r1/OpenR1-Math-220k\\n- yentinglin/s1K-1.1-trl-format\\n- simplescaling/s1K-1.1\\nlanguage:\\n- en\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- reasoning\\nmodel-index:\\n- name: yentinglin/Mistral-Small-24B-Instruct-2501-reasoning\\n  results:\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: MATH-500\\n      type: MATH\\n    metrics:\\n    - type: pass@1\\n      value: 0.95\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: AIME 2025\\n      type: AIME\\n    metrics:\\n    - type: pass@1\\n      value: 0.5333\\n      name: pass@1\\n      verified: false\\n    - type: pass@1\\n      value: 0.6667\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\\n  - task:\\n      type: text-generation\\n    dataset:\\n      name: GPQA Diamond\\n      type: GPQA\\n    metrics:\\n    - type: pass@1\\n      value: 0.62022\\n      name: pass@1\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/yentinglin/zhtw-reasoning-eval-leaderboard\\n      name: yentinglin/zhtw-reasoning-eval-leaderboard\", \"transformersInfo\": null, \"_id\": \"67b0c9c398a97493ce3475c5\", \"modelId\": \"yentinglin/Mistral-Small-24B-Instruct-2501-reasoning\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Levontriz/Alita",
            "metadata": "{\"id\": \"Levontriz/Alita\", \"author\": \"Levontriz\", \"sha\": \"6c1b780f442825003d7d32d038476062cf56314b\", \"last_modified\": \"2025-02-27 23:20:18+00:00\", \"created_at\": \"2025-02-27 23:09:22+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"fastai\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"fastai\", \"text-classification\", \"en\", \"dataset:facebook/natural_reasoning\", \"dataset:open-thoughts/OpenThoughts-114k\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"region:us\"], \"pipeline_tag\": \"text-classification\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- facebook/natural_reasoning\\n- open-thoughts/OpenThoughts-114k\\nlanguage:\\n- en\\nlibrary_name: fastai\\nmetrics:\\n- character\\npipeline_tag: text-classification\\nnew_version: mistralai/Mistral-Small-24B-Instruct-2501\", \"widget_data\": [{\"text\": \"I like you. I love you\"}], \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-27 23:20:18+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- facebook/natural_reasoning\\n- open-thoughts/OpenThoughts-114k\\nlanguage:\\n- en\\nlibrary_name: fastai\\nmetrics:\\n- character\\npipeline_tag: text-classification\\nnew_version: mistralai/Mistral-Small-24B-Instruct-2501\", \"transformersInfo\": null, \"_id\": \"67c0f0a2a261b07d0f762ec6\", \"modelId\": \"Levontriz/Alita\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Mathieu-Thomas-JOSSET/1",
            "metadata": "{\"id\": \"Mathieu-Thomas-JOSSET/1\", \"author\": \"Mathieu-Thomas-JOSSET\", \"sha\": \"148697844c903a22622e0cc6200fc558a5d7d0f6\", \"last_modified\": \"2025-01-31 06:20:31+00:00\", \"created_at\": \"2025-01-31 06:20:24+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"text-generation-inference\", \"unsloth\", \"mistral\", \"trl\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if 'role' in messages[0] %}{% for message in messages %}{% if message['role'] == 'user' %}{{'<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% else %}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|im_start|>user\\n' + message['value'] + '<|im_end|>\\n'}}{% elif message['from'] == 'gpt' %}{{'<|im_start|>assistant\\n' + message['value'] + '<|im_end|>\\n' }}{% else %}{{ '<|im_start|>system\\n' + message['value'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}{% endif %}\", \"eos_token\": \"<|im_end|>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-01-31 06:20:31+00:00\", \"cardData\": \"base_model: mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- mistral\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"679c6ba8131e1cf8ab252a49\", \"modelId\": \"Mathieu-Thomas-JOSSET/1\", \"usedStorage\": 17078049}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Ansh989/Ansh",
            "metadata": "{\"id\": \"Ansh989/Ansh\", \"author\": \"Ansh989\", \"sha\": \"ec4b6a5fc7fecdc318e5539e055b54d58ff2e195\", \"last_modified\": \"2025-02-20 02:57:05+00:00\", \"created_at\": \"2025-02-20 02:07:54+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"code\", \"text-generation\", \"en\", \"dataset:fka/awesome-chatgpt-prompts\", \"dataset:open-thoughts/OpenThoughts-114k\", \"dataset:open-r1/OpenR1-Math-220k\", \"dataset:Congliu/Chinese-DeepSeek-R1-Distill-data-110k\", \"dataset:bespokelabs/Bespoke-Stratos-17k\", \"dataset:cognitivecomputations/dolphin-r1\", \"dataset:ServiceNow-AI/R1-Distill-SFT\", \"dataset:simplescaling/s1K\", \"dataset:saiyan-world/Goku-MovieGenBenc\", \"dataset:agentica-org/DeepScaleR-Preview-Dataset\", \"dataset:FreedomIntelligence/medical-o1-reasoning-SFT\", \"dataset:MLCommons/unsupervised_peoples_speech\", \"dataset:google/Synthetic-Persona-Chat\", \"dataset:MaziyarPanahi/synthetic-medical-conversations-deepseek-v3-chat\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- fka/awesome-chatgpt-prompts\\n- open-thoughts/OpenThoughts-114k\\n- open-r1/OpenR1-Math-220k\\n- Congliu/Chinese-DeepSeek-R1-Distill-data-110k\\n- bespokelabs/Bespoke-Stratos-17k\\n- cognitivecomputations/dolphin-r1\\n- ServiceNow-AI/R1-Distill-SFT\\n- simplescaling/s1K\\n- saiyan-world/Goku-MovieGenBenc\\n- agentica-org/DeepScaleR-Preview-Dataset\\n- FreedomIntelligence/medical-o1-reasoning-SFT\\n- MLCommons/unsupervised_peoples_speech\\n- google/Synthetic-Persona-Chat\\n- MaziyarPanahi/synthetic-medical-conversations-deepseek-v3-chat\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- code\\nnew_version: deepseek-ai/DeepSeek-R1\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-20 02:57:05+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\ndatasets:\\n- fka/awesome-chatgpt-prompts\\n- open-thoughts/OpenThoughts-114k\\n- open-r1/OpenR1-Math-220k\\n- Congliu/Chinese-DeepSeek-R1-Distill-data-110k\\n- bespokelabs/Bespoke-Stratos-17k\\n- cognitivecomputations/dolphin-r1\\n- ServiceNow-AI/R1-Distill-SFT\\n- simplescaling/s1K\\n- saiyan-world/Goku-MovieGenBenc\\n- agentica-org/DeepScaleR-Preview-Dataset\\n- FreedomIntelligence/medical-o1-reasoning-SFT\\n- MLCommons/unsupervised_peoples_speech\\n- google/Synthetic-Persona-Chat\\n- MaziyarPanahi/synthetic-medical-conversations-deepseek-v3-chat\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\nmetrics:\\n- accuracy\\npipeline_tag: text-generation\\ntags:\\n- code\\nnew_version: deepseek-ai/DeepSeek-R1\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67b68e7a0fbb5ded331b5403\", \"modelId\": \"Ansh989/Ansh\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bartowski/Arcee-Blitz-exl2",
            "metadata": "{\"id\": \"bartowski/Arcee-Blitz-exl2\", \"author\": \"bartowski\", \"sha\": \"53e3e84437b8775b21e6f301a805aabdad7d1156\", \"last_modified\": \"2025-02-24 20:43:22+00:00\", \"created_at\": \"2025-02-24 20:43:21+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 5, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"text-generation\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\\nquantized_by: bartowski\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='measurement.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-24 20:43:22+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\\nquantized_by: bartowski\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67bcd9e9370c692d350ced31\", \"modelId\": \"bartowski/Arcee-Blitz-exl2\", \"usedStorage\": 85339527147}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "AlexBefest/CardProjector-24B-v1",
            "metadata": "{\"id\": \"AlexBefest/CardProjector-24B-v1\", \"author\": \"AlexBefest\", \"sha\": \"a0c7afea8b39d9a6da42e4a0d4f13ee6fc85eca5\", \"last_modified\": \"2025-03-03 16:29:22+00:00\", \"created_at\": \"2025-02-21 09:16:32+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 209, \"downloads_all_time\": null, \"likes\": 28, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"not-for-all-audiences\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- not-for-all-audiences\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are CardGenerator, trained by WoonaAI, you help generate character cards for role-playing games for SillyTavern in json format in chara_card_v2 specification.\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Samples/Kitty.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Samples/Sky.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Samples/Supernova.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-03 16:29:22+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- not-for-all-audiences\", \"transformersInfo\": null, \"_id\": \"67b84470c5b2d0bd2ecac5ab\", \"modelId\": \"AlexBefest/CardProjector-24B-v1\", \"usedStorage\": 47161926909}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "Mawdistical/Mawdistic-Purrvance-24b",
            "metadata": "{\"id\": \"Mawdistical/Mawdistic-Purrvance-24b\", \"author\": \"Mawdistical\", \"sha\": \"85e31c929aa581b14e1b39c9df02ee9eaaa4af36\", \"last_modified\": \"2025-03-27 13:36:01+00:00\", \"created_at\": \"2025-03-22 02:04:13+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 25, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"nsfw\", \"explicit\", \"roleplay\", \"unaligned\", \"dangerous\", \"en\", \"base_model:mistralai/Mistral-Small-24B-Instruct-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Instruct-2501\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\nlicense_name: m\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\nthumbnail: https://cdn-uploads.huggingface.co/production/uploads/67c10cfba43d7939d60160ff/YkH6RmKVLs8IaBSh_tJhz.png\\ninference: false\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"<|eot_id|>\", \"pad_token\": \"<|finetune_right_pad_id|>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572423680}, \"total\": 23572423680}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-03-27 13:36:01+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Instruct-2501\\nlanguage:\\n- en\\nlicense: apache-2.0\\nlicense_name: m\\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\\ntags:\\n- nsfw\\n- explicit\\n- roleplay\\n- unaligned\\n- dangerous\\nthumbnail: https://cdn-uploads.huggingface.co/production/uploads/67c10cfba43d7939d60160ff/YkH6RmKVLs8IaBSh_tJhz.png\\ninference: false\", \"transformersInfo\": null, \"_id\": \"67de1a9d1aec41929f28648e\", \"modelId\": \"Mawdistical/Mawdistic-Purrvance-24b\", \"usedStorage\": 47161968258}",
            "depth": 2,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/OmnipotentFreyja",
            "metadata": "{\"id\": \"bigband/OmnipotentFreyja\", \"author\": \"bigband\", \"sha\": \"f570b4c0cdc6032028603e9d109fac449526a9d4\", \"last_modified\": \"2025-02-04 11:35:09+00:00\", \"created_at\": \"2025-02-04 11:30:27+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 8, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 11:35:09+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1fa537b4ad443f35317c1\", \"modelId\": \"bigband/OmnipotentFreyja\", \"usedStorage\": 47176728532}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/HealerApollo",
            "metadata": "{\"id\": \"bigband/HealerApollo\", \"author\": \"bigband\", \"sha\": \"837c43e230c6c908d3c25ac21c6c8226d72dc196\", \"last_modified\": \"2025-02-04 10:16:27+00:00\", \"created_at\": \"2025-02-04 10:03:48+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 11, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 10:16:27+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1e6048747511e7ba349f5\", \"modelId\": \"bigband/HealerApollo\", \"usedStorage\": 47176728468}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "jhu-clsp/rank1-mistral-2501-24b",
            "metadata": "{\"id\": \"jhu-clsp/rank1-mistral-2501-24b\", \"author\": \"jhu-clsp\", \"sha\": \"039dec6eb7b2242320e4e65b1639dc0f23f28005\", \"last_modified\": \"2025-02-26 13:44:00+00:00\", \"created_at\": \"2025-02-18 16:37:30+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 90, \"downloads_all_time\": null, \"likes\": 2, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"reranker\", \"retrieval\", \"text-generation\", \"conversational\", \"en\", \"dataset:jhu-clsp/rank1-training-data\", \"arxiv:2502.18418\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:mit\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- jhu-clsp/rank1-training-data\\nlanguage:\\n- en\\nlicense: mit\\npipeline_tag: text-generation\\ntags:\\n- reranker\\n- retrieval\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ content }}{% elif message['role'] == 'assistant' %}{{ content }}{% endif %}{% endfor %}\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23573058560}, \"total\": 23573058560}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-26 13:44:00+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- jhu-clsp/rank1-training-data\\nlanguage:\\n- en\\nlicense: mit\\npipeline_tag: text-generation\\ntags:\\n- reranker\\n- retrieval\", \"transformersInfo\": null, \"_id\": \"67b4b74abe8951a892ab93cb\", \"modelId\": \"jhu-clsp/rank1-mistral-2501-24b\", \"usedStorage\": 47163237894}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "SaisExperiments/Not-So-Small-Alpaca-24B",
            "metadata": "{\"id\": \"SaisExperiments/Not-So-Small-Alpaca-24B\", \"author\": \"SaisExperiments\", \"sha\": \"870c4d17238b872e25bf43654f013fec61efd9dd\", \"last_modified\": \"2025-02-08 12:15:12+00:00\", \"created_at\": \"2025-02-07 09:16:30+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 25, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"safetensors\", \"mistral\", \"en\", \"dataset:anthracite-org/kalo-opus-instruct-22k-no-refusal\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"model-index\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\nlanguage:\\n- en\\nlicense: apache-2.0\\nmodel-index:\\n- name: Not-So-Small-Alpaca-24B\\n  results:\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: IFEval (0-Shot)\\n      type: HuggingFaceH4/ifeval\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: inst_level_strict_acc and prompt_level_strict_acc\\n      value: 62.44\\n      name: strict accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: BBH (3-Shot)\\n      type: BBH\\n      args:\\n        num_few_shot: 3\\n    metrics:\\n    - type: acc_norm\\n      value: 33.02\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MATH Lvl 5 (4-Shot)\\n      type: hendrycks/competition_math\\n      args:\\n        num_few_shot: 4\\n    metrics:\\n    - type: exact_match\\n      value: 18.05\\n      name: exact match\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: GPQA (0-shot)\\n      type: Idavidrein/gpqa\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 14.54\\n      name: acc_norm\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MuSR (0-shot)\\n      type: TAUR-Lab/MuSR\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 12.09\\n      name: acc_norm\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MMLU-PRO (5-shot)\\n      type: TIGER-Lab/MMLU-Pro\\n      config: main\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 29.94\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\", \"widget_data\": null, \"model_index\": [{\"name\": \"Not-So-Small-Alpaca-24B\", \"results\": [{\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"IFEval (0-Shot)\", \"type\": \"HuggingFaceH4/ifeval\", \"args\": {\"num_few_shot\": 0}}, \"metrics\": [{\"type\": \"inst_level_strict_acc and prompt_level_strict_acc\", \"value\": 62.44, \"name\": \"strict accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"BBH (3-Shot)\", \"type\": \"BBH\", \"args\": {\"num_few_shot\": 3}}, \"metrics\": [{\"type\": \"acc_norm\", \"value\": 33.02, \"name\": \"normalized accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"MATH Lvl 5 (4-Shot)\", \"type\": \"hendrycks/competition_math\", \"args\": {\"num_few_shot\": 4}}, \"metrics\": [{\"type\": \"exact_match\", \"value\": 18.05, \"name\": \"exact match\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"GPQA (0-shot)\", \"type\": \"Idavidrein/gpqa\", \"args\": {\"num_few_shot\": 0}}, \"metrics\": [{\"type\": \"acc_norm\", \"value\": 14.54, \"name\": \"acc_norm\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"MuSR (0-shot)\", \"type\": \"TAUR-Lab/MuSR\", \"args\": {\"num_few_shot\": 0}}, \"metrics\": [{\"type\": \"acc_norm\", \"value\": 12.09, \"name\": \"acc_norm\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"MMLU-PRO (5-shot)\", \"type\": \"TIGER-Lab/MMLU-Pro\", \"config\": \"main\", \"split\": \"test\", \"args\": {\"num_few_shot\": 5}}, \"metrics\": [{\"type\": \"acc\", \"value\": 29.94, \"name\": \"accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\", \"name\": \"Open LLM Leaderboard\"}}]}], \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if 'role' in messages[0] %}{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] + '\\n\\n' }}{% set loop_messages = messages[1:] %}{% else %}{{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.' + '\\n\\n' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '### Instruction:\\n' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ '### Response:\\n' + message['content'] + eos_token + '\\n\\n' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '### Response:\\n' }}{% endif %}{% else %}{{ bos_token }}{% if messages[0]['from'] == 'system' %}{{ messages[0]['value'] + '\\n\\n' }}{% set loop_messages = messages[1:] %}{% else %}{{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.' + '\\n\\n' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['from'] == 'human' %}{{ '### Instruction:\\n' + message['value'] + '\\n\\n' }}{% elif message['from'] == 'gpt' %}{{ '### Response:\\n' + message['value'] + eos_token + '\\n\\n' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '### Response:\\n' }}{% endif %}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-08 12:15:12+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- anthracite-org/kalo-opus-instruct-22k-no-refusal\\nlanguage:\\n- en\\nlicense: apache-2.0\\nmodel-index:\\n- name: Not-So-Small-Alpaca-24B\\n  results:\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: IFEval (0-Shot)\\n      type: HuggingFaceH4/ifeval\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: inst_level_strict_acc and prompt_level_strict_acc\\n      value: 62.44\\n      name: strict accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: BBH (3-Shot)\\n      type: BBH\\n      args:\\n        num_few_shot: 3\\n    metrics:\\n    - type: acc_norm\\n      value: 33.02\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MATH Lvl 5 (4-Shot)\\n      type: hendrycks/competition_math\\n      args:\\n        num_few_shot: 4\\n    metrics:\\n    - type: exact_match\\n      value: 18.05\\n      name: exact match\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: GPQA (0-shot)\\n      type: Idavidrein/gpqa\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 14.54\\n      name: acc_norm\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MuSR (0-shot)\\n      type: TAUR-Lab/MuSR\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 12.09\\n      name: acc_norm\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MMLU-PRO (5-shot)\\n      type: TIGER-Lab/MMLU-Pro\\n      config: main\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 29.94\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard?query=SaisExperiments/Not-So-Small-Alpaca-24B\\n      name: Open LLM Leaderboard\", \"transformersInfo\": null, \"_id\": \"67a5cf6ecb56f346c177e230\", \"modelId\": \"SaisExperiments/Not-So-Small-Alpaca-24B\", \"usedStorage\": 47161926909}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "v2ray/GPT4chan-24B",
            "metadata": "{\"id\": \"v2ray/GPT4chan-24B\", \"author\": \"v2ray\", \"sha\": \"a8d1b892da40fed76bb026db591c5b4ce0104c9c\", \"last_modified\": \"2025-02-05 04:26:26+00:00\", \"created_at\": \"2025-02-04 01:16:37+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 151, \"downloads_all_time\": null, \"likes\": 10, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"transformers\", \"safetensors\", \"mistral\", \"text-generation\", \"en\", \"dataset:v2ray/4chan\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:mit\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- v2ray/4chan\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\npipeline_tag: text-generation\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"ruslanmv/convert_to_gguf\", \"K00B404/LLM_Quantization\"], \"safetensors\": {\"parameters\": {\"BF16\": 23572423680}, \"total\": 23572423680}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-05 04:26:26+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\ndatasets:\\n- v2ray/4chan\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\npipeline_tag: text-generation\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a16a75713e930e9f41f7f4\", \"modelId\": \"v2ray/GPT4chan-24B\", \"usedStorage\": 94306857883}",
            "depth": 1,
            "children": [],
            "children_count": 0
        },
        {
            "model_id": "bigband/ProsperousTezcatlipoca",
            "metadata": "{\"id\": \"bigband/ProsperousTezcatlipoca\", \"author\": \"bigband\", \"sha\": \"2c0f6db67d93de7d32004636dc3eb709aef24f9a\", \"last_modified\": \"2025-02-04 08:47:09+00:00\", \"created_at\": \"2025-02-04 08:36:19+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 11, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"vllm\", \"gguf\": null, \"inference\": null, \"inference_provider_mapping\": null, \"tags\": [\"vllm\", \"safetensors\", \"mistral\", \"text-generation\", \"transformers\", \"conversational\", \"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", \"zh\", \"ja\", \"ru\", \"ko\", \"base_model:mistralai/Mistral-Small-24B-Base-2501\", \"base_model:finetune:mistralai/Mistral-Small-24B-Base-2501\", \"license:apache-2.0\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n        {{- '[INST]' + message['content'] + '[/INST]' }}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- message['content'] + eos_token }}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='SYSTEM_PROMPT.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00009-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00010-of-00010.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tekken.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 23572403200}, \"total\": 23572403200}, \"security_repo_status\": null, \"xet_enabled\": null, \"lastModified\": \"2025-02-04 08:47:09+00:00\", \"cardData\": \"base_model:\\n- mistralai/Mistral-Small-24B-Base-2501\\nlanguage:\\n- en\\n- fr\\n- de\\n- es\\n- it\\n- pt\\n- zh\\n- ja\\n- ru\\n- ko\\nlibrary_name: vllm\\nlicense: apache-2.0\\ntags:\\n- transformers\\ninference: false\\nextra_gated_description: If you want to learn more about how we process your personal\\n  data, please read our <a href=\\\"https://mistral.ai/terms/\\\">Privacy Policy</a>.\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67a1d183458124ec0ab375de\", \"modelId\": \"bigband/ProsperousTezcatlipoca\", \"usedStorage\": 94321574612}",
            "depth": 1,
            "children": [],
            "children_count": 0
        }
    ]
}