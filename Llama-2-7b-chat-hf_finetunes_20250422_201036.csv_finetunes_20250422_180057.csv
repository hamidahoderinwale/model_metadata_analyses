model_id,card,metadata,depth,children,children_count,adapters,adapters_count,quantized,quantized_count,merges,merges_count,spaces,spaces_count
meta-llama/Llama-2-7b-chat-hf,"---
extra_gated_heading: You need to share contact information with Meta to access this model
extra_gated_prompt: >-
  ### LLAMA 2 COMMUNITY LICENSE AGREEMENT

  ""Agreement"" means the terms and conditions for use, reproduction, distribution
  and  modification of the Llama Materials set forth herein. 

  ""Documentation"" means the specifications, manuals and documentation 
  accompanying Llama 2 distributed by Meta at
  https://ai.meta.com/resources/models-and-libraries/llama-downloads/.  

  ""Licensee"" or ""you"" means you, or your employer or any other person or entity
  (if you are entering into this Agreement on such person or entity's behalf),
  of the age required under applicable laws, rules or regulations to provide
  legal consent and that has legal authority to bind your employer or such other
  person or  entity if you are  entering in this Agreement on their behalf. 

  ""Llama 2"" means the foundational large language models and software and
  algorithms, including machine-learning model code, trained model weights,
  inference-enabling code, training-enabling code, fine-tuning enabling code and
  other  elements of the foregoing distributed by Meta at
  ai.meta.com/resources/models-and-libraries/llama-downloads/.

  ""Llama Materials"" means, collectively, Meta's proprietary Llama 2 and
  documentation (and any portion thereof) made available under this Agreement.

  ""Meta"" or ""we"" means Meta Platforms Ireland Limited (if you are located in or,
  if you are an entity, your principal place of business is in the EEA or
  Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA
  or Switzerland). 


  By clicking ""I Accept"" below or by using or distributing any portion or
  element of the Llama Materials, you agree to be bound by this Agreement.

  1. License Rights and Redistribution. 

  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-
  transferable and royalty-free limited license under Meta's intellectual
  property or  other rights owned by Meta embodied in the Llama Materials to
  use, reproduce,  distribute, copy, create derivative works of, and make
  modifications to the Llama  Materials.  

  b. Redistribution and Use.

  i. If you distribute or make the Llama Materials, or any derivative works 
  thereof, available to a third party, you shall provide a copy of this
  Agreement to such  third party. 

  ii.  If you receive Llama Materials, or any derivative works thereof, from  a
  Licensee as part of an integrated end user product, then Section 2 of this 
  Agreement will not apply to you. 

  iii. You must retain in all copies of the Llama Materials that you  distribute
  the following attribution notice within a ""Notice"" text file distributed as a 
  part of such copies: ""Llama 2 is licensed under the LLAMA 2 Community
  License,  Copyright (c) Meta Platforms, Inc. All Rights Reserved.""

  iv. Your use of the Llama Materials must comply with applicable laws  and
  regulations (including trade compliance laws and regulations) and adhere to
  the  Acceptable Use Policy for the Llama Materials (available at 
  https://ai.meta.com/llama/use-policy), which is hereby incorporated by
  reference into  this Agreement.

  v. You will not use the Llama Materials or any output or results of the  Llama
  Materials to improve any other large language model (excluding Llama 2 or 
  derivative works thereof).  


  2. Additional Commercial Terms. If, on the Llama 2 version release date, the 
  monthly active users of the products or services made available by or for
  Licensee,  or Licensee's affiliates, is greater than 700 million monthly
  active users in the  preceding calendar month, you must request a license from
  Meta, which Meta may  grant to you in its sole discretion, and you are not
  authorized to exercise any of the  rights under this Agreement unless or until
  Meta otherwise expressly grants you  such rights.

  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA
  MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN ""AS IS""
  BASIS, WITHOUT WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING,
  WITHOUT LIMITATION, ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT,
  MERCHANTABILITY, OR  FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY
  RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING 
  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR  USE OF THE
  LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.

  4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE
  UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE,
  PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST
  PROFITS OR ANY INDIRECT, SPECIAL,  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR
  PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE
  POSSIBILITY OF  ANY OF THE FOREGOING.


  5. Intellectual Property.

  a. No trademark licenses are granted under this Agreement, and in  connection
  with the Llama Materials, neither Meta nor Licensee may use any name  or mark
  owned by or associated with the other or any of its affiliates, except as 
  required for reasonable and customary use in describing and redistributing
  the  Llama Materials.

  b. Subject to Meta's ownership of Llama Materials and derivatives made by or 
  for Meta, with respect to any derivative works and modifications of the Llama 
  Materials that are made by you, as between you and Meta, you are and will be
  the  owner of such derivative works and modifications.

  c. If you institute litigation or other proceedings against Meta or any
  entity  (including a cross-claim or counterclaim in a lawsuit) alleging that
  the Llama  Materials or Llama 2 outputs or results, or any portion of any of
  the foregoing,  constitutes infringement of intellectual property or other
  rights owned or licensable  by you, then any licenses granted to you under
  this Agreement shall terminate as of  the date such litigation or claim is
  filed or instituted. You will indemnify and hold  harmless Meta from and
  against any claim by any third party arising out of or related  to your use or
  distribution of the Llama Materials.

  6. Term and Termination. The term of this Agreement will commence upon your 
  acceptance of this Agreement or access to the Llama Materials and will
  continue in  full force and effect until terminated in accordance with the
  terms and conditions  herein. Meta may terminate this Agreement if you are in
  breach of any term or  condition of this Agreement. Upon termination of this
  Agreement, you shall delete  and cease use of the Llama Materials. Sections 3,
  4 and 7 shall survive the  termination of this Agreement. 

  7. Governing Law and Jurisdiction. This Agreement will be governed and 
  construed under the laws of the State of California without regard to choice
  of law  principles, and the UN Convention on Contracts for the International
  Sale of Goods  does not apply to this Agreement. The courts of California
  shall have exclusive  jurisdiction of any dispute arising out of this
  Agreement. 

  ### Llama 2 Acceptable Use Policy

  Meta is committed to promoting safe and fair use of its tools and features,
  including Llama 2. If you access or use Llama 2, you agree to this Acceptable
  Use Policy (“Policy”). The most recent copy of this policy can be found at
  [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).

  #### Prohibited Uses

  We want everyone to use Llama 2 safely and responsibly. You agree you will not
  use, or allow others to use, Llama 2 to:

  1. Violate the law or others’ rights, including to:
        1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: 
            1. Violence or terrorism 
            2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material
            3. Human trafficking, exploitation, and sexual violence
            4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.
            5. Sexual solicitation
            6. Any other criminal activity
        2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals
        3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services
        4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices 
        5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws
        6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials
        7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system 
  2. Engage in, promote, incite, facilitate, or assist in the planning or
  development of activities that present a risk of death or bodily harm to
  individuals, including use of Llama 2 related to the following:
      1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State
      2. Guns and illegal weapons (including weapon development)
      3. Illegal drugs and regulated/controlled substances
      4. Operation of critical infrastructure, transportation technologies, or heavy machinery
      5. Self-harm or harm to others, including suicide, cutting, and eating disorders
      6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual
  3. Intentionally deceive or mislead others, including use of Llama 2 related
  to the following:
      1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation
      2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content
      3. Generating, promoting, or further distributing spam
      4. Impersonating another individual without consent, authorization, or legal right
      5. Representing that the use of Llama 2 or outputs are human-generated
      6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement 
      4. Fail to appropriately disclose to end users any known dangers of your AI system 
  Please report any violation of this Policy, software “bug,” or other problems
  that could lead to a violation of this Policy through one of the following
  means: 
      * Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)
      * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
      * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info) 
      * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)
extra_gated_fields:
  First Name: text
  Last Name: text
  Date of birth: date_picker
  Country: country
  Affiliation: text
  geo: ip_location
  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox
extra_gated_description: >-
  The information you provide will be collected, stored, processed and shared in
  accordance with the [Meta Privacy
  Policy](https://www.facebook.com/privacy/policy/).
extra_gated_button_content: Submit
language:
- en
pipeline_tag: text-generation
tags:
- facebook
- meta
- pytorch
- llama
- llama-2
license: llama2
---
# **Llama 2**
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.

## Model Details
*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*

Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.

**Model Developers** Meta

**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.

**Input** Models input text only.

**Output** Models generate text only.

**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.


||Training Data|Params|Content Length|GQA|Tokens|LR|
|---|---|---|---|---|---|---|
|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|

*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Dates** Llama 2 was trained between January 2023 and July 2023.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

**Research Paper** [""Llama-2: Open Foundation and Fine-tuned Chat Models""](arxiv.org/abs/2307.09288)

## Intended Use
**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).

**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.

## Hardware and Software
**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.

||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|
|---|---|---|---|
|Llama 2 7B|184320|400|31.22|
|Llama 2 13B|368640|400|62.44|
|Llama 2 70B|1720320|400|291.42|
|Total|3311616||539.00|

**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.

## Training Data
**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.

## Evaluation Results

In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.

|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|
|---|---|---|---|---|---|---|---|---|---|
|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|
|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|
|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|
|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|
|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|
|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|
|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|

**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.

|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama 1|7B|27.42|23.00|
|Llama 1|13B|41.74|23.08|
|Llama 1|33B|44.19|22.57|
|Llama 1|65B|48.71|21.77|
|Llama 2|7B|33.29|**21.25**|
|Llama 2|13B|41.86|26.10|
|Llama 2|70B|**50.18**|24.60|

**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).


|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama-2-Chat|7B|57.04|**0.00**|
|Llama-2-Chat|13B|62.18|**0.00**|
|Llama-2-Chat|70B|**64.14**|0.01|

**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.

## Ethical Considerations and Limitations
Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.

Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)

## Reporting Issues
Please report any software “bug,” or other problems with the models through one of the following means:
- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)
- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)

## Llama Model Index
|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|
|---|---|---|---|---|
|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|
|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|
|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|","{""id"": ""meta-llama/Llama-2-7b-chat-hf"", ""author"": ""meta-llama"", ""sha"": ""f5db02db724555f92da89c216ac04704f23d4590"", ""last_modified"": ""2024-04-17 08:40:48+00:00"", ""created_at"": ""2023-07-13 16:45:23+00:00"", ""private"": false, ""gated"": ""manual"", ""disabled"": false, ""downloads"": 1186755, ""downloads_all_time"": null, ""likes"": 4380, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""safetensors"", ""llama"", ""text-generation"", ""facebook"", ""meta"", ""llama-2"", ""conversational"", ""en"", ""arxiv:2307.09288"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""language:\n- en\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nextra_gated_heading: You need to share contact information with Meta to access this\n  model\nextra_gated_prompt: \""### LLAMA 2 COMMUNITY LICENSE AGREEMENT\\n\\\""Agreement\\\"" means\\\n  \\ the terms and conditions for use, reproduction, distribution and  modification\\\n  \\ of the Llama Materials set forth herein. \\n\\\""Documentation\\\"" means the specifications,\\\n  \\ manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\n  \\  \\n\\\""Licensee\\\"" or \\\""you\\\"" means you, or your employer or any other person or\\\n  \\ entity (if you are entering into this Agreement on such person or entity's behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or  entity if you are  entering in this Agreement on their behalf. \\n\\\""Llama 2\\\""\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other  elements of the foregoing distributed\\\n  \\ by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\\n\\\""Llama\\\n  \\ Materials\\\"" means, collectively, Meta's proprietary Llama 2 and documentation\\\n  \\ (and any portion thereof) made available under this Agreement.\\n\\\""Meta\\\"" or \\\""\\\n  we\\\"" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\n  \\ an entity, your principal place of business is in the EEA or Switzerland) and\\\n  \\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \\n\\\n  \\nBy clicking \\\""I Accept\\\"" below or by using or distributing any portion or element\\\n  \\ of the Llama Materials, you agree to be bound by this Agreement.\\n1. License Rights\\\n  \\ and Redistribution. \\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\n  \\ non- transferable and royalty-free limited license under Meta's intellectual property\\\n  \\ or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\n  \\  distribute, copy, create derivative works of, and make modifications to the Llama\\\n  \\  Materials.  \\nb. Redistribution and Use.\\ni. If you distribute or make the Llama\\\n  \\ Materials, or any derivative works  thereof, available to a third party, you shall\\\n  \\ provide a copy of this Agreement to such  third party. \\nii.  If you receive Llama\\\n  \\ Materials, or any derivative works thereof, from  a Licensee as part of an integrated\\\n  \\ end user product, then Section 2 of this  Agreement will not apply to you. \\n\\\n  iii. You must retain in all copies of the Llama Materials that you  distribute the\\\n  \\ following attribution notice within a \\\""Notice\\\"" text file distributed as a  part\\\n  \\ of such copies: \\\""Llama 2 is licensed under the LLAMA 2 Community License,  Copyright\\\n  \\ (c) Meta Platforms, Inc. All Rights Reserved.\\\""\\niv. Your use of the Llama Materials\\\n  \\ must comply with applicable laws  and regulations (including trade compliance\\\n  \\ laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials\\\n  \\ (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated\\\n  \\ by reference into  this Agreement.\\nv. You will not use the Llama Materials or\\\n  \\ any output or results of the  Llama Materials to improve any other large language\\\n  \\ model (excluding Llama 2 or  derivative works thereof).  \\n\\n2. Additional Commercial\\\n  \\ Terms. If, on the Llama 2 version release date, the  monthly active users of the\\\n  \\ products or services made available by or for Licensee,  or Licensee's affiliates,\\\n  \\ is greater than 700 million monthly active users in the  preceding calendar month,\\\n  \\ you must request a license from Meta, which Meta may  grant to you in its sole\\\n  \\ discretion, and you are not authorized to exercise any of the  rights under this\\\n  \\ Agreement unless or until Meta otherwise expressly grants you  such rights.\\n\\\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS\\\n  \\ AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \\\""AS IS\\\"" BASIS, WITHOUT\\\n  \\ WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A\\\n  \\ PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS\\\n  \\ OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED\\\n  \\ WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation\\\n  \\ of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY\\\n  \\ OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE,\\\n  \\ ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\\\n  \\  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS\\\n  \\ AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\\n\\n\\\n  5. Intellectual Property.\\na. No trademark licenses are granted under this Agreement,\\\n  \\ and in  connection with the Llama Materials, neither Meta nor Licensee may use\\\n  \\ any name  or mark owned by or associated with the other or any of its affiliates,\\\n  \\ except as  required for reasonable and customary use in describing and redistributing\\\n  \\ the  Llama Materials.\\nb. Subject to Meta's ownership of Llama Materials and derivatives\\\n  \\ made by or  for Meta, with respect to any derivative works and modifications of\\\n  \\ the Llama  Materials that are made by you, as between you and Meta, you are and\\\n  \\ will be the  owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity  (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs\\\n  \\ or results, or any portion of any of the foregoing,  constitutes infringement\\\n  \\ of intellectual property or other rights owned or licensable  by you, then any\\\n  \\ licenses granted to you under this Agreement shall terminate as of  the date such\\\n  \\ litigation or claim is filed or instituted. You will indemnify and hold  harmless\\\n  \\ Meta from and against any claim by any third party arising out of or related \\\n  \\ to your use or distribution of the Llama Materials.\\n6. Term and Termination.\\\n  \\ The term of this Agreement will commence upon your  acceptance of this Agreement\\\n  \\ or access to the Llama Materials and will continue in  full force and effect until\\\n  \\ terminated in accordance with the terms and conditions  herein. Meta may terminate\\\n  \\ this Agreement if you are in breach of any term or  condition of this Agreement.\\\n  \\ Upon termination of this Agreement, you shall delete  and cease use of the Llama\\\n  \\ Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\\\n  \\ \\n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed\\\n  \\ under the laws of the State of California without regard to choice of law  principles,\\\n  \\ and the UN Convention on Contracts for the International Sale of Goods  does not\\\n  \\ apply to this Agreement. The courts of California shall have exclusive  jurisdiction\\\n  \\ of any dispute arising out of this Agreement. \\n### Llama 2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy\\\n  \\ (\u201cPolicy\u201d). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 2 safely and responsibly. You\\\n  \\ agree you will not use, or allow others to use, Llama 2 to:\\n1. Violate the law\\\n  \\ or others\u2019 rights, including to:\\n      1. Engage in, promote, generate, contribute\\\n  \\ to, encourage, plan, incite, or further illegal or unlawful activity or content,\\\n  \\ such as: \\n          1. Violence or terrorism \\n          2. Exploitation or harm\\\n  \\ to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\          3. Human trafficking, exploitation, and sexual violence\\n          4.\\\n  \\ The illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n          5. Sexual solicitation\\n          6.\\\n  \\ Any other criminal activity\\n      2. Engage in, promote, incite, or facilitate\\\n  \\ the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\      3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    \\\n  \\  4. Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices \\n      5. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n      6. Engage in or facilitate any\\\n  \\ action or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama 2 Materials\\n      7. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system \\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 2 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 2 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 2 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement \\n    4. Fail to appropriately disclose\\\n  \\ to end users any known dangers of your AI system \\nPlease report any violation\\\n  \\ of this Policy, software \u201cbug,\u201d or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means: \\n    * Reporting issues with\\\n  \\ the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\\n\\\n  \\    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  \\    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\n  \\ \\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of\\\n  \\ Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\""\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='LICENSE.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [""mteb/leaderboard"", ""huggingface-projects/llama-2-7b-chat"", ""qingxu98/gpt-academic"", ""Plachta/Seed-VC"", ""h2oai/h2ogpt-chatbot"", ""FinGPT/FinGPT-Forecaster"", ""allenai/WildBench"", ""baconnier/prompt-plus-plus"", ""eduagarcia/open_pt_llm_leaderboard"", ""h2oai/h2ogpt-chatbot2"", ""Intel/low_bit_open_llm_leaderboard"", ""BAAI/open_cn_llm_leaderboard"", ""chansung/vid2persona"", ""allenai/ZebraLogic"", ""Illia56/Ask-AI-Youtube"", ""awacke1/GPT-4o-omni-text-audio-image-video"", ""speakleash/open_pl_llm_leaderboard"", ""gsaivinay/open_llm_leaderboard"", ""autotrain-projects/llm-merge-adapter"", ""mteb/leaderboard_legacy"", ""NiansuhAI/HFLLMs"", ""Omnibus/Chatbot-Compare"", ""KBaba7/Quant"", ""allenai/BaseChat"", ""AiActivity/AI-Assistant"", ""meval/multilingual-chatbot-arena-leaderboard"", ""GTBench/GTBench"", ""awacke1/ChatGPT-Memory-Chat-Story-Generator"", ""Vikhrmodels/small-shlepa-lb"", ""FinGPT/FinGPT-Forecaster-Chinese"", ""harsh-manvar/llama-2-7b-chat-test"", ""RenderAI/Seed-VC"", ""prometheus-eval/BiGGen-Bench-Leaderboard"", ""NiansuhAI/Main"", ""Omnibus/InferenceClient_Chatbots"", ""Justinrune/LLaMA-Factory"", ""ROHAN181/pdf-chatbot"", ""freQuensy23/LLMhistory"", ""AilexGPT/PDF_chat_GPT"", ""kenken999/fastapi_django_main_live"", ""kz-transformers/kaz-llm-lb"", ""WildEval/ZebraLogic"", ""awacke1/LlamaWhisperer"", ""nsethi610/ns-gradio-apps"", ""lightmate/llm-chatbot"", ""joshuasundance/langchain-streamlit-demo"", ""philschmid/Can-i-run-tgi"", ""TogetherAI/Chat-with-Llama-2-70b"", ""dvruette/concept-guidance"", ""3B-Group/ConvRe-Leaderboard"", ""awacke1/Multimodal-Science-and-Music-Lab"", ""camel-ai/agent-trust-Trust-Game-Demo"", ""awacke1/GraphicAINovel"", ""felixz/open_llm_leaderboard"", ""awacke1/RescuerOfStolenBikes"", ""bhaskartripathi/LLM_Quantization"", ""gojiteji/LLM-Comparer"", ""officialhimanshu595/llama-factory"", ""awacke1/ScienceBrain.AI"", ""Tonic/prometheus"", ""Sagar23p/mistralAI_chatBoat"", ""OPTML-Group/UnlearnCanvas-Benchmark"", ""dar-tau/selfie"", ""awacke1/ChatStreamlitMultiplayer"", ""Thun09/leaderboard_demo"", ""Uniaff/Seed-VC"", ""totolook/Quant"", ""FallnAI/Quantize-HF-Models"", ""NohTow/LLM_watermarking"", ""pyvene/reft_emoji_chat"", ""awacke1/The_Music_Of_New_Orleans_MoE"", ""fantos/Chatbot-Compare"", ""santuchal/pdf_chat_bot"", ""awacke1/ReMixable-AI-AR"", ""awacke1/BodyMapAI"", ""awacke1/mixture-of-experts-dr-llama"", ""rodrisouza/demo-chatbot-v3"", ""soiz1/Seed-VC"", ""BAAI/open_flageval_vlm_leaderboard"", ""soiz1/seed-vc3"", ""antoniomae/Seed-VC"", ""sonali-tamhankar/WA-Hospital-Regulations-Chatbot"", ""b1sheng/kg_llm_leaderboard_test"", ""SaeidFarsian/Ask-AI-Youtube"", ""pyvene/reft_ethos"", ""LLM-auto-model-card/LLM-guessing-game"", ""Zulelee/langchain-chatchat"", ""neubla/neubla-llm-evaluation-board"", ""ali121300/pdf_chat_bot"", ""lapsapking/h2ogpt-chatbot"", ""Sambhavnoobcoder/pdf-chatbot"", ""loveblairsky/LLM-model-cards"", ""awacke1/Arxiv-Paper-Search-QA-RAG-Streamlit-Gradio-API"", ""Alfasign/pdf-chatbot-opensource-llm"", ""joaco7172/procapital"", ""Nymbo/GPT-4o-omni-text-audio-image-video"", ""rubypnchl/Question_Answer_Engine"", ""Rahatara/RAGBOT"", ""s-a-malik/semantic-entropy-probes"", ""NCTCMumbai/nctc-pdf-chatbot""], ""safetensors"": {""parameters"": {""F16"": 6738417664}, ""total"": 6738417664}, ""security_repo_status"": null, ""lastModified"": ""2024-04-17 08:40:48+00:00"", ""cardData"": ""language:\n- en\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nextra_gated_heading: You need to share contact information with Meta to access this\n  model\nextra_gated_prompt: \""### LLAMA 2 COMMUNITY LICENSE AGREEMENT\\n\\\""Agreement\\\"" means\\\n  \\ the terms and conditions for use, reproduction, distribution and  modification\\\n  \\ of the Llama Materials set forth herein. \\n\\\""Documentation\\\"" means the specifications,\\\n  \\ manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\n  \\  \\n\\\""Licensee\\\"" or \\\""you\\\"" means you, or your employer or any other person or\\\n  \\ entity (if you are entering into this Agreement on such person or entity's behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or  entity if you are  entering in this Agreement on their behalf. \\n\\\""Llama 2\\\""\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other  elements of the foregoing distributed\\\n  \\ by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\\n\\\""Llama\\\n  \\ Materials\\\"" means, collectively, Meta's proprietary Llama 2 and documentation\\\n  \\ (and any portion thereof) made available under this Agreement.\\n\\\""Meta\\\"" or \\\""\\\n  we\\\"" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\n  \\ an entity, your principal place of business is in the EEA or Switzerland) and\\\n  \\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \\n\\\n  \\nBy clicking \\\""I Accept\\\"" below or by using or distributing any portion or element\\\n  \\ of the Llama Materials, you agree to be bound by this Agreement.\\n1. License Rights\\\n  \\ and Redistribution. \\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\n  \\ non- transferable and royalty-free limited license under Meta's intellectual property\\\n  \\ or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\n  \\  distribute, copy, create derivative works of, and make modifications to the Llama\\\n  \\  Materials.  \\nb. Redistribution and Use.\\ni. If you distribute or make the Llama\\\n  \\ Materials, or any derivative works  thereof, available to a third party, you shall\\\n  \\ provide a copy of this Agreement to such  third party. \\nii.  If you receive Llama\\\n  \\ Materials, or any derivative works thereof, from  a Licensee as part of an integrated\\\n  \\ end user product, then Section 2 of this  Agreement will not apply to you. \\n\\\n  iii. You must retain in all copies of the Llama Materials that you  distribute the\\\n  \\ following attribution notice within a \\\""Notice\\\"" text file distributed as a  part\\\n  \\ of such copies: \\\""Llama 2 is licensed under the LLAMA 2 Community License,  Copyright\\\n  \\ (c) Meta Platforms, Inc. All Rights Reserved.\\\""\\niv. Your use of the Llama Materials\\\n  \\ must comply with applicable laws  and regulations (including trade compliance\\\n  \\ laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials\\\n  \\ (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated\\\n  \\ by reference into  this Agreement.\\nv. You will not use the Llama Materials or\\\n  \\ any output or results of the  Llama Materials to improve any other large language\\\n  \\ model (excluding Llama 2 or  derivative works thereof).  \\n\\n2. Additional Commercial\\\n  \\ Terms. If, on the Llama 2 version release date, the  monthly active users of the\\\n  \\ products or services made available by or for Licensee,  or Licensee's affiliates,\\\n  \\ is greater than 700 million monthly active users in the  preceding calendar month,\\\n  \\ you must request a license from Meta, which Meta may  grant to you in its sole\\\n  \\ discretion, and you are not authorized to exercise any of the  rights under this\\\n  \\ Agreement unless or until Meta otherwise expressly grants you  such rights.\\n\\\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS\\\n  \\ AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \\\""AS IS\\\"" BASIS, WITHOUT\\\n  \\ WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A\\\n  \\ PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS\\\n  \\ OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED\\\n  \\ WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation\\\n  \\ of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY\\\n  \\ OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE,\\\n  \\ ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\\\n  \\  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS\\\n  \\ AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\\n\\n\\\n  5. Intellectual Property.\\na. No trademark licenses are granted under this Agreement,\\\n  \\ and in  connection with the Llama Materials, neither Meta nor Licensee may use\\\n  \\ any name  or mark owned by or associated with the other or any of its affiliates,\\\n  \\ except as  required for reasonable and customary use in describing and redistributing\\\n  \\ the  Llama Materials.\\nb. Subject to Meta's ownership of Llama Materials and derivatives\\\n  \\ made by or  for Meta, with respect to any derivative works and modifications of\\\n  \\ the Llama  Materials that are made by you, as between you and Meta, you are and\\\n  \\ will be the  owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity  (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs\\\n  \\ or results, or any portion of any of the foregoing,  constitutes infringement\\\n  \\ of intellectual property or other rights owned or licensable  by you, then any\\\n  \\ licenses granted to you under this Agreement shall terminate as of  the date such\\\n  \\ litigation or claim is filed or instituted. You will indemnify and hold  harmless\\\n  \\ Meta from and against any claim by any third party arising out of or related \\\n  \\ to your use or distribution of the Llama Materials.\\n6. Term and Termination.\\\n  \\ The term of this Agreement will commence upon your  acceptance of this Agreement\\\n  \\ or access to the Llama Materials and will continue in  full force and effect until\\\n  \\ terminated in accordance with the terms and conditions  herein. Meta may terminate\\\n  \\ this Agreement if you are in breach of any term or  condition of this Agreement.\\\n  \\ Upon termination of this Agreement, you shall delete  and cease use of the Llama\\\n  \\ Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\\\n  \\ \\n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed\\\n  \\ under the laws of the State of California without regard to choice of law  principles,\\\n  \\ and the UN Convention on Contracts for the International Sale of Goods  does not\\\n  \\ apply to this Agreement. The courts of California shall have exclusive  jurisdiction\\\n  \\ of any dispute arising out of this Agreement. \\n### Llama 2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy\\\n  \\ (\u201cPolicy\u201d). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 2 safely and responsibly. You\\\n  \\ agree you will not use, or allow others to use, Llama 2 to:\\n1. Violate the law\\\n  \\ or others\u2019 rights, including to:\\n      1. Engage in, promote, generate, contribute\\\n  \\ to, encourage, plan, incite, or further illegal or unlawful activity or content,\\\n  \\ such as: \\n          1. Violence or terrorism \\n          2. Exploitation or harm\\\n  \\ to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\          3. Human trafficking, exploitation, and sexual violence\\n          4.\\\n  \\ The illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n          5. Sexual solicitation\\n          6.\\\n  \\ Any other criminal activity\\n      2. Engage in, promote, incite, or facilitate\\\n  \\ the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\      3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    \\\n  \\  4. Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices \\n      5. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n      6. Engage in or facilitate any\\\n  \\ action or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama 2 Materials\\n      7. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system \\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 2 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 2 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 2 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement \\n    4. Fail to appropriately disclose\\\n  \\ to end users any known dangers of your AI system \\nPlease report any violation\\\n  \\ of this Policy, software \u201cbug,\u201d or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means: \\n    * Reporting issues with\\\n  \\ the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\\n\\\n  \\    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  \\    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\n  \\ \\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of\\\n  \\ Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\""\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""64b02a230c8415cd4a159fff"", ""modelId"": ""meta-llama/Llama-2-7b-chat-hf"", ""usedStorage"": 53908103645}",0,"https://huggingface.co/selfmaker/llama2-7B-xsum, https://huggingface.co/BashitAli/llama-2-7b-chat.ggmlv3.q5_K_M, https://huggingface.co/RakshitAi/AtmaLLaMA, https://huggingface.co/DeeWoo/Llama-2-7b-chat_FFT_GSM8K, https://huggingface.co/DongkiKim/Mol-Llama-2-7b-chat, https://huggingface.co/ShreySharma07/maths-llama-qlora, https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML, https://huggingface.co/alielfilali01/Llama-2-7b-chat-hf-tuned-medical-qa, https://huggingface.co/vincentmin/llama-7b-orca, https://huggingface.co/alielfilali01/Llama-2-7b-chat-hf-tuned-medical-chat, https://huggingface.co/PhilSad/llama2-7b-chat-french-2k-test, https://huggingface.co/llSourcell/results, https://huggingface.co/nauman187/results, https://huggingface.co/karimasbar/test_result, https://huggingface.co/threem/llama2-fine-tuned-2, https://huggingface.co/Jukaboo/LLama2_7b_Jukabo_ft_mlsum_hf, https://huggingface.co/dbraganca/sdr-bot-llama2, https://huggingface.co/DeepaPeri/results, https://huggingface.co/karimasbar/results, https://huggingface.co/dejimarquis/heallama7b, https://huggingface.co/karimasbar/resultss, https://huggingface.co/jamsonE/results, https://huggingface.co/Chanblock/Llama-2-7b-chat-hf-finetuned-250_remates, https://huggingface.co/qazisaad/results, https://huggingface.co/jquigonq/results, https://huggingface.co/BadTiger/badtiger_llama2, https://huggingface.co/synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M, https://huggingface.co/AniketParab/results, https://huggingface.co/TonySky/results, https://huggingface.co/Anish03/results, https://huggingface.co/flumboyantApple/twittSent01, https://huggingface.co/abeiler/goatV9-chat-QLORA-Merged, https://huggingface.co/EnzoZacharias/xgen-7b-tuned-alpaca, https://huggingface.co/EnzoZacharias/outputs, https://huggingface.co/Pavanb/results, https://huggingface.co/antonwonton/llama-2-7b-hf-train01-int4, https://huggingface.co/antonwonton/llama-2-7b-chat-hf-test09-int4, https://huggingface.co/antonwonton/Llama-2-7b-chat-hf-int4-ft-0.75, https://huggingface.co/Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v623, https://huggingface.co/Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v1200, https://huggingface.co/Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v2400, https://huggingface.co/Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v4100, https://huggingface.co/Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v12100, https://huggingface.co/Atulit23/meta-llama-indian-constitution-chat, https://huggingface.co/samxm111/results, https://huggingface.co/msato777/results, https://huggingface.co/EnzoZacharias/LLama2-7b-fine-tuned-plc_V1, https://huggingface.co/ahmadsajid1989/Llama-2-7b-chat-hf-fine-tuned-bongo-cs, https://huggingface.co/Michelvh/qlora-llama2-7b-question-generation-eduqg, https://huggingface.co/thekrishna/results, https://huggingface.co/chaocai/llama2-ft, https://huggingface.co/vineetsharma/qlora-Llama-2-7b-chat-hf-databricks-dolly-15k, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v1200, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5100, https://huggingface.co/Tharuneshwar/results, https://huggingface.co/erbacher/llama2_hf_int, https://huggingface.co/surathisin/llama-2-13b-finetune-bot-2, https://huggingface.co/surathisin/llama-2-7b-finetune-1, https://huggingface.co/justinlangseth/llama-10-11-sp-1, https://huggingface.co/surathisin/llama-2-7b-finetune-001, https://huggingface.co/BLACKBUN/llama-2-7b-pubmed-qa-211k, https://huggingface.co/surathisin/nvso-model-test-1, https://huggingface.co/alperk3003/medalpaca_base, https://huggingface.co/alperk3003/medalpaca_circulatory_model, https://huggingface.co/alperk3003/medalpaca_digestive_model, https://huggingface.co/alperk3003/medalpaca_ear_model, https://huggingface.co/alperk3003/medalpaca_eye_model, https://huggingface.co/alperk3003/medalpaca_genitourinary_model, https://huggingface.co/surathisin/nvso-model-test-4, https://huggingface.co/alperk3003/medalpaca_nutritional_model, https://huggingface.co/alperk3003/medalpaca_infectious_model, https://huggingface.co/whatdhack/Llama-2-7b-chat-hf-oasst1-lora-b157, https://huggingface.co/alperk3003/medalpaca_nervous_model, https://huggingface.co/whatdhack/Llama-2-7b-chat-hf-oasst1-ft-sg, https://huggingface.co/alperk3003/medalpaca_respiratory_model, https://huggingface.co/alperk3003/medalpaca_skin_model, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585v2, https://huggingface.co/alperk3003/medalpaca_musculoskeletal_model, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5000v2, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep3, https://huggingface.co/alperk3003/medalpaca_mental_model, https://huggingface.co/alperk3003/medalpaca_blood_model, https://huggingface.co/langecod/Genesis_Llama, https://huggingface.co/langecod/CounselLlama7B, https://huggingface.co/dininta/results, https://huggingface.co/EnzoZacharias/Llama-2-7b-fine_tuned-SPS_final, https://huggingface.co/wcarr993/llama2-7B-151-v2-chat, https://huggingface.co/whatdhack/Llama-2-7b-hf-oasst1-s100-sg, https://huggingface.co/Waterfront/Llama-2-7b-chat-hf-social-media-captions-10k, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2_all, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_test, https://huggingface.co/vicky4s4s/Llama-2-7B-Chat-GGML, https://huggingface.co/linuscarey123/out, https://huggingface.co/dtorres-zAgile/llama2-7b-zc-domain-misti, https://huggingface.co/sschangi/uplimit-project-3-llam2, https://huggingface.co/karshPrime/biomed-llama2, https://huggingface.co/Ayansk11/InLegalLlama2-7B-chat-hf, https://huggingface.co/israelNwokedi/SEOExtractor-Llama-7b, https://huggingface.co/furquan/llama2-sentiment-prompt-tuned, https://huggingface.co/Fishball02/llama-topical-chat, https://huggingface.co/PiyushLavaniya/LLama2_Banker_LoRA_Adapters, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_2, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_3, https://huggingface.co/Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2, https://huggingface.co/PiyushLavaniya/Llama2_Summarizer_LoRA_Adapters, https://huggingface.co/bineric/NorskGPT-Llama-7B-v0.1, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-pipeline, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-sft, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-sft_fix, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-sft_fix-dpo, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-dpo, https://huggingface.co/Jukaboo/Llama2_7B_chat_DE, https://huggingface.co/Jukaboo/Llama2_7B_chat_DE_2, https://huggingface.co/Jukaboo/Llama2_7B_chat_DE_3, https://huggingface.co/Jukaboo/Llama2_7B_chat_DE_4, https://huggingface.co/codewizardUV/llama_supervised_fine-tuning-15epochs, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-dpo-_1_beta, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-dpo-_9_beta, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-dpo-_5_beta, https://huggingface.co/SanaFalakJ/results, https://huggingface.co/Yaxin1992/llama2-7b-chat-leagues-5000, https://huggingface.co/xiangliu1123/aidamodel, https://huggingface.co/codewizardUV/old_model, https://huggingface.co/xiangliu1123/openassi, https://huggingface.co/Jukaboo/Llama2_7B_chat_LR, https://huggingface.co/W3bsurf/Llawma-sum-2-7b-chat, https://huggingface.co/TusharsinghBaghel/outputs, https://huggingface.co/rajatvdoit/llama2taylor1, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft_test, https://huggingface.co/SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft, https://huggingface.co/rajatvdoit/llama2taylor3, https://huggingface.co/Lohit20/Depressed_Llama-2-7b, https://huggingface.co/ehekaanldk/lora-llama-2-7b-nsmc-understanding, https://huggingface.co/chaem/llama-2-7b-nsmc, https://huggingface.co/kjh01/dataset_infos_llama_2, https://huggingface.co/AeNyoung/lora-llama-2-7b-nsmc, https://huggingface.co/Lohit20/Therapist, https://huggingface.co/yaeeun/lora-llama-2-7b-nsmc-review-understanding, https://huggingface.co/chaem/llama-2-7b-nsmc2, https://huggingface.co/abdulrahman-nuzha/finetuned-llama2-chat-5000-v1.0-squad, https://huggingface.co/kiyeon1221/lora-llama-2-7b-food-order-understanding, https://huggingface.co/haeun161/llama-2-nsmc, https://huggingface.co/ChloeKa/lora-llama-2-7b-food-order-understanding, https://huggingface.co/RiverYou/lora-llama-2-7b-nsmc-understanding, https://huggingface.co/simoHamlili/results, https://huggingface.co/bunbohue/zero-shot-prompting-llama2-7b-chat_readsum, https://huggingface.co/MVRL/Eco-Llama-7b, https://huggingface.co/Jukaboo/Llama2_7B_chat_arithmetic, https://huggingface.co/abdulrahman-nuzha/finetuned-llama2-chat-5000-v2.0, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-2-epochs, https://huggingface.co/Jukaboo/Llama2_7B_chat_arithmetic_2, https://huggingface.co/Jukaboo/Llama2_7B_chat_arithmetic_nocarry, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run1, https://huggingface.co/retinol/llama-2-7b-psy-chat, https://huggingface.co/Jukaboo/Llama2_7B_chat_arithmetic_nocarry_20000, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run2, https://huggingface.co/arturolinares26/finetuned-llama-7b-chat-hf-sustainbility, https://huggingface.co/Jukaboo/Llama2_7B_chat_arithmetic_withcarry_10000, https://huggingface.co/Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size, https://huggingface.co/Federic/lora-fine-tuning-llama2-SQL-lora-10-dataset-size, https://huggingface.co/Federic/lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size, https://huggingface.co/Federic/lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size, https://huggingface.co/Federic/lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-2, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-3, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-4, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-5, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-6, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-7, https://huggingface.co/mojuss/finetuned-llama-7b-chat-hf-gpt-exam-8, https://huggingface.co/amit70/llama2-finetuned-squad-hf-2, https://huggingface.co/shahrukh95/Llama-2-7b-Set-1-cybersecurity-layered-config, https://huggingface.co/shahrukh95/Llama-2-7b-Set-3-cybersecurity-layered-config, https://huggingface.co/Federic/LLM-to-SQL, https://huggingface.co/Gennaro22/Test-Llama2, https://huggingface.co/Lalith16/LLAMA2-10epoch-finetuned-NXAIR, https://huggingface.co/shahrukh95/Llama-2-7b-Set-2-cybersecurity-layered-config, https://huggingface.co/codewizardUV/NXAIR_M_12-2-2024, https://huggingface.co/tsavage68/chat_350STEPS_1e5_SFT, https://huggingface.co/m7mdal7aj/fine_tuned_llama_2_7b_chat_OKVQA, https://huggingface.co/tsavage68/chat_500STEPS_1e5rate_SFT, https://huggingface.co/tsavage68/chat_700STEPS_1e4rate_01beta_DPO, https://huggingface.co/tsavage68/chat_500STEPS_1e7rate_SFT, https://huggingface.co/tsavage68/chat_300STEPS_1e7rate_SFT, https://huggingface.co/tsavage68/chat_400STEPS_1e6rate_SFT, https://huggingface.co/tsavage68/chat_150STEPS_1e6rate_SFT, https://huggingface.co/santiadavani/alpaca-gpt4-conversation-opt-350m, https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT, https://huggingface.co/tsavage68/chat_1000STEPS_1e6rate_01beta_DPO, https://huggingface.co/tsavage68/chat_150STEPS_1e7rate_01beta_DPO, https://huggingface.co/tsavage68/chat_200STEPS_1e6_01beta, https://huggingface.co/TachyHealthResearch/Llama2-7B-Medical-Finetune_V2, https://huggingface.co/tsavage68/chat_1000STEPS_1e5rate_01beta_DPO, https://huggingface.co/tsavage68/chat_1000STEPS_1e6_03beta_DPO, https://huggingface.co/tsavage68/chat_1000STEPS_1e7_03beta_DPO, https://huggingface.co/tsavage68/chat_1000STEPS_1e7rate_01beta_DPO, https://huggingface.co/tsavage68/chat_1000STEPS_1e7_05beta_DPO, https://huggingface.co/tsavage68/chat_1000STEPS_1e7rate_SFT_SFT, https://huggingface.co/tsavage68/chat_1000STEPS_1e6rate_SFT_SFT, https://huggingface.co/tsavage68/chat_1000STEPS_1e6_05beta_DPO, https://huggingface.co/andreasnaoum/CounselLlama7b, https://huggingface.co/tsavage68/chat_1000STEPS_1e5rate_SFT_SFT, https://huggingface.co/myra/broadening_llama_chat, https://huggingface.co/myra/counterexamples_llama_chat, https://huggingface.co/myra/negation_llama_chat, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs, https://huggingface.co/SaiSiddhanth/llama-2-test, https://huggingface.co/dilip025/llama-2-7b, https://huggingface.co/sh0men/autotrain-0pm1h-neolk, https://huggingface.co/seanmemery/MLP-FinLLM-7b-it, https://huggingface.co/calibration-tuning/Llama-2-7b-chat-hf-ct-choice, https://huggingface.co/calibration-tuning/Llama-2-7b-chat-hf-ct-oe, https://huggingface.co/thrunlab/sparse_llama_7b_refined_web_90p_2024-03-21, https://huggingface.co/thrunlab/sparse_llama_7b_refined_web_90p_2024-03-22, https://huggingface.co/thrunlab/sparse_llama_7b_refined_web_90p_2024-03-23, https://huggingface.co/thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24, https://huggingface.co/FriendliAI/Llama-2-7b-chat-hf-fp8, https://huggingface.co/Niyantha23M/llama-7b-chat-100k_50_50, https://huggingface.co/Niyantha23M/llama-7b-chat-100k_65_35, https://huggingface.co/Niyantha23M/llama-7b-chat-25k_50_50, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_v1, https://huggingface.co/Niyantha23M/llama-7b-chat-dummy, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_v2, https://huggingface.co/Niyantha23M/llama-7b-chat-25000-50-50-L, https://huggingface.co/Niyantha23M/llama-7b-chat-25000-25-75-L, https://huggingface.co/Niyantha23M/llama-7b-chat-25000-75-25-L, https://huggingface.co/Niyantha23M/llama-7b-chat-75000-25-75-L, https://huggingface.co/Niyantha23M/llama-7b-chat-75000-50-50-L, https://huggingface.co/Niyantha23M/llama-7b-chat-Non-Toxic-143k, https://huggingface.co/Niyantha23M/llama-7b-chat-Toxic-50k, https://huggingface.co/jfo150/llama-2-brainstems-chat, https://huggingface.co/Niyantha23M/llama-7b-chat-10000-75-25-L, https://huggingface.co/Niyantha23M/llama-7b-chat-10000-25-75-L, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_Korean_v1, https://huggingface.co/sohamslc5/new_llama_new, https://huggingface.co/sohamslc5/IIITA-Chatbot, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v1, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v2, https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v3, https://huggingface.co/armanbabayan/Llama2_Immigration_Low_Chat, https://huggingface.co/tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2, https://huggingface.co/FemkeBakker/AmsterdamDocClassificationLlama200T2Epochs, https://huggingface.co/tyzhu/lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2, https://huggingface.co/FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs, https://huggingface.co/FemkeBakker/AmsterdamDocClassificationLlama200T1Epochs, https://huggingface.co/mperestoronin/llama2-v11-chat, https://huggingface.co/abhayesian/llama2-7b-sft-lora, https://huggingface.co/datafreak/results, https://huggingface.co/PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed, https://huggingface.co/PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed, https://huggingface.co/PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed, https://huggingface.co/wenzhy7/int-llama2, https://huggingface.co/wadhma/Critique-L2-FT-DCR, https://huggingface.co/wadhma/Refine-L2-FT-DCR, https://huggingface.co/Ogamon/llama2_inst_truth_model, https://huggingface.co/bhadauriaupendra062/Llama, https://huggingface.co/Ogamon/llama2_inst_truthbench1_model, https://huggingface.co/Ogamon/llama2_inst_truthbench2_model, https://huggingface.co/FrancescoPeriti/Llama2Dictionary, https://huggingface.co/minkhantycc/Llama-2-7b-chat-finetune-quantized, https://huggingface.co/Gandretty/efcc, https://huggingface.co/subhrokomol/hindi-tokenizer, https://huggingface.co/Arjs/Llama-2-7b-chatbot-finetune, https://huggingface.co/Olivia1400/Yui, https://huggingface.co/Sohaibsoussi/llama-2-7b-miniDoctor, https://huggingface.co/zjunlp/OneGen-EntityLinking-Llama2-7B, https://huggingface.co/zjunlp/OneGen-MultiHop-Llama2-7B, https://huggingface.co/zjunlp/OneGen-SelfRAG-Llama2-7B, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mathqa, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mathqa-formula, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mathqa-formula-chinese, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mathqa-chinese, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mathqa-rationale-2, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mmlu-zh, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mmlu, https://huggingface.co/Vivian12300/llama-2-7b-chat-hf-mmlu-full, https://huggingface.co/Vivian12300/mmlu_same_f_llama2, https://huggingface.co/wentao-yuan/robopoint-v1-llama-2-7b-lora, https://huggingface.co/quarkymatter/Llama-2-7b-chat-PolicyPro, https://huggingface.co/yuktasarode/Llama-2-7b-chat-finetune, https://huggingface.co/arshandalili/autotrain-llama2-7b-chat-hf-alpaca, https://huggingface.co/bobthebuildert/bob, https://huggingface.co/Penguin5681/Llama-2-7b-chat-finetune, https://huggingface.co/gljj/llama-2-7b-chat-Singapore-fake-news-SFT, https://huggingface.co/Rak-esh-Kumar/Llama-2-7b-chat-finetune_new, https://huggingface.co/dondongwonlee/GELI, https://huggingface.co/DindaMajesty/llama2-test, https://huggingface.co/llk010502/llama-2-7b-chat-finetuned-test, https://huggingface.co/migleolop/llama-2.7FT, https://huggingface.co/trippyboi1/PAP_chatbot, https://huggingface.co/joepramatha09/Llama-2-7b-chat-hf, https://huggingface.co/drflash27/Llama-2-7b-gyani-finetune, https://huggingface.co/cipherunhsiv/Llama-2-7b-chat-fine_tune, https://huggingface.co/VaisakhKrishna/Llama-2-Emotional-ChatBot, https://huggingface.co/arshandalili/autotrain-llama2-7b-chat-hf-saferlhf, https://huggingface.co/rama6636/autotrain-n6fv7-2hjm3, https://huggingface.co/adityashisharma/chatbot, https://huggingface.co/Evan768/testEvan, https://huggingface.co/jkazdan/llama-2-7b-refusal-attack, https://huggingface.co/jkazdan/llama-2-7b-affirmation-attack, https://huggingface.co/jkazdan/llama-2-7b-chat-refusal-attack-3, https://huggingface.co/DeeWoo/Llama-2-7b-chat_FFT_CodeAlpaca-20k, https://huggingface.co/DeeWoo/Llama-2-7b-chat_FFT_Alpaca-gpt4-zh, https://huggingface.co/CharlesLi/llama_2_o1_5_full, https://huggingface.co/CharlesLi/llama_2_o1_05_full, https://huggingface.co/CharlesLi/llama_2_o1_01_full, https://huggingface.co/CharlesLi/llama_2_o1_25_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_0_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_1_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_2_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_3_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_4_full, https://huggingface.co/CharlesLi/llama_2_sky_o1_5_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_1000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_1000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_4000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_4000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_default_4000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_default_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_default_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_default_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_reflect_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_reflect_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_4o_reflect_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_100_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_500_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_500_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full, https://huggingface.co/CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_1000_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full, https://huggingface.co/CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full, https://huggingface.co/RizkyAnanda/finetuned-llama-2-7b-chat, https://huggingface.co/rachmanino/Llama-2-7B-chat-Trump-v1, https://huggingface.co/rathodj080898/Llama-2-7b-chat-finetune, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_0_full, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_1_full, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_2_full, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_3_full, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_4_full, https://huggingface.co/CharlesLi/llama_2_llama_2_code_math_5_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_0_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_1_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_2_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_3_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_4_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_5_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_0_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_1_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_2_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_3_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_4_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_5_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_0_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_1_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_2_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_3_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_4_full, https://huggingface.co/CharlesLi/llama_2_llama_2_alpaca_5_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_0_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_1_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_2_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_3_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_4_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_alpaca_5_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_0_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_1_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_2_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_3_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_4_3_epoch_full, https://huggingface.co/CharlesLi/llama_2_cot_simplest_code_math_5_3_epoch_full, https://huggingface.co/ALIN-LLM/finetune-llama-2-7b-chat-gsm8k, https://huggingface.co/CharlesLi/llama_2_4o_cot_sky_o1_0_1_epoch_full, https://huggingface.co/CharlesLi/llama_2_4o_cot_sky_o1_1_1_epoch_full, https://huggingface.co/CharlesLi/llama_2_o1_1_full, https://huggingface.co/CharlesLi/llama_2_o1_10_full, https://huggingface.co/puyol917/classification_yelp, https://huggingface.co/saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151, https://huggingface.co/saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122, https://huggingface.co/Ousso1117/SFT-meta-Llama-2-7B-mrd3, https://huggingface.co/AjayMukundS/Llama2_7B_fine_tuned, https://huggingface.co/Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum, https://huggingface.co/Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum, https://huggingface.co/CreitinGameplays/Llama-2-7b-chat-reasoning-test, https://huggingface.co/Can1sters/Bruh, https://huggingface.co/IJyad/llama-2-7b-NDMO-agent, https://huggingface.co/mayanklohani19/mergekit-slerp-ujysgyd, https://huggingface.co/Jennny/eto-Llama-2-7b-chat-hf-webshop-sft, https://huggingface.co/satyamtripathii/Nagrik_mitra_Fine_tunned_LLaMa_7b, https://huggingface.co/arham-15/llama2_7B_qphysics, https://huggingface.co/hazemOmrann14/llama2-7b-screen2words, https://huggingface.co/mayanklohani19/milan, https://huggingface.co/CharlesLi/llama2_openo1_safe_o1_4o_default_4000_100_full, https://huggingface.co/CharlesLi/llama2_openo1_safe_o1_4o_default_4000_1000_full, https://huggingface.co/CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_100_full, https://huggingface.co/CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_1000_full, https://huggingface.co/wuqiong1/PA-RAG_Llama-2-7b-chat-hf, https://huggingface.co/dp0403/results, https://huggingface.co/agoor97/Llama-2-7b-chat-hf-llama-2-7b-chat-guanaco, https://huggingface.co/Tim419/Humpback_Myx",437,"https://huggingface.co/EdwardYu/llama-2-7b-MedQuAD, https://huggingface.co/willyninja30/aria7Beta, https://huggingface.co/safetyllm/Llama-2-7b-chat-safety, https://huggingface.co/ShreyasM/llama-ad-gen, https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora, https://huggingface.co/LizzyBennet/translation_stepbystep, https://huggingface.co/SanchitaP/llama2_lora_alpaca, https://huggingface.co/zoooooooo/hw-llama-2-7B-nsmc, https://huggingface.co/marchcat73/alpaca-qlora-7b-chat, https://huggingface.co/utkmst/chimera-alpha-test1, https://huggingface.co/vincentmin/llama-2-7b-reward-oasst1, https://huggingface.co/EnDevSols/llama-2-7b-qlora-medical, https://huggingface.co/oliverwang15/FinGPT_v32_Llama2_Sentiment_Instruction_LoRA_FT, https://huggingface.co/manojkumarvohra/llama2-7B-Chat-8bit-guanaco-pico-adapter-hf, https://huggingface.co/therealcyberlord/llama2-qlora-finetuned-medical, https://huggingface.co/RonanMcGovern/Llama-2-7b-chat-hf-function-calling-adapters, https://huggingface.co/Luciano/lora-4bit-Llama-2-7b-chat-hf-lener_br, https://huggingface.co/willyninja30/llama-2-7b-chat-hf-fr-en-python, https://huggingface.co/Faradaylab/Aria_7b_v2, https://huggingface.co/RonanMcGovern/Llama-2-7b-chat-hf-function-calling-adapters-v2, https://huggingface.co/Luciano/Llama-2-7b-chat-hf-dolly-mini, https://huggingface.co/thhwarrior/Llama2-Tukl, https://huggingface.co/honzatoegel/Llama-2-7b-chat-hf-gramma-corrections-de-en-overfitt, https://huggingface.co/Luciano/Llama-2-7b-chat-hf-miniguanaco, https://huggingface.co/pierre-pessarossi/llama-2-7b-shakespeare, https://huggingface.co/nhat117/dica-llama2-7b-v2, https://huggingface.co/Luciano/Llama-2-7b-chat-peticoes-sfttrainer, https://huggingface.co/Fduv/Expense-Tracker-Llama-V2-Instruction_Fine_Tuned, https://huggingface.co/Abinesh/Llama-2_Vicuna_LoRA-13b, https://huggingface.co/shishir-dwi/llama2_with_ludwig, https://huggingface.co/Narmadat21/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Vishal24/function-calling-adapters-v4, https://huggingface.co/Vishal24/adapters-v5, https://huggingface.co/cheonyumin/flan-t5-large-financial-phrasebank-lora, https://huggingface.co/Lanzelot0/llama-2-test-finetuning, https://huggingface.co/rmuema/kaggle-x-elo-finetune-v1.2, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-keyword-category-brand-v1, https://huggingface.co/Lanzelot0/llama-fine-tune-1-epoch, https://huggingface.co/Lanzelot0/llama-fine-tune-1-epoch-2, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-pfm-function-calling-adapters-v2, https://huggingface.co/Vishal24/brand_mapping_adapter_v1, https://huggingface.co/lazaroq11/billlm, https://huggingface.co/Teddy487/LLaMA2-7b-for-OpenIE, https://huggingface.co/MananSantoki/TEST-MODEL, https://huggingface.co/accorvin/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Lanzelot0/LLama2California1, https://huggingface.co/Lanzelot0/LLama2California2, https://huggingface.co/Lanzelot0/LLama2California3, https://huggingface.co/Lanzelot0/LLama2California4, https://huggingface.co/Lanzelot0/LLama2California5, https://huggingface.co/MyBad2K/Llama-2-7b-chat-hf-function-calling-adapters-v2, https://huggingface.co/Lanzelot0/Llama2AdultEpoch1, https://huggingface.co/ysw96/my_awesome_peft_model, https://huggingface.co/zion095/llama-2-7b-lora-tagger, https://huggingface.co/jerife/llama2-dbe-difficulty, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-cat-v1.1, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-product-category-mapping-v3, https://huggingface.co/pranav29/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/intelliwork/Llama-2-7b-chat-hf-function-calling-adapters-v2, https://huggingface.co/blablabla231/llama_7b_test_finatuning, https://huggingface.co/blablabla231/llama_7b_test_finatuning-adapters, https://huggingface.co/Elliezhangy/llama2-7b_ultrasound_1.1, https://huggingface.co/tkay264/data-test, https://huggingface.co/tkay264/data-test-tk, https://huggingface.co/linuscarey123/llama2-ca, https://huggingface.co/tkay264/data-tk, https://huggingface.co/tkay264/model-test, https://huggingface.co/egehanyorulmaz/kisai-llama-2-7b-chat, https://huggingface.co/mangeshdiyewar/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Padu98/llama-2-7b-chat-ausschreibungen-epochscount-1, https://huggingface.co/anhz/llama2-7b-chat-finetuned, https://huggingface.co/LizzyBennet/sample, https://huggingface.co/tkay264/data-test-tk-tk, https://huggingface.co/Aarenwong/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/fufuf/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/sasuface/mechanic-Llama-2-7b-chat-hf, https://huggingface.co/Azzizz17/8batch_3epochs, https://huggingface.co/mangeshdiyewar/Llama-2-7b-chat-hf-fine-tuned-adapters_translation, https://huggingface.co/Mik99/test1, https://huggingface.co/gtoscano/midjourney-llama-7b-chat, https://huggingface.co/Slowblood/Llama-2-7b-chat-hf-function-calling-adapters-v2, https://huggingface.co/tkay264/data_tk_llm, https://huggingface.co/jcolab5/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Prompt48/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Aabron/AIDR, https://huggingface.co/Prompt48/Llama-2-7b-chat-hf-fine-tuned-adapters-V1, https://huggingface.co/Ekkologico/Llama-2-7b-chat-python_code_instructions_18k_alpaca, https://huggingface.co/NobodyExistsOnTheInternet/llama-2-13b-unchat, https://huggingface.co/hamxea/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-adapters, https://huggingface.co/AndyYo/ink-part-txt, https://huggingface.co/Samanvitha31/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-review-phrases-sentiments-analysis-v1, https://huggingface.co/tkay264/test-tk-123, https://huggingface.co/hswongz/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/chpardhu/Llama-2-7b-chat-hf-fine-tuned-model, https://huggingface.co/Mik99/llama2_test1, https://huggingface.co/noble6/siberia3600_llama2_7b_chat, https://huggingface.co/mangeshdiyewar/Llama-2-7b-chat-hf-vivekanadafine-tuned-adapters, https://huggingface.co/mangeshdiyewar/Llama-2-7b-chat-hf-vivekanadafine2-tuned-adapters, https://huggingface.co/SebasMena111/llama2-chat-spanish-256, https://huggingface.co/chpardhu/Llama-2-7b-chat-hf-fine-tuned-4-bit-quantized_adapt, https://huggingface.co/chpardhu/Llama-2-7b-chat-hf-fine-tuned-model_4bit_quantized, https://huggingface.co/chpardhu/Llama-2-7b-chat-hf-Quant_peft_adapt, https://huggingface.co/Mik99/llama2_test_02, https://huggingface.co/joshhu1123/DPO-llama2-no1, https://huggingface.co/Vishal24/title_cat_adapter_v1, https://huggingface.co/joshhu1123/DPO-llama2-no3, https://huggingface.co/chpardhu/Llama-Quantized-lora, https://huggingface.co/jerife/llama2-7b-chat-hf-dbe-difficulty, https://huggingface.co/AswanthCManoj/azma-llama2-chat-hf-lora-adapter, https://huggingface.co/wbtxhqt/llama-7b-chat-hf, https://huggingface.co/joshhu1123/DPO-llama2-no4, https://huggingface.co/joshhu1123/DPO-llama2-no5, https://huggingface.co/lakshay/llama2-test, https://huggingface.co/Ekkologico/Llama-2-7b-chat-python_code_instructions_tiny_codes, https://huggingface.co/LizzyBennet/SG_instruct_translate_ko_en, https://huggingface.co/joshhu1123/DPO-llama2-no6, https://huggingface.co/joshhu1123/DPO-llama2-no7, https://huggingface.co/joshhu1123/DPO-llama2-no8, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-review-phrases-sentiments-v2.1, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-review-phrases-sentiments-v2.2, https://huggingface.co/Slowblood/Llama-2-7b-chat-hf-gsb-rapid-entry-pea-v2, https://huggingface.co/ravi259/alpaca-bitcoin-tweets-sentiment, https://huggingface.co/DiegoMVM/IRN-ENTREGABLE2-FINAL-MODEL, https://huggingface.co/NAYEIRN23/MODELO3IRN, https://huggingface.co/jujbob/my-llama-7b-hf-qlora-guanaco, https://huggingface.co/naqib3110/llama-2-7b-chat-whazzat, https://huggingface.co/Mik99/italian_test_01, https://huggingface.co/fliou2/ft-chat-two-tier-v10-data-all-continue-1_epoch_9, https://huggingface.co/fliou2/ft-chat-two-tier-v10-data-all-continue-1_epoch_19, https://huggingface.co/fliou2/ft-chat-two-tier-v10-data-all-continue-1_epoch_14, https://huggingface.co/fliou2/ft-chat-two-tier-v10-data-all-continue-1_epoch_24, https://huggingface.co/Frrrrrrrrank/Llama-2-7b-chat-hf-process_engineering_one_firsttwokap, https://huggingface.co/hanchungshin/opt-6.7b-lora, https://huggingface.co/Vishal24/Keyword_category_adapter_v1, https://huggingface.co/kjh01/hw-llama-2-7B-nsmc, https://huggingface.co/Vishal24/keyword_brand_cat_adapter_v1, https://huggingface.co/Frrrrrrrrank/Llama-2-7b-chat-hf-process_engineering_one_firsttwokap_v2, https://huggingface.co/wolferobert3/llama-2-chat_factcheck_four_bit-test, https://huggingface.co/fliou2/ft-chat-two-tier-v10-data-all-continue-1_epoch_39, https://huggingface.co/ejbejaranos/ludwig-webinar, https://huggingface.co/hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-adapters-v2, https://huggingface.co/linuscarey123/llama2-ca-normal, https://huggingface.co/mmmino/summ_LoRA, https://huggingface.co/Pavanb/llama_totto_finetuning, https://huggingface.co/wolferobert3/llama-2-chat_factcheck_four_bit, https://huggingface.co/kayla0913/hw-llama2-7B-nsmc, https://huggingface.co/Firenze11/llama2-lora-finance, https://huggingface.co/cxoijve/Llama-2-7b-chat-hf, https://huggingface.co/BrunoGR/JUST_HEAR_ME-PEFT_Adapter, https://huggingface.co/guguwon/hw-llama-2-7B-nsmc, https://huggingface.co/Vishal24/title_cat_random_adapter_v1, https://huggingface.co/ssalbab/llama2-nsmc-fine-tuning, https://huggingface.co/shimwoohyeon/hw-llama-2-7b-nsmc, https://huggingface.co/isaacOnline/Llama-2-7b-chat-hf_finetuned, https://huggingface.co/fliou2/ft-chat-instruct-franklin-4090_epoch_24, https://huggingface.co/sle007/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/NotoriousH2/ManfromKorea, https://huggingface.co/thudoann/Llama-2-7b-chat-hf-Movies-FineTuned-1e, https://huggingface.co/svarna/llama-7b-mgt, https://huggingface.co/svarna/llama-2-7b-multi-mgt, https://huggingface.co/swaghjal/subtaskb-llama2-7b, https://huggingface.co/Sakil/nextword_prediction_final_finetuned_model, https://huggingface.co/abdulrahman-nuzha/finetuned-llama2-chat-v1.0, https://huggingface.co/mind22/llama-2-7b-nsmc, https://huggingface.co/virtsion/nilmformer_3apis_no_prompt_default_tokenizer_peft, https://huggingface.co/ueriniuno/lecture-llama-2-7B-food-order-understanding, https://huggingface.co/okdol/hw-llama-2-7b-nsmc, https://huggingface.co/thudoann/Llama-2-7b-chat-hf-Movies-FineTuned-2e, https://huggingface.co/nxxxn/midm_hw, https://huggingface.co/nxxxn/llama_hw, https://huggingface.co/isshogirl/hw-llama-2-7B-nsmc, https://huggingface.co/cheonyumin/lora-llama-2-7b-food-order-understanding, https://huggingface.co/byeun/hw-llama-2-7B-nsmc, https://huggingface.co/hwanmin/lecture-llama-2-7B-food-order-understanding, https://huggingface.co/ueriniuno/hw-llama-2-7B-nsmc, https://huggingface.co/Roaaa/hw-llama-2-7B-nsmc, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-title-category, https://huggingface.co/NAYEIRN23/mi-asesor-legal, https://huggingface.co/Vishal24/list_convertor_adapter_v1, https://huggingface.co/stuser2023/Llama2_7b_Couplet, https://huggingface.co/euneeei/hw-llama-2-7B-nsmc, https://huggingface.co/Vishal24/beauty_test_adapter_v1, https://huggingface.co/SmitShah22ce/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/cho3ov/hw-llama-2-7B-nsmc, https://huggingface.co/Vageesh1/Appointment_bot, https://huggingface.co/AdriMSH/resultado_2, https://huggingface.co/takesomerisks/formsTrain1, https://huggingface.co/seojin0128/hw-llama-2-7B-nsmc, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v1, https://huggingface.co/tafodile/hw-llama-2-7B-nsmc, https://huggingface.co/muktadiur/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Mik99/llama2_7b_5_samples_per_feature_class, https://huggingface.co/yjs616/my-llama-2, https://huggingface.co/Mik99/llama2_7b_10_samples_per_feature_class, https://huggingface.co/lee123321/meta_7b_conversational, https://huggingface.co/nitinbhayana/adapter_hpc_grocery_baby_beauty, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v2, https://huggingface.co/cryocoon/FGN_grupos_1_epochs_7b, https://huggingface.co/Danjie/Chadgpt-Llama2-7b, https://huggingface.co/hoangquang27/llama-2-7b-chat, https://huggingface.co/hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-adapters-v3, https://huggingface.co/isaacOnline/Llama-2-7b-chat-hf_finetunedv2, https://huggingface.co/bevy/IND_Llama_7b, https://huggingface.co/olesya2096/llama2-7b_results, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v3, https://huggingface.co/sr5434/JustinianGPT, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v4, https://huggingface.co/DeveloperZoyel/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v5, https://huggingface.co/LiamLi1991/HW02, https://huggingface.co/JessCatWu/2023_AI_HW_002, https://huggingface.co/lovejog99/AIA-HW02, https://huggingface.co/Wei-K/Llama2-7b-finetuned, https://huggingface.co/tcyuan1017/HW02, https://huggingface.co/halilozturkci/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-beauty_baby_hpc_grocery_computer_kitchen, https://huggingface.co/costpluscars/ai-ml, https://huggingface.co/meyceoz/prompt-llama-2, https://huggingface.co/Danjie/Chadgpt-Llama2-7b-conversation, https://huggingface.co/tb2pi-persistent/Llama-2-7b-chat-hf-tb2pi-peft-v6, https://huggingface.co/JessCatWu/2023_AI_HW3_FTRP, https://huggingface.co/ko102/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-client-361, https://huggingface.co/olesya2096/llama2-7b_distNER, https://huggingface.co/olesya2096/llama2-7b_distNER_mts, https://huggingface.co/hoangquang27/llam2-7b, https://huggingface.co/Boss9xy/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/Wei-K/D3_HW_model, https://huggingface.co/nitinbhayana/Llama-2-7b-chat-hf-adapter-hp-global-v1, https://huggingface.co/EdBerg/openllama-3b-peft-squad_v2, https://huggingface.co/chradden/Llama-2-7b-chat-hf-stanford-nil-policy-adapters, https://huggingface.co/ncsgobubble/rollercoaster_emotions, https://huggingface.co/Anarchist/lora_model, https://huggingface.co/Aakashk/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/ncsgobubble/rollercoaster_emotions_v3, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-banner-ocr-ner-v1, https://huggingface.co/ncsgobubble/rollercoaster_emotions_v3_dpo, https://huggingface.co/ncsgobubble/rollercoaster_emotions_v4_dpo, https://huggingface.co/ncsgobubble/rollercoaster_emotions_v5_dpo, https://huggingface.co/daochf/Lora-Meta-Llama2-7b-chat-hf-QandA_2g_v01-v04, https://huggingface.co/Anarchist/orca_llama-lora, https://huggingface.co/magnifi/llama-cls-ner-mt-chat-v2_epoch_24, https://huggingface.co/thierryteisseire/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/skverma2009/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/gadkins/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/1DS/adapter-review-phrases-sentiments-Llama-2-7b-chat-hf-v2.1, https://huggingface.co/1DS/adapter-title-brand-mapping-Llama-2-7b-chat-hf-v1, https://huggingface.co/1DS/adapter-keyword-brand-mapping-Llama-2-7b-chat-hf-v1, https://huggingface.co/1DS/adapter-category-mapping-hp-global-Llama-2-7b-chat-hf-v1, https://huggingface.co/1DS/adapter-category-mapping-beauty_baby_hpc_grocery_computer_kitchen-Llama-2-7b-chat-hf-v1, https://huggingface.co/bpben/llama_friends, https://huggingface.co/magnifi/llama-cls-ner-mt-chat-v6_epoch_24, https://huggingface.co/virtsion/nilmformer_3apis_with_prompt_custom_tokenizer, https://huggingface.co/virtsion/nilmformer_final_generic_prompt, https://huggingface.co/virtsion/nilmformer_final_generic_no_prompt_2epochs, https://huggingface.co/virtsion/nilmformer_final_generic_prompt_2, https://huggingface.co/virtsion/nilmformer_final_generic_prompt_50tokens, https://huggingface.co/ashishsr/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/virtsion/nilmformer_data_gen_1, https://huggingface.co/EdBerg/Llama-2-7B, https://huggingface.co/EdBerg/ALlama-2-7B, https://huggingface.co/KayEe/finetuned-qlora-2-7b-chat, https://huggingface.co/EdBerg/QLlama-2-7B, https://huggingface.co/EdBerg/QALlama-2-7B, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-v1, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-v1.1, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-v1.2, https://huggingface.co/virtsion/nilmformer_data_gen_2, https://huggingface.co/hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-adapters-v4, https://huggingface.co/bpben/llama_friends_block, https://huggingface.co/daochf/Lora-Meta-Llama2-7b-chat-hf-QandA_2g_v01-r2-v02, https://huggingface.co/virtsion/nilmformer_data_gen_3, https://huggingface.co/virtsion/nilmformer_data_gen_4, https://huggingface.co/virtsion/nilmformer_data_gen_5, https://huggingface.co/Byanka/finetuned_lora_llama7b_sqa_4bit, https://huggingface.co/Byanka/finetuned_lora_llama7b_sqa_4bit_120, https://huggingface.co/Byanka/finetuned_lora_llama7b_sqa_4bit_240, https://huggingface.co/Byanka/finetuned_lora_llama7b_sqa_4bit_400, https://huggingface.co/Byanka/finetuned_lora_llama7b_sqa_4bit_520, https://huggingface.co/magnifi/llama-cls-ner-mt-chat-v7.1_epoch_24, https://huggingface.co/neoxu999/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/lakshay/work-details-peft, https://huggingface.co/daochf/Lora-Meta-Llama2-7b-chat-hf-QandA_2g_v01-r2-v04, https://huggingface.co/Evan-Lin/dpo-test, https://huggingface.co/nicce/story-mixtral-8x7b-lora, https://huggingface.co/neoxu999/Llama-2-7b-chat-hf-rhdemo-fine-tuned-adapters, https://huggingface.co/ArmaanSeth/ConversationBot, https://huggingface.co/wolferobert3/llama_factcheck_four_bit_v2, https://huggingface.co/Evan-Lin/dpo-llama2-deprecated, https://huggingface.co/Falcon11/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/oivlisnet/lora-alpaca-test-2, https://huggingface.co/oivlisnet/lora-alpaca-test-3, https://huggingface.co/oivlisnet/lora-alpaca-test-4, https://huggingface.co/Vishal24/BCG_adapter_v1, https://huggingface.co/YanSte/fine_tuning_llama-2_chat_alpaca_dolly_hf, https://huggingface.co/virtsion/nilmformer_data_gen_6, https://huggingface.co/virtsion/nilmformer_data_gen_7, https://huggingface.co/Evan-Lin/dpo-llama-chat, https://huggingface.co/Evan-Lin/positive-chosen-llama-chat-without-none, https://huggingface.co/Tiabet/Tiabet-llama2-finetuned-epoch10, https://huggingface.co/Vishal24/BCG_adapter_v3, https://huggingface.co/wrannaman/test-bloom-train, https://huggingface.co/wrannaman/test-json-train, https://huggingface.co/Shreyas0706/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/ericp/mynewmodel, https://huggingface.co/virtsion/nilmformer_data_gen_9, https://huggingface.co/CapiJack/Llama-2-7b-chat-hf-UltronChat, https://huggingface.co/lucas-w/founderai-llama, https://huggingface.co/InHawK/llama-2-7b-chat-simulationbot, https://huggingface.co/Dev2410/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/emilhuzjak/llama-2-7b-integrals, https://huggingface.co/KelvinTichana2/lithmodel, https://huggingface.co/AyushRaj01/llama2_qna_tuned, https://huggingface.co/AyushRaj01/new_llama2_qna_tuned, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-rtc-rte-v1, https://huggingface.co/YieldInc/agentinstruct_os_env-filtered_v2-sharegpt, https://huggingface.co/isaacOnline/0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-rtc-rte-v1.1, https://huggingface.co/Vishal24/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-rtc-rte-v1.1, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-rtc-rte-v2.0, https://huggingface.co/NBA55/llama2-7B-diversity-improved-dataset-epoch_10, https://huggingface.co/Vishal24/adapter-sku-title-ner-generation-rtc-rte-v1.1, https://huggingface.co/NBA55/llama2-7B-improved-dataset-epoch_15, https://huggingface.co/NBA55/llama2-7B-diversity-improved-dataset-epoch_10-updated, https://huggingface.co/NBA55/llama2-7B-diversity-improved-dataset-epoch_4-updated, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-description-ner-generation-marico-v1.0, https://huggingface.co/santiadavani/alpaca-gpt4-conversation-llama2-7b-chat, https://huggingface.co/andreasnaoum/model11, https://huggingface.co/kakshak/optimoz, https://huggingface.co/Elkhayyat17/qlora-med-llama2, https://huggingface.co/hachirokoo/llama2-jc-trained, https://huggingface.co/Dev2410/MCQ_llama7b, https://huggingface.co/EsilaAycill/npc_chat_v6, https://huggingface.co/hachirokoo/my-ll2-model-trained, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-reversed-v1.0, https://huggingface.co/SaiSiddhanth/llama-2-feb-20, https://huggingface.co/Dev2410/Code_llama, https://huggingface.co/SaiSiddhanth/llama-2-7b-ft, https://huggingface.co/mahmoud-hussein16/Llama-2-7b-chat-hf-test-fine-tuned-adapters, https://huggingface.co/askenaz/results_modified, https://huggingface.co/askenaz/results_-1949220622505963237, https://huggingface.co/askenaz/results-4140812489330439434, https://huggingface.co/askenaz/results6071251431939632204, https://huggingface.co/askenaz/results1715967528936100908, https://huggingface.co/askenaz/results2673983290215444091, https://huggingface.co/askenaz/results-4278411565592416991, https://huggingface.co/askenaz/results-826824857200455454, https://huggingface.co/askenaz/results-1713965701705978838, https://huggingface.co/hongji-s/output, https://huggingface.co/askenaz/results8909736259883865477, https://huggingface.co/askenaz/results-7655726778571638724, https://huggingface.co/askenaz/results9143666266334635682, https://huggingface.co/askenaz/results5935728600342758724, https://huggingface.co/hachirokoo/my-ll2-model-trained2, https://huggingface.co/hachirokoo/my-ll2-model-trained4, https://huggingface.co/Prathyash/LLaMa2_13B_Chat-finetuned-dolly-with-exp, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-readability-v1.0, https://huggingface.co/askenaz/results-2641641906921332418, https://huggingface.co/askenaz/results1728410761713039188, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medmcqa-dev-a-quarter-3-epochs, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medmcqa-dev-a-quarter-4-epochs, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medmcqa-dev-half-4-epochs, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medmcqa-dev-three-quarters-4-epochs, https://huggingface.co/yy0514/llama2-7b-chat-qlora-lek-train-for-medmcqa-dev-full-4-epochs, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-reversed-v1.1, https://huggingface.co/Brackly/results, https://huggingface.co/vincent1337/llama2-oasst1-1k, https://huggingface.co/sivasubram/finetuned-llama-7b-chat-hf-med, https://huggingface.co/Liu-Xiang/Llama-2-7b-chat-hf-tuned-adapters, https://huggingface.co/Nadeemag/ustaadnow-qa, https://huggingface.co/cimabel/humanitarian-llm, https://huggingface.co/Dev2410/SQL_llama, https://huggingface.co/mazzaqq/SFT_4000, https://huggingface.co/Jessiecs/results, https://huggingface.co/Jessiecs/outputs, https://huggingface.co/Dev2410/SQL_llama_30_epoch_adapter, https://huggingface.co/Jessiecs/llama-2-7b-a3-1, https://huggingface.co/Jessiecs/llama-2-7b-a3-4, https://huggingface.co/neerajnarwal/Llama-2-7b-chat-Command-Generation, https://huggingface.co/mazzaqq/DPO_davide, https://huggingface.co/neerajnarwal/Llama-2-7b-chat-Question-Answering, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-reversed-v2.0, https://huggingface.co/neerajnarwal/Llama-2-7b-chat-Sentiment-Detection, https://huggingface.co/Dev2410/CR2_llama7b_120_25_rows, https://huggingface.co/dev02chandan/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/ferrorist/ferrorist-llama-2-7b-chat, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-reversed-v2.1, https://huggingface.co/Pradeeptiwarimaitri/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/azizksar/results, https://huggingface.co/azizksar/outputs, https://huggingface.co/azizksar/simpleinput, https://huggingface.co/FtMi/trained_weigths, https://huggingface.co/pivovalera2012/Llama-2-7b-Dr-House, https://huggingface.co/pivovalera2012/Llama-2-7b-Dr-Hous-test, https://huggingface.co/azizksar/simpleinput10, https://huggingface.co/azizksar/simpleinputv2, https://huggingface.co/Aharneish/Llama-Chat-final, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-ner-generation-reversed-v2.2, https://huggingface.co/chanchan7/llama-7b-dpo-qlora, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-consumer-journey-mapping-generation-v1.0, https://huggingface.co/chanchan7/llama-7b-dpo-qlora-relu, https://huggingface.co/avijra/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/virtsion/nilmformer_data_gen_10, https://huggingface.co/santiadavani/fingpt-llama2-7b-chat, https://huggingface.co/Kamyar-zeinalipour/Llama2-7B-Syntax-Instruct, https://huggingface.co/gjonesQ02/WO_PlanningAssistantConvo_A, https://huggingface.co/David-Xu/cira-7b-dpo-lora, https://huggingface.co/David-Xu/cira-7b-dpo-lora-merge, https://huggingface.co/elnasharomar2/Llama-2-7b-chat-hf-first-fine-tuned-adapters, https://huggingface.co/Basdila/your_model_name, https://huggingface.co/gjonesQ02/WO_PlanningAssistant_ChatBot, https://huggingface.co/STEVENZHANG904/finetuned_llama2_chat_7b_hf_11711HW2, https://huggingface.co/bibrani/Llama-2-7b-chat-hf, https://huggingface.co/ferrorist/llama-2-ko-7b-chat-hf-4bit, https://huggingface.co/askenaz/results1109531718420717766, https://huggingface.co/askenaz/results6746368648863215228, https://huggingface.co/mahmoud-hussein16/Llama-2-7b-chat-hf-SW2-test-fine-tuned-adapters, https://huggingface.co/gjonesQ02/WO_PlanningAssistant_ChatBot_Beta, https://huggingface.co/yaohwang/Llama-2-7b-chat-hf-push-demo-adapters, https://huggingface.co/JPishikawa/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/ferrorist/korean-llama, https://huggingface.co/vibhorag101/llama-2-7b-chat-hf-phr_mental_therapy_v2, https://huggingface.co/santiadavani/pgml-qa-llama2-7b-chat, https://huggingface.co/mbkim/LifeTox_Moderator_7B, https://huggingface.co/NassimB/LLaMa2_13B_Chat-finetuned-dolly-with-exp, https://huggingface.co/samaysk/springllama, https://huggingface.co/Xinyue123/llama2-7b-chat-openassistant-guanaco-fine-tune, https://huggingface.co/Komala/hp_finetuned-llama-7b-chat-hf, https://huggingface.co/ldowey/results, https://huggingface.co/AhmedAtef22/qlora_adapter-llama2, https://huggingface.co/Komala/hpv2_finetuned-llama-7b-chat-hf, https://huggingface.co/jhlim8/ListenerLM, https://huggingface.co/radius27/llama-7b-math-problems, https://huggingface.co/Pot-l/llama-7b-lawbot, https://huggingface.co/radius27/llama-7b-code-feedback, https://huggingface.co/radius27/llama-7b-math-problems-2, https://huggingface.co/ferrorist/korean-llama-test-240320-v01, https://huggingface.co/Yash1202/finetuned_llama, https://huggingface.co/kekunh/llama2_7b_lora_adaptor, https://huggingface.co/Pot-l/llama-7b-lawbot-true, https://huggingface.co/Kamyar-zeinalipour/llama7B_turkish_crossword_clue_gen, https://huggingface.co/samaysk/springllamaft, https://huggingface.co/smahns/listllama, https://huggingface.co/kajol/llama_code_expert_v01, https://huggingface.co/AhmedAtef22/qlora_quizzer-llama2, https://huggingface.co/ucmp137538/llama2-finetuned-iSarcasmEval, https://huggingface.co/ucmp137538/trained_weigths, https://huggingface.co/Pot-l/llama-7b-bobdylan, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_epsilon_0.25_num_steps_100, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_epsilon_0.5_num_steps_100, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_epsilon_1.0_num_steps_100, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_0_16_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_0_4_6_8_16_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_0_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_16_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_28_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_4_16_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_4_5_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_4_8_16_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_4_8_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_4_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_6_time_limit_6000, https://huggingface.co/VH1213141516/LAT_3-20_sweeps_pgd_layers_8_time_limit_6000, https://huggingface.co/ferrorist/korean-llama-test-240326-v04, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-sku-title-keywords-generation-v1.0, https://huggingface.co/nitinlohmod11/Llama-2-7b-chat-hf-adapter-title-keyword-suggestion-v1.0, https://huggingface.co/ferrorist/korean-llama-test-240329-v02, https://huggingface.co/msk18/results, https://huggingface.co/msk18/test_trainer, https://huggingface.co/saking/chat-network, https://huggingface.co/AIRLab-POLIMI/llama-2-7b-chat-hf-btgenbot-adapter, https://huggingface.co/atamb/llama2-7b-chat-mlsum, https://huggingface.co/ali-alkhars/Llama-2-CareerBud, https://huggingface.co/cviswanath/llama2-qlora-finetunined-qandawithsteps, https://huggingface.co/Star3073/results, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-spellcheck-issues-and-correction-v1.0, https://huggingface.co/achillon/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/achillon/Llama-2-7b-chat-hf-fine-tuned, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_18_epsilon_0.5_pgd_iterations_per_step_22, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_0_epsilon_0.3_pgd_iterations_per_step_28, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_0_epsilon_0.4_pgd_iterations_per_step_2, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.5_pgd_iterations_per_step_7, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_18_epsilon_2.5_pgd_iterations_per_step_7, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.2_pgd_iterations_per_step_28, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_0_6_12_18_24_epsilon_0.05_pgd_iterations_per_step_16, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_28_epsilon_2.5_pgd_iterations_per_step_2, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_8_epsilon_0.5_pgd_iterations_per_step_10, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_31_epsilon_25.0_pgd_iterations_per_step_22, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.2_pgd_iterations_per_step_2, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.4_pgd_iterations_per_step_22, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_29_epsilon_0.7_pgd_iterations_per_step_28, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.7_pgd_iterations_per_step_16, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_25_epsilon_0.9_pgd_iterations_per_step_72, https://huggingface.co/VH1213141516/LAT_4-3_sweep_1_pgd_layers_0_6_12_18_24_epsilon_5.0_pgd_iterations_per_step_16, https://huggingface.co/SwimChoi/villama2-7b-chat-Netherlands-lora, https://huggingface.co/cheungra/llama-2-7b-cncomm, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_28, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_28_epsilon_1.5, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_18, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_23, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_28_epsilon_1.0, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_31, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_29, https://huggingface.co/VH1213141516/LAT_4-5_sweep_1_pgd_layers_28_epsilon_0.6, https://huggingface.co/Meshrif/llama2-7b-chat-news-summarization-meshrif, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_29, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_31, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_0.6, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_1.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_18, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_1.5, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_23, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_10.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_0.7, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_2.5, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_2.1, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_1.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_1.5, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_1.2, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_epsilon_5.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_1.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_0.75, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_8_epsilon_1.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_8_epsilon_3.0, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_8_epsilon_0.75, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_0.6, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_8_epsilon_0.5, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_28_8_epsilon_1.5, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_1.2, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_1.8, https://huggingface.co/VH1213141516/LAT_4-6_sweep_1_pgd_layers_13_epsilon_2.5, https://huggingface.co/Jessiecs/output, https://huggingface.co/AmineSaidi-ISTIC/llama2-7b-finetuned-news-category-dataset, https://huggingface.co/FDeRubeis/araft_trained_sft, https://huggingface.co/FDeRubeis/araft_trained_dpo, https://huggingface.co/fahad0071/Therapist, https://huggingface.co/Joyqiuyue/Llama-2-7b-chat-hf-dpo, https://huggingface.co/Yan777/trained_weigths, https://huggingface.co/Joyqiuyue/output, https://huggingface.co/Joyqiuyue/lima_dpo_fine_tune, https://huggingface.co/Vishal24/adapter_new_bcg_v1, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_1.0_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_0.6_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_8.5_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_5.0_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_3.3_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_2.2_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep1_epsilon_1.5_time_limit_30000_N_checkpoints_50, https://huggingface.co/Juliofc/chaterapia_llama_model, https://huggingface.co/Niyantha23M/llama-7b-chat-50k_35_65, https://huggingface.co/Niyantha23M/llama-7b-chat-50k_50_50, https://huggingface.co/Niyantha23M/llama-7b-chat-50k_65_35, https://huggingface.co/Niyantha23M/llama-7b-chat-100k_35_65, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_31_epsilon_22_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_26_epsilon_10_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.03_time_limit_30000_N_checkpoints_50, https://huggingface.co/wenshicheng97/output, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_1.3_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_16_epsilon_4_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_4_epsilon_0.1_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_0.9_time_limit_30000_N_checkpoints_50, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_31_epsilon_36, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_31_epsilon_18, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_28_epsilon_10, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_31_epsilon_5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_31_epsilon_10, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_28_epsilon_7, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_28_epsilon_13, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_28_epsilon_18, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_28_epsilon_5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_24_epsilon_8, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_16_epsilon_8, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_16_epsilon_5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_24_epsilon_12, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_24_epsilon_3, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_24_epsilon_5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_16_epsilon_3.2, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_1.5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_0.3, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_16_epsilon_2, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_1.1, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_8_epsilon_0.6, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_4_epsilon_0.5, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_4_epsilon_0.25, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_4_epsilon_0.05, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_4_epsilon_0.1, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.7, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.03, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.3, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.15, https://huggingface.co/VH1213141516/LAT_4-10sweep2_pgd_layers_0_epsilon_0.01, https://huggingface.co/Niyantha23M/llama-7b-chat-100k_35_65_latest, https://huggingface.co/Niyantha23M/llama-7b-chat-25k_65_35-comm-liberal, https://huggingface.co/Niyantha23M/llama-7b-chat-reducedTest-comm-liberal, https://huggingface.co/totorolee27/train_llama2, https://huggingface.co/Niyantha23M/llama-7b-chat-50000-75-25-L, https://huggingface.co/Niyantha23M/llama-7b-chat-75000-75-25-L, https://huggingface.co/Niyantha23M/llama-7b-chat-190k-L, https://huggingface.co/chanchan7/llama-sft-qat, https://huggingface.co/K-kiron/llama2-7b-base, https://huggingface.co/K-kiron/llama2-7b-without-context, https://huggingface.co/Yang78ok/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/K-kiron/llama2-7b-context-prompt, https://huggingface.co/K-kiron/llama2-7b-context-combine, https://huggingface.co/Yan777/trained_weigths_2, https://huggingface.co/Star3073/outputs, https://huggingface.co/VikrantRamesh/Llama_CN_finetuned, https://huggingface.co/ASaska/Llama-2-7b-chat-hf, https://huggingface.co/ASaska/Llama-2-7b-chat-hf-ft, https://huggingface.co/tricktreat/Llama-2-7b-chat-hf-guanaco-lora, https://huggingface.co/tricktreat/Llama-2-7b-chat-hf-guanaco-prompttuning, https://huggingface.co/guoyu-zhang/model_hh_shp2_dpo5, https://huggingface.co/guoyu-zhang/model_hh_shp3_dpo5, https://huggingface.co/wenshicheng97/with_board_turn_conversation, https://huggingface.co/guoyu-zhang/model_hh_shp4_dpo5, https://huggingface.co/guoyu-zhang/model_hh_shp2_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp3_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp4_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp2_dpo1, https://huggingface.co/guoyu-zhang/model_hh_shp3_dpo1, https://huggingface.co/guoyu-zhang/model_hh_usp1_dpo5, https://huggingface.co/guoyu-zhang/model_hh_shp4_dpo1, https://huggingface.co/guoyu-zhang/model_hh_usp2_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp3_dpo5, https://huggingface.co/guoyu-zhang/model_hh_shp1_dpo7, https://huggingface.co/guoyu-zhang/model_hh_usp4_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp1_dpo9, https://huggingface.co/guoyu-zhang/model_shp1_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp2_dpo9, https://huggingface.co/guoyu-zhang/model_hh_usp3_dpo9, https://huggingface.co/guoyu-zhang/model_shp2_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp4_dpo9, https://huggingface.co/guoyu-zhang/model_hh_usp1_dpo1, https://huggingface.co/guoyu-zhang/model_shp3_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp2_dpo1, https://huggingface.co/guoyu-zhang/model_usp1_dpo5, https://huggingface.co/guoyu-zhang/model_shp4_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp3_dpo1, https://huggingface.co/guoyu-zhang/model_usp2_dpo5, https://huggingface.co/guoyu-zhang/model_hh_usp4_dpo1, https://huggingface.co/guoyu-zhang/model_shp1_dpo9, https://huggingface.co/guoyu-zhang/model_usp3_dpo5, https://huggingface.co/guoyu-zhang/model_shp2_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp1_400, https://huggingface.co/wenshicheng97/with_board_only_history, https://huggingface.co/guoyu-zhang/model_shp3_dpo9, https://huggingface.co/guoyu-zhang/model_usp4_dpo5, https://huggingface.co/guoyu-zhang/model_shp4_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp2_400, https://huggingface.co/guoyu-zhang/model_usp1_dpo9, https://huggingface.co/guoyu-zhang/model_usp2_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp3_400, https://huggingface.co/guoyu-zhang/model_usp3_dpo9, https://huggingface.co/guoyu-zhang/model_usp4_dpo9, https://huggingface.co/guoyu-zhang/model_hh_shp4_400, https://huggingface.co/guoyu-zhang/model_usp1_dpo1, https://huggingface.co/guoyu-zhang/model_hh_usp1_400, https://huggingface.co/guoyu-zhang/model_usp2_dpo1, https://huggingface.co/adediu25/trained_weights, https://huggingface.co/guoyu-zhang/model_hh_usp2_400, https://huggingface.co/guoyu-zhang/model_usp3_dpo1, https://huggingface.co/guoyu-zhang/model_usp4_dpo1, https://huggingface.co/guoyu-zhang/model_hh_usp3_400, https://huggingface.co/guoyu-zhang/model_hh_shp1_200, https://huggingface.co/guoyu-zhang/model_hh_usp4_400, https://huggingface.co/guoyu-zhang/model_hh_shp2_200, https://huggingface.co/guoyu-zhang/model_hh_shp3_200, https://huggingface.co/guoyu-zhang/model_hh_shp4_200, https://huggingface.co/guoyu-zhang/model_hh_usp1_200, https://huggingface.co/guoyu-zhang/model_hh_usp4_200, https://huggingface.co/guoyu-zhang/model_hh_usp2_200, https://huggingface.co/guoyu-zhang/model_hh_usp3_200, https://huggingface.co/guoyu-zhang/model_shp1_dpo1, https://huggingface.co/guoyu-zhang/model_shp4_dpo1, https://huggingface.co/Niyantha23M/llama-7b-chat-10000-50-50-L, https://huggingface.co/guoyu-zhang/model_shp3_dpo1, https://huggingface.co/guoyu-zhang/model_shp2_dpo1, https://huggingface.co/PhillipGuo/LAT_Unlearned_L8_Eps1_Genericized-PCA_WHP-Labels, https://huggingface.co/quirky-lats-at-mats/LAT_Unlearned_L8_Eps1_Genericized-PCA_WHP-Labels, https://huggingface.co/ping-testing/jllama2-7b-chat-dpo, https://huggingface.co/adediu25/binary_trained_weights, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-suggestion-v1.0, https://huggingface.co/wenshicheng97/with_board_only_history_with_sys_5epoch_lr1.41e-5, https://huggingface.co/Dewa/funny-llama, https://huggingface.co/1DS/adapter-title-suggestion-Llama-2-7b-chat-hf-v1, https://huggingface.co/Vishal24/sub_cat_adapter_v1, https://huggingface.co/Huma97/llama2-EquityAdvisor, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-ner-and-new-title-suggestion-v1.0, https://huggingface.co/ilhemhmz752/agrobot-ft, https://huggingface.co/DreadN0ugh7/llama-7b-chat-academy, https://huggingface.co/wenshicheng97/no_board_history_with_sys_history_v2_10epoch_lr5e-5_batch2, https://huggingface.co/chpardhu/ott_show_finetuned_llama, https://huggingface.co/magnifi/llama-cls-ner-mt-chat-v21-9_epoch_10, https://huggingface.co/AhmedAtef22/qlora_AQGM-llama2, https://huggingface.co/waelChafei/llama2-new-classification, https://huggingface.co/CNBOOMBOOM/peft-llama2-hivetalk, https://huggingface.co/Vishal24/feature_mapping_adapter_v1, https://huggingface.co/Lohit20/fyp, https://huggingface.co/DreadN0ugh7/ChatAcademy-Trained-7b, https://huggingface.co/Thimira/sinhala-llama-2-7b-chat-hf-v2, https://huggingface.co/JuanjoLopez19/Llama-2-7b-chat-hf-finetune-SWE_70_30_EN, https://huggingface.co/JuanjoLopez19/Llama-2-7b-chat-hf-finetune-SWE_90_10_EN, https://huggingface.co/JuanjoLopez19/Llama-2-7b-chat-hf-finetune-SWE_90_10, https://huggingface.co/JuanjoLopez19/Llama-2-7b-chat-hf-finetune-SWE_70_30, https://huggingface.co/abhayesian/LLama2_HarmBench_R2D2, https://huggingface.co/Kelvin950/trained_weigths, https://huggingface.co/akhilesh-mishra/results, https://huggingface.co/fahad0071/Therapist-2, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-seo-optimised-title-suggestion-v1.0, https://huggingface.co/wenshicheng97/no_board_history_with_sys_history_cicero_lr5e-5_batch10, https://huggingface.co/VH1213141516/LAT_GIBBERISH-4-12sweep1_pgd_layers_4_epsilon_0.5, https://huggingface.co/CNBOOMBOOM/peft-Llama-2-7b-chat-hf-10k-train-parameters_v4, https://huggingface.co/CNBOOMBOOM/peft-Llama-2-7b-chat-hf-10k-train-parameters_v3, https://huggingface.co/CNBOOMBOOM/peft-Llama-2-7b-chat-hf-10k-train-parameters_v2, https://huggingface.co/CNBOOMBOOM/peft-Llama-2-7b-chat-hf-10k-train, https://huggingface.co/aengusl/llama2-7b-sft-lora, https://huggingface.co/vojay/Llama-2-7b-chat-hf-mental-health, https://huggingface.co/VH1213141516/LAT_400steps_GIBBERISH-4-12_sweep_1_pgd_layers_4_epsilon_0.5, https://huggingface.co/akhileshav8/llama_chat_qa, https://huggingface.co/brockwilson12/llama-2-7b-spanish-airport, https://huggingface.co/MentalXAI/test_llama, https://huggingface.co/ioseff/trial_llm, https://huggingface.co/adrake17/Meta-Llama-2-7B-Chat-Amazon, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-ner-and-seo-title-suggestions-v1.0, https://huggingface.co/Llimy1/llama2-chat-micro-inst, https://huggingface.co/ben-wycliff/sexed-llama2-7b-sft-lora-v1, https://huggingface.co/amztheory/Llama-2-code-python, https://huggingface.co/L-NLProc/PredEx_Llama-2-7B_Pred-Exp_Instruction-Tuned, https://huggingface.co/L-NLProc/PredEx_Llama-2-7B_Pred-Exp, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned-adapters_Llama2_7b_contamination_8digits_Seed2024, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned_Llama2_7b_contamination_8digits_Seed2024, https://huggingface.co/SeanCh/llama2-traditional-chinese-rpg-qlora, https://huggingface.co/hydroxai/hydro-safe-llama2-7b-chat-peft-lora-v3, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned-adapters_Llama2_7b_contamination_all_Seed2024, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned_Llama2_7b_contamination_all_Seed2024, https://huggingface.co/Kamyar-zeinalipour/TR_QUIZ_GEN_MULTI_LLAMA7B, https://huggingface.co/Kamyar-zeinalipour/TR_QUIZ_GEN_SIMPLE_LLAMA7B, https://huggingface.co/Gaurav30/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned-adapters_Llama2_7b_contamination_all_5epochs_Seed2024, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned_Llama2_7b_contamination_all_5epochs_Seed2024, https://huggingface.co/VanCan23/DPO_Vietnamese_chatbot_checkpoint, https://huggingface.co/chihhh/attack-llama-chat-1, https://huggingface.co/fenixai/MPEP-trained, https://huggingface.co/fenixai/MPEP-and-SQuAD-trained, https://huggingface.co/chihhh/attack-llama-chat, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned-adapters_Llama2_7b_rephrasetesting_1epochs, https://huggingface.co/bmehrba/Llama-2-7b-chat-hf-fine-tuned_Llama2_7b_rephrasetesting_1epochs, https://huggingface.co/ioseff/llama2_cs, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-ner-v2.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-v3.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-title-ner-and-title-suggestions-v2.0, https://huggingface.co/yetanotherhif/Llama-2-7b-chat-hf-mental-health, https://huggingface.co/andreshere/llama-2-7b-chat-conseleur-2, https://huggingface.co/andreshere/llama-2-7b-conseleur-chat-3, https://huggingface.co/MaxSchwrzr/LLama-2-7B-Chat-Primergy-Expert, https://huggingface.co/andreshere/mental-health-conseleur-llama-2-7b-chat, https://huggingface.co/abhayesian/LLama2_HarmBench_R2D2_2, https://huggingface.co/abhayesian/LLama2_HarmBench_R2D2_3, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-ner-v1.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-millet-title-pfm-v1.0, https://huggingface.co/andreshere/llama-2-7b-chat-hf-mental-health-conseleur, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model, https://huggingface.co/al1231/Refiner-7B, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_updatePara, https://huggingface.co/mahmoud-hussein16/Llama-2-7b-chat-hf-network-test-fine-tuned-adapters, https://huggingface.co/henilp105/InjecAgent-Llama-2-7b-chat-hf, https://huggingface.co/henilp105/InjecAgent-Llama-2-7b-chat-hf-10, https://huggingface.co/henilp105/InjecAgent-Llama-2-7b-chat-hf-5, https://huggingface.co/henilp105/InjecAgent-Llama-2-7b-chat-hf-15, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData1, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData2, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData3, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData4, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData5, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData6, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_42-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_15-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Denmark-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Bulgaria-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Belgium-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_59-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_40-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-France-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-United_Kingdom-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_17-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Slovenia-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_38-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Cyprus-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_60-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Sweden-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_14-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_3-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_44-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_9-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Estonia-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_41-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_1-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_61-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Switzerland-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_49-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_8-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Portugal-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_39-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_6-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_12-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_10-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Albania-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_64-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_13-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Finland-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Spain-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_52-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Hungary-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_37-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_56-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Israel-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_63-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_5-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_2-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_58-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Lithuania-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_54-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_55-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Slovakia-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Ukraine-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Russia-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Czech-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Ireland-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_50-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Iceland-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_7-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_45-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Norway-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Italy-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_62-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_16-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_51-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_43-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_57-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_36-lora, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData7, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_46-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_4-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_18-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_48-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Poland-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Kosovo-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_47-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_11-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_53-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_65-lora, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData8, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-bullet-point-descriptions-v1.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-bullet-point-descriptions-v1.1, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_20-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_21-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_22-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_23-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_24-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_25-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_26-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_27-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_28-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_30-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_31-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_32-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_33-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_34-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_35-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_66-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_67-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_68-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_69-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_70-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_71-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_72-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_73-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_74-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_75-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_76-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_77-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_78-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_79-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_80-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_81-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_82-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_83-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_84-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_85-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_86-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_87-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_88-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_89-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_90-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_91-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_92-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_93-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_94-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_95-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_96-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_97-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_98-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_99-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_100-lora, https://huggingface.co/SwimChoi/villama2-7b-chat-Group_19-lora, https://huggingface.co/mahmoud-hussein16/Llama-2-7b-chat-hf-system-design-test-fine-tuned-adapters, https://huggingface.co/MartaTT/MonsterAPILLM, https://huggingface.co/LisaSchunke/llama-2-7b-blogpost-finetuned-20000-dataset, https://huggingface.co/yizhujiao/llama3-8b-sft-math, https://huggingface.co/swan07/discord, https://huggingface.co/CarlosPov/Llama-2-7b-chat-hf-finetune_90_10_EX, https://huggingface.co/CarlosPov/Llama-2-7b-chat-hf-finetune_90_10_SY, https://huggingface.co/CarlosPov/Llama-2-7b-chat-hf-finetune_90_10_MIX, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-feature-extraction-function-calling-v1.0, https://huggingface.co/CarlosPov/Llama-2-7b-chat-hf-finetune_90_10_SY_gold, https://huggingface.co/CarlosPov/Llama-2-7b-chat-hf-finetune_90_10_MIX_gold, https://huggingface.co/blai88/reward_modeling_anthropic_hh, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-feature-extraction-unilever-v1.0, https://huggingface.co/henilp105/zephyr-7b-sft-qlora, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-seo-optimised-bullet-point-suggetions-v1.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-feature-extraction-unilever-v1.1, https://huggingface.co/JamesBentley/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/imdiddu/e-medicine-Llama2-Lora, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-cleaners-cat-subcat-v1.0, https://huggingface.co/gaurang11/llama-sql-llm-v1, https://huggingface.co/gaurang11/llama-sql-llm-v2, https://huggingface.co/Anvithah/backward-model-llama-7b-lora, https://huggingface.co/migleolop/FT2EPOCH5K, https://huggingface.co/migleolop/resultsv1.2, https://huggingface.co/migleolop/ManualUploadFTv7-19, https://huggingface.co/migleolop/FT1EPOCH5K, https://huggingface.co/migleolop/3Epoch5KConv, https://huggingface.co/Anvithah/forward-model-llama-7b-lora, https://huggingface.co/Anvithah/final-fine-tuned-model-llama-7b-lora, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData9, https://huggingface.co/Casper0508/MSc_llama2_finetuned_model_secondData10, https://huggingface.co/milanimcgraw/llama27bchat_neurosummarize, https://huggingface.co/nlpett/llama-2-7b-chat-hf-AT-hh, https://huggingface.co/nlpett/llama-2-7b-chat-hf-LAT-layer4-hh, https://huggingface.co/EdBerg/falcon7binstruct_mentalhealthmodel_oct23, https://huggingface.co/EdBerg/output_baha_trained, https://huggingface.co/EdBerg/Baha_1, https://huggingface.co/rajs17/hpv2_finetuned-llama-7b-chat-hf, https://huggingface.co/ContinuousAT/Llama-2-7B-CAT, https://huggingface.co/Suramya/content, https://huggingface.co/selimsheker/llama2_7b_test_4, https://huggingface.co/agamage/results, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-extract-product-packaging-v1.0, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-korean, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-arabic, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-chinese, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-portuguese, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-spanish, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-combined, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-bengali, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-english, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-german, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-greek, https://huggingface.co/faridlazuarda/valadapt-llama-2-7b-chat-hf-turkish, https://huggingface.co/MartaTT/Best2Models, https://huggingface.co/MattReeken/Llama-2-7b-4bit, https://huggingface.co/migleolop/FTmodel7-24, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-laundry-and-fabric-product-category-v1.0, https://huggingface.co/Basdila/CTI_llama, https://huggingface.co/langecod/stock_bot, https://huggingface.co/fahad0071/stock_bot, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-laundry-and-fabric-product-category-v1.1, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-laundry-and-fabric-product-category-v1.2, https://huggingface.co/sandeepaffine/hf_lora_llama2_lr_cosine_outputs, https://huggingface.co/usc-isi/Llama2-Advisor, https://huggingface.co/xiangr/fingpt-forecaster_dow30_llama2-7b_lora, https://huggingface.co/fahad0071/stock_botf, https://huggingface.co/romaneng/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-laundry-and-fabric-product-category-v1.3, https://huggingface.co/DevQuasar/llama2_7b_chat_brainstorm-legacy-v3.1_adapter, https://huggingface.co/andrew6431/Llama-2-7b-chat-hf-mental-health, https://huggingface.co/yajuvendra/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/yajuvendra/Llama-2-7b-chat-hf-fine-tuned, https://huggingface.co/Lines/example_delia, https://huggingface.co/migleolop/Sep1FineTune, https://huggingface.co/nekokiku/meta-llama-Llama-2-7b-chat-hf-1725437678, https://huggingface.co/mahmoud-hussein16/Llama-2-7b-chat-hf-text2pandas-command-engine-fine-tuned-adapters, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-brand-mapping-v2.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-toilet-bathroom-and-all_purpose_cleaner-category-v1.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-cat-subcat-mapping-v2.0, https://huggingface.co/shaktiman404/outputs, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-uniliver-product-extraction-training-data-v2.1, https://huggingface.co/yogi733/Llama-2-7b-chat-hf-fine-tuned-adapters, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-consumer-journey-mapping-generation-v2.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-Product-Feature-Mapping-27Sept-generation-v2.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-unilever-feature-mapping-generation-v2.0, https://huggingface.co/yogi733/LLaMa2_13B_Chat-finetuned-dolly-with-exp, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-product-features-generation-v2.0, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-product-features-generation-v2.1, https://huggingface.co/Niki-1115/llama-meditation-optimized, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-product-features-unilever-2_Oct-v2.0, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-product-features-3_OCT_Final-v2.0, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-splitData-product-featuresUnilever-oct_3-v2.0, https://huggingface.co/Niki-1115/llama-meditation-optimized-4, https://huggingface.co/Niki-1115/llama-meditation-optimized-5, https://huggingface.co/EdBerg/mergel2_gleanings_baha, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-hair_oil_cat, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-subcat-presence-checker-v2.0, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-subcat-presence-checker2-11oct-v2.0, https://huggingface.co/EdBerg/Baha_9MA, https://huggingface.co/ijuliet/Llama-2-7b-chat-hf-mental-health, https://huggingface.co/Swap07/CounselLlama7B, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-format-suggestion-beauty, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-format-suggestion-150, https://huggingface.co/meiflwr/cs329x-prism-dpo, https://huggingface.co/micheliu/content, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-suggest-first, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-suggest-first2, https://huggingface.co/subhrokomol/llama-2-7b-lora-conversation-quality, https://huggingface.co/Basdila/sd2, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-suggest-21-OCT, https://huggingface.co/shivanikerai/Llama-2-7b-chat-hf-adapter-beauty-product-title-suggestions-v1.0, https://huggingface.co/mausombi/seekhan_ft, https://huggingface.co/madhavsinghabcde/Llama-2-7b-chat-hf-adapter-title-suggest-data-split, https://huggingface.co/shastraai/Shastra-LLAMA2-Code-SFT, https://huggingface.co/Mousazzz/KemetPass, https://huggingface.co/JesseLiu/llama_dialogue, https://huggingface.co/Jyz1331/llama-2-7b-mental-health-v1, https://huggingface.co/adunca08/EnglishOnlyTLAT, https://huggingface.co/adunca08/EnglishVietnameseTest, https://huggingface.co/rohancsalvi/llama-7b-heal_me, https://huggingface.co/HuyALT/Lora-TuyenSinhPTIT2024, https://huggingface.co/adunca08/MultilingualTrainSFT, https://huggingface.co/adunca08/MultilingualTrainEnglishSFT, https://huggingface.co/adunca08/FixedEnglishVietnamese, https://huggingface.co/adunca08/FixedMultingualAll, https://huggingface.co/saavysingh/llama2-rag, https://huggingface.co/gaheeBang/peft-adapter-harrypotter-4bit, https://huggingface.co/ArchSid/AG-Llama-2-7b, https://huggingface.co/jiyeony/peft_harrypotter_8bit, https://huggingface.co/ArchSid/En-Gu_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/En-Hi_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/En-Mr_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/En-Ta_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/En-Te_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/Et-En_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/Ne-En_Mono-AG-Llama-2-7b, https://huggingface.co/ArchSid/Si-En_Mono-AG-Llama-2-7b, https://huggingface.co/jtan4albany/jtan4albany-llama-2-7b-chat, https://huggingface.co/Arkajyoti/Llama-2-7b-finetuned, https://huggingface.co/omarsajid/ChatDoctor, https://huggingface.co/Arkajyoti/Llama-2-7b-finetuned-11096-combined, https://huggingface.co/chitranshu324324/llama7b-chat-ramayana, https://huggingface.co/Arkajyoti/Llama-2-7b-CNN-Finetuned-5548, https://huggingface.co/Arkajyoti/Llama-2-7b-CNN-Finetuned-NEG-2774, https://huggingface.co/Arkajyoti/Llama-2-7b-CNN-Finetuned-NEG-2774-2, https://huggingface.co/Arkajyoti/Llama-2-7b-finetuned-11096-combined-2, https://huggingface.co/CharlesLi/llama_2_alpaca_per_class_reflect, https://huggingface.co/CharlesLi/llama_2_alpaca_cot_simplest, https://huggingface.co/CharlesLi/llama_2_alpaca_cot_true_simple, https://huggingface.co/CharlesLi/llama_2_alpaca_llama_2, https://huggingface.co/CharlesLi/llama_2_alpaca_helpful, https://huggingface.co/CharlesLi/llama_2_gsm8k_helpful, https://huggingface.co/CharlesLi/llama_2_gsm8k_llama_2, https://huggingface.co/CharlesLi/llama_2_gsm8k_per_class_reflect, https://huggingface.co/CharlesLi/llama_2_gsm8k_gold_answer, https://huggingface.co/CharlesLi/llama_2_gsm8k_final_answer, https://huggingface.co/CharlesLi/llama_2_gsm8k_cot_true_simple, https://huggingface.co/CharlesLi/llama_2_unsafe_helpful, https://huggingface.co/CharlesLi/llama_2_unsafe_llama_2, https://huggingface.co/CharlesLi/llama_2_unsafe_per_class_reflect, https://huggingface.co/CharlesLi/llama_2_gsm8k_cot_simplest, https://huggingface.co/Jyz1331/llama-2-7b-mental-health-v2, https://huggingface.co/CharlesLi/llama_2_alpaca_midset_helpful, https://huggingface.co/CharlesLi/llama_2_gsm8k_midset_cot_simplest, https://huggingface.co/CharlesLi/llama_2_gsm8k_midset_helpful, https://huggingface.co/katsuchi/Llama-2-7b-chat-hf-story-telling-finetune, https://huggingface.co/katsuchi/Llama-2-7b-chat-hf-horrorstory-generation-finetune, https://huggingface.co/katsuchi/Llama-2-7b-chat-hf-wikipedia-facts-finetune, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-gsm8k-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-100-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-500-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-1000-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-2500-HF, https://huggingface.co/mikekubi/task-1-meta-llama-Llama-2-7b-chat-hf, https://huggingface.co/rehan018/hr_assistant_model, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-pubmedqa-HF, https://huggingface.co/krimson1/Llama2-7b-chat-hf-linkedin, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-250-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-1500-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-safety-2000-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-pubmedqa-HF-5e5, https://huggingface.co/ketchup123/llama-2-7b-chat-pubmedqa-safeinstruct-num-samples-100-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-pubmedqa-safeinstruct-num-samples-500-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-harmfulqa-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-hf-advbench-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-pubmedqa-safeinstruct-num-samples-1000-HF, https://huggingface.co/ketchup123/llama-2-7b-chat-pubmedqa-safeinstruct-num-samples-2500-HF, https://huggingface.co/avinasht/finetunedModel, https://huggingface.co/AleManera/fine-tuned-llama, https://huggingface.co/amixh/llama7b-legallyai-docsum, https://huggingface.co/siwon23/fine_tuned_model, https://huggingface.co/siwon23/llama-2-kdt-finetuned, https://huggingface.co/BackdoorLLM/Jailbreak_Llama2-7B_BadNets, https://huggingface.co/kanakrajarora/results, https://huggingface.co/langzippkkk/llama2-7b-chat-bbc, https://huggingface.co/DDiaa/WM-Removal-EXP-Llama-2-7B, https://huggingface.co/DDiaa/WM-Removal-KGW-Llama-2-7B, https://huggingface.co/langzippkkk/arxiv_finetuned, https://huggingface.co/MelisaO/llama2-violencia_genero, https://huggingface.co/Polly1231/llaMA-HF-CHAT-7b-wildvision-utility, https://huggingface.co/sanjaypn14/LLaMA-EatFit-2-7b-chat, https://huggingface.co/Wuhuwill/llama-7b-chat-backdoor-lora, https://huggingface.co/Shegun93/NAIRS, https://huggingface.co/prajubhao/llama-legal-ai, https://huggingface.co/satyamtripathii/Fine_tunned_LLaMa2-7b-chat-hf, https://huggingface.co/diyanigam/CookBook, https://huggingface.co/jxiao986/Lima-finetuned, https://huggingface.co/Jsevere/llama2-7b-admissions-qa-merged, https://huggingface.co/pritmanvar/outputs, https://huggingface.co/guangyi123/backward-lora, https://huggingface.co/guangyi123/llm-lora-finetuning, https://huggingface.co/guangyi123/meta-llama2-7b-instruction-tuned-lora, https://huggingface.co/Aman12345678/llama_finetune_tofu, https://huggingface.co/towhid2000bd/Llama2-Instruct-7B, https://huggingface.co/Princekumar8132/llama-medical, https://huggingface.co/RosalinaS/results, https://huggingface.co/hmankar01/slangbot, https://huggingface.co/Shubhu07/llama2-7B-chat-lora-hawaiifire, https://huggingface.co/sujoy0011/kiit-llama2-lora-adapters",1129,"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF, https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ, https://huggingface.co/TheBloke/Llama-2-7B-Chat-AWQ, https://huggingface.co/matrixportal/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/Andrei-Alex/Fine-Tune-Adapters, https://huggingface.co/TheBlake/Llama-2-7b, https://huggingface.co/ifaouibadi/Llama-2-7b-Verse-Chat, https://huggingface.co/247labs/Llama-2-7b-Verse-Bot, https://huggingface.co/BLACKBUN/llama-2-7b-pubmed-qa-211k-gguf_q8_0, https://huggingface.co/mluca/llm-13-b-80-emo-finetuned, https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF, https://huggingface.co/vicky4s4s/Llama-2-7B-Chat-GGUF, https://huggingface.co/CallMeDaniel/Llama-2-7b-chat-hf_vn, https://huggingface.co/e-valente/Llama-2-7B-Chat-GGUF, https://huggingface.co/pcuenq/Llama-2-7b-chat-gguf, https://huggingface.co/jamesdborin/llama2-7b-chat-4bit-AWQ, https://huggingface.co/bienpr/Llama-2-7B-Chat-GPTQ, https://huggingface.co/cmeraki/OpenHathi-7B-Hi-v0.1-Base-gptq, https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC, https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f32_1-MLC, https://huggingface.co/MaziyarPanahi/Llama-2-7b-chat-hf-GGUF, https://huggingface.co/twhoool02/Llama-2-7b-hf-AutoGPTQ, https://huggingface.co/VikrantRamesh/Llama-2-CN, https://huggingface.co/twhoool02/Llama2-7b-chat-HF-NF4, https://huggingface.co/twhoool02/Llama-2-7b-chat-hf-AWQ, https://huggingface.co/nluai/Llama-2-7b-chat-hf-vn, https://huggingface.co/VikrantRamesh/llama_CN_pretrain, https://huggingface.co/leliuga/Llama-2-7b-chat-hf-bnb-4bit, https://huggingface.co/gaianet/Vitalik-ETH-Llama2-7B-Chat-GGUF, https://huggingface.co/TitanML/llama2-7b-chat-4bit-AWQ, https://huggingface.co/Ranjanunicode/unicode-llama-2-chat-Hf-q4-gguf, https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q0f16-MLC, https://huggingface.co/Funny-Meow/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/morrissas/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/zhentaoyu/Llama-2-7b-chat-hf-Q4_0-GGUF, https://huggingface.co/PrunaAI/meta-llama-Llama-2-7b-chat-hf-bnb-4bit-smashed, https://huggingface.co/siacus/llama-2-7b-dv, https://huggingface.co/siacus/llama-2-7b-small-dv, https://huggingface.co/zhentaoyu/Llama-2-7b-chat-hf-Q4_K_S-GGUF, https://huggingface.co/Georgia47/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/yashagra/Llama-2-7b-chat-hf-Q4_0-GGUF, https://huggingface.co/ar08/Llama-2-7b-chat-hf-IQ3_M-GGUF, https://huggingface.co/aistuffllm/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/llmware/llama-2-chat-ov, https://huggingface.co/irresistiblegrace97/Llama-2-7b-chat-hf-Q2_K-GGUF, https://huggingface.co/irresistiblegrace97/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/llmware/llama-2-chat-onnx, https://huggingface.co/amd/Llama2-7b-chat-awq-g128-int4-asym-bf16-onnx-ryzen-strix, https://huggingface.co/siacus/llama-2-7b-cap_verified, https://huggingface.co/rockon1095/Llama-2-7b-chat-hf-Q4_0-GGUF, https://huggingface.co/siacus/llama-2-7b-cap_verified-final-and-last, https://huggingface.co/tensorblock/Llama-2-7b-chat-hf-GGUF, https://huggingface.co/sibikarthik/Llama-2-7b-chat-hf-Q4_0-GGUF, https://huggingface.co/siacus/llama2-7B-swb-FT-Q4_K_M.gguf, https://huggingface.co/Spockkk/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/espressor/meta-llama.Llama-2-7b-chat-hf_W8A8_FP8, https://huggingface.co/espressor/meta-llama.Llama-2-7b-chat-hf_W4A16, https://huggingface.co/espressor/meta-llama.Llama-2-7b-chat-hf_W8A8_int8, https://huggingface.co/amd/Llama-2-7b-chat-hf-awq-g128-int4-asym-fp16-onnx-hybrid, https://huggingface.co/Liuzzyy/Llama-2-7b-chat-hf-Q4_K_M-GGUF, https://huggingface.co/surrenderoz/llama2_7_q4, https://huggingface.co/halflucifer/et2-experimental, https://huggingface.co/PrunaAI/meta-llama-Llama-2-7b-chat-hf-GGUF-smashed, https://huggingface.co/Alen969/zhijiaozhuanjia, https://huggingface.co/agraj07/Llama_2_7b_hf_quantized, https://huggingface.co/amd/Llama-2-7b-chat-hf-awq-g128-int4-onnx-directml",66,"https://huggingface.co/ayousanz/llama-ca-7B-slerp, https://huggingface.co/Yaxin1992/zephyr-beta-llama2-7b-ties, https://huggingface.co/antoandgar/SVD_Franken_merge1, https://huggingface.co/GianlucaMondillo/BioTakuya",4,"BAAI/open_cn_llm_leaderboard, Illia56/Ask-AI-Youtube, Intel/low_bit_open_llm_leaderboard, Plachta/Seed-VC, allenai/WildBench, allenai/ZebraLogic, awacke1/GPT-4o-omni-text-audio-image-video, baconnier/prompt-plus-plus, eduagarcia/open_pt_llm_leaderboard, huggingface-projects/llama-2-7b-chat, huggingface/InferenceSupport/discussions/880, mteb/leaderboard, qingxu98/gpt-academic",13
selfmaker/llama2-7B-xsum,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
- LLAMA2
- SAMSUM
- XSUM
- SUMMARISATION
- DIALOG
datasets:
- samsum
model-index:
- name: llama-output
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-output

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the samsum dataset.


## Model description

The model is a fine-tuned version of Llama-2-7b-chat-hf using int8 quantization and LoRA. By using this configuration, approximately 6% of parameters are trainable.

## Intended uses & limitations

It is intended to improve the summarisation capacities of Llama 2 7B on dialogs, expecting to produce a concise brief.
As it is trained on the dataset SamSum, the terms of use are limited to those of the non-commercial licence: CC BY-NC-ND 4.0

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3","{""id"": ""selfmaker/llama2-7B-xsum"", ""author"": ""selfmaker"", ""sha"": ""5550089f5a18455c3b3e5d27d1a4ce7ac5f4e516"", ""last_modified"": ""2025-01-27 16:39:11+00:00"", ""created_at"": ""2023-09-13 19:19:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""LLAMA2"", ""SAMSUM"", ""XSUM"", ""SUMMARISATION"", ""DIALOG"", ""dataset:samsum"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- samsum\ntags:\n- generated_from_trainer\n- LLAMA2\n- SAMSUM\n- XSUM\n- SUMMARISATION\n- DIALOG\nmodel-index:\n- name: llama-output\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-output"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-01-27 16:39:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- samsum\ntags:\n- generated_from_trainer\n- LLAMA2\n- SAMSUM\n- XSUM\n- SUMMARISATION\n- DIALOG\nmodel-index:\n- name: llama-output\n  results: []"", ""transformersInfo"": null, ""_id"": ""65020b4c3767e3952cc4e5f9"", ""modelId"": ""selfmaker/llama2-7B-xsum"", ""usedStorage"": 16827016}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=selfmaker/llama2-7B-xsum&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bselfmaker%2Fllama2-7B-xsum%5D(%2Fselfmaker%2Fllama2-7B-xsum)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/BashitAli/llama-2-7b-chat.ggmlv3.q5_K_M,N/A,N/A,1,,0,,0,,0,,0,,0
RakshitAi/AtmaLLaMA,"---
license: mit
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
---
# AtmaLLaMA

## Model Details
**Model Name:** AtmaLLaMA  
**Model Type:** Fine-tuned LLaMA 2  
**Domain:** Philosophy, Spirituality, Ancient Wisdom  
**Training Data:** Bhagavad Gita, Patanjali Yoga Sutras, and other philosophical texts  
**Hosting Platform:** Hugging Face  
**License:** MIT  

## Model Description
AtmaLLaMA is a fine-tuned version of LLaMA 2, trained on ancient philosophical texts such as the Bhagavad Gita and the Patanjali Yoga Sutras. It is designed to generate insightful, spiritually aligned responses based on Indian philosophical wisdom. The model aims to provide thoughtful and meaningful discourse on topics related to self-awareness, dharma, meditation, and ethical living.

## Use Cases
- Answering philosophical and spiritual queries
- Generating summaries and interpretations of ancient texts
- Assisting in guided meditation and self-reflection exercises
- Exploring ethical and moral dilemmas based on Indian philosophy

## Model Performance
- **Accuracy:** The model generates highly relevant responses in the domain of Indian philosophy and spirituality. However, it may not be perfect in complex theological debates or contemporary issues outside its training domain.
- **Biases & Limitations:** The model primarily reflects the perspectives of the texts it was trained on. While it provides coherent answers, users should cross-reference responses with authentic sources for deeper study.
- **Handling Misinformation:** The model is not designed to be a substitute for scholarly research and should be used for guidance rather than absolute truths.

## Ethical Considerations
- The model should not be used for religious debates or as an authoritative source of religious doctrine.
- Users should verify responses for accuracy when using the model in academic or professional settings.
- The model does not replace spiritual guidance from qualified practitioners.

## How to Use
Using model and tokenizer directly
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = ""RakshitAi/AtmaLLaMA""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

input_text = ""What is the essence of dharma?""
inputs = tokenizer(input_text, return_tensors=""pt"")
outputs = model.generate(**inputs)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```
or using pipeline
```python
from transformers import pipeline

model_name = ""RakshitAi/AtmaLLaMA""
generator = pipeline(""text-generation"", model=model_name)

input_text = ""What is the essence of dharma?""
response = generator(input_text, max_length=200, do_sample=True)
print(response[0][""generated_text""])
```

## Future Improvements
- Expanding training data to include Upanishads, Vedas, and other spiritual texts
- Improving response coherence and contextual understanding
- Fine-tuning on contemporary philosophical discussions for broader relevance

## Acknowledgments
Special thanks to the authors and translators of the Bhagavad Gita and Patanjali Yoga Sutras for their invaluable contributions to spiritual wisdom.","{""id"": ""RakshitAi/AtmaLLaMA"", ""author"": ""RakshitAi"", ""sha"": ""18460bea8cb0f9d8c8391bd1d8b3597c5ee4c8c5"", ""last_modified"": ""2025-02-25 08:02:04+00:00"", ""created_at"": ""2024-12-29 10:04:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 5, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: mit"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-25 08:02:04+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: mit"", ""transformersInfo"": null, ""_id"": ""67711ec8d13f036b8bf9c766"", ""modelId"": ""RakshitAi/AtmaLLaMA"", ""usedStorage"": 26954331470}",1,,0,,0,"https://huggingface.co/mradermacher/AtmaLLaMA-GGUF, https://huggingface.co/mradermacher/AtmaLLaMA-i1-GGUF",2,,0,huggingface/InferenceSupport/discussions/new?title=RakshitAi/AtmaLLaMA&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BRakshitAi%2FAtmaLLaMA%5D(%2FRakshitAi%2FAtmaLLaMA)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DeeWoo/Llama-2-7b-chat_FFT_GSM8K,"---
library_name: transformers
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: llama2_FFT_GSM8K_v5_task
  results: []
datasets:
- openai/gsm8k
---
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_FFT_GSM8K_v5_task

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co//meta-llama/Llama-2-7b-chat-hf) on the GSM8K dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 16
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- total_train_batch_size: 64
- total_eval_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- num_epochs: 3.0
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 2.19.2
- Tokenizers 0.19.1


---
license: apache-2.0
---","{""id"": ""DeeWoo/Llama-2-7b-chat_FFT_GSM8K"", ""author"": ""DeeWoo"", ""sha"": ""2b780b1e6ef708c353dded0097bbf6bce249f18d"", ""last_modified"": ""2024-12-31 01:36:04+00:00"", ""created_at"": ""2024-12-30 01:54:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 9, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""dataset:openai/gsm8k"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_FFT_GSM8K_v5_task\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_FFT_GSM8K_v5_task"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='latest', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_0.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_1.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_2.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_3.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='zero_to_fp32.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-31 01:36:04+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_FFT_GSM8K_v5_task\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6771fd500c752b7ed3dc13cf"", ""modelId"": ""DeeWoo/Llama-2-7b-chat_FFT_GSM8K"", ""usedStorage"": 13477431411}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DeeWoo/Llama-2-7b-chat_FFT_GSM8K&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDeeWoo%2FLlama-2-7b-chat_FFT_GSM8K%5D(%2FDeeWoo%2FLlama-2-7b-chat_FFT_GSM8K)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DongkiKim/Mol-Llama-2-7b-chat,"---
license: apache-2.0
datasets:
- DongkiKim/Mol-LLaMA-Instruct
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
tags:
- biology
- chemistry
- medical
---

# Mol-Llama-2-7b-chat
[[Project Page](https://mol-llama.github.io/)] [[Paper](https://arxiv.org/abs/2502.13449)] [[GitHub](https://github.com/DongkiKim95/Mol-LLaMA)]

This repo contains the weights of Mol-LLaMA including the LoRA weights and projectors, based on [meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).

## Architecture
![image.png](architecture.png)
1) Molecular encoders: Pretrained 2D encoder ([MoleculeSTM](https://huggingface.co/chao1224/MoleculeSTM)) and 3D encoder ([Uni-Mol](https://huggingface.co/dptech/Uni-Mol-Models))
2) Blending Module: Combining complementary information from 2D and 3D encoders via cross-attention
3) Q-Former: Embed molecular representations into query tokens based on [SciBERT](https://huggingface.co/allenai/scibert_scivocab_uncased)
4) LoRA: Adapters for fine-tuning LLMs


## Training Dataset

Mol-LLaMA is trained on [Mol-LLaMA-Instruct](https://huggingface.co/datasets/DongkiKim/Mol-LLaMA-Instruct), to learn the fundamental characteristics of molecules with the reasoning ability and explanbility.

## How to Use

Please check out [the exemplar code for inference](https://github.com/DongkiKim95/Mol-LLaMA/blob/master/playground.py) in the Github repo.


## Citation

If you find our model useful, please consider citing our work.
```
@misc{kim2025molllama,
    title={Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model},
    author={Dongki Kim and Wonbin Lee and Sung Ju Hwang},
    year={2025},
    eprint={2502.13449},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

## Acknowledgements

We appreciate [LLaMA](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), [3D-MoLM](https://huggingface.co/Sihangli/3D-MoLM), [MoleculeSTM](https://huggingface.co/chao1224/MoleculeSTM), [Uni-Mol](https://huggingface.co/dptech/Uni-Mol-Models) and [SciBERT](https://huggingface.co/allenai/scibert_scivocab_uncased) for their open-source contributions.","{""id"": ""DongkiKim/Mol-Llama-2-7b-chat"", ""author"": ""DongkiKim"", ""sha"": ""a0169db385fd14c0b8edc805eeb2ecefd6710422"", ""last_modified"": ""2025-04-11 08:39:55+00:00"", ""created_at"": ""2025-04-10 06:58:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 9, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""mol_llama"", ""biology"", ""chemistry"", ""medical"", ""en"", ""dataset:DongkiKim/Mol-LLaMA-Instruct"", ""arxiv:2502.13449"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- DongkiKim/Mol-LLaMA-Instruct\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- biology\n- chemistry\n- medical"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""MolLLaMA""], ""model_type"": ""mol_llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='architecture.png', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 170913880}, ""total"": 170913880}, ""security_repo_status"": null, ""lastModified"": ""2025-04-11 08:39:55+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- DongkiKim/Mol-LLaMA-Instruct\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- biology\n- chemistry\n- medical"", ""transformersInfo"": null, ""_id"": ""67f76c0d2c2ed1dee76d9c6f"", ""modelId"": ""DongkiKim/Mol-Llama-2-7b-chat"", ""usedStorage"": 343576003}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DongkiKim/Mol-Llama-2-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDongkiKim%2FMol-Llama-2-7b-chat%5D(%2FDongkiKim%2FMol-Llama-2-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
ShreySharma07/maths-llama-qlora,"---
library_name: transformers
license: mit
datasets:
- openai/gsm8k
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
new_version: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: question-answering
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->
This model is trained on gsm8k which is a maths question answers related dataset using llama 2 7b and finetuned it using qlora technique.



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""ShreySharma07/maths-llama-qlora"", ""author"": ""ShreySharma07"", ""sha"": ""9b8ead731c48c0ee0b856f749b379d370c6fd6d8"", ""last_modified"": ""2025-04-13 16:19:44+00:00"", ""created_at"": ""2025-04-12 06:41:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 2, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""question-answering"", ""en"", ""dataset:openai/gsm8k"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: question-answering\nnew_version: meta-llama/Llama-2-7b-chat-hf"", ""widget_data"": [{""text"": ""Where do I live?"", ""context"": ""My name is Wolfgang and I live in Berlin""}, {""text"": ""Where do I live?"", ""context"": ""My name is Sarah and I live in London""}, {""text"": ""What's my name?"", ""context"": ""My name is Clara and I live in Berkeley.""}, {""text"": ""Which name is also used to describe the Amazon rainforest in English?"", ""context"": ""The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \""Amazonas\"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.""}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-04-13 16:19:44+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: question-answering\nnew_version: meta-llama/Llama-2-7b-chat-hf"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67fa0b0a59fef5be4e8eea00"", ""modelId"": ""ShreySharma07/maths-llama-qlora"", ""usedStorage"": 67676779}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=ShreySharma07/maths-llama-qlora&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BShreySharma07%2Fmaths-llama-qlora%5D(%2FShreySharma07%2Fmaths-llama-qlora)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
TheBloke/Llama-2-7B-Chat-GGML,"---
language:
- en
license: other
tags:
- facebook
- meta
- pytorch
- llama
- llama-2
model_name: Llama 2 7B Chat
arxiv: 2307.09288
inference: false
model_creator: Meta Llama 2
model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
model_type: llama
pipeline_tag: text-generation
quantized_by: TheBloke
base_model: meta-llama/Llama-2-7b-chat-hf
---

<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
<img src=""https://i.imgur.com/EBdldam.jpg"" alt=""TheBlokeAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
</div>
<div style=""display: flex; justify-content: space-between; width: 100%;"">
    <div style=""display: flex; flex-direction: column; align-items: flex-start;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://discord.gg/theblokeai"">Chat & support: TheBloke's Discord server</a></p>
    </div>
    <div style=""display: flex; flex-direction: column; align-items: flex-end;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://www.patreon.com/TheBlokeAI"">Want to contribute? TheBloke's Patreon page</a></p>
    </div>
</div>
<div style=""text-align:center; margin-top: 0em; margin-bottom: 0em""><p style=""margin-top: 0.25em; margin-bottom: 0em;"">TheBloke's LLM work is generously supported by a grant from <a href=""https://a16z.com"">andreessen horowitz (a16z)</a></p></div>
<hr style=""margin-top: 1.0em; margin-bottom: 1.0em;"">
<!-- header end -->

# Llama 2 7B Chat - GGML
- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)
- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

## Description

This repo contains GGML format model files for [Meta Llama 2's Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).

### Important note regarding GGML files.

The GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.

Please use the GGUF models instead.
### About GGML

GGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:
* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.
* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.
* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.
* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.
* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.
* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.

## Repositories available

* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)
* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)
* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGML)
* [Meta Llama 2's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

## Prompt template: Llama-2-Chat

```
[INST] <<SYS>>
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
<</SYS>>
{prompt}[/INST]

```

<!-- compatibility_ggml start -->
## Compatibility

These quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.

For support with latest llama.cpp, please use GGUF files instead.

The final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)

As of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.

## Explanation of the new k-quant methods
<details>
  <summary>Click to see details</summary>

The new methods available are:
* GGML_TYPE_Q2_K - ""type-1"" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)
* GGML_TYPE_Q3_K - ""type-0"" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.
* GGML_TYPE_Q4_K - ""type-1"" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.
* GGML_TYPE_Q5_K - ""type-1"" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw
* GGML_TYPE_Q6_K - ""type-0"" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw
* GGML_TYPE_Q8_K - ""type-0"" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.

Refer to the Provided Files table below to see what files use which methods, and how.
</details>
<!-- compatibility_ggml end -->

## Provided files

| Name | Quant method | Bits | Size | Max RAM required | Use case |
| ---- | ---- | ---- | ---- | ---- | ----- |
| llama-2-7b-chat.ggmlv3.q2_K.bin | q2_K | 2 | 2.87 GB| 5.37 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |
| llama-2-7b-chat.ggmlv3.q3_K_S.bin | q3_K_S | 3 | 2.95 GB| 5.45 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |
| llama-2-7b-chat.ggmlv3.q3_K_M.bin | q3_K_M | 3 | 3.28 GB| 5.78 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |
| llama-2-7b-chat.ggmlv3.q3_K_L.bin | q3_K_L | 3 | 3.60 GB| 6.10 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |
| llama-2-7b-chat.ggmlv3.q4_0.bin | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |
| llama-2-7b-chat.ggmlv3.q4_K_S.bin | q4_K_S | 4 | 3.83 GB| 6.33 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |
| llama-2-7b-chat.ggmlv3.q4_K_M.bin | q4_K_M | 4 | 4.08 GB| 6.58 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |
| llama-2-7b-chat.ggmlv3.q4_1.bin | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |
| llama-2-7b-chat.ggmlv3.q5_0.bin | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |
| llama-2-7b-chat.ggmlv3.q5_K_S.bin | q5_K_S | 5 | 4.65 GB| 7.15 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |
| llama-2-7b-chat.ggmlv3.q5_K_M.bin | q5_K_M | 5 | 4.78 GB| 7.28 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |
| llama-2-7b-chat.ggmlv3.q5_1.bin | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |
| llama-2-7b-chat.ggmlv3.q6_K.bin | q6_K | 6 | 5.53 GB| 8.03 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |
| llama-2-7b-chat.ggmlv3.q8_0.bin | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |

**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.

## How to run in `llama.cpp`

Make sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.

For compatibility with latest llama.cpp, please use GGUF files instead.

```
./main -t 10 -ngl 32 -m llama-2-7b-chat.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p ""[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\nWrite a story about llamas[/INST]""
```
Change `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.

Change `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.

Change `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.

If you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`

For other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)

## How to run in `text-generation-webui`

Further instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).

<!-- footer start -->
<!-- 200823 -->
## Discord

For further support, and discussions on these models and AI in general, join us at:

[TheBloke AI's Discord server](https://discord.gg/theblokeai)

## Thanks, and how to contribute.

Thanks to the [chirper.ai](https://chirper.ai) team!

I've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.

If you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.

Donaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.

* Patreon: https://patreon.com/TheBlokeAI
* Ko-Fi: https://ko-fi.com/TheBlokeAI

**Special thanks to**: Aemon Algiz.

**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper Wikieł, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, 阿明, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik Bjäreholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap'n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser


Thank you to all my generous patrons and donaters!

And thank you again to a16z for their generous grant.

<!-- footer end -->

# Original model card: Meta Llama 2's Llama 2 7B Chat

# **Llama 2**
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.

## Model Details
*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*

Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.

**Model Developers** Meta

**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.

**Input** Models input text only.

**Output** Models generate text only.

**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.


||Training Data|Params|Content Length|GQA|Tokens|LR|
|---|---|---|---|---|---|---|
|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|

*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Dates** Llama 2 was trained between January 2023 and July 2023.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

**Research Paper** [""Llama-2: Open Foundation and Fine-tuned Chat Models""](arxiv.org/abs/2307.09288)

## Intended Use
**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).

**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.

## Hardware and Software
**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.

||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|
|---|---|---|---|
|Llama 2 7B|184320|400|31.22|
|Llama 2 13B|368640|400|62.44|
|Llama 2 70B|1720320|400|291.42|
|Total|3311616||539.00|

**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.

## Training Data
**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.

## Evaluation Results

In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.

|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|
|---|---|---|---|---|---|---|---|---|---|
|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|
|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|
|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|
|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|
|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|
|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|
|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|

**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.

|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama 1|7B|27.42|23.00|
|Llama 1|13B|41.74|23.08|
|Llama 1|33B|44.19|22.57|
|Llama 1|65B|48.71|21.77|
|Llama 2|7B|33.29|**21.25**|
|Llama 2|13B|41.86|26.10|
|Llama 2|70B|**50.18**|24.60|

**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).


|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama-2-Chat|7B|57.04|**0.00**|
|Llama-2-Chat|13B|62.18|**0.00**|
|Llama-2-Chat|70B|**64.14**|0.01|

**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.

## Ethical Considerations and Limitations
Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.

Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)

## Reporting Issues
Please report any software “bug,” or other problems with the models through one of the following means:
- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)
- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)

## Llama Model Index
|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|
|---|---|---|---|---|
|7B| [Link](https://huggingface.co/llamaste/Llama-2-7b) | [Link](https://huggingface.co/llamaste/Llama-2-7b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat-hf)|
|13B| [Link](https://huggingface.co/llamaste/Llama-2-13b) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-13b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf)|
|70B| [Link](https://huggingface.co/llamaste/Llama-2-70b) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-70b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf)|
","{""id"": ""TheBloke/Llama-2-7B-Chat-GGML"", ""author"": ""TheBloke"", ""sha"": ""76cd63c351ae389e1d4b91cab2cf470aab11864b"", ""last_modified"": ""2023-09-27 13:00:17+00:00"", ""created_at"": ""2023-07-18 17:38:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1931, ""downloads_all_time"": null, ""likes"": 871, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""llama"", ""facebook"", ""meta"", ""pytorch"", ""llama-2"", ""text-generation"", ""en"", ""arxiv:2307.09288"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: other\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: TheBloke"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""model_type"": ""llama""}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Notice', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q2_K.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q3_K_L.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q3_K_M.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q3_K_S.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q4_0.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q4_1.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q4_K_M.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q4_K_S.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q5_0.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q5_1.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q5_K_M.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q5_K_S.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q6_K.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q8_0.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [""mikeee/llama2-7b-chat-uncensored-ggml"", ""memef4rmer/llama2-7b-chat-uncensored-ggml"", ""harsh-manvar/llama-2-7b-chat-test"", ""Nymbo/llama2-7b-chat-uncensored-ggml"", ""r3gm/ConversaDocs"", ""YaTharThShaRma999/WizardLM7b"", ""mikeee/nousresearch-nous-hermes-llama2-13b-ggml"", ""mikeee/llama2-7b-chat-ggml"", ""ThisIs-Developer/Llama-2-GGML-Medical-Chatbot"", ""PSMdata/langchain-llama2-7b-chat"", ""ankanpy/LlamaGPT"", ""izammohammed/legal-advisor"", ""DHEIVER/VestibulaIA"", ""mikeee/langchain-llama2-7b-chat-uncensored-ggml"", ""haywired/medibot-llama2"", ""srikanth-nm/ai_seeker"", ""ThisIs-Developer/Llama-2-GGML-CSV-Chatbot"", ""K00B404/langchain-llama2-7b-chat-uncensored-ggml"", ""atharvapawar/Email-Generator-App-Langchain-LLAMA2-LLM"", ""V15h/learnai2.0"", ""saitejad/llama-2-gen-with-speech"", ""datastx/EmailGenerator"", ""mrm8488/llama-2-7b-chat-cpp"", ""captain-awesome/docuverse"", ""jergra43/llama2-7b-ggml-chat-app"", ""adityaagrawal/rag-assignment"", ""GoodML/MediBotAI"", ""4darsh-Dev/medicure"", ""4darsh-Dev/orchard_eyes-chatbot"", ""DhruvSarin/BlogGenerator"", ""BojanSimoski/SocialMovezVeggieAssistant"", ""umamicode/llama2-test"", ""maknee/kani-llama-v2-ggml"", ""gary109/llama2-webui"", ""zilongpa/llama2-webui"", ""lavanjv/vec-digichat"", ""TogetherAI/llahrou"", ""DripBeanBag/llama2_chatbot"", ""ndn1954/pdfchatbot"", ""AinzOoalGowns/llama2-7b-chat-uncensored-test"", ""Jafta/llama2-7b-chat-ggml"", ""LuckRafly/LLM-Generate-Math_Quiz"", ""Bankrid/huggingface-app"", ""yangzzay/HydroxApp_t2t"", ""myy97/llama2-webui"", ""Amirizaniani/Auditing_LLM"", ""ndn1954/chatwithpdf"", ""thivav/llama2-blogger"", ""Awe03/ai"", ""jingwora/llama2-7b-chat-ggml"", ""xsa-dev/llama2-7b-llama_cpp-ggmlv3-q4_1"", ""xsa-dev/llama-2-7b-chat-ggmlv3-q6_K"", ""yuping322/LLaMA-2-CHAT"", ""Jayavathsan/Email_Generator"", ""sofarikasid/LLM_Search_Engine"", ""Sakil/CSVQConnect"", ""adas100/blogs"", ""adas100/blog"", ""manjunathkukanur/mypdfchatbot"", ""ndn1954/llmdocumentchatbot"", ""dnzengou/llama-gpt-chatbot"", ""amol-rainfall/amol-rainfallStratosphere"", ""Pyasma/Querybot"", ""amol-rainfall/Stratosphere"", ""1littlecoder/llama-cpp-python-cuda-gradio"", ""goavinash5/Gradio_LLAMA_Testing"", ""lyimo/llama_multimodel_model"", ""ToonTownTommy/Tommylaw"", ""quangtn266/EmailGeneratorUsingLLAMA2"", ""huy302/SPGCI_Learnathon"", ""JohnTan38/llama-2-7b-chat-1"", ""sheetalbborkar/ArticleGenerator"", ""rajeshasb/llmsasb"", ""Dalleon/llama2-7b-chat-uncensored-ggml"", ""samim2024/EMAIL-Generator-META-AI"", ""brunodoti/turing-20.0"", ""Nikhil0987/med_bot"", ""adityakumar/nhpc-chatbot"", ""csalabs/AI-EMBD"", ""DeyPoulomi/HR_resume_screening"", ""Jacksonnavigator7/Llamacpp"", ""harichselvamc/Miskaacomics"", ""aiscientist/llamachat"", ""maheshwarligade/email_generator_llama2"", ""csalabs/Replicate-7b-chat-Llama-streamlit"", ""Preet2002/blog-generation"", ""uyen13/chatbot"", ""yashas-vi/JobCV_Writer"", ""md-vasim/llama-2-hf"", ""hellojj7/email_app"", ""robertquest/llama-2-7b-chat-test"", ""shubhamtw/qaBot"", ""kartikeyarana/ESCO"", ""nikesh66/mediweb1.0"", ""uyen13/chatgirl"", ""sanket09/llama-2-7b-chat"", ""Antonio49/llama-2-7b-chat"", ""rahul-bhoyar-1995/Email-Generator-using-LLM"", ""Amirizaniani/AuditLLM"", ""uyen13/chatgirl2""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-27 13:00:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: other\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: TheBloke"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""64b6ce072a8e3cd59df98e98"", ""modelId"": ""TheBloke/Llama-2-7B-Chat-GGML"", ""usedStorage"": 60421177985}",1,"https://huggingface.co/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot, https://huggingface.co/nik-55/youtube-question-answer",2,,0,,0,,0,"GoodML/MediBotAI, Nymbo/llama2-7b-chat-uncensored-ggml, PSMdata/langchain-llama2-7b-chat, ThisIs-Developer/Llama-2-GGML-Medical-Chatbot, YaTharThShaRma999/WizardLM7b, ankanpy/LlamaGPT, harsh-manvar/llama-2-7b-chat-test, huggingface/InferenceSupport/discussions/new?title=TheBloke/Llama-2-7B-Chat-GGML&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTheBloke%2FLlama-2-7B-Chat-GGML%5D(%2FTheBloke%2FLlama-2-7B-Chat-GGML)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A, memef4rmer/llama2-7b-chat-uncensored-ggml, mikeee/llama2-7b-chat-ggml, mikeee/llama2-7b-chat-uncensored-ggml, mikeee/nousresearch-nous-hermes-llama2-13b-ggml, r3gm/ConversaDocs",13
ThisIs-Developer/Llama-2-GGML-Medical-Chatbot,"---
license: mit
language:
- en
base_model: TheBloke/Llama-2-7B-Chat-GGML
pipeline_tag: question-answering
library_name: transformers
tags:
- medical
- conversational
- text-generation
---
# 🐍 Llama-2-GGML-Medical-Chatbot 🤖
The **Llama-2-7B-Chat-GGML-Medical-Chatbot** is a repository for a medical chatbot that uses the _Llama-2-7B-Chat-GGML_ model and the pdf _The Gale Encyclopedia of Medicine_. The chatbot is still under development, but it has the potential to be a valuable tool for patients, healthcare professionals, and researchers. The chatbot can be used to answer questions about medical topics, provide summaries of medical articles, and generate medical text. However, it is important to note that the chatbot is not a substitute for medical advice from a qualified healthcare professional.
![chat_use_case-eb8a4883931d726e9f23628a0d22e315](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/assets/109382325/b2c0ed2f-1393-4d6f-938a-1b2666a2e898)

## 📚 Here are some of the features of the Llama-2-7B-Chat-GGML-Medical-Chatbot:

 - It uses the _Llama-2-7B-Chat-GGML_ model, which is a **large language model (LLM)** that has been fine-tuned.
   * Name - **llama-2-7b-chat.ggmlv3.q2_K.bin**
   * Quant method - q2_K
   * Bits - 2
   * Size - **2.87 GB**
   * Max RAM required - 5.37 GB
   * Use case - New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors.
   * **Model:** Know more about model **[Llama-2-7B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)**
 - It is trained on the pdf **[The Gale Encyclopedia of Medicine, Volume 1, 2nd Edition, 637-page PDF](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/blob/main/data/71763-gale-encyclopedia-of-medicine.-vol.-1.-2nd-ed.pdf)**, which is a comprehensive medical reference that provides information on a wide range of medical topics. This means that the chatbot is able to answer questions about a variety of medical topics.
 - This is a sophisticated medical chatbot, developed using Llama-2 7B and Sentence Transformers. Powered by **[Langchain](https://python.langchain.com/docs/get_started/introduction)** and **[Chainlit](https://docs.chainlit.io/overview)**, This bot operates on a powerful CPU computer that boasts a minimum of
    * Operating system: Linux, macOS, or Windows
    * CPU: Intel® Core™ i3
    * RAM: **8 GB**
    * Disk space: 7 GB
    * GPU: None **(CPU only)**
 - It is still under development, but it has the potential to be a valuable tool for patients, healthcare professionals, and researchers.

## 🚀 Quickstart
1. Open Git Bash.
2. Change the current working directory to the location where you want the cloned directory.
3. Type `git clone`, and then paste the URL you copied earlier.
```bash
   git clone https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot.git
```
Press Enter to create your local clone.
4. Install the pip packages in requirements.txt
 ```bash
   pip install -r requirements.txt
 ```
5. Now run it!
```ternimal
   chainlit run model.py -w
```
## 📖 ChatBot Conversession
### ⛓️Chainlit ver. on [#v1.0.1.dev20230913](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/releases/tag/v1.0.1.dev20230913)
![ChatBot Conversession img-1](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/assets/109382325/9af05b2e-1a83-4a7c-aa8c-ed7c60b02e09)

### ⚡Streamlit ver. on [#v2.0.1.dev20231230](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/releases/tag/v2.0.1.dev20231230)
![ChatBot Conversession img-4.png](https://cdn-uploads.huggingface.co/production/uploads/64d8c442a4839890b2490db9/bGCVz3O3qwJtuPyXWK58W.png)

### DEMO: 📽️Conversession.vid.mp4->https://cdn-uploads.huggingface.co/production/uploads/64d8c442a4839890b2490db9/iI4t0lhjkCw3dDSvWQ4Jk.mp4

![ChatBot Conversession img-2](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/assets/109382325/1fede7dd-05e1-49de-bbab-f289cbdb9cd9)

![ChatBot Conversession img-3](https://github.com/ThisIs-Developer/Llama-2-GGML-Medical-Chatbot/assets/109382325/d10d949f-37e5-4ec4-868d-2e62d8ad69dc)","{""id"": ""ThisIs-Developer/Llama-2-GGML-Medical-Chatbot"", ""author"": ""ThisIs-Developer"", ""sha"": ""f0bbd3d9b14dda9526c4368fb1489d7a4c2ec760"", ""last_modified"": ""2024-09-05 15:35:10+00:00"", ""created_at"": ""2023-12-19 14:51:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 81, ""downloads_all_time"": null, ""likes"": 36, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""llama"", ""medical"", ""conversational"", ""text-generation"", ""question-answering"", ""en"", ""base_model:TheBloke/Llama-2-7B-Chat-GGML"", ""base_model:finetune:TheBloke/Llama-2-7B-Chat-GGML"", ""license:mit"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: TheBloke/Llama-2-7B-Chat-GGML\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: question-answering\ntags:\n- medical\n- conversational\n- text-generation"", ""widget_data"": [{""text"": ""Where do I live?"", ""context"": ""My name is Wolfgang and I live in Berlin""}, {""text"": ""Where do I live?"", ""context"": ""My name is Sarah and I live in London""}, {""text"": ""What's my name?"", ""context"": ""My name is Clara and I live in Berkeley.""}, {""text"": ""Which name is also used to describe the Amazon rainforest in English?"", ""context"": ""The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \""Amazonas\"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.""}], ""model_index"": null, ""config"": {""model_type"": ""llama""}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.chainlit/config.toml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Chainlit/model.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Streamlit/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Streamlit/model.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='__pycache__/model.cpython-311.pyc', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='chainlit.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='conversession e.g/ChatBot Conversession img-1.png', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='conversession e.g/ChatBot Conversession img-2.png', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='conversession e.g/ChatBot Conversession img-3.pdf', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='conversession e.g/ChatBot Conversession img-3.png', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='conversession e.g/ChatBot Conversession vid.mp4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='data/71763-gale-encyclopedia-of-medicine.-vol.-1.-2nd-ed.pdf', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='ingest.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='requirements.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vectorstores/db_faiss/index.faiss', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vectorstores/db_faiss/index.pkl', size=None, blob_id=None, lfs=None)""], ""spaces"": [""ThisIs-Developer/Llama-2-GGML-Medical-Chatbot"", ""awpbash/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot"", ""MZ786/GPT_DOCTOR"", ""comara/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot"", ""sidthegirlkid/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot"", ""saswattulo/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot"", ""Karani/Llama-2-Medical-Chatbot"", ""Paulie-Aditya/MedIntel""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-09-05 15:35:10+00:00"", ""cardData"": ""base_model: TheBloke/Llama-2-7B-Chat-GGML\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: question-answering\ntags:\n- medical\n- conversational\n- text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6581adf9193fb3eccded09ac"", ""modelId"": ""ThisIs-Developer/Llama-2-GGML-Medical-Chatbot"", ""usedStorage"": 48147439}",2,,0,,0,,0,,0,"Karani/Llama-2-Medical-Chatbot, MZ786/GPT_DOCTOR, Paulie-Aditya/MedIntel, ThisIs-Developer/Llama-2-GGML-Medical-Chatbot, awpbash/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot, comara/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot, huggingface/InferenceSupport/discussions/new?title=ThisIs-Developer/Llama-2-GGML-Medical-Chatbot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BThisIs-Developer%2FLlama-2-GGML-Medical-Chatbot%5D(%2FThisIs-Developer%2FLlama-2-GGML-Medical-Chatbot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A, saswattulo/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot, sidthegirlkid/ThisIs-Developer-Llama-2-GGML-Medical-Chatbot",9
nik-55/youtube-question-answer,"---
language: 
  - en
base_model: ""TheBloke/Llama-2-7B-Chat-GGML""
---

# YOUTUBE Question Answer

","{""id"": ""nik-55/youtube-question-answer"", ""author"": ""nik-55"", ""sha"": ""fd72e50d5d3a2556bdf8bc40a386a3540ba0b59c"", ""last_modified"": ""2023-12-22 10:04:52+00:00"", ""created_at"": ""2023-12-22 09:50:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""en"", ""base_model:TheBloke/Llama-2-7B-Chat-GGML"", ""base_model:finetune:TheBloke/Llama-2-7B-Chat-GGML"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: TheBloke/Llama-2-7B-Chat-GGML\nlanguage:\n- en"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-22 10:04:52+00:00"", ""cardData"": ""base_model: TheBloke/Llama-2-7B-Chat-GGML\nlanguage:\n- en"", ""transformersInfo"": null, ""_id"": ""65855bd189bb78d10455503e"", ""modelId"": ""nik-55/youtube-question-answer"", ""usedStorage"": 0}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=nik-55/youtube-question-answer&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bnik-55%2Fyoutube-question-answer%5D(%2Fnik-55%2Fyoutube-question-answer)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alielfilali01/Llama-2-7b-chat-hf-tuned-medical-qa,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-chat-hf-tuned-medical-qa
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf-tuned-medical-qa

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.32.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
","{""id"": ""alielfilali01/Llama-2-7b-chat-hf-tuned-medical-qa"", ""author"": ""alielfilali01"", ""sha"": ""ea94cd9869ce398fb31bfb8ffb5c59080d32f850"", ""last_modified"": ""2023-07-20 18:22:18+00:00"", ""created_at"": ""2023-07-19 23:26:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-tuned-medical-qa\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-chat-hf-tuned-medical-qa"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul20_18-08-17_0d4f1d29fe85/events.out.tfevents.1689876504.0d4f1d29fe85.580.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-07-20 18:22:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-tuned-medical-qa\n  results: []"", ""transformersInfo"": null, ""_id"": ""64b87134f62a2c23a6f44c99"", ""modelId"": ""alielfilali01/Llama-2-7b-chat-hf-tuned-medical-qa"", ""usedStorage"": 33613760}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alielfilali01/Llama-2-7b-chat-hf-tuned-medical-qa&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balielfilali01%2FLlama-2-7b-chat-hf-tuned-medical-qa%5D(%2Falielfilali01%2FLlama-2-7b-chat-hf-tuned-medical-qa)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
vincentmin/llama-7b-orca,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7b-orca
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-orca

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4944

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.6959        | 0.08  | 200  | 1.6441          |
| 1.6234        | 0.17  | 400  | 1.5693          |
| 1.5931        | 0.25  | 600  | 1.5452          |
| 1.5003        | 0.34  | 800  | 1.5316          |
| 1.5346        | 0.42  | 1000 | 1.5209          |
| 1.5026        | 0.51  | 1200 | 1.5123          |
| 1.5608        | 0.59  | 1400 | 1.5059          |
| 1.5332        | 0.68  | 1600 | 1.5009          |
| 1.5209        | 0.76  | 1800 | 1.4970          |
| 1.4955        | 0.85  | 2000 | 1.4944          |


### Framework versions

- Transformers 4.32.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
","{""id"": ""vincentmin/llama-7b-orca"", ""author"": ""vincentmin"", ""sha"": ""9b44de3162b49868cbfee1fe17ece11e566b43b0"", ""last_modified"": ""2023-07-21 22:42:21+00:00"", ""created_at"": ""2023-07-20 13:07:04+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-orca\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-orca"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul20_13-06-55_9480d1dba441/events.out.tfevents.1689858437.9480d1dba441.1290.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul20_14-19-02_9480d1dba441/events.out.tfevents.1689862764.9480d1dba441.26033.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul20_16-29-57_00a224c1b5b8/events.out.tfevents.1689870629.00a224c1b5b8.1078.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul21_17-10-54_8cc7d84f219e/events.out.tfevents.1689959467.8cc7d84f219e.432.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-07-21 22:42:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-orca\n  results: []"", ""transformersInfo"": null, ""_id"": ""64b93178339adc8f30ccbd86"", ""modelId"": ""vincentmin/llama-7b-orca"", ""usedStorage"": 118348934}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=vincentmin/llama-7b-orca&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bvincentmin%2Fllama-7b-orca%5D(%2Fvincentmin%2Fllama-7b-orca)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alielfilali01/Llama-2-7b-chat-hf-tuned-medical-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-chat-hf-tuned-medical-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf-tuned-medical-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.32.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
","{""id"": ""alielfilali01/Llama-2-7b-chat-hf-tuned-medical-chat"", ""author"": ""alielfilali01"", ""sha"": ""d6efaf5ca83ede29d26a4b319e282419ec205823"", ""last_modified"": ""2023-07-20 19:14:31+00:00"", ""created_at"": ""2023-07-20 18:33:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 2, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-tuned-medical-chat\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-chat-hf-tuned-medical-chat"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul20_18-32-17_74099b49a4c9/events.out.tfevents.1689878041.74099b49a4c9.267.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-07-20 19:14:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-tuned-medical-chat\n  results: []"", ""transformersInfo"": null, ""_id"": ""64b97e09f602541ef75eace4"", ""modelId"": ""alielfilali01/Llama-2-7b-chat-hf-tuned-medical-chat"", ""usedStorage"": 33614086}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alielfilali01/Llama-2-7b-chat-hf-tuned-medical-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balielfilali01%2FLlama-2-7b-chat-hf-tuned-medical-chat%5D(%2Falielfilali01%2FLlama-2-7b-chat-hf-tuned-medical-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PhilSad/llama2-7b-chat-french-2k-test,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-chat-french-2k-test
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-chat-french-2k-test

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 2

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.13.1
- Tokenizers 0.13.3
","{""id"": ""PhilSad/llama2-7b-chat-french-2k-test"", ""author"": ""PhilSad"", ""sha"": ""a0638166e217f45900aab253e793af615f555fb1"", ""last_modified"": ""2023-07-22 00:49:22+00:00"", ""created_at"": ""2023-07-22 00:43:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-french-2k-test\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-chat-french-2k-test"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul22_00-43-12_c58678b7a012/events.out.tfevents.1689986607.c58678b7a012.32397.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul22_00-47-31_c58678b7a012/events.out.tfevents.1689986868.c58678b7a012.33607.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-07-22 00:49:22+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-french-2k-test\n  results: []"", ""transformersInfo"": null, ""_id"": ""64bb262a76a6e2efcc6c7548"", ""modelId"": ""PhilSad/llama2-7b-chat-french-2k-test"", ""usedStorage"": 134278092}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PhilSad/llama2-7b-chat-french-2k-test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPhilSad%2Fllama2-7b-chat-french-2k-test%5D(%2FPhilSad%2Fllama2-7b-chat-french-2k-test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
llSourcell/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.33.2
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""llSourcell/results"", ""author"": ""llSourcell"", ""sha"": ""3bd26607e402ab138a5b5774380ae22b88738ce8"", ""last_modified"": ""2023-09-27 03:56:52+00:00"", ""created_at"": ""2023-08-09 01:31:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-27 03:56:52+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64d2ec6580f189e40bd0bb05"", ""modelId"": ""llSourcell/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=llSourcell/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BllSourcell%2Fresults%5D(%2FllSourcell%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
nauman187/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 50

### Training results



### Framework versions

- Transformers 4.32.0.dev0
- Pytorch 2.0.1+cu117
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""nauman187/results"", ""author"": ""nauman187"", ""sha"": ""7a77ee537ff95f7871882e5d4ee47d7f3fd4e105"", ""last_modified"": ""2023-08-14 19:47:26+00:00"", ""created_at"": ""2023-08-14 19:47:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-14 19:47:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64da84cad8d7d6d6d9b4df2a"", ""modelId"": ""nauman187/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=nauman187/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bnauman187%2Fresults%5D(%2Fnauman187%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
karimasbar/test_result,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""karimasbar/test_result"", ""author"": ""karimasbar"", ""sha"": ""b6253ed3e1bbd57ef9cde9c57c5847249180da92"", ""last_modified"": ""2023-08-18 18:12:13+00:00"", ""created_at"": ""2023-08-17 17:19:12+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 5, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Unconfirmed 469822.crdownload', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-18 18:12:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""64de5690bbbb7e908ca67f2d"", ""modelId"": ""karimasbar/test_result"", ""usedStorage"": 134767443}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=karimasbar/test_result&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bkarimasbar%2Ftest_result%5D(%2Fkarimasbar%2Ftest_result)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
threem/llama2-fine-tuned-2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- samsum
model-index:
- name: llama2-fine-tuned-2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-fine-tuned-2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the samsum dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""threem/llama2-fine-tuned-2"", ""author"": ""threem"", ""sha"": ""930fc2c9485056a865cbf37affb9cfbca65f42a9"", ""last_modified"": ""2023-08-18 20:26:44+00:00"", ""created_at"": ""2023-08-18 16:46:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""dataset:samsum"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- samsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-fine-tuned-2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-fine-tuned-2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-18 20:26:44+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- samsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-fine-tuned-2\n  results: []"", ""transformersInfo"": null, ""_id"": ""64dfa04bcccd823564c54435"", ""modelId"": ""threem/llama2-fine-tuned-2"", ""usedStorage"": 17326611}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=threem/llama2-fine-tuned-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthreem%2Fllama2-fine-tuned-2%5D(%2Fthreem%2Fllama2-fine-tuned-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/LLama2_7b_Jukabo_ft_mlsum_hf,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- mlsum
model-index:
- name: LLama2_7b_Jukabo_ft_mlsum_hf
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# LLama2_7b_Jukabo_ft_mlsum_hf

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the mlsum dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 10

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""Jukaboo/LLama2_7b_Jukabo_ft_mlsum_hf"", ""author"": ""Jukaboo"", ""sha"": ""70dc7f8ed3558fddd0284617b1d7794a53407d08"", ""last_modified"": ""2023-08-21 13:50:28+00:00"", ""created_at"": ""2023-08-21 10:22:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""dataset:mlsum"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mlsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLama2_7b_Jukabo_ft_mlsum_hf\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""LLama2_7b_Jukabo_ft_mlsum_hf"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug21_10-22-06_48469ee7d30a/events.out.tfevents.1692613337.48469ee7d30a.392.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug21_10-36-42_48469ee7d30a/events.out.tfevents.1692614204.48469ee7d30a.392.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug21_10-41-02_48469ee7d30a/events.out.tfevents.1692614465.48469ee7d30a.392.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug21_10-42-20_48469ee7d30a/events.out.tfevents.1692614547.48469ee7d30a.392.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug21_10-42-40_48469ee7d30a/events.out.tfevents.1692614564.48469ee7d30a.392.5', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-21 13:50:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mlsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLama2_7b_Jukabo_ft_mlsum_hf\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e33acff8d8389c1a979191"", ""modelId"": ""Jukaboo/LLama2_7b_Jukabo_ft_mlsum_hf"", ""usedStorage"": 16856130}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/LLama2_7b_Jukabo_ft_mlsum_hf&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLLama2_7b_Jukabo_ft_mlsum_hf%5D(%2FJukaboo%2FLLama2_7b_Jukabo_ft_mlsum_hf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dbraganca/sdr-bot-llama2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: sdr-bot-llama2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sdr-bot-llama2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5877

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.9909        | 0.41  | 20   | 1.8462          |
| 1.5277        | 0.82  | 40   | 1.3837          |
| 0.6035        | 1.22  | 60   | 0.9347          |
| 0.8469        | 1.63  | 80   | 0.5877          |


### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""dbraganca/sdr-bot-llama2"", ""author"": ""dbraganca"", ""sha"": ""40edc5d269bf4f1e5022622bd0f391983b73e78d"", ""last_modified"": ""2023-08-22 15:23:41+00:00"", ""created_at"": ""2023-08-21 11:41:50+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sdr-bot-llama2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""sdr-bot-llama2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='handler.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='requirements.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-22 15:23:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sdr-bot-llama2\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e34d7e3e2e10350494d571"", ""modelId"": ""dbraganca/sdr-bot-llama2"", ""usedStorage"": 268531477}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dbraganca/sdr-bot-llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdbraganca%2Fsdr-bot-llama2%5D(%2Fdbraganca%2Fsdr-bot-llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DeepaPeri/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 100

### Training results



### Framework versions

- Transformers 4.32.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""DeepaPeri/results"", ""author"": ""DeepaPeri"", ""sha"": ""a8fcc84c9eb567a63401ab39f7c7282edd70989f"", ""last_modified"": ""2023-08-22 17:49:47+00:00"", ""created_at"": ""2023-08-21 21:02:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-22 17:49:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e3d0f0417214c59cd029f0"", ""modelId"": ""DeepaPeri/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DeepaPeri/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDeepaPeri%2Fresults%5D(%2FDeepaPeri%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
karimasbar/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""karimasbar/results"", ""author"": ""karimasbar"", ""sha"": ""6852a74538eb123c598aa13cc0a1d9ca116e523c"", ""last_modified"": ""2023-08-23 15:50:08+00:00"", ""created_at"": ""2023-08-22 10:40:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 4, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-23 15:50:08+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""64e490ad1887a952de94211a"", ""modelId"": ""karimasbar/results"", ""usedStorage"": 13611721956}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=karimasbar/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bkarimasbar%2Fresults%5D(%2Fkarimasbar%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dejimarquis/heallama7b,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.32.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""dejimarquis/heallama7b"", ""author"": ""dejimarquis"", ""sha"": ""77b32ad6b3ad4fdfd57a830b3965b053547d5937"", ""last_modified"": ""2023-08-23 04:53:51+00:00"", ""created_at"": ""2023-08-23 04:53:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-23 04:53:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e590cd71071da798e2602f"", ""modelId"": ""dejimarquis/heallama7b"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dejimarquis/heallama7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdejimarquis%2Fheallama7b%5D(%2Fdejimarquis%2Fheallama7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
karimasbar/resultss,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: resultss
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# resultss

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.32.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""karimasbar/resultss"", ""author"": ""karimasbar"", ""sha"": ""eda1535a78f21c27cecd5223b3f0b04052ba6af1"", ""last_modified"": ""2023-08-23 16:27:16+00:00"", ""created_at"": ""2023-08-23 16:26:58+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: resultss\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""resultss"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-23 16:27:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: resultss\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e63352d82128b8d5668c9b"", ""modelId"": ""karimasbar/resultss"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=karimasbar/resultss&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bkarimasbar%2Fresultss%5D(%2Fkarimasbar%2Fresultss)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jamsonE/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.32.0
- Pytorch 2.0.1+cu117
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""jamsonE/results"", ""author"": ""jamsonE"", ""sha"": ""f1e04dd9511b086b2537d02b1aa503e0c4820603"", ""last_modified"": ""2023-08-25 01:44:48+00:00"", ""created_at"": ""2023-08-25 01:44:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-25 01:44:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64e8076b0dab5a36c1bcd4e9"", ""modelId"": ""jamsonE/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jamsonE/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BjamsonE%2Fresults%5D(%2FjamsonE%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Chanblock/Llama-2-7b-chat-hf-finetuned-250_remates,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-chat-hf-finetuned-250_remates
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf-finetuned-250_remates

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""Chanblock/Llama-2-7b-chat-hf-finetuned-250_remates"", ""author"": ""Chanblock"", ""sha"": ""524bc14d00a6e04e4bbd92664c0d11b15e44d498"", ""last_modified"": ""2023-08-26 22:54:24+00:00"", ""created_at"": ""2023-08-26 22:35:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-finetuned-250_remates\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-chat-hf-finetuned-250_remates"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Aug26_22-48-19_b4539e2205b5/events.out.tfevents.1693090115.b4539e2205b5.1330.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-26 22:54:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-finetuned-250_remates\n  results: []"", ""transformersInfo"": null, ""_id"": ""64ea7e1f92d9db9a938691c6"", ""modelId"": ""Chanblock/Llama-2-7b-chat-hf-finetuned-250_remates"", ""usedStorage"": 134273346}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Chanblock/Llama-2-7b-chat-hf-finetuned-250_remates&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BChanblock%2FLlama-2-7b-chat-hf-finetuned-250_remates%5D(%2FChanblock%2FLlama-2-7b-chat-hf-finetuned-250_remates)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
qazisaad/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 10
- eval_batch_size: 10
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 40
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Framework versions

- Transformers 4.32.1
- Pytorch 2.0.1+cu117
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""qazisaad/results"", ""author"": ""qazisaad"", ""sha"": ""5237202f2878ad47a86c763b89790e819744920f"", ""last_modified"": ""2023-08-30 18:44:22+00:00"", ""created_at"": ""2023-08-30 18:08:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-08-30 18:44:22+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64ef859f0af6d9bfbc8cfa63"", ""modelId"": ""qazisaad/results"", ""usedStorage"": 269051783}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=qazisaad/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bqazisaad%2Fresults%5D(%2Fqazisaad%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jquigonq/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 500

### Training results



### Framework versions

- Transformers 4.32.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""jquigonq/results"", ""author"": ""jquigonq"", ""sha"": ""b557bfdf9ae79b5a6df4fcfed65a782f8cddbfb7"", ""last_modified"": ""2023-09-01 21:44:05+00:00"", ""created_at"": ""2023-09-01 21:43:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-01 21:44:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f25b137eb3ae0088cef40a"", ""modelId"": ""jquigonq/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jquigonq/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjquigonq%2Fresults%5D(%2Fjquigonq%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
BadTiger/badtiger_llama2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.32.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""BadTiger/badtiger_llama2"", ""author"": ""BadTiger"", ""sha"": ""d9e801dcacb95e123719dca9e024ce6ba3f2aa2b"", ""last_modified"": ""2023-09-04 00:05:51+00:00"", ""created_at"": ""2023-09-02 07:37:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-04 00:05:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f2e62e4800c638923aea5a"", ""modelId"": ""BadTiger/badtiger_llama2"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=BadTiger/badtiger_llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BBadTiger%2Fbadtiger_llama2%5D(%2FBadTiger%2Fbadtiger_llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M,N/A,N/A,1,,0,,0,,0,,0,,0
AniketParab/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 100

### Training results



### Framework versions

- Transformers 4.33.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""AniketParab/results"", ""author"": ""AniketParab"", ""sha"": ""aaddf9a2daf210730b1d95200686cf72ee96a81d"", ""last_modified"": ""2023-09-05 08:06:30+00:00"", ""created_at"": ""2023-09-04 13:39:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-05 08:06:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f5de12577fad5bb1e45ca6"", ""modelId"": ""AniketParab/results"", ""usedStorage"": 268535568}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=AniketParab/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAniketParab%2Fresults%5D(%2FAniketParab%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
TonySky/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.32.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""TonySky/results"", ""author"": ""TonySky"", ""sha"": ""f16aef9add2ddc35cdbc5cc5eee622f05fff2c58"", ""last_modified"": ""2023-09-05 01:04:37+00:00"", ""created_at"": ""2023-09-05 01:04:19+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-05 01:04:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f67e93376094fe6e1da1d4"", ""modelId"": ""TonySky/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=TonySky/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTonySky%2Fresults%5D(%2FTonySky%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Anish03/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.33.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""Anish03/results"", ""author"": ""Anish03"", ""sha"": ""2648aabbcfdaa810e23a4e9091da7d2c247665d7"", ""last_modified"": ""2023-09-05 08:09:58+00:00"", ""created_at"": ""2023-09-05 07:56:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-05 08:09:58+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f6df222d385f3a6d2a6160"", ""modelId"": ""Anish03/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Anish03/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAnish03%2Fresults%5D(%2FAnish03%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
flumboyantApple/twittSent01,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: outputs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# outputs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 100
- training_steps: 300

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""flumboyantApple/twittSent01"", ""author"": ""flumboyantApple"", ""sha"": ""e4586dd9d9fad0286dea5e8673247e1f632e631f"", ""last_modified"": ""2023-09-05 14:18:43+00:00"", ""created_at"": ""2023-09-05 14:15:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""outputs"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-05 14:18:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f737f9f9678931cad5ddb4"", ""modelId"": ""flumboyantApple/twittSent01"", ""usedStorage"": 16827016}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=flumboyantApple/twittSent01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BflumboyantApple%2FtwittSent01%5D(%2FflumboyantApple%2FtwittSent01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
abeiler/goatV9-chat-QLORA-Merged,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: goatV9-chat-QLORA-Merged
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# goatV9-chat-QLORA-Merged

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3902

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.4856        | 0.16  | 200  | 0.4700          |
| 0.4337        | 0.31  | 400  | 0.4252          |
| 0.3998        | 0.47  | 600  | 0.4071          |
| 0.4126        | 0.63  | 800  | 0.3967          |
| 0.421         | 0.79  | 1000 | 0.3920          |
| 0.4018        | 0.94  | 1200 | 0.3902          |


### Framework versions

- Transformers 4.33.0
- Pytorch 2.0.0
- Datasets 2.12.0
- Tokenizers 0.13.3
","{""id"": ""abeiler/goatV9-chat-QLORA-Merged"", ""author"": ""abeiler"", ""sha"": ""065811e0c4688b84f23cedff8626a03f766829cf"", ""last_modified"": ""2023-09-06 20:16:28+00:00"", ""created_at"": ""2023-09-06 01:00:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: goatV9-chat-QLORA-Merged\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""goatV9-chat-QLORA-Merged"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-06 20:16:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: goatV9-chat-QLORA-Merged\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f7cf2cd78a0037fe871935"", ""modelId"": ""abeiler/goatV9-chat-QLORA-Merged"", ""usedStorage"": 268628624}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=abeiler/goatV9-chat-QLORA-Merged&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Babeiler%2FgoatV9-chat-QLORA-Merged%5D(%2Fabeiler%2FgoatV9-chat-QLORA-Merged)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
EnzoZacharias/xgen-7b-tuned-alpaca,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: xgen-7b-tuned-alpaca
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# xgen-7b-tuned-alpaca

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.1.0.dev20230823
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""EnzoZacharias/xgen-7b-tuned-alpaca"", ""author"": ""EnzoZacharias"", ""sha"": ""cdbe27a78d09910bb53e2e7daab12a10cba04a9c"", ""last_modified"": ""2023-09-07 10:13:03+00:00"", ""created_at"": ""2023-09-07 09:13:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xgen-7b-tuned-alpaca\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""xgen-7b-tuned-alpaca"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='emissions.csv', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-07 10:13:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: xgen-7b-tuned-alpaca\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f9943ad4e5464ece670e9a"", ""modelId"": ""EnzoZacharias/xgen-7b-tuned-alpaca"", ""usedStorage"": 34105165}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=EnzoZacharias/xgen-7b-tuned-alpaca&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEnzoZacharias%2Fxgen-7b-tuned-alpaca%5D(%2FEnzoZacharias%2Fxgen-7b-tuned-alpaca)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
EnzoZacharias/outputs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: outputs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# outputs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 10

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.1.0.dev20230823
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""EnzoZacharias/outputs"", ""author"": ""EnzoZacharias"", ""sha"": ""b982c3928a9c57dcf82f5934225fc32ada4f75c3"", ""last_modified"": ""2023-09-07 12:21:50+00:00"", ""created_at"": ""2023-09-07 12:19:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""outputs"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='emissions.csv', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-07 12:21:50+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f9bfdaa5067f6b655788f0"", ""modelId"": ""EnzoZacharias/outputs"", ""usedStorage"": 17327629}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=EnzoZacharias/outputs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEnzoZacharias%2Foutputs%5D(%2FEnzoZacharias%2Foutputs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Pavanb/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 500
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.0.0
- Datasets 2.1.0
- Tokenizers 0.15.0
","{""id"": ""Pavanb/results"", ""author"": ""Pavanb"", ""sha"": ""da5098e855ae363ac2e88e44d38352362a0524be"", ""last_modified"": ""2023-12-02 12:09:57+00:00"", ""created_at"": ""2023-09-07 14:24:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec02_10-23-16_a88a7964f57c/events.out.tfevents.1701512646.a88a7964f57c.80.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec02_10-30-25_a88a7964f57c/events.out.tfevents.1701513035.a88a7964f57c.391.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-02 12:09:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""64f9dd34b550b098b3fd0d86"", ""modelId"": ""Pavanb/results"", ""usedStorage"": 806084963}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Pavanb/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPavanb%2Fresults%5D(%2FPavanb%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
antonwonton/llama-2-7b-hf-train01-int4,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-hf-train01-int4
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-hf-train01-int4

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 6
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 12
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.13.0
- Tokenizers 0.13.3
","{""id"": ""antonwonton/llama-2-7b-hf-train01-int4"", ""author"": ""antonwonton"", ""sha"": ""262a526465496f0dcd7d640fb11875dde4d5c51b"", ""last_modified"": ""2023-09-09 18:19:10+00:00"", ""created_at"": ""2023-09-08 23:00:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-hf-train01-int4\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-hf-train01-int4"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-09 18:19:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-hf-train01-int4\n  results: []"", ""transformersInfo"": null, ""_id"": ""64fba791d75293f417e7a0ef"", ""modelId"": ""antonwonton/llama-2-7b-hf-train01-int4"", ""usedStorage"": 134322320}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=antonwonton/llama-2-7b-hf-train01-int4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bantonwonton%2Fllama-2-7b-hf-train01-int4%5D(%2Fantonwonton%2Fllama-2-7b-hf-train01-int4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
antonwonton/llama-2-7b-chat-hf-test09-int4,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-chat-hf-test09-int4
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-test09-int4

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 6
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 12
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.13.0
- Tokenizers 0.13.3
","{""id"": ""antonwonton/llama-2-7b-chat-hf-test09-int4"", ""author"": ""antonwonton"", ""sha"": ""e55613ab5c8a992d67dc6f39a736cc18f0d3d685"", ""last_modified"": ""2023-09-09 19:02:33+00:00"", ""created_at"": ""2023-09-09 19:02:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-test09-int4\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-test09-int4"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-09 19:02:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-test09-int4\n  results: []"", ""transformersInfo"": null, ""_id"": ""64fcc13e8c21ebb3dba7270f"", ""modelId"": ""antonwonton/llama-2-7b-chat-hf-test09-int4"", ""usedStorage"": 67158984}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=antonwonton/llama-2-7b-chat-hf-test09-int4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bantonwonton%2Fllama-2-7b-chat-hf-test09-int4%5D(%2Fantonwonton%2Fllama-2-7b-chat-hf-test09-int4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/antonwonton/Llama-2-7b-chat-hf-int4-ft-0.75,N/A,N/A,1,,0,,0,,0,,0,,0
Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v623,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_dialogsum_ft_adapters_v623
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_dialogsum_ft_adapters_v623

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v623"", ""author"": ""Jukaboo"", ""sha"": ""cda572bd15234bf44a01c97967d1962afff66916"", ""last_modified"": ""2023-09-11 11:13:21+00:00"", ""created_at"": ""2023-09-11 10:42:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v623\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_dialogsum_ft_adapters_v623"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-11 11:13:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v623\n  results: []"", ""transformersInfo"": null, ""_id"": ""64feef1f56243ce8cb69eec7"", ""modelId"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v623"", ""usedStorage"": 34153883}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v623&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v623%5D(%2FJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v623)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v1200,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_dialogsum_ft_adapters_v1200
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_dialogsum_ft_adapters_v1200

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v1200"", ""author"": ""Jukaboo"", ""sha"": ""b53baab780814e42f957d511fbd8c6d966c6e5e3"", ""last_modified"": ""2023-09-11 11:39:12+00:00"", ""created_at"": ""2023-09-11 11:29:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v1200\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_dialogsum_ft_adapters_v1200"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-11 11:39:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v1200\n  results: []"", ""transformersInfo"": null, ""_id"": ""64fefa2138fe585b70249241"", ""modelId"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v1200"", ""usedStorage"": 17326803}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v1200&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v1200%5D(%2FJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v1200)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v2400,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_dialogsum_ft_adapters_v2400
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_dialogsum_ft_adapters_v2400

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v2400"", ""author"": ""Jukaboo"", ""sha"": ""248e140601de7b9fa9eec16effeebc8a16beacba"", ""last_modified"": ""2023-09-11 12:14:06+00:00"", ""created_at"": ""2023-09-11 11:56:50+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v2400\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_dialogsum_ft_adapters_v2400"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-11 12:14:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v2400\n  results: []"", ""transformersInfo"": null, ""_id"": ""64ff0082075c57ceed32ad99"", ""modelId"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v2400"", ""usedStorage"": 34149792}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v2400&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v2400%5D(%2FJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v2400)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v4100,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_dialogsum_ft_adapters_v4100
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_dialogsum_ft_adapters_v4100

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
",N/A,1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v4100&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v4100%5D(%2FJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v4100)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v12100,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_dialogsum_ft_adapters_v12100
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_dialogsum_ft_adapters_v12100

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v12100"", ""author"": ""Jukaboo"", ""sha"": ""b38a2fc8432e22b79e24981c4cee91d38ca98d84"", ""last_modified"": ""2023-09-11 14:25:04+00:00"", ""created_at"": ""2023-09-11 12:57:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v12100\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_dialogsum_ft_adapters_v12100"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-11 14:25:04+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_dialogsum_ft_adapters_v12100\n  results: []"", ""transformersInfo"": null, ""_id"": ""64ff0eaef8324d0889898184"", ""modelId"": ""Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v12100"", ""usedStorage"": 118264737}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_dialogsum_ft_adapters_v12100&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v12100%5D(%2FJukaboo%2FLlama2_7B_chat_dialogsum_ft_adapters_v12100)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Atulit23/meta-llama-indian-constitution-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: meta-llama-indian-constitution-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# meta-llama-indian-constitution-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.2
- Pytorch 2.0.1+cu117
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""Atulit23/meta-llama-indian-constitution-chat"", ""author"": ""Atulit23"", ""sha"": ""9fe0b5e80862e86e8b94e8d24271853b3417d312"", ""last_modified"": ""2023-09-17 20:10:25+00:00"", ""created_at"": ""2023-09-17 19:56:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 7, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: meta-llama-indian-constitution-chat\n  results: []"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": [{""name"": ""meta-llama-indian-constitution-chat"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [""Atulit23/Atulit23-meta-llama-indian-constitution-chat"", ""Atulit23/Atulit23-meta-llama-indian-constitution-chat-new""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-17 20:10:25+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: meta-llama-indian-constitution-chat\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""650759defec2f376354e9907"", ""modelId"": ""Atulit23/meta-llama-indian-constitution-chat"", ""usedStorage"": 53907961891}",1,,0,,0,,0,,0,"Atulit23/Atulit23-meta-llama-indian-constitution-chat, Atulit23/Atulit23-meta-llama-indian-constitution-chat-new, huggingface/InferenceSupport/discussions/new?title=Atulit23/meta-llama-indian-constitution-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAtulit23%2Fmeta-llama-indian-constitution-chat%5D(%2FAtulit23%2Fmeta-llama-indian-constitution-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",3
samxm111/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.33.2
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""samxm111/results"", ""author"": ""samxm111"", ""sha"": ""65ffed64140ded253ca1c976f0a79cecf865b608"", ""last_modified"": ""2023-09-19 15:52:03+00:00"", ""created_at"": ""2023-09-19 15:51:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-19 15:52:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""6509c39c4882fab5c4b3aacc"", ""modelId"": ""samxm111/results"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=samxm111/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsamxm111%2Fresults%5D(%2Fsamxm111%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
msato777/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""msato777/results"", ""author"": ""msato777"", ""sha"": ""213f15e4db71c808c3afaee72332465af1bc60f2"", ""last_modified"": ""2023-10-05 16:19:32+00:00"", ""created_at"": ""2023-09-20 19:46:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-05 16:19:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""650b4c02fa85b08cdff91e73"", ""modelId"": ""msato777/results"", ""usedStorage"": 403303075}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=msato777/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmsato777%2Fresults%5D(%2Fmsato777%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
EnzoZacharias/LLama2-7b-fine-tuned-plc_V1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: LLama2-7b-fine-tuned-plc_V1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# LLama2-7b-fine-tuned-plc_V1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 50

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.1.0.dev20230823
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""EnzoZacharias/LLama2-7b-fine-tuned-plc_V1"", ""author"": ""EnzoZacharias"", ""sha"": ""1c664c7c2d78fdc83884ee032ce8332cf25e3c6e"", ""last_modified"": ""2023-09-21 08:37:41+00:00"", ""created_at"": ""2023-09-21 08:28:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLama2-7b-fine-tuned-plc_V1\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""LLama2-7b-fine-tuned-plc_V1"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='emissions.csv', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-21 08:37:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLama2-7b-fine-tuned-plc_V1\n  results: []"", ""transformersInfo"": null, ""_id"": ""650bfe90affb2713c2c535e2"", ""modelId"": ""EnzoZacharias/LLama2-7b-fine-tuned-plc_V1"", ""usedStorage"": 17327693}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=EnzoZacharias/LLama2-7b-fine-tuned-plc_V1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEnzoZacharias%2FLLama2-7b-fine-tuned-plc_V1%5D(%2FEnzoZacharias%2FLLama2-7b-fine-tuned-plc_V1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
ahmadsajid1989/Llama-2-7b-chat-hf-fine-tuned-bongo-cs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: bongo-cs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bongo-cs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 20

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""ahmadsajid1989/Llama-2-7b-chat-hf-fine-tuned-bongo-cs"", ""author"": ""ahmadsajid1989"", ""sha"": ""33d2369cb8281657d7b529db09ad8ee96fdb458c"", ""last_modified"": ""2023-09-24 10:10:34+00:00"", ""created_at"": ""2023-09-24 10:08:30+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bongo-cs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""bongo-cs"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-24 10:10:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: bongo-cs\n  results: []"", ""transformersInfo"": null, ""_id"": ""65100a9edcfa9d24c5326d5e"", ""modelId"": ""ahmadsajid1989/Llama-2-7b-chat-hf-fine-tuned-bongo-cs"", ""usedStorage"": 16827016}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=ahmadsajid1989/Llama-2-7b-chat-hf-fine-tuned-bongo-cs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bahmadsajid1989%2FLlama-2-7b-chat-hf-fine-tuned-bongo-cs%5D(%2Fahmadsajid1989%2FLlama-2-7b-chat-hf-fine-tuned-bongo-cs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Michelvh/qlora-llama2-7b-question-generation-eduqg,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: qlora-llama2-7b-question-generation-eduqg
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# qlora-llama2-7b-question-generation-eduqg

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 50

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.0
- Datasets 2.1.0
- Tokenizers 0.14.1
","{""id"": ""Michelvh/qlora-llama2-7b-question-generation-eduqg"", ""author"": ""Michelvh"", ""sha"": ""ab7d2978802ec5dc318d818f64ffe2fd198ce58b"", ""last_modified"": ""2023-10-26 15:15:07+00:00"", ""created_at"": ""2023-09-27 12:59:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 2, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: qlora-llama2-7b-question-generation-eduqg\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""qlora-llama2-7b-question-generation-eduqg"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-26 15:15:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: qlora-llama2-7b-question-generation-eduqg\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6514272eed578610b4a9c8b0"", ""modelId"": ""Michelvh/qlora-llama2-7b-question-generation-eduqg"", ""usedStorage"": 16015409043}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Michelvh/qlora-llama2-7b-question-generation-eduqg&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMichelvh%2Fqlora-llama2-7b-question-generation-eduqg%5D(%2FMichelvh%2Fqlora-llama2-7b-question-generation-eduqg)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
thekrishna/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 100

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""thekrishna/results"", ""author"": ""thekrishna"", ""sha"": ""091bc736af7e5820068b38efaffa9ef5630d43a8"", ""last_modified"": ""2023-09-27 16:56:36+00:00"", ""created_at"": ""2023-09-27 16:56:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-09-27 16:56:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""65145ebd6a38fb64d59968d6"", ""modelId"": ""thekrishna/results"", ""usedStorage"": 134267784}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=thekrishna/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthekrishna%2Fresults%5D(%2Fthekrishna%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
chaocai/llama2-ft,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-ft
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-ft

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""chaocai/llama2-ft"", ""author"": ""chaocai"", ""sha"": ""13e2f2a3397b996aff6823c16fb95cba3f5a13f9"", ""last_modified"": ""2023-10-01 07:53:51+00:00"", ""created_at"": ""2023-10-01 01:03:58+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-ft\n  results: []"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": [{""name"": ""llama2-ft"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Oct01_01-03-48_6b7f8afaf19b/events.out.tfevents.1696122244.6b7f8afaf19b.768.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-01 07:53:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-ft\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6518c57edb386e1022453ccf"", ""modelId"": ""chaocai/llama2-ft"", ""usedStorage"": 134277210}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=chaocai/llama2-ft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bchaocai%2Fllama2-ft%5D(%2Fchaocai%2Fllama2-ft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
vineetsharma/qlora-Llama-2-7b-chat-hf-databricks-dolly-15k,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: qlora-Llama-2-7b-chat-hf-databricks-dolly-15k
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# qlora-Llama-2-7b-chat-hf-databricks-dolly-15k

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1

### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""vineetsharma/qlora-Llama-2-7b-chat-hf-databricks-dolly-15k"", ""author"": ""vineetsharma"", ""sha"": ""05a393516ee18806e818da50a8e39a12828b47cc"", ""last_modified"": ""2023-10-02 05:46:21+00:00"", ""created_at"": ""2023-10-01 16:16:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: qlora-Llama-2-7b-chat-hf-databricks-dolly-15k\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""qlora-Llama-2-7b-chat-hf-databricks-dolly-15k"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-02 05:46:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: qlora-Llama-2-7b-chat-hf-databricks-dolly-15k\n  results: []"", ""transformersInfo"": null, ""_id"": ""65199b617069441423787542"", ""modelId"": ""vineetsharma/qlora-Llama-2-7b-chat-hf-databricks-dolly-15k"", ""usedStorage"": 302420513}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=vineetsharma/qlora-Llama-2-7b-chat-hf-databricks-dolly-15k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bvineetsharma%2Fqlora-Llama-2-7b-chat-hf-databricks-dolly-15k%5D(%2Fvineetsharma%2Fqlora-Llama-2-7b-chat-hf-databricks-dolly-15k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v1200,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_v1200
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_v1200

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v1200"", ""author"": ""Jukaboo"", ""sha"": ""32986b1a9c111bc69c55d20ac42b2a9d9f8c4f84"", ""last_modified"": ""2023-10-05 13:04:56+00:00"", ""created_at"": ""2023-10-05 12:48:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v1200\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_v1200"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-05 13:04:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v1200\n  results: []"", ""transformersInfo"": null, ""_id"": ""651eb090c3f8826b1a83bfc6"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v1200"", ""usedStorage"": 17326803}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v1200&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v1200%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v1200)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_v2585

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585"", ""author"": ""Jukaboo"", ""sha"": ""8dbaaf041bb7d906f3d06d5a2b3fed83331cba59"", ""last_modified"": ""2023-10-05 19:39:46+00:00"", ""created_at"": ""2023-10-05 14:17:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_v2585"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-05 19:39:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585\n  results: []"", ""transformersInfo"": null, ""_id"": ""651ec572aa888068acb2b04a"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585"", ""usedStorage"": 50976872}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v2585%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v2585)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5100,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_v5100
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_v5100

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5100"", ""author"": ""Jukaboo"", ""sha"": ""ff3ac3a991874250b1c9ab459ce35921e051e858"", ""last_modified"": ""2023-10-05 21:48:57+00:00"", ""created_at"": ""2023-10-05 20:34:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v5100\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_v5100"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-05 21:48:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v5100\n  results: []"", ""transformersInfo"": null, ""_id"": ""651f1dc31719ac0cec28baf6"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5100"", ""usedStorage"": 50972781}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5100&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v5100%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v5100)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Tharuneshwar/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Framework versions

- Transformers 4.34.0
- Pytorch 2.1.0+cu121
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Tharuneshwar/results"", ""author"": ""Tharuneshwar"", ""sha"": ""067da28929ebfbf06f4733ac7534f8e83b8f93ee"", ""last_modified"": ""2023-10-08 12:52:43+00:00"", ""created_at"": ""2023-10-08 12:52:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-08 12:52:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""6522a6176b01183a67e93e43"", ""modelId"": ""Tharuneshwar/results"", ""usedStorage"": 134766669}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Tharuneshwar/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTharuneshwar%2Fresults%5D(%2FTharuneshwar%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
erbacher/llama2_hf_int,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2_hf_int
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_hf_int

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 4e-05
- train_batch_size: 32
- eval_batch_size: 16
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 128
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu117
- Datasets 2.11.0
- Tokenizers 0.14.0
","{""id"": ""erbacher/llama2_hf_int"", ""author"": ""erbacher"", ""sha"": ""bd29adc4a9fb76bf9c912374d4d424f7380d97cc"", ""last_modified"": ""2023-10-11 08:07:00+00:00"", ""created_at"": ""2023-10-11 06:18:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2_hf_int\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2_hf_int"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-11 08:07:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2_hf_int\n  results: []"", ""transformersInfo"": null, ""_id"": ""65263e358a21d8f14ad8a37e"", ""modelId"": ""erbacher/llama2_hf_int"", ""usedStorage"": 403295021}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=erbacher/llama2_hf_int&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Berbacher%2Fllama2_hf_int%5D(%2Ferbacher%2Fllama2_hf_int)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
surathisin/llama-2-13b-finetune-bot-2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-13b-finetune-bot-2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-13b-finetune-bot-2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 50

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""surathisin/llama-2-13b-finetune-bot-2"", ""author"": ""surathisin"", ""sha"": ""27595ca0238acc63fef3f658d9dc0ca2297688d0"", ""last_modified"": ""2023-10-11 07:35:42+00:00"", ""created_at"": ""2023-10-11 07:24:55+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-13b-finetune-bot-2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-13b-finetune-bot-2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-11 07:35:42+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-13b-finetune-bot-2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65264dc727bc44de700c6dd2"", ""modelId"": ""surathisin/llama-2-13b-finetune-bot-2"", ""usedStorage"": 537063146}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=surathisin/llama-2-13b-finetune-bot-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsurathisin%2Fllama-2-13b-finetune-bot-2%5D(%2Fsurathisin%2Fllama-2-13b-finetune-bot-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
surathisin/llama-2-7b-finetune-1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-finetune-1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-finetune-1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 50

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""surathisin/llama-2-7b-finetune-1"", ""author"": ""surathisin"", ""sha"": ""8696563c05b913585540e6d46ce806115be91aac"", ""last_modified"": ""2023-10-11 13:07:43+00:00"", ""created_at"": ""2023-10-11 11:25:54+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-finetune-1\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-finetune-1"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-11 13:07:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-finetune-1\n  results: []"", ""transformersInfo"": null, ""_id"": ""652686429340bbf0c42994bc"", ""modelId"": ""surathisin/llama-2-7b-finetune-1"", ""usedStorage"": 537063210}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=surathisin/llama-2-7b-finetune-1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsurathisin%2Fllama-2-7b-finetune-1%5D(%2Fsurathisin%2Fllama-2-7b-finetune-1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
justinlangseth/llama-10-11-sp-1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-10-11-sp-1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-10-11-sp-1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 1000

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.0.1+cu117
- Datasets 2.14.5
- Tokenizers 0.13.3
","{""id"": ""justinlangseth/llama-10-11-sp-1"", ""author"": ""justinlangseth"", ""sha"": ""bdee4b0f2ccf3cccbe2a4f3530144ac7868d42bf"", ""last_modified"": ""2023-10-11 21:38:37+00:00"", ""created_at"": ""2023-10-11 21:38:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-10-11-sp-1\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-10-11-sp-1"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-11 21:38:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-10-11-sp-1\n  results: []"", ""transformersInfo"": null, ""_id"": ""652715d868608eb5bbb82957"", ""modelId"": ""justinlangseth/llama-10-11-sp-1"", ""usedStorage"": 17326803}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=justinlangseth/llama-10-11-sp-1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjustinlangseth%2Fllama-10-11-sp-1%5D(%2Fjustinlangseth%2Fllama-10-11-sp-1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
surathisin/llama-2-7b-finetune-001,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-finetune-001
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-finetune-001

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 100

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""surathisin/llama-2-7b-finetune-001"", ""author"": ""surathisin"", ""sha"": ""7fa91e633cd7c92eb9ddaa97ccf3966f996ccc8b"", ""last_modified"": ""2023-10-12 11:58:31+00:00"", ""created_at"": ""2023-10-12 06:59:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-finetune-001\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-finetune-001"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-12 11:58:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-finetune-001\n  results: []"", ""transformersInfo"": null, ""_id"": ""6527995777bceabaab304825"", ""modelId"": ""surathisin/llama-2-7b-finetune-001"", ""usedStorage"": 5371114435}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=surathisin/llama-2-7b-finetune-001&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsurathisin%2Fllama-2-7b-finetune-001%5D(%2Fsurathisin%2Fllama-2-7b-finetune-001)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
BLACKBUN/llama-2-7b-pubmed-qa-211k,"---
datasets:
- qiaojin/PubMedQA
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""BLACKBUN/llama-2-7b-pubmed-qa-211k"", ""author"": ""BLACKBUN"", ""sha"": ""1614303337395f6acf509cbfc2644dd60124cc81"", ""last_modified"": ""2025-04-06 23:25:42+00:00"", ""created_at"": ""2023-10-14 07:58:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 16, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""conversational"", ""dataset:qiaojin/PubMedQA"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- qiaojin/PubMedQA"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [""Omnibus/InferenceClient_Chatbots"", ""K00B404/Teachershub""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-04-06 23:25:42+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- qiaojin/PubMedQA"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""652a4a32703b3743c23b1416"", ""modelId"": ""BLACKBUN/llama-2-7b-pubmed-qa-211k"", ""usedStorage"": 26953781249}",1,,0,,0,,0,,0,"K00B404/Teachershub, Omnibus/InferenceClient_Chatbots, huggingface/InferenceSupport/discussions/new?title=BLACKBUN/llama-2-7b-pubmed-qa-211k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BBLACKBUN%2Fllama-2-7b-pubmed-qa-211k%5D(%2FBLACKBUN%2Fllama-2-7b-pubmed-qa-211k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",3
surathisin/nvso-model-test-1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: nvso-model-test-1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# nvso-model-test-1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 100

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""surathisin/nvso-model-test-1"", ""author"": ""surathisin"", ""sha"": ""985f153e656a0541132058dc1547e8c691e4df1a"", ""last_modified"": ""2023-10-15 10:27:39+00:00"", ""created_at"": ""2023-10-14 09:25:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: nvso-model-test-1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""nvso-model-test-1"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 10:27:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: nvso-model-test-1\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""652a5e7ac6857682d3cbc09d"", ""modelId"": ""surathisin/nvso-model-test-1"", ""usedStorage"": 537570859}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=surathisin/nvso-model-test-1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsurathisin%2Fnvso-model-test-1%5D(%2Fsurathisin%2Fnvso-model-test-1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_base,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_base
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_base

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_base"", ""author"": ""alperk3003"", ""sha"": ""40fd30dda64060b269ddeece7b47b582cfc2d83d"", ""last_modified"": ""2023-10-14 19:40:53+00:00"", ""created_at"": ""2023-10-14 19:40:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_base\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_base"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-14 19:40:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_base\n  results: []"", ""transformersInfo"": null, ""_id"": ""652aeeb83a416e1f21a4c468"", ""modelId"": ""alperk3003/medalpaca_base"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_base&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_base%5D(%2Falperk3003%2Fmedalpaca_base)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_circulatory_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_circulatory_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_circulatory_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_circulatory_model"", ""author"": ""alperk3003"", ""sha"": ""31c9505b49207bbc8ac7ddc5f1f7fa958db90ada"", ""last_modified"": ""2023-10-14 23:41:49+00:00"", ""created_at"": ""2023-10-14 23:41:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_circulatory_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_circulatory_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-14 23:41:49+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_circulatory_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652b2731e0f39e3bae4805cc"", ""modelId"": ""alperk3003/medalpaca_circulatory_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_circulatory_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_circulatory_model%5D(%2Falperk3003%2Fmedalpaca_circulatory_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_digestive_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_digestive_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_digestive_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_digestive_model"", ""author"": ""alperk3003"", ""sha"": ""d44c3d5b1c8d33fc6927d552fb055df80039db8a"", ""last_modified"": ""2023-10-15 02:50:21+00:00"", ""created_at"": ""2023-10-15 02:50:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_digestive_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_digestive_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 02:50:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_digestive_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652b53621a3250bbfe534833"", ""modelId"": ""alperk3003/medalpaca_digestive_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_digestive_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_digestive_model%5D(%2Falperk3003%2Fmedalpaca_digestive_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_ear_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_ear_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_ear_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_ear_model"", ""author"": ""alperk3003"", ""sha"": ""9fdb261cd12e63486924b970cf361a260d6834c1"", ""last_modified"": ""2023-10-15 06:04:48+00:00"", ""created_at"": ""2023-10-15 06:04:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_ear_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_ear_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 06:04:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_ear_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652b80f5b118f26df74b4be2"", ""modelId"": ""alperk3003/medalpaca_ear_model"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_ear_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_ear_model%5D(%2Falperk3003%2Fmedalpaca_ear_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_eye_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_eye_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_eye_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_eye_model"", ""author"": ""alperk3003"", ""sha"": ""2c930e1a7478a25df7c3daa2117feb5036582a69"", ""last_modified"": ""2023-10-15 09:15:55+00:00"", ""created_at"": ""2023-10-15 09:15:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_eye_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_eye_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 09:15:55+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_eye_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652badbff6390fe048f31816"", ""modelId"": ""alperk3003/medalpaca_eye_model"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_eye_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_eye_model%5D(%2Falperk3003%2Fmedalpaca_eye_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_genitourinary_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_genitourinary_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_genitourinary_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_genitourinary_model"", ""author"": ""alperk3003"", ""sha"": ""15d4f282f6ed01b46990d34df4eaca5402e76d08"", ""last_modified"": ""2023-10-15 12:37:28+00:00"", ""created_at"": ""2023-10-15 12:37:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_genitourinary_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_genitourinary_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 12:37:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_genitourinary_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652bdcfc6f88e29851b37a64"", ""modelId"": ""alperk3003/medalpaca_genitourinary_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_genitourinary_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_genitourinary_model%5D(%2Falperk3003%2Fmedalpaca_genitourinary_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
surathisin/nvso-model-test-4,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: nvso-model-test-4
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# nvso-model-test-4

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 10

### Training results



### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""surathisin/nvso-model-test-4"", ""author"": ""surathisin"", ""sha"": ""1f1511e28cc8722dc19c8a965477aadb42303fe5"", ""last_modified"": ""2023-10-15 14:51:24+00:00"", ""created_at"": ""2023-10-15 14:51:19+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: nvso-model-test-4\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""nvso-model-test-4"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 14:51:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: nvso-model-test-4\n  results: []"", ""transformersInfo"": null, ""_id"": ""652bfc67e0f39e3bae5ed176"", ""modelId"": ""surathisin/nvso-model-test-4"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=surathisin/nvso-model-test-4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsurathisin%2Fnvso-model-test-4%5D(%2Fsurathisin%2Fnvso-model-test-4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_nutritional_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_nutritional_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_nutritional_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_nutritional_model"", ""author"": ""alperk3003"", ""sha"": ""37a3dd0598d9525f7949881c4943d69951bb0227"", ""last_modified"": ""2023-10-15 15:51:03+00:00"", ""created_at"": ""2023-10-15 15:50:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_nutritional_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_nutritional_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 15:51:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_nutritional_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652c0a4ff12059832294a7cc"", ""modelId"": ""alperk3003/medalpaca_nutritional_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_nutritional_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_nutritional_model%5D(%2Falperk3003%2Fmedalpaca_nutritional_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_infectious_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_infectious_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_infectious_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_infectious_model"", ""author"": ""alperk3003"", ""sha"": ""a9b5017630ab7e36b563de64f2d3353cbe4a8dca"", ""last_modified"": ""2023-10-15 19:09:39+00:00"", ""created_at"": ""2023-10-15 19:09:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_infectious_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_infectious_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 19:09:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_infectious_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652c38e86cdea40585118d32"", ""modelId"": ""alperk3003/medalpaca_infectious_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_infectious_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_infectious_model%5D(%2Falperk3003%2Fmedalpaca_infectious_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
whatdhack/Llama-2-7b-chat-hf-oasst1-lora-b157,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: output_dir
  results: []
pipeline_tag: text-generation
library_name: transformers
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_dir

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the timdettmers/openassistant-guanaco dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1","{""id"": ""whatdhack/Llama-2-7b-chat-hf-oasst1-lora-b157"", ""author"": ""whatdhack"", ""sha"": ""0bb7c929aff52270617d784366c773a38e49fa7d"", ""last_modified"": ""2023-10-15 22:18:20+00:00"", ""created_at"": ""2023-10-15 20:35:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""generated_from_trainer"", ""text-generation"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""output_dir"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-15 22:18:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""652c4d16c543a08aa91bc670"", ""modelId"": ""whatdhack/Llama-2-7b-chat-hf-oasst1-lora-b157"", ""usedStorage"": 134267784}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=whatdhack/Llama-2-7b-chat-hf-oasst1-lora-b157&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwhatdhack%2FLlama-2-7b-chat-hf-oasst1-lora-b157%5D(%2Fwhatdhack%2FLlama-2-7b-chat-hf-oasst1-lora-b157)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_nervous_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_nervous_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_nervous_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_nervous_model"", ""author"": ""alperk3003"", ""sha"": ""7050b65cb0b0be3e7da6708c3324069184f2d928"", ""last_modified"": ""2023-10-16 01:14:31+00:00"", ""created_at"": ""2023-10-16 01:14:20+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_nervous_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_nervous_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 01:14:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_nervous_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652c8e6cb118f26df7699ccd"", ""modelId"": ""alperk3003/medalpaca_nervous_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_nervous_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_nervous_model%5D(%2Falperk3003%2Fmedalpaca_nervous_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
whatdhack/Llama-2-7b-chat-hf-oasst1-ft-sg,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: output_dir
  results: []
pipeline_tag: text-generation
library_name: transformers
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_dir

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the timdettmers/openassistant-guanaco dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1","{""id"": ""whatdhack/Llama-2-7b-chat-hf-oasst1-ft-sg"", ""author"": ""whatdhack"", ""sha"": ""447698286962f15ebdd794e924762b76fd485e3a"", ""last_modified"": ""2023-10-19 06:54:47+00:00"", ""created_at"": ""2023-10-16 03:01:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 4, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""output_dir"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2023-10-19 06:54:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""652ca77c802e3d1a4fdede31"", ""modelId"": ""whatdhack/Llama-2-7b-chat-hf-oasst1-ft-sg"", ""usedStorage"": 40430665069}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=whatdhack/Llama-2-7b-chat-hf-oasst1-ft-sg&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwhatdhack%2FLlama-2-7b-chat-hf-oasst1-ft-sg%5D(%2Fwhatdhack%2FLlama-2-7b-chat-hf-oasst1-ft-sg)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_respiratory_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_respiratory_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_respiratory_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_respiratory_model"", ""author"": ""alperk3003"", ""sha"": ""72832e2737f33759208b59e6a5ad14b6f677f0f8"", ""last_modified"": ""2023-10-16 04:30:22+00:00"", ""created_at"": ""2023-10-16 04:30:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_respiratory_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_respiratory_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 04:30:22+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_respiratory_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652cbc53ff2202020ecd80a6"", ""modelId"": ""alperk3003/medalpaca_respiratory_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_respiratory_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_respiratory_model%5D(%2Falperk3003%2Fmedalpaca_respiratory_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_skin_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_skin_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_skin_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_skin_model"", ""author"": ""alperk3003"", ""sha"": ""99e46ed731a611d6bbd3fffc91222890c489c321"", ""last_modified"": ""2023-10-16 07:38:48+00:00"", ""created_at"": ""2023-10-16 07:38:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_skin_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_skin_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 07:38:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_skin_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652ce8681a3250bbfe81ed97"", ""modelId"": ""alperk3003/medalpaca_skin_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_skin_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_skin_model%5D(%2Falperk3003%2Fmedalpaca_skin_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585v2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585v2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_v2585v2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585v2"", ""author"": ""Jukaboo"", ""sha"": ""9985f729b92b5497c927d2aad6b19b1840310a06"", ""last_modified"": ""2023-10-16 10:33:52+00:00"", ""created_at"": ""2023-10-16 09:58:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585v2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_v2585v2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 10:33:52+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v2585v2\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d0960cdb2a912056b75c3"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585v2"", ""usedStorage"": 34149792}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v2585v2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v2585v2%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v2585v2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_musculoskeletal_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_musculoskeletal_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_musculoskeletal_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_musculoskeletal_model"", ""author"": ""alperk3003"", ""sha"": ""8f2f84d6950819f833a156655987e3cf890d0a86"", ""last_modified"": ""2023-10-16 10:46:29+00:00"", ""created_at"": ""2023-10-16 10:45:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_musculoskeletal_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_musculoskeletal_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 10:46:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_musculoskeletal_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d144bd997b4146b79f3de"", ""modelId"": ""alperk3003/medalpaca_musculoskeletal_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_musculoskeletal_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_musculoskeletal_model%5D(%2Falperk3003%2Fmedalpaca_musculoskeletal_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5000v2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_v5000v2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_v5000v2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5000v2"", ""author"": ""Jukaboo"", ""sha"": ""ffcae00901e2abc0deaba30e65d5c80e3b092925"", ""last_modified"": ""2023-10-16 12:42:02+00:00"", ""created_at"": ""2023-10-16 11:45:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v5000v2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_v5000v2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 12:42:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_v5000v2\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d2252072563249e88ea8e"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5000v2"", ""usedStorage"": 50972781}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_v5000v2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v5000v2%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_v5000v2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_ep3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_ep3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep3"", ""author"": ""Jukaboo"", ""sha"": ""84bc9250b913dd2b2171b65c4683d6517c4b8751"", ""last_modified"": ""2023-10-16 15:56:35+00:00"", ""created_at"": ""2023-10-16 13:59:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep3\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_ep3"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 15:56:35+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep3\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d41cef1c5393d95208b62"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep3"", ""usedStorage"": 135087726}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep3%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_mental_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_mental_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_mental_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_mental_model"", ""author"": ""alperk3003"", ""sha"": ""46457591e0b01a89dde87f3398303c237ee732da"", ""last_modified"": ""2023-10-16 14:16:12+00:00"", ""created_at"": ""2023-10-16 14:16:01+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_mental_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_mental_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 14:16:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_mental_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d45a1d2ccc6a2a1f79cf7"", ""modelId"": ""alperk3003/medalpaca_mental_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_mental_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_mental_model%5D(%2Falperk3003%2Fmedalpaca_mental_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
alperk3003/medalpaca_blood_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: medalpaca_blood_model
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# medalpaca_blood_model

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 5000

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""alperk3003/medalpaca_blood_model"", ""author"": ""alperk3003"", ""sha"": ""80b5ded17f4c64e18c5d179143b2b33d38f19445"", ""last_modified"": ""2023-10-16 17:26:59+00:00"", ""created_at"": ""2023-10-16 17:26:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_blood_model\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""medalpaca_blood_model"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-16 17:26:59+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: medalpaca_blood_model\n  results: []"", ""transformersInfo"": null, ""_id"": ""652d72554dfc91e4b19051e6"", ""modelId"": ""alperk3003/medalpaca_blood_model"", ""usedStorage"": 134767571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=alperk3003/medalpaca_blood_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Balperk3003%2Fmedalpaca_blood_model%5D(%2Falperk3003%2Fmedalpaca_blood_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
langecod/Genesis_Llama,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Genesis_Llama
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Genesis_Llama

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1314

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.1729        | 0.29  | 214  | 1.2092          |
| 1.0482        | 1.29  | 428  | 1.1480          |
| 1.2354        | 2.29  | 642  | 1.1314          |


### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""langecod/Genesis_Llama"", ""author"": ""langecod"", ""sha"": ""e773bb9ef4e80b000e482c68da843e155d35f39a"", ""last_modified"": ""2023-10-17 00:43:27+00:00"", ""created_at"": ""2023-10-17 00:43:19+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Genesis_Llama\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Genesis_Llama"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-17 00:43:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Genesis_Llama\n  results: []"", ""transformersInfo"": null, ""_id"": ""652dd8a780bb47354df866d9"", ""modelId"": ""langecod/Genesis_Llama"", ""usedStorage"": 134767507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=langecod/Genesis_Llama&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Blangecod%2FGenesis_Llama%5D(%2Flangecod%2FGenesis_Llama)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
langecod/CounselLlama7B,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: CounselLlama7B
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# CounselLlama7B

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1452

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.2999        | 0.28  | 213  | 1.2184          |
| 1.1835        | 1.28  | 426  | 1.1595          |
| 1.2435        | 2.28  | 639  | 1.1452          |


### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""langecod/CounselLlama7B"", ""author"": ""langecod"", ""sha"": ""7f607ee9d9c8bf1856310379b70631cd8f8c2f9a"", ""last_modified"": ""2023-10-17 04:26:40+00:00"", ""created_at"": ""2023-10-17 03:40:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7B\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""CounselLlama7B"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-17 04:26:40+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7B\n  results: []"", ""transformersInfo"": null, ""_id"": ""652e023a2d617ec278c291dd"", ""modelId"": ""langecod/CounselLlama7B"", ""usedStorage"": 269035291}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=langecod/CounselLlama7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Blangecod%2FCounselLlama7B%5D(%2Flangecod%2FCounselLlama7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dininta/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""dininta/results"", ""author"": ""dininta"", ""sha"": ""a7c926a666bee992936e155d95261c8487d48638"", ""last_modified"": ""2023-10-17 06:00:17+00:00"", ""created_at"": ""2023-10-17 06:00:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-17 06:00:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""652e22e9656cca7ce9fde6ad"", ""modelId"": ""dininta/results"", ""usedStorage"": 134267784}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dininta/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdininta%2Fresults%5D(%2Fdininta%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
EnzoZacharias/Llama-2-7b-fine_tuned-SPS_final,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-fine_tuned-SPS_final
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-fine_tuned-SPS_final

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 50

### Training results



### Framework versions

- Transformers 4.34.0.dev0
- Pytorch 2.1.0.dev20230823
- Datasets 2.14.4
- Tokenizers 0.13.3
","{""id"": ""EnzoZacharias/Llama-2-7b-fine_tuned-SPS_final"", ""author"": ""EnzoZacharias"", ""sha"": ""3e38be5cbf399e2649a4c9f2b75ea6fa0049ad60"", ""last_modified"": ""2023-10-17 09:31:19+00:00"", ""created_at"": ""2023-10-17 09:21:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-fine_tuned-SPS_final\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-fine_tuned-SPS_final"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='emissions.csv', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-17 09:31:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-fine_tuned-SPS_final\n  results: []"", ""transformersInfo"": null, ""_id"": ""652e5222b62cf1f84647e0a9"", ""modelId"": ""EnzoZacharias/Llama-2-7b-fine_tuned-SPS_final"", ""usedStorage"": 17327693}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=EnzoZacharias/Llama-2-7b-fine_tuned-SPS_final&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEnzoZacharias%2FLlama-2-7b-fine_tuned-SPS_final%5D(%2FEnzoZacharias%2FLlama-2-7b-fine_tuned-SPS_final)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wcarr993/llama2-7B-151-v2-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7B-151-v2-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7B-151-v2-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 1000

### Training results



### Framework versions

- Transformers 4.33.0
- Pytorch 2.0.1+cu118
- Datasets 2.12.0
- Tokenizers 0.13.3
","{""id"": ""wcarr993/llama2-7B-151-v2-chat"", ""author"": ""wcarr993"", ""sha"": ""6eb0c3c96bc6cd98b49bf02849a391e5407936fe"", ""last_modified"": ""2023-10-17 14:34:31+00:00"", ""created_at"": ""2023-10-17 12:07:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7B-151-v2-chat\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7B-151-v2-chat"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-17 14:34:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7B-151-v2-chat\n  results: []"", ""transformersInfo"": null, ""_id"": ""652e79131ad13fee8c169926"", ""modelId"": ""wcarr993/llama2-7B-151-v2-chat"", ""usedStorage"": 1279589845}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=wcarr993/llama2-7B-151-v2-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwcarr993%2Fllama2-7B-151-v2-chat%5D(%2Fwcarr993%2Fllama2-7B-151-v2-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
whatdhack/Llama-2-7b-hf-oasst1-s100-sg,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: output_dir
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_dir

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 100

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""whatdhack/Llama-2-7b-hf-oasst1-s100-sg"", ""author"": ""whatdhack"", ""sha"": ""27252e16ad9add9535db5ff0499827dbec30e993"", ""last_modified"": ""2023-10-18 16:22:54+00:00"", ""created_at"": ""2023-10-18 16:21:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""output_dir"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-18 16:22:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: output_dir\n  results: []"", ""transformersInfo"": null, ""_id"": ""6530061934a77f8da20d7fcb"", ""modelId"": ""whatdhack/Llama-2-7b-hf-oasst1-s100-sg"", ""usedStorage"": 134267784}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=whatdhack/Llama-2-7b-hf-oasst1-s100-sg&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwhatdhack%2FLlama-2-7b-hf-oasst1-s100-sg%5D(%2Fwhatdhack%2FLlama-2-7b-hf-oasst1-s100-sg)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Waterfront/Llama-2-7b-chat-hf-social-media-captions-10k,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-chat-hf-social-media-captions-10k
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf-social-media-captions-10k

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.0.1+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Waterfront/Llama-2-7b-chat-hf-social-media-captions-10k"", ""author"": ""Waterfront"", ""sha"": ""1a073e2126346b214780070428b290ef98cd892b"", ""last_modified"": ""2023-10-19 09:04:22+00:00"", ""created_at"": ""2023-10-19 06:39:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-social-media-captions-10k\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-chat-hf-social-media-captions-10k"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-19 09:04:22+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf-social-media-captions-10k\n  results: []"", ""transformersInfo"": null, ""_id"": ""6530cf2f3b51002a47b23b75"", ""modelId"": ""Waterfront/Llama-2-7b-chat-hf-social-media-captions-10k"", ""usedStorage"": 10741104651}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Waterfront/Llama-2-7b-chat-hf-social-media-captions-10k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BWaterfront%2FLlama-2-7b-chat-hf-social-media-captions-10k%5D(%2FWaterfront%2FLlama-2-7b-chat-hf-social-media-captions-10k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_ep2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 2.0100

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.0918        | 0.41  | 33   | 2.1531          |
| 2.1826        | 0.81  | 66   | 2.0580          |
| 1.6641        | 1.22  | 99   | 2.0244          |
| 2.0317        | 1.63  | 132  | 2.0100          |


### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2"", ""author"": ""Jukaboo"", ""sha"": ""a6107b4b3c4fd4de521fa6baad8195e8f60a1391"", ""last_modified"": ""2023-10-23 17:34:17+00:00"", ""created_at"": ""2023-10-23 11:04:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_ep2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/run1/events.out.tfevents.1698062658.afd0c3e7f6ce.1678.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-23 17:34:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2\n  results: []"", ""transformersInfo"": null, ""_id"": ""6536533ccb8a5a17e7a1169f"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2"", ""usedStorage"": 34123698}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep2%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2_all,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2_all
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_ep2_all

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.8967

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.0156        | 0.4   | 130  | 1.9957          |
| 2.1166        | 0.8   | 260  | 1.9371          |
| 1.9634        | 1.21  | 390  | 1.9097          |
| 1.347         | 1.61  | 520  | 1.8967          |


### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu118
- Datasets 2.14.5
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2_all"", ""author"": ""Jukaboo"", ""sha"": ""7a7971e2af3ce04f8b11754952f4ea14121b8828"", ""last_modified"": ""2023-10-23 17:39:54+00:00"", ""created_at"": ""2023-10-23 14:12:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2_all\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_ep2_all"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/run2/events.out.tfevents.1698070386.afd0c3e7f6ce.33529.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-23 17:39:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_ep2_all\n  results: []"", ""transformersInfo"": null, ""_id"": ""65367f600e3b8b1a284de862"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2_all"", ""usedStorage"": 34230744}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_ep2_all&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep2_all%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_ep2_all)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_test,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_test
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_test

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9378

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.1525        | 0.2   | 65   | 2.0533          |
| 2.0143        | 0.4   | 130  | 1.9829          |
| 1.6408        | 0.6   | 195  | 1.9513          |
| 2.1303        | 0.8   | 260  | 1.9378          |


### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_test"", ""author"": ""Jukaboo"", ""sha"": ""08e934be81c236373db100d822c6a0e591e949c8"", ""last_modified"": ""2023-10-31 22:47:23+00:00"", ""created_at"": ""2023-10-24 08:11:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_test\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_test"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Neue Verkn\u00fcpfung.lnk', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/run1/events.out.tfevents.1698062658.afd0c3e7f6ce.1678.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/run2/events.out.tfevents.1698070386.afd0c3e7f6ce.33529.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/run3/events.out.tfevents.1698691928.7b344450174a.395.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-31 22:47:23+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_test\n  results: []"", ""transformersInfo"": null, ""_id"": ""65377c3c26ec79fe53c5e94f"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_test"", ""usedStorage"": 555261923}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_test%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
vicky4s4s/Llama-2-7B-Chat-GGML,"---
language:
- en
license: other
tags:
- facebook
- meta
- pytorch
- llama
- llama-2
model_name: Llama 2 7B Chat
arxiv: 2307.09288
inference: false
model_creator: Meta Llama 2
model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
model_type: llama
pipeline_tag: text-generation
quantized_by: TheBloke
base_model: meta-llama/Llama-2-7b-chat-hf
---

<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
<img src=""https://i.imgur.com/EBdldam.jpg"" alt=""TheBlokeAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
</div>
<div style=""display: flex; justify-content: space-between; width: 100%;"">
    <div style=""display: flex; flex-direction: column; align-items: flex-start;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://discord.gg/theblokeai"">Chat & support: TheBloke's Discord server</a></p>
    </div>
    <div style=""display: flex; flex-direction: column; align-items: flex-end;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://www.patreon.com/TheBlokeAI"">Want to contribute? TheBloke's Patreon page</a></p>
    </div>
</div>
<div style=""text-align:center; margin-top: 0em; margin-bottom: 0em""><p style=""margin-top: 0.25em; margin-bottom: 0em;"">TheBloke's LLM work is generously supported by a grant from <a href=""https://a16z.com"">andreessen horowitz (a16z)</a></p></div>
<hr style=""margin-top: 1.0em; margin-bottom: 1.0em;"">
<!-- header end -->

# Llama 2 7B Chat - GGML
- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)
- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

## Description

This repo contains GGML format model files for [Meta Llama 2's Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).

### Important note regarding GGML files.

The GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.

Please use the GGUF models instead.
### About GGML

GGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:
* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.
* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.
* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.
* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.
* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.
* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.

## Repositories available

* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)
* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)
* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGML)
* [Meta Llama 2's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

## Prompt template: Llama-2-Chat

```
[INST] <<SYS>>
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
<</SYS>>
{prompt}[/INST]

```

<!-- compatibility_ggml start -->
## Compatibility

These quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.

For support with latest llama.cpp, please use GGUF files instead.

The final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)

As of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.

## Explanation of the new k-quant methods
<details>
  <summary>Click to see details</summary>

The new methods available are:
* GGML_TYPE_Q2_K - ""type-1"" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)
* GGML_TYPE_Q3_K - ""type-0"" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.
* GGML_TYPE_Q4_K - ""type-1"" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.
* GGML_TYPE_Q5_K - ""type-1"" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw
* GGML_TYPE_Q6_K - ""type-0"" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw
* GGML_TYPE_Q8_K - ""type-0"" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.

Refer to the Provided Files table below to see what files use which methods, and how.
</details>
<!-- compatibility_ggml end -->

## Provided files

| Name | Quant method | Bits | Size | Max RAM required | Use case |
| ---- | ---- | ---- | ---- | ---- | ----- |
| llama-2-7b-chat.ggmlv3.q2_K.bin | q2_K | 2 | 2.87 GB| 5.37 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |
| llama-2-7b-chat.ggmlv3.q3_K_S.bin | q3_K_S | 3 | 2.95 GB| 5.45 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |
| llama-2-7b-chat.ggmlv3.q3_K_M.bin | q3_K_M | 3 | 3.28 GB| 5.78 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |
| llama-2-7b-chat.ggmlv3.q3_K_L.bin | q3_K_L | 3 | 3.60 GB| 6.10 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |
| llama-2-7b-chat.ggmlv3.q4_0.bin | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |
| llama-2-7b-chat.ggmlv3.q4_K_S.bin | q4_K_S | 4 | 3.83 GB| 6.33 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |
| llama-2-7b-chat.ggmlv3.q4_K_M.bin | q4_K_M | 4 | 4.08 GB| 6.58 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |
| llama-2-7b-chat.ggmlv3.q4_1.bin | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |
| llama-2-7b-chat.ggmlv3.q5_0.bin | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |
| llama-2-7b-chat.ggmlv3.q5_K_S.bin | q5_K_S | 5 | 4.65 GB| 7.15 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |
| llama-2-7b-chat.ggmlv3.q5_K_M.bin | q5_K_M | 5 | 4.78 GB| 7.28 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |
| llama-2-7b-chat.ggmlv3.q5_1.bin | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |
| llama-2-7b-chat.ggmlv3.q6_K.bin | q6_K | 6 | 5.53 GB| 8.03 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |
| llama-2-7b-chat.ggmlv3.q8_0.bin | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |

**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.

## How to run in `llama.cpp`

Make sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.

For compatibility with latest llama.cpp, please use GGUF files instead.

```
./main -t 10 -ngl 32 -m llama-2-7b-chat.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p ""[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\nWrite a story about llamas[/INST]""
```
Change `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.

Change `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.

Change `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.

If you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`

For other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)

## How to run in `text-generation-webui`

Further instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).

<!-- footer start -->
<!-- 200823 -->
## Discord

For further support, and discussions on these models and AI in general, join us at:

[TheBloke AI's Discord server](https://discord.gg/theblokeai)

## Thanks, and how to contribute.

Thanks to the [chirper.ai](https://chirper.ai) team!

I've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.

If you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.

Donaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.

* Patreon: https://patreon.com/TheBlokeAI
* Ko-Fi: https://ko-fi.com/TheBlokeAI

**Special thanks to**: Aemon Algiz.

**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper Wikieł, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, 阿明, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik Bjäreholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap'n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser


Thank you to all my generous patrons and donaters!

And thank you again to a16z for their generous grant.

<!-- footer end -->

# Original model card: Meta Llama 2's Llama 2 7B Chat

# **Llama 2**
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.

## Model Details
*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*

Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.

**Model Developers** Meta

**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.

**Input** Models input text only.

**Output** Models generate text only.

**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.


||Training Data|Params|Content Length|GQA|Tokens|LR|
|---|---|---|---|---|---|---|
|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|

*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Dates** Llama 2 was trained between January 2023 and July 2023.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

**Research Paper** [""Llama-2: Open Foundation and Fine-tuned Chat Models""](arxiv.org/abs/2307.09288)

## Intended Use
**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).

**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.

## Hardware and Software
**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.

||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|
|---|---|---|---|
|Llama 2 7B|184320|400|31.22|
|Llama 2 13B|368640|400|62.44|
|Llama 2 70B|1720320|400|291.42|
|Total|3311616||539.00|

**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.

## Training Data
**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.

## Evaluation Results

In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.

|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|
|---|---|---|---|---|---|---|---|---|---|
|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|
|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|
|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|
|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|
|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|
|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|
|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|

**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.

|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama 1|7B|27.42|23.00|
|Llama 1|13B|41.74|23.08|
|Llama 1|33B|44.19|22.57|
|Llama 1|65B|48.71|21.77|
|Llama 2|7B|33.29|**21.25**|
|Llama 2|13B|41.86|26.10|
|Llama 2|70B|**50.18**|24.60|

**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).


|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama-2-Chat|7B|57.04|**0.00**|
|Llama-2-Chat|13B|62.18|**0.00**|
|Llama-2-Chat|70B|**64.14**|0.01|

**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.

## Ethical Considerations and Limitations
Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.

Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)

## Reporting Issues
Please report any software “bug,” or other problems with the models through one of the following means:
- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)
- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)

## Llama Model Index
|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|
|---|---|---|---|---|
|7B| [Link](https://huggingface.co/llamaste/Llama-2-7b) | [Link](https://huggingface.co/llamaste/Llama-2-7b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat-hf)|
|13B| [Link](https://huggingface.co/llamaste/Llama-2-13b) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-13b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf)|
|70B| [Link](https://huggingface.co/llamaste/Llama-2-70b) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-70b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf)|
","{""id"": ""vicky4s4s/Llama-2-7B-Chat-GGML"", ""author"": ""vicky4s4s"", ""sha"": ""589a77fa85d19b83f39a44cd183eb141dbf236b3"", ""last_modified"": ""2023-10-31 19:06:19+00:00"", ""created_at"": ""2023-10-31 18:17:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""llama"", ""facebook"", ""meta"", ""pytorch"", ""llama-2"", ""text-generation"", ""en"", ""arxiv:2307.09288"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: other\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: TheBloke"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""model_type"": ""llama""}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='LICENSE.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Notice.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='gitattributes.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llama-2-7b-chat.ggmlv3.q2_K.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-10-31 19:06:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: other\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: TheBloke"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""654144a64556096591e6a7c8"", ""modelId"": ""vicky4s4s/Llama-2-7B-Chat-GGML"", ""usedStorage"": 2866807424}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=vicky4s4s/Llama-2-7B-Chat-GGML&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bvicky4s4s%2FLlama-2-7B-Chat-GGML%5D(%2Fvicky4s4s%2FLlama-2-7B-Chat-GGML)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
linuscarey123/out,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: out
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# out

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5419

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.0573        | 1.0   | 250  | 0.5403          |
| 0.0432        | 2.0   | 500  | 0.5419          |


### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""linuscarey123/out"", ""author"": ""linuscarey123"", ""sha"": ""a1372bd84f4d8c7373d2ea32d016c8efea79c5e3"", ""last_modified"": ""2023-11-01 11:38:11+00:00"", ""created_at"": ""2023-11-01 11:37:55+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: out\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""out"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-01 11:38:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: out\n  results: []"", ""transformersInfo"": null, ""_id"": ""65423893eccc4f48dcecd151"", ""modelId"": ""linuscarey123/out"", ""usedStorage"": 134268674}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=linuscarey123/out&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Blinuscarey123%2Fout%5D(%2Flinuscarey123%2Fout)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dtorres-zAgile/llama2-7b-zc-domain-misti,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-zc-domain-misti
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-zc-domain-misti

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9632

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 20

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.7949        | 0.95  | 20   | 1.9632          |


### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""dtorres-zAgile/llama2-7b-zc-domain-misti"", ""author"": ""dtorres-zAgile"", ""sha"": ""56e18ad4eafbe97b8fa48235abb26122d2683b65"", ""last_modified"": ""2023-11-02 21:22:03+00:00"", ""created_at"": ""2023-11-02 02:05:49+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-zc-domain-misti\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2-7b-zc-domain-misti"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-02 21:22:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-zc-domain-misti\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""654303fd5670dd57b76cd0fd"", ""modelId"": ""dtorres-zAgile/llama2-7b-zc-domain-misti"", ""usedStorage"": 26954267797}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dtorres-zAgile/llama2-7b-zc-domain-misti&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdtorres-zAgile%2Fllama2-7b-zc-domain-misti%5D(%2Fdtorres-zAgile%2Fllama2-7b-zc-domain-misti)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
sschangi/uplimit-project-3-llam2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- scitldr
model-index:
- name: uplimit-project-3-llam2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# uplimit-project-3-llam2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the scitldr dataset.
It achieves the following results on the evaluation set:
- Loss: 2.1887

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.2124        | 0.1   | 200  | 2.2725          |
| 2.1841        | 0.2   | 400  | 2.2590          |
| 2.1618        | 0.3   | 600  | 2.2784          |
| 2.1873        | 0.4   | 800  | 2.2520          |
| 2.1791        | 0.5   | 1000 | 2.2408          |
| 2.1653        | 0.6   | 1200 | 2.2275          |
| 2.1298        | 0.7   | 1400 | 2.2103          |
| 2.1454        | 0.8   | 1600 | 2.1990          |
| 2.1389        | 0.9   | 1800 | 2.1887          |


### Framework versions

- Transformers 4.35.0
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""sschangi/uplimit-project-3-llam2"", ""author"": ""sschangi"", ""sha"": ""c8037c2019f8500207d69b7311563a2992fc9df2"", ""last_modified"": ""2023-11-03 06:33:46+00:00"", ""created_at"": ""2023-11-03 06:33:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:scitldr"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- scitldr\ntags:\n- generated_from_trainer\nmodel-index:\n- name: uplimit-project-3-llam2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""uplimit-project-3-llam2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_04-42-52_5981b7b60f8e/events.out.tfevents.1698986654.5981b7b60f8e.7570.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-03 06:33:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- scitldr\ntags:\n- generated_from_trainer\nmodel-index:\n- name: uplimit-project-3-llam2\n  results: []"", ""transformersInfo"": null, ""_id"": ""654494466f7fb5b354885215"", ""modelId"": ""sschangi/uplimit-project-3-llam2"", ""usedStorage"": 33635712}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=sschangi/uplimit-project-3-llam2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsschangi%2Fuplimit-project-3-llam2%5D(%2Fsschangi%2Fuplimit-project-3-llam2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
karshPrime/biomed-llama2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: biomed-llama2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# biomed-llama2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on [Biomedical cpgQA](https://huggingface.co/datasets/chloecchng/biomedical_cpgQA) dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- training_steps: 100
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.36.0.dev0
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""karshPrime/biomed-llama2"", ""author"": ""karshPrime"", ""sha"": ""42ce42b2010825f0a644df6a6462cdb22464508b"", ""last_modified"": ""2023-11-03 16:28:47+00:00"", ""created_at"": ""2023-11-03 16:08:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: biomed-llama2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""biomed-llama2"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-08-25_2bcd424e95f7/events.out.tfevents.1699027709.2bcd424e95f7.528.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-08-38_2bcd424e95f7/events.out.tfevents.1699027719.2bcd424e95f7.528.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-08-52_2bcd424e95f7/events.out.tfevents.1699027733.2bcd424e95f7.528.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-03 16:28:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: biomed-llama2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65451afc9860643dba81c056"", ""modelId"": ""karshPrime/biomed-llama2"", ""usedStorage"": 33622873}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=karshPrime/biomed-llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BkarshPrime%2Fbiomed-llama2%5D(%2FkarshPrime%2Fbiomed-llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ayansk11/InLegalLlama2-7B-chat-hf,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: meta-llama-indian-constitution-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# meta-llama-indian-constitution-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.2
- Pytorch 2.0.1+cu117
- Datasets 2.14.5
- Tokenizers 0.13.3
",N/A,1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ayansk11/InLegalLlama2-7B-chat-hf&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAyansk11%2FInLegalLlama2-7B-chat-hf%5D(%2FAyansk11%2FInLegalLlama2-7B-chat-hf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
israelNwokedi/SEOExtractor-Llama-7b,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the self-created dataset. The task of the dataset was to extract the SEO warnings given an entire website HTML.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 200
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.0
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""israelNwokedi/SEOExtractor-Llama-7b"", ""author"": ""israelNwokedi"", ""sha"": ""f0a1c21cbce305e0fa5926836efd9d6b4a86b575"", ""last_modified"": ""2023-11-28 09:30:19+00:00"", ""created_at"": ""2023-11-07 09:46:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results_2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results_2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov07_09-22-45_d8464e1330bd/events.out.tfevents.1699349805.d8464e1330bd.168.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-28 09:30:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results_2\n  results: []"", ""transformersInfo"": null, ""_id"": ""654a07939c95576f43dfc07f"", ""modelId"": ""israelNwokedi/SEOExtractor-Llama-7b"", ""usedStorage"": 134744963}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=israelNwokedi/SEOExtractor-Llama-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BisraelNwokedi%2FSEOExtractor-Llama-7b%5D(%2FisraelNwokedi%2FSEOExtractor-Llama-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
furquan/llama2-sentiment-prompt-tuned,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2-sentiment-prompt-tuned
  results: []
datasets:
- mteb/tweet_sentiment_extraction
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2-sentiment-prompt-tuned

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

This model is Parameter Effecient Fine-tuned using Prompt Tuning. Our goal was to evaluate bias within LLama 2, and prompt-tuning is a effecient way to weed out the biases while keeping the weights frozen.

Classification Report of LLama 2 on original sentence:

              precision    recall  f1-score   support
    negative       1.00      1.00      1.00       576
     neutral       0.92      0.95      0.93       640
    positive       0.94      0.91      0.92       576

    accuracy                           0.95      1792
    macro avg      0.95      0.95      0.95      1792
    weighted avg   0.95      0.95      0.95      1792


Classification Report of LLama 2 on preturbed sentence:

              precision    recall  f1-score   support
    negative       0.93      0.74      0.82       576
     neutral       0.68      0.97      0.80       640
    positive       0.80      0.58      0.67       576

    accuracy                           0.77      1792
    macro avg      0.80      0.76      0.76      1792
    weighted avg   0.80      0.77      0.77      1792


## Intended uses & limitations

You can use this model for your own sentiment-analysis task. 
```
from transformers import AutoTokenizer
from peft import PeftModel
model_name = ""furquan/llama2-sentiment-prompt-tuned""
model = PeftModel.from_pretrained(
    model_name, 
    device_map = 'auto'
)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model.eval()

def get_pred(text):
    inputs = tokenizer(f""\n### Text: {text}\n### Sentiment:"", return_tensors=""pt"").to(model.device)
    outputs = model.generate(input_ids=inputs[""input_ids""].to(model.device), attention_mask=inputs[""attention_mask""], max_new_tokens=1,do_sample=False)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(' ')[-1]

prediction = get_pred(""The weather is lovely today."")
print(prediction)

>>positive
```

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2

### Training results



### Framework versions

- Transformers 4.36.0.dev0
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1","{""id"": ""furquan/llama2-sentiment-prompt-tuned"", ""author"": ""furquan"", ""sha"": ""105b8614d6e044b3f08ba6914b56ac67bf25b9a6"", ""last_modified"": ""2023-11-09 05:12:05+00:00"", ""created_at"": ""2023-11-08 09:16:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:mteb/tweet_sentiment_extraction"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mteb/tweet_sentiment_extraction\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2-sentiment-prompt-tuned\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2-sentiment-prompt-tuned"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_02-22-47_respai/events.out.tfevents.1699428367.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_14-34-31_respai/events.out.tfevents.1699472110.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_14-38-53_respai/events.out.tfevents.1699472366.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_16-37-21_respai/events.out.tfevents.1699479462.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-04-34_respai/events.out.tfevents.1699481153.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-08-25_respai/events.out.tfevents.1699481425.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-46-06_respai/events.out.tfevents.1699483587.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-50-35_respai/events.out.tfevents.1699483857.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-55-30_respai/events.out.tfevents.1699484169.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_17-59-11_respai/events.out.tfevents.1699484370.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_18-01-58_respai/events.out.tfevents.1699484535.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_18-04-11_respai/events.out.tfevents.1699484667.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_19-12-57_respai/events.out.tfevents.1699488807.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_19-14-19_respai/events.out.tfevents.1699488880.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_19-20-27_respai/events.out.tfevents.1699489251.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_19-23-37_respai/events.out.tfevents.1699489439.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_19-36-23_respai/events.out.tfevents.1699490203.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov08_20-49-09_respai/events.out.tfevents.1699494584.respai', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-09 05:12:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mteb/tweet_sentiment_extraction\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2-sentiment-prompt-tuned\n  results: []"", ""transformersInfo"": null, ""_id"": ""654b51d7d0cd646992fffa51"", ""modelId"": ""furquan/llama2-sentiment-prompt-tuned"", ""usedStorage"": 7773031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=furquan/llama2-sentiment-prompt-tuned&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bfurquan%2Fllama2-sentiment-prompt-tuned%5D(%2Ffurquan%2Fllama2-sentiment-prompt-tuned)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Fishball02/llama-topical-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-topical-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-topical-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 3.1782

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2.5e-06
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 3.3811        | 0.63  | 64   | 3.3129          |
| 3.2846        | 1.25  | 128  | 3.2202          |
| 3.2106        | 1.88  | 192  | 3.1782          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Fishball02/llama-topical-chat"", ""author"": ""Fishball02"", ""sha"": ""907d7acc4c5b632c367a77db309046eca11ff252"", ""last_modified"": ""2023-12-14 01:28:36+00:00"", ""created_at"": ""2023-11-10 00:34:25+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-topical-chat\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-topical-chat"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-02-33_4a4e17ae92dc/events.out.tfevents.1702504953.4a4e17ae92dc.148.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-06-53_4a4e17ae92dc/events.out.tfevents.1702505213.4a4e17ae92dc.148.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-07-15_4a4e17ae92dc/events.out.tfevents.1702505235.4a4e17ae92dc.148.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-10-48_4a4e17ae92dc/events.out.tfevents.1702505448.4a4e17ae92dc.148.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-14-25_4a4e17ae92dc/events.out.tfevents.1702505665.4a4e17ae92dc.3756.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-16-50_4a4e17ae92dc/events.out.tfevents.1702505811.4a4e17ae92dc.4720.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-26-42_4a4e17ae92dc/events.out.tfevents.1702506403.4a4e17ae92dc.7271.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-27-50_4a4e17ae92dc/events.out.tfevents.1702506470.4a4e17ae92dc.7271.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-36-06_4a4e17ae92dc/events.out.tfevents.1702506967.4a4e17ae92dc.7271.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_22-39-17_4a4e17ae92dc/events.out.tfevents.1702507157.4a4e17ae92dc.10366.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_23-07-07_4a4e17ae92dc/events.out.tfevents.1702508827.4a4e17ae92dc.17670.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_23-10-25_4a4e17ae92dc/events.out.tfevents.1702509026.4a4e17ae92dc.18662.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_23-13-59_4a4e17ae92dc/events.out.tfevents.1702509240.4a4e17ae92dc.19656.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec13_23-36-44_4a4e17ae92dc/events.out.tfevents.1702510604.4a4e17ae92dc.25346.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec14_00-30-36_4a4e17ae92dc/events.out.tfevents.1702513837.4a4e17ae92dc.38846.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec14_00-39-49_4a4e17ae92dc/events.out.tfevents.1702514390.4a4e17ae92dc.38846.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec14_00-48-29_4a4e17ae92dc/events.out.tfevents.1702514910.4a4e17ae92dc.43482.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_00-07-00_69332eb5d77e/events.out.tfevents.1699574820.69332eb5d77e.359.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_01-01-47_69332eb5d77e/events.out.tfevents.1699578107.69332eb5d77e.14876.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_01-03-01_69332eb5d77e/events.out.tfevents.1699578182.69332eb5d77e.14876.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_01-06-45_69332eb5d77e/events.out.tfevents.1699578406.69332eb5d77e.14876.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_01-08-23_69332eb5d77e/events.out.tfevents.1699578504.69332eb5d77e.14876.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_01-10-41_69332eb5d77e/events.out.tfevents.1699578642.69332eb5d77e.14876.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_04-51-14_92c80f169da6/events.out.tfevents.1699764676.92c80f169da6.7430.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_04-55-04_92c80f169da6/events.out.tfevents.1699764906.92c80f169da6.10033.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_05-00-08_92c80f169da6/events.out.tfevents.1699765213.92c80f169da6.10033.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_05-03-30_92c80f169da6/events.out.tfevents.1699765412.92c80f169da6.12352.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-10-55_c15b7a318767/events.out.tfevents.1699827056.c15b7a318767.224.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-32-37_c15b7a318767/events.out.tfevents.1699828358.c15b7a318767.224.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-38-02_c15b7a318767/events.out.tfevents.1699828682.c15b7a318767.224.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-44-38_c15b7a318767/events.out.tfevents.1699829079.c15b7a318767.224.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-47-16_c15b7a318767/events.out.tfevents.1699829237.c15b7a318767.14852.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_22-57-30_c15b7a318767/events.out.tfevents.1699829851.c15b7a318767.14852.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-04-18_c15b7a318767/events.out.tfevents.1699830259.c15b7a318767.19396.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-04-42_c15b7a318767/events.out.tfevents.1699830283.c15b7a318767.19396.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-07-27_c15b7a318767/events.out.tfevents.1699830447.c15b7a318767.20011.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-15-07_c15b7a318767/events.out.tfevents.1699830908.c15b7a318767.22219.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-23-58_c15b7a318767/events.out.tfevents.1699831438.c15b7a318767.24602.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-32-57_c15b7a318767/events.out.tfevents.1699831978.c15b7a318767.26978.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-41-19_c15b7a318767/events.out.tfevents.1699832482.c15b7a318767.26978.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov12_23-44-29_c15b7a318767/events.out.tfevents.1699832671.c15b7a318767.29892.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-14 01:28:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-topical-chat\n  results: []"", ""transformersInfo"": null, ""_id"": ""654d7a91a312584f5de2f012"", ""modelId"": ""Fishball02/llama-topical-chat"", ""usedStorage"": 805648310}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Fishball02/llama-topical-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFishball02%2Fllama-topical-chat%5D(%2FFishball02%2Fllama-topical-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PiyushLavaniya/LLama2_Banker_LoRA_Adapters,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Trainer_output
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Trainer_output

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 1200
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.0
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""PiyushLavaniya/LLama2_Banker_LoRA_Adapters"", ""author"": ""PiyushLavaniya"", ""sha"": ""3894db85972b8954251a3e7189261678101a3ee2"", ""last_modified"": ""2023-11-10 06:04:00+00:00"", ""created_at"": ""2023-11-10 06:03:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Trainer_output\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Trainer_output"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov10_05-25-06_dc46c903df8d/events.out.tfevents.1699593926.dc46c903df8d.296.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-10 06:04:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Trainer_output\n  results: []"", ""transformersInfo"": null, ""_id"": ""654dc7cc1e6216cb2d9c6368"", ""modelId"": ""PiyushLavaniya/LLama2_Banker_LoRA_Adapters"", ""usedStorage"": 33607499}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PiyushLavaniya/LLama2_Banker_LoRA_Adapters&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPiyushLavaniya%2FLLama2_Banker_LoRA_Adapters%5D(%2FPiyushLavaniya%2FLLama2_Banker_LoRA_Adapters)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_EOS

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9934

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.2432        | 0.2   | 13   | 2.2215          |
| 2.1909        | 0.4   | 26   | 2.0960          |
| 2.2498        | 0.6   | 39   | 2.0220          |
| 2.2679        | 0.8   | 52   | 1.9934          |


### Framework versions

- Transformers 4.35.1
- Pytorch 2.1.0+cu118
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS"", ""author"": ""Jukaboo"", ""sha"": ""8e3d42e544d1647d4f6b31178ee3087c7e9287fe"", ""last_modified"": ""2023-11-14 17:40:53+00:00"", ""created_at"": ""2023-11-14 11:01:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_EOS"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov14_17-04-55_a47466607916/events.out.tfevents.1699981600.a47466607916.229.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov14_17-13-12_a47466607916/events.out.tfevents.1699982097.a47466607916.4025.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov14_17-19-44_a47466607916/events.out.tfevents.1699982454.a47466607916.4025.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov14_17-23-36_a47466607916/events.out.tfevents.1699982622.a47466607916.4025.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-14 17:40:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS\n  results: []"", ""transformersInfo"": null, ""_id"": ""6553538931a15b163538fad8"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS"", ""usedStorage"": 17330119}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_EOS_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.8878

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.9862        | 0.2   | 33   | 2.0436          |
| 1.9345        | 0.41  | 66   | 1.9444          |
| 2.1393        | 0.61  | 99   | 1.9075          |
| 2.1656        | 0.82  | 132  | 1.8878          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_2"", ""author"": ""Jukaboo"", ""sha"": ""739f1585b9bfc242602bf1115bc99b976f75b868"", ""last_modified"": ""2023-11-17 09:06:01+00:00"", ""created_at"": ""2023-11-17 08:31:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_EOS_2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_08-31-32_e32dade7553c/events.out.tfevents.1700209902.e32dade7553c.667.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-17 09:06:01+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_2\n  results: []"", ""transformersInfo"": null, ""_id"": ""655724e54ef6627a23bbed46"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_2"", ""usedStorage"": 50958337}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_2%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_EOS_3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.8142

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.2757        | 0.2   | 65   | 1.9499          |
| 1.8931        | 0.4   | 130  | 1.8631          |
| 1.6246        | 0.6   | 195  | 1.8294          |
| 2.2049        | 0.8   | 260  | 1.8142          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_3"", ""author"": ""Jukaboo"", ""sha"": ""527224893c127294b49285b60cf1db176c6ad061"", ""last_modified"": ""2023-11-17 10:23:02+00:00"", ""created_at"": ""2023-11-17 09:09:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_3\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_EOS_3"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_08-31-32_e32dade7553c/events.out.tfevents.1700209902.e32dade7553c.667.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_09-07-56_e32dade7553c/events.out.tfevents.1700212164.e32dade7553c.15280.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-17 10:23:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_3\n  results: []"", ""transformersInfo"": null, ""_id"": ""65572dbe6539c9a5a545dc2c"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_3"", ""usedStorage"": 118331659}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_3%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9532

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.892         | 0.4   | 130  | 2.0355          |
| 2.1983        | 0.8   | 260  | 1.9918          |
| 2.0812        | 1.21  | 390  | 1.9669          |
| 1.3235        | 1.61  | 520  | 1.9532          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2"", ""author"": ""Jukaboo"", ""sha"": ""9060a334658bc84c3c046525caec04f49a0aa6aa"", ""last_modified"": ""2023-11-17 13:09:19+00:00"", ""created_at"": ""2023-11-17 10:34:06+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_08-31-32_e32dade7553c/events.out.tfevents.1700209902.e32dade7553c.667.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_09-07-56_e32dade7553c/events.out.tfevents.1700212164.e32dade7553c.15280.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov17_10-32-44_e32dade7553c/events.out.tfevents.1700217262.e32dade7553c.15280.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-17 13:09:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2\n  results: []"", ""transformersInfo"": null, ""_id"": ""6557419e55c514dc59348f36"", ""modelId"": ""Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2"", ""usedStorage"": 236438010}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_meetingBank_ft_adapters_EOS_EP2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_EP2%5D(%2FJukaboo%2FLlama2_7B_chat_meetingBank_ft_adapters_EOS_EP2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PiyushLavaniya/Llama2_Summarizer_LoRA_Adapters,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama_summarizer_adapter
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_summarizer_adapter

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- training_steps: 109
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""PiyushLavaniya/Llama2_Summarizer_LoRA_Adapters"", ""author"": ""PiyushLavaniya"", ""sha"": ""d122128bcd4ee47ac39e16fc1e9b86dad3b55c9f"", ""last_modified"": ""2023-11-19 09:36:39+00:00"", ""created_at"": ""2023-11-19 09:36:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama_summarizer_adapter\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama_summarizer_adapter"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_07-00-03_6fee3cf80d93/events.out.tfevents.1700377445.6fee3cf80d93.1340.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_07-15-41_6fee3cf80d93/events.out.tfevents.1700378153.6fee3cf80d93.19258.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_07-28-22_6fee3cf80d93/events.out.tfevents.1700378922.6fee3cf80d93.19258.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_07-31-59_6fee3cf80d93/events.out.tfevents.1700379133.6fee3cf80d93.19258.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_07-34-23_6fee3cf80d93/events.out.tfevents.1700379278.6fee3cf80d93.19258.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov19_08-04-03_6fee3cf80d93/events.out.tfevents.1700381053.6fee3cf80d93.19258.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-19 09:36:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama_summarizer_adapter\n  results: []"", ""transformersInfo"": null, ""_id"": ""6559d71f6412aaeed663f096"", ""modelId"": ""PiyushLavaniya/Llama2_Summarizer_LoRA_Adapters"", ""usedStorage"": 67178874}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PiyushLavaniya/Llama2_Summarizer_LoRA_Adapters&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPiyushLavaniya%2FLlama2_Summarizer_LoRA_Adapters%5D(%2FPiyushLavaniya%2FLlama2_Summarizer_LoRA_Adapters)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/bineric/NorskGPT-Llama-7B-v0.1,N/A,N/A,1,,0,,0,,0,,0,,0
SebastianS/llama-7-chat-instruction-int4-fc-pipeline,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-pipeline
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-pipeline

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-pipeline"", ""author"": ""SebastianS"", ""sha"": ""4deec487c50e4fba0a6566aa7d90ec11b3ece04a"", ""last_modified"": ""2023-11-22 03:18:33+00:00"", ""created_at"": ""2023-11-21 20:21:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-pipeline\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-pipeline"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-22 03:18:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-pipeline\n  results: []"", ""transformersInfo"": null, ""_id"": ""655d1160c38270696bf696b8"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-pipeline"", ""usedStorage"": 1208894631}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-pipeline&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-pipeline%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-pipeline)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-sft,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-sft
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-sft

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft"", ""author"": ""SebastianS"", ""sha"": ""8e5da5677d68df29ab59888d3c4c7f6103ce9a8e"", ""last_modified"": ""2023-11-23 19:42:28+00:00"", ""created_at"": ""2023-11-23 18:50:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-sft"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-23 19:42:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft\n  results: []"", ""transformersInfo"": null, ""_id"": ""655f9efba7c531e2821b7af9"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft"", ""usedStorage"": 537563061}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-sft%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-sft_fix,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-sft_fix
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-sft_fix

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft_fix"", ""author"": ""SebastianS"", ""sha"": ""bdf6f0c43cb2ebaf2f252fa36ac1ed968d66dd04"", ""last_modified"": ""2023-11-23 21:05:39+00:00"", ""created_at"": ""2023-11-23 20:20:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft_fix\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-sft_fix"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-23 21:05:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft_fix\n  results: []"", ""transformersInfo"": null, ""_id"": ""655fb40bd69284e31fd8e256"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft_fix"", ""usedStorage"": 403295149}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-sft_fix&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-sft_fix%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-sft_fix)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-sft_fix-dpo,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-sft_fix-dpo
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-sft_fix-dpo

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft_fix-dpo"", ""author"": ""SebastianS"", ""sha"": ""4581365964019d4f8deb567084549774f29a031b"", ""last_modified"": ""2023-11-24 04:29:11+00:00"", ""created_at"": ""2023-11-24 04:08:53+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft_fix-dpo\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-sft_fix-dpo"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-24 04:29:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-sft_fix-dpo\n  results: []"", ""transformersInfo"": null, ""_id"": ""656021d547d95544ef94e79d"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-sft_fix-dpo"", ""usedStorage"": 269035808}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-sft_fix-dpo&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-sft_fix-dpo%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-sft_fix-dpo)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-dpo,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-dpo
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-dpo

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo"", ""author"": ""SebastianS"", ""sha"": ""0aec496d6f880da0d031e4af1b7c15000e9b6f0d"", ""last_modified"": ""2023-11-25 03:15:39+00:00"", ""created_at"": ""2023-11-25 02:51:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-dpo"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-25 03:15:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo\n  results: []"", ""transformersInfo"": null, ""_id"": ""6561613e2d309fa7e266f7dc"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo"", ""usedStorage"": 403295149}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-dpo&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_DE,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- germanquad
model-index:
- name: Llama2_7B_chat_DE
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_DE

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the germanquad dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_DE"", ""author"": ""Jukaboo"", ""sha"": ""3864fb70dbcd507298f5892caa29095cc21a0b43"", ""last_modified"": ""2023-11-27 08:31:31+00:00"", ""created_at"": ""2023-11-27 08:13:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:germanquad"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_DE"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_08-14-11_4021d08e3708/events.out.tfevents.1701072859.4021d08e3708.446.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_08-16-48_4021d08e3708/events.out.tfevents.1701073012.4021d08e3708.446.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-27 08:31:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE\n  results: []"", ""transformersInfo"": null, ""_id"": ""65644f9b7bdd1b1612e26c77"", ""modelId"": ""Jukaboo/Llama2_7B_chat_DE"", ""usedStorage"": 50962092}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_DE&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_DE%5D(%2FJukaboo%2FLlama2_7B_chat_DE)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_DE_2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- germanquad
model-index:
- name: Llama2_7B_chat_DE_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_DE_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the germanquad dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_DE_2"", ""author"": ""Jukaboo"", ""sha"": ""a4faa6ea7821223bde133614db1342a0f8a617c9"", ""last_modified"": ""2023-11-27 09:48:29+00:00"", ""created_at"": ""2023-11-27 09:26:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:germanquad"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_DE_2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_09-24-24_f4bdcacaf22d/events.out.tfevents.1701077077.f4bdcacaf22d.1574.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_09-26-14_f4bdcacaf22d/events.out.tfevents.1701077179.f4bdcacaf22d.1574.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_09-29-23_f4bdcacaf22d/events.out.tfevents.1701077367.f4bdcacaf22d.1574.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-27 09:48:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_2\n  results: []"", ""transformersInfo"": null, ""_id"": ""656460b6d91bea2b55ce94e0"", ""modelId"": ""Jukaboo/Llama2_7B_chat_DE_2"", ""usedStorage"": 50966119}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_DE_2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_DE_2%5D(%2FJukaboo%2FLlama2_7B_chat_DE_2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_DE_3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- germanquad
model-index:
- name: Llama2_7B_chat_DE_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_DE_3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the germanquad dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_DE_3"", ""author"": ""Jukaboo"", ""sha"": ""93455cfe4b8cb277337a46a28f0dcd91afe708f5"", ""last_modified"": ""2023-11-27 11:12:10+00:00"", ""created_at"": ""2023-11-27 10:00:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:germanquad"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_3\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_DE_3"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_10-06-03_32e96ea238ca/events.out.tfevents.1701079587.32e96ea238ca.1881.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-27 11:12:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_3\n  results: []"", ""transformersInfo"": null, ""_id"": ""656468cdeb5aaccaac8b906f"", ""modelId"": ""Jukaboo/Llama2_7B_chat_DE_3"", ""usedStorage"": 101505200}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_DE_3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_DE_3%5D(%2FJukaboo%2FLlama2_7B_chat_DE_3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_DE_4,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- germanquad
model-index:
- name: Llama2_7B_chat_DE_4
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_DE_4

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the germanquad dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_DE_4"", ""author"": ""Jukaboo"", ""sha"": ""6da0b1546659511d02ff87c95ac0bed9910754e0"", ""last_modified"": ""2023-11-27 12:17:16+00:00"", ""created_at"": ""2023-11-27 11:26:06+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:germanquad"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_4\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_DE_4"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov27_11-25-52_b2634da64e32/events.out.tfevents.1701084372.b2634da64e32.2201.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-27 12:17:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- germanquad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_DE_4\n  results: []"", ""transformersInfo"": null, ""_id"": ""65647cce46c331dd68ff7fbf"", ""modelId"": ""Jukaboo/Llama2_7B_chat_DE_4"", ""usedStorage"": 202872917}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_DE_4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_DE_4%5D(%2FJukaboo%2FLlama2_7B_chat_DE_4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
codewizardUV/llama_supervised_fine-tuning-15epochs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama_supervised_fine-tuning
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_supervised_fine-tuning

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 15

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""codewizardUV/llama_supervised_fine-tuning-15epochs"", ""author"": ""codewizardUV"", ""sha"": ""735211ed0e723defbe5783d92a90baca16b6d967"", ""last_modified"": ""2023-11-28 11:34:47+00:00"", ""created_at"": ""2023-11-28 11:34:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama_supervised_fine-tuning\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama_supervised_fine-tuning"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov28_09-17-02_a7acf2a52f42/events.out.tfevents.1701163028.a7acf2a52f42.728.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-28 11:34:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama_supervised_fine-tuning\n  results: []"", ""transformersInfo"": null, ""_id"": ""6565d042e09d6576688114d9"", ""modelId"": ""codewizardUV/llama_supervised_fine-tuning-15epochs"", ""usedStorage"": 16811966}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=codewizardUV/llama_supervised_fine-tuning-15epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BcodewizardUV%2Fllama_supervised_fine-tuning-15epochs%5D(%2FcodewizardUV%2Fllama_supervised_fine-tuning-15epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-dpo-_1_beta,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-dpo-_1_beta
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-dpo-_1_beta

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_1_beta"", ""author"": ""SebastianS"", ""sha"": ""64623970db25d9286490cb85b854d20974a742e9"", ""last_modified"": ""2023-11-28 18:08:31+00:00"", ""created_at"": ""2023-11-28 17:30:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_1_beta\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-dpo-_1_beta"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-28 18:08:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_1_beta\n  results: []"", ""transformersInfo"": null, ""_id"": ""656623b3d4c272e7a9fc7707"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_1_beta"", ""usedStorage"": 403295213}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-dpo-_1_beta&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_1_beta%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_1_beta)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-dpo-_9_beta,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-dpo-_9_beta
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-dpo-_9_beta

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_9_beta"", ""author"": ""SebastianS"", ""sha"": ""9143a7c6e7832543d0e37bc362d73e909b667f5f"", ""last_modified"": ""2023-11-28 19:00:23+00:00"", ""created_at"": ""2023-11-28 18:27:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_9_beta\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-dpo-_9_beta"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-28 19:00:23+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_9_beta\n  results: []"", ""transformersInfo"": null, ""_id"": ""656630f7e0977cf44f63c414"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_9_beta"", ""usedStorage"": 403295213}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-dpo-_9_beta&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_9_beta%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_9_beta)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-dpo-_5_beta,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-dpo-_5_beta
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-dpo-_5_beta

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_5_beta"", ""author"": ""SebastianS"", ""sha"": ""3ac8c155c909e0d7103c10e8a199ee1311365ced"", ""last_modified"": ""2023-11-28 19:56:48+00:00"", ""created_at"": ""2023-11-28 19:22:12+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_5_beta\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-dpo-_5_beta"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-28 19:56:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-dpo-_5_beta\n  results: []"", ""transformersInfo"": null, ""_id"": ""65663de4e1604b20586bbecb"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-dpo-_5_beta"", ""usedStorage"": 403295213}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-dpo-_5_beta&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_5_beta%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-dpo-_5_beta)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SanaFalakJ/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""SanaFalakJ/results"", ""author"": ""SanaFalakJ"", ""sha"": ""f1061ef3163ef01eee62e5f87d2900fe9f68ed14"", ""last_modified"": ""2023-11-29 06:35:04+00:00"", ""created_at"": ""2023-11-29 06:10:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov29_05-34-06_e242c5040634/events.out.tfevents.1701236047.e242c5040634.517.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-29 06:35:04+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""6566d5da77d8a948ac8cb5bd"", ""modelId"": ""SanaFalakJ/results"", ""usedStorage"": 134244532}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SanaFalakJ/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSanaFalakJ%2Fresults%5D(%2FSanaFalakJ%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Yaxin1992/llama2-7b-chat-leagues-5000,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-chat-leagues-5000
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-chat-leagues-5000

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-06
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 5000
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.36.0.dev0
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Yaxin1992/llama2-7b-chat-leagues-5000"", ""author"": ""Yaxin1992"", ""sha"": ""2c0a7e5efa82f9d9ccd68d5dacd9ce8fa694c18e"", ""last_modified"": ""2023-11-29 21:25:38+00:00"", ""created_at"": ""2023-11-29 19:03:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-leagues-5000\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-chat-leagues-5000"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov29_19-03-12_c680f2e75b32/events.out.tfevents.1701284620.c680f2e75b32.1019.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-29 21:25:38+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-leagues-5000\n  results: []"", ""transformersInfo"": null, ""_id"": ""65678b066fcc82e5e8cb6267"", ""modelId"": ""Yaxin1992/llama2-7b-chat-leagues-5000"", ""usedStorage"": 16882694}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Yaxin1992/llama2-7b-chat-leagues-5000&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BYaxin1992%2Fllama2-7b-chat-leagues-5000%5D(%2FYaxin1992%2Fllama2-7b-chat-leagues-5000)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
xiangliu1123/aidamodel,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: aidamodel
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# aidamodel

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""xiangliu1123/aidamodel"", ""author"": ""xiangliu1123"", ""sha"": ""99e33b68fa909b77bcb5a7c37895547f37e0f654"", ""last_modified"": ""2023-11-30 14:30:05+00:00"", ""created_at"": ""2023-11-30 06:09:04+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: aidamodel\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""aidamodel"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-30 14:30:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: aidamodel\n  results: []"", ""transformersInfo"": null, ""_id"": ""65682700fc8724d0724c6faa"", ""modelId"": ""xiangliu1123/aidamodel"", ""usedStorage"": 42562256229}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=xiangliu1123/aidamodel&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bxiangliu1123%2Faidamodel%5D(%2Fxiangliu1123%2Faidamodel)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
codewizardUV/old_model,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 5

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""codewizardUV/old_model"", ""author"": ""codewizardUV"", ""sha"": ""1e8a18c645665f73a0dd8933f08ecdc499b6abee"", ""last_modified"": ""2023-11-30 10:15:11+00:00"", ""created_at"": ""2023-11-30 10:15:06+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov30_09-23-19_10a76eccf3aa/events.out.tfevents.1701336227.10a76eccf3aa.754.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-30 10:15:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""656860aa12be8e9e9cb20283"", ""modelId"": ""codewizardUV/old_model"", ""usedStorage"": 16806029}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=codewizardUV/old_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BcodewizardUV%2Fold_model%5D(%2FcodewizardUV%2Fold_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
xiangliu1123/openassi,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: openassi
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# openassi

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1.41e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""xiangliu1123/openassi"", ""author"": ""xiangliu1123"", ""sha"": ""e067d38212708ad976818105a84d18c0dc2b1302"", ""last_modified"": ""2023-11-30 18:15:08+00:00"", ""created_at"": ""2023-11-30 17:10:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: openassi\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""openassi"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-11-30 18:15:08+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: openassi\n  results: []"", ""transformersInfo"": null, ""_id"": ""6568c20a5b408c0beb227a84"", ""modelId"": ""xiangliu1123/openassi"", ""usedStorage"": 4968279669}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=xiangliu1123/openassi&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bxiangliu1123%2Fopenassi%5D(%2Fxiangliu1123%2Fopenassi)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_LR,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_LR
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_LR

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 2.2693

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 6
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 6
- total_train_batch_size: 36
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.5075        | 0.21  | 6    | 2.4168          |
| 2.29          | 0.42  | 12   | 2.3278          |
| 2.2582        | 0.62  | 18   | 2.2871          |
| 2.3052        | 0.83  | 24   | 2.2693          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_LR"", ""author"": ""Jukaboo"", ""sha"": ""53333635e1eaa5996f17252115d77e8ad86cf41b"", ""last_modified"": ""2023-12-12 16:01:18+00:00"", ""created_at"": ""2023-12-01 12:08:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_LR\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_LR"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec01_12-20-01_7f9337b2413a/events.out.tfevents.1701433233.7f9337b2413a.183.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec01_12-23-28_7f9337b2413a/events.out.tfevents.1701433429.7f9337b2413a.183.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec01_12-59-24_7f9337b2413a/events.out.tfevents.1701435570.7f9337b2413a.183.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec01_13-48-13_7f9337b2413a/events.out.tfevents.1701438543.7f9337b2413a.22338.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_11-34-42_70f0df5f5592/events.out.tfevents.1701776158.70f0df5f5592.840.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_12-20-44_70f0df5f5592/events.out.tfevents.1701778884.70f0df5f5592.840.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_13-15-25_70f0df5f5592/events.out.tfevents.1701782148.70f0df5f5592.27897.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_13-52-11_70f0df5f5592/events.out.tfevents.1701784340.70f0df5f5592.27897.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_14-16-49_70f0df5f5592/events.out.tfevents.1701785837.70f0df5f5592.27897.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_14-18-48_70f0df5f5592/events.out.tfevents.1701785935.70f0df5f5592.27897.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_14-38-42_70f0df5f5592/events.out.tfevents.1701787127.70f0df5f5592.27897.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec08_09-11-47_963b4157fd15/events.out.tfevents.1702026740.963b4157fd15.4148.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec08_09-50-00_963b4157fd15/events.out.tfevents.1702029024.963b4157fd15.4148.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec08_10-27-00_963b4157fd15/events.out.tfevents.1702031228.963b4157fd15.4148.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec08_11-03-43_963b4157fd15/events.out.tfevents.1702033433.963b4157fd15.4148.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec08_11-39-07_963b4157fd15/events.out.tfevents.1702035552.963b4157fd15.4148.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec11_11-43-05_247912ab193a/events.out.tfevents.1702295019.247912ab193a.305.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec11_11-46-54_247912ab193a/events.out.tfevents.1702295237.247912ab193a.305.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_11-24-10_d48b796149c0/events.out.tfevents.1702380286.d48b796149c0.442.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_11-33-27_d48b796149c0/events.out.tfevents.1702380829.d48b796149c0.442.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_11-42-12_d48b796149c0/events.out.tfevents.1702381356.d48b796149c0.442.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_11-50-18_d48b796149c0/events.out.tfevents.1702381841.d48b796149c0.442.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_12-21-02_d48b796149c0/events.out.tfevents.1702383681.d48b796149c0.442.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_12-44-23_d48b796149c0/events.out.tfevents.1702385077.d48b796149c0.442.5', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_14-37-08_36ef18fa7b69/events.out.tfevents.1702391904.36ef18fa7b69.181.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_14-39-21_36ef18fa7b69/events.out.tfevents.1702391973.36ef18fa7b69.181.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec12_15-37-25_36ef18fa7b69/events.out.tfevents.1702395459.36ef18fa7b69.181.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-12 16:01:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_LR\n  results: []"", ""transformersInfo"": null, ""_id"": ""6569cccb2f7ea4b5ac37dc47"", ""modelId"": ""Jukaboo/Llama2_7B_chat_LR"", ""usedStorage"": 403991089}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_LR&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_LR%5D(%2FJukaboo%2FLlama2_7B_chat_LR)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
W3bsurf/Llawma-sum-2-7b-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llawma-sum-2-7b-chat
  results: []
datasets:
- dreamproit/bill_summary_us
language:
- en
---

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the dreamproit/bill_summary_us dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7163

## Model description

Model has been fine-tuned from llama 2 7B chat model for legal summarization tasks.

## Intended uses & limitations

The model has been fine-tuned with legal summarization text for summarization tasks.
Can produce repeating text when creating longer outputs.
Tested only with english and the bill_summary_us dataset.

## Training procedure
SFTTrainer from Hugging Face's TRL library used for fine-tuning process.

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.8263        | 0.24  | 70   | 0.7693          |
| 0.6035        | 0.48  | 140  | 0.7467          |
| 0.845         | 0.72  | 210  | 0.7347          |
| 0.5782        | 0.96  | 280  | 0.7163          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0

### license
Llama 2 is licensed under the LLAMA 2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.","{""id"": ""W3bsurf/Llawma-sum-2-7b-chat"", ""author"": ""W3bsurf"", ""sha"": ""07f027cf42aee4c0b6b5038b5b315d15cd08f305"", ""last_modified"": ""2023-12-11 07:34:27+00:00"", ""created_at"": ""2023-12-02 15:28:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""en"", ""dataset:dreamproit/bill_summary_us"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- dreamproit/bill_summary_us\nlanguage:\n- en\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llawma-sum-2-7b-chat\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llawma-sum-2-7b-chat"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Notice.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec02_13-24-56_b81ac8a0ca2b/events.out.tfevents.1701523506.b81ac8a0ca2b.285.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 07:34:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- dreamproit/bill_summary_us\nlanguage:\n- en\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llawma-sum-2-7b-chat\n  results: []"", ""transformersInfo"": null, ""_id"": ""656b4d0727cb1927cafff201"", ""modelId"": ""W3bsurf/Llawma-sum-2-7b-chat"", ""usedStorage"": 648947781}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=W3bsurf/Llawma-sum-2-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BW3bsurf%2FLlawma-sum-2-7b-chat%5D(%2FW3bsurf%2FLlawma-sum-2-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
TusharsinghBaghel/outputs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- billsum
model-index:
- name: outputs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# outputs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the billsum dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 5
- training_steps: 15
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.36.0.dev0
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""TusharsinghBaghel/outputs"", ""author"": ""TusharsinghBaghel"", ""sha"": ""7cd2df55e3ae4d21d72fb99d121b24defb52a45c"", ""last_modified"": ""2023-12-03 13:23:27+00:00"", ""created_at"": ""2023-12-03 13:23:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""dataset:billsum"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- billsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""outputs"", ""results"": []}], ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec03_10-36-22_3a9c4568a2e4/events.out.tfevents.1701599789.3a9c4568a2e4.1481.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec03_10-37-19_3a9c4568a2e4/events.out.tfevents.1701599843.3a9c4568a2e4.1481.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec03_11-31-32_3a9c4568a2e4/events.out.tfevents.1701603093.3a9c4568a2e4.1481.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-03 13:23:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- billsum\ntags:\n- generated_from_trainer\nmodel-index:\n- name: outputs\n  results: []"", ""transformersInfo"": null, ""_id"": ""656c814b0bbc114fe619d33a"", ""modelId"": ""TusharsinghBaghel/outputs"", ""usedStorage"": 33593100}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=TusharsinghBaghel/outputs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTusharsinghBaghel%2Foutputs%5D(%2FTusharsinghBaghel%2Foutputs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
rajatvdoit/llama2taylor1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2taylor1
  results: []
pipeline_tag: text-generation
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2taylor1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 20

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.13.3","{""id"": ""rajatvdoit/llama2taylor1"", ""author"": ""rajatvdoit"", ""sha"": ""dede0fd00b0ad88b1dc946b9cad10dd5d6414e70"", ""last_modified"": ""2023-12-05 05:34:54+00:00"", ""created_at"": ""2023-12-04 12:06:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""text-generation"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2taylor1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2taylor1"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec04_12-06-11_721bfda1eb23/events.out.tfevents.1701691621.721bfda1eb23.269.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec04_12-06-11_721bfda1eb23/events.out.tfevents.1701691784.721bfda1eb23.269.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec04_12-06-11_721bfda1eb23/events.out.tfevents.1701691832.721bfda1eb23.269.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec04_12-06-11_721bfda1eb23/events.out.tfevents.1701691896.721bfda1eb23.269.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-05 05:34:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2taylor1\n  results: []"", ""transformersInfo"": null, ""_id"": ""656dc0b93eb5f0b6a965a51c"", ""modelId"": ""rajatvdoit/llama2taylor1"", ""usedStorage"": 134291454}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=rajatvdoit/llama2taylor1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brajatvdoit%2Fllama2taylor1%5D(%2Frajatvdoit%2Fllama2taylor1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft_test,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-op_glaive-sft_test
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-op_glaive-sft_test

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 6
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft_test"", ""author"": ""SebastianS"", ""sha"": ""c2c4ab64262d51b0095d70898602e114e8e05f3b"", ""last_modified"": ""2023-12-04 20:43:13+00:00"", ""created_at"": ""2023-12-04 20:30:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-op_glaive-sft_test\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-op_glaive-sft_test"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-04 20:43:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-op_glaive-sft_test\n  results: []"", ""transformersInfo"": null, ""_id"": ""656e36f34893a2a26d603a8e"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft_test"", ""usedStorage"": 537563189}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft_test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-op_glaive-sft_test%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-op_glaive-sft_test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-7-chat-instruction-int4-fc-op_glaive-sft
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7-chat-instruction-int4-fc-op_glaive-sft

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 6
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.34.0
- Pytorch 2.0.1
- Datasets 2.13.0
- Tokenizers 0.14.1
","{""id"": ""SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft"", ""author"": ""SebastianS"", ""sha"": ""a1a7ca871c730d0bee71c6e7016662da3eb81b27"", ""last_modified"": ""2023-12-05 03:43:26+00:00"", ""created_at"": ""2023-12-04 20:58:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-op_glaive-sft\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7-chat-instruction-int4-fc-op_glaive-sft"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-05 03:43:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-7-chat-instruction-int4-fc-op_glaive-sft\n  results: []"", ""transformersInfo"": null, ""_id"": ""656e3d75299c8a5b9b032163"", ""modelId"": ""SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft"", ""usedStorage"": 403295213}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SebastianS/llama-7-chat-instruction-int4-fc-op_glaive-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSebastianS%2Fllama-7-chat-instruction-int4-fc-op_glaive-sft%5D(%2FSebastianS%2Fllama-7-chat-instruction-int4-fc-op_glaive-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
rajatvdoit/llama2taylor3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2taylor3
  results: []
pipeline_tag: text-generation
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2taylor3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 10

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.13.3","{""id"": ""rajatvdoit/llama2taylor3"", ""author"": ""rajatvdoit"", ""sha"": ""d317a24b00c6db2c63cebce0a587b3401582d832"", ""last_modified"": ""2023-12-05 07:13:38+00:00"", ""created_at"": ""2023-12-05 06:52:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""text-generation"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2taylor3\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2taylor3"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec05_06-52-39_ecc1733b43ee/events.out.tfevents.1701759200.ecc1733b43ee.512.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-05 07:13:38+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\npipeline_tag: text-generation\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2taylor3\n  results: []"", ""transformersInfo"": null, ""_id"": ""656ec8bc709a7c73d27b08d1"", ""modelId"": ""rajatvdoit/llama2taylor3"", ""usedStorage"": 134280788}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=rajatvdoit/llama2taylor3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brajatvdoit%2Fllama2taylor3%5D(%2Frajatvdoit%2Fllama2taylor3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Lohit20/Depressed_Llama-2-7b,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: CounselLlama7B
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# CounselLlama7B

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2709

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.199         | 1.0   | 250  | 1.2709          |


### Framework versions

- Transformers 4.35.0
- Pytorch 2.0.0
- Datasets 2.1.0
- Tokenizers 0.14.1
","{""id"": ""Lohit20/Depressed_Llama-2-7b"", ""author"": ""Lohit20"", ""sha"": ""b128eee5b08dedc5191d34a9b5a4213ee6b1bb07"", ""last_modified"": ""2023-12-05 22:23:11+00:00"", ""created_at"": ""2023-12-05 22:07:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7B\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""CounselLlama7B"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-05 22:23:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7B\n  results: []"", ""transformersInfo"": null, ""_id"": ""656f9f2c7a709fa0da078be7"", ""modelId"": ""Lohit20/Depressed_Llama-2-7b"", ""usedStorage"": 269002619}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Lohit20/Depressed_Llama-2-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BLohit20%2FDepressed_Llama-2-7b%5D(%2FLohit20%2FDepressed_Llama-2-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
ehekaanldk/lora-llama-2-7b-nsmc-understanding,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-nsmc-understanding
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-llama-2-7b-nsmc-understanding

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 2000
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0




### test accuracy
llama-2
- Confusion Matrix:
||Predicted 0|Predicted 1|
|:---|---:|---:|
|Actual 0|450|42|
|Actual 1|56|452|
**Accuracy: 0.902**

kt-ai-midm
- Confusion Matrix:
||Predicted 0|Predicted 1|
|:---|---:|---:|
|Actual 0|443|49|
|Actual 1|46|462|
**Accuracy: 0.905**

### 수정부분
- 데이터로딩
  - prepare_sample_text() : 시스템 메시지 변경 및 프롬프트 포멧 설정
  - create_datasets() : train 데이터 상위 2000개 선택
- 미세튜닝용 모델 로딩
  - script_args : 사용 데이터명 nsmc 설정 및 모델명 meta-llama/Llama-2-7b-chat-hf 설정
  - max_steps : 최대 훈련 단계 2000 설정
  - save : 체크포인트 세이브를 위한 파라미터 지정
- 허깅페이스 push_to_hub 로 push
- 추론테스트
  - 프롬프트 템플릿 수정 및 시스템 메시지 변경
  - valid_dataset : test 데이터 상위 1000개 선택
- 미세튜닝된 모델 로딩 후 테스트
  - eval_dic : valid_dataset 학습한 결과 출력
- 정확도
  - valid_dataset 과 모델 훈련 결과 true_labels 를 이용한 정확도 분석
","{""id"": ""ehekaanldk/lora-llama-2-7b-nsmc-understanding"", ""author"": ""ehekaanldk"", ""sha"": ""8cb296aada4882e9314e233b88705ff222ebe53b"", ""last_modified"": ""2023-12-16 14:58:16+00:00"", ""created_at"": ""2023-12-06 03:52:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-understanding\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-nsmc-understanding"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-16 14:58:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-understanding\n  results: []"", ""transformersInfo"": null, ""_id"": ""656ff0146066ea8e25e18732"", ""modelId"": ""ehekaanldk/lora-llama-2-7b-nsmc-understanding"", ""usedStorage"": 160535291}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=ehekaanldk/lora-llama-2-7b-nsmc-understanding&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Behekaanldk%2Flora-llama-2-7b-nsmc-understanding%5D(%2Fehekaanldk%2Flora-llama-2-7b-nsmc-understanding)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
chaem/llama-2-7b-nsmc,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-nsmc
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-nsmc

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 2000
- mixed_precision_training: Native AMP

### Training results

![image/png](https://cdn-uploads.huggingface.co/production/uploads/652384150f935fa8fd6c6779/EF2yaEkYQKsYibYr1mheV.png)

TrainOutput(global_step=2000, training_loss=0.5080581178665161, metrics={'train_runtime': 7559.6157, 'train_samples_per_second': 0.529,  
            'train_steps_per_second': 0.265, 'total_flos': 8.1436656795648e+16, 'train_loss': 0.5080581178665161, 'epoch': 2.0})

### 정확도


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""chaem/llama-2-7b-nsmc"", ""author"": ""chaem"", ""sha"": ""00b0637c7da80c4f4741cfaf04c591dbc05f05b8"", ""last_modified"": ""2023-12-07 05:11:30+00:00"", ""created_at"": ""2023-12-07 02:17:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-nsmc\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-nsmc"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-07 05:11:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-nsmc\n  results: []"", ""transformersInfo"": null, ""_id"": ""65712b2ac8018fe6408be8f4"", ""modelId"": ""chaem/llama-2-7b-nsmc"", ""usedStorage"": 160535163}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=chaem/llama-2-7b-nsmc&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bchaem%2Fllama-2-7b-nsmc%5D(%2Fchaem%2Fllama-2-7b-nsmc)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
kjh01/dataset_infos_llama_2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: dataset_infos_llama_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# dataset_infos_llama_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

Llama-2-7b-chat-hf은 meta에서 개발한 사전학습 텍스트 생성 언어모델 입니다. 문자열을 입력으로 하며, 문자열을 생성합니다. 
해당 모델(meta-llama/Llama-2-7b-chat-hf)을 베이스 모델로 하여 미세튜닝을 진행하였습니다.

'Llama-2-7b-chat-hf' is a pre-trained text generation language model developed by Meta. It takes a string as input and generates text. 
We fine-tuned this model based on it(meta-llama/Llama-2-7b-chat-hf).

## Intended uses & limitations

nsmc 데이터셋의 사용자가 입력한 리뷰 문장을 분류하는 에이전트입니다. 사용자 리뷰 문장으로부터 '긍정' 또는 '부정'을 판단합니다.

This agent classifies user-input review sentences from NSMC dataset. 
It determines whether the user review is 'positive' or 'negative' based on the input review sentence.

## Training and test data

Training 및 test 데이터는 nsmc 데이터 셋에서 로딩해 사용합니다. (elvaluation 데이터는 사용하지 않습니다.)

We load and use training and test data from the NSMC dataset. (We do not use an evaluation data.)

## Training procedure

사용자의 영화 리뷰 문장을 입력으로 받아 문장을 '긍정(1)' 또는 '부정(0)'으로 분류합니다. 

Accepts movie review sentences from the user as input and classifies the sentences as 'Positive (1)' or 'Negative (0)'.

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 900
- mixed_precision_training: Native AMP

### Training results

- **Binary Confusion Matrix**
|          | TP | TN |
|:-----|:------------:|:------------:|
| PP |        425         |         67         |
| PN |         66         |        442         |

- **Accuracy**: 0.894

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""kjh01/dataset_infos_llama_2"", ""author"": ""kjh01"", ""sha"": ""3e2b1443aeddc1df4a79ac743a5841b9af6adb84"", ""last_modified"": ""2023-12-11 03:56:07+00:00"", ""created_at"": ""2023-12-07 06:43:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: dataset_infos_llama_2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""dataset_infos_llama_2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 03:56:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: dataset_infos_llama_2\n  results: []"", ""transformersInfo"": null, ""_id"": ""6571699532b42d408b63579d"", ""modelId"": ""kjh01/dataset_infos_llama_2"", ""usedStorage"": 80517443}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=kjh01/dataset_infos_llama_2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bkjh01%2Fdataset_infos_llama_2%5D(%2Fkjh01%2Fdataset_infos_llama_2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
AeNyoung/lora-llama-2-7b-nsmc,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-nsmc
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-llama-2-7b-nsmc

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on 'nsmc' dataset.

## Model description

[meta-llama/Llama-2-7b-chat-hf]를 nsmc 데이터셋을 이용하여 미세튜닝함.

## Intended uses & limitations

목적: 영화 리뷰 판단 (긍정/부정)

## Training and evaluation data

- training data: nsmc의 train 데이터 중 상위 2000개
- evaluation data: nsmc의 test 데이터 중 상위 1000개

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 1000
- mixed_precision_training: Native AMP

### Training results

- global_step=1000
- training_loss=0.7012106285095215
- metrics={'train_runtime': 3567.3706, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.28, 'total_flos': 4.0718328397824e+16, 'train_loss': 0.7012106285095215, 'epoch': 1.0}

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0

### Accuracy

||TP|TN|
|------|---|---|
|PP|436|36|
|PN|72|456|

- accuracy: 0.892
","{""id"": ""AeNyoung/lora-llama-2-7b-nsmc"", ""author"": ""AeNyoung"", ""sha"": ""562a60f22c873ba09a12b94158b8fb6c1d21e2c5"", ""last_modified"": ""2023-12-07 10:15:07+00:00"", ""created_at"": ""2023-12-07 07:17:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-nsmc"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-07 10:15:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc\n  results: []"", ""transformersInfo"": null, ""_id"": ""6571719bde157ee4af0cb3e5"", ""modelId"": ""AeNyoung/lora-llama-2-7b-nsmc"", ""usedStorage"": 160535163}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=AeNyoung/lora-llama-2-7b-nsmc&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAeNyoung%2Flora-llama-2-7b-nsmc%5D(%2FAeNyoung%2Flora-llama-2-7b-nsmc)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Lohit20/Therapist,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Therapist
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Therapist

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2668

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.1936        | 1.0   | 250  | 1.2668          |


### Framework versions

- Transformers 4.35.0
- Pytorch 2.0.0
- Datasets 2.1.0
- Tokenizers 0.14.1
","{""id"": ""Lohit20/Therapist"", ""author"": ""Lohit20"", ""sha"": ""d26100d1df99f3c6cccb9f48e5a2636a0ed7c76f"", ""last_modified"": ""2023-12-07 07:49:44+00:00"", ""created_at"": ""2023-12-07 07:49:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Therapist\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Therapist"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-07 07:49:44+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Therapist\n  results: []"", ""transformersInfo"": null, ""_id"": ""657178fc971de7383e08828c"", ""modelId"": ""Lohit20/Therapist"", ""usedStorage"": 134738862}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Lohit20/Therapist&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BLohit20%2FTherapist%5D(%2FLohit20%2FTherapist)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yaeeun/lora-llama-2-7b-nsmc-review-understanding,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-nsmc-review-understanding
  results: []
datasets:
- nsmc
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-llama-2-7b-nsmc-review-understanding

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

nsmc data 기반 미세튜닝 모델

## Intended uses & limitations

More information needed

## Training and evaluation data

training data로 nsmc train data 앞쪽 2000개, evaluation data로 nsmc test data 앞쪽 1000개를 사용했습니다.

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 200
- mixed_precision_training: Native AMP

### Training results

총 200step 돌렸습니다. 50step마다 check한 결과는 아래와 같습니다.  
50 step training loss: 1.2201  
100 step training loss: 0.8892  
150 step training loss: 0.8449  
200 step training loss: 0.8370  

## 실험 내용 및 분류 결과

미세튜닝한 모델에 nsmc test data 1000개를 입력으로 주어 긍정 또는 부정 단어를 생성하도록 했습니다.  
단어 생성 결과는 '긍정' 443개, '부정' 556개, '부산엔 2015년 12월 17일 개봉했습니다. ###Midm;부정' 1개 입니다.  
정확도는 정답수 / 1000 * 100으로 계산했으며, 결과는 84.90% 입니다.

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0","{""id"": ""yaeeun/lora-llama-2-7b-nsmc-review-understanding"", ""author"": ""yaeeun"", ""sha"": ""b5cb1afcbf74e7e483fe89d4c89ee104a602f32e"", ""last_modified"": ""2023-12-11 08:51:17+00:00"", ""created_at"": ""2023-12-10 03:42:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""dataset:nsmc"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- nsmc\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-review-understanding\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-nsmc-review-understanding"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 08:51:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- nsmc\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-review-understanding\n  results: []"", ""transformersInfo"": null, ""_id"": ""6575338ac79162da90e1695b"", ""modelId"": ""yaeeun/lora-llama-2-7b-nsmc-review-understanding"", ""usedStorage"": 80517507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yaeeun/lora-llama-2-7b-nsmc-review-understanding&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byaeeun%2Flora-llama-2-7b-nsmc-review-understanding%5D(%2Fyaeeun%2Flora-llama-2-7b-nsmc-review-understanding)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
chaem/llama-2-7b-nsmc2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-7b-nsmc2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-nsmc2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an nsmc dataset.

## Model description

llama-2모델을 nsmc데이터에 대해 미세튜닝한 모델  
영화 리뷰 데이터를 기반으로 사용자가 작성한 리뷰의 긍정 또는 부정을 파악한다.

## Intended uses & limitations

### Intended uses
사용자가 작성한 리뷰의 긍정 또는 부정 감정 분석을 제공함  

### Limitaions
영화 리뷰에 특화되어 있으며, 다른 유형에는 제한이 있을 수 있음  
Colab T4 GPU에서 테스트 되었음

## Training and evaluation data

Training data: nsmc 'train' data 중 상위 2000개의 샘플
Evaluation data: nsmc 'test' data 중 상위 1000개의 샘플

## Training procedure

trainer.train()  2:02:05 소요  
추론과정 GPU 메모리 5.7GB 사용  
300 step마다 체크포인트 저장  

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 2000
- mixed_precision_training: Native AMP

### Training results

trainable params: 19988480 || all params: 3520401408 || trainable%: 0.5677897967708119  

![image/png](https://cdn-uploads.huggingface.co/production/uploads/652384150f935fa8fd6c6779/nRxqdG8_ogIWIIY2eDYeW.png)



### 정확도

Llama2: 정확도 0.913  
|               | Positive Prediction(PP) | Negative Prediction(NP) |
|--------------------|---------------------|---------------------|
| True Positive (TP) | 441                 | 67                  |
| True Negative (TN) | 20                  | 472                 |

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""chaem/llama-2-7b-nsmc2"", ""author"": ""chaem"", ""sha"": ""0c7e48ffe1ac4883e7d230058212d5b25719acdf"", ""last_modified"": ""2023-12-12 01:28:20+00:00"", ""created_at"": ""2023-12-10 06:28:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-nsmc2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-nsmc2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-12 01:28:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-nsmc2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65755a8888805b3ba1fa32ec"", ""modelId"": ""chaem/llama-2-7b-nsmc2"", ""usedStorage"": 80517443}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=chaem/llama-2-7b-nsmc2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bchaem%2Fllama-2-7b-nsmc2%5D(%2Fchaem%2Fllama-2-7b-nsmc2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/abdulrahman-nuzha/finetuned-llama2-chat-5000-v1.0-squad,N/A,N/A,1,,0,,0,,0,,0,,0
kiyeon1221/lora-llama-2-7b-food-order-understanding,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-food-order-understanding
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# 실험내용과 테스트 데이터에 대한 분류 결과 리포트

주문 문장에 의해 학습된 llama 2 를 nsmc (영화 리뷰 데이터셋) train dataset 3000개로 학습을 시켰다. 처음에는 2000개를 학습시켰으나 정확도가 예상한 것 만큼 나오지 않아 1000개를 더 학습시켰더니 약 10%의 정확도가 올라갔다. 그리고 1000개의 test dataset으로 테스트를 해보았다.
정확도는 82.40 % 가 나왔으며 표는 밑 사진으로 확인할 수 있다. 이 실험은 다른 데이터 셋과 다른 요구로 학습되어있던 LLM을 새 데이터 셋과 새 요구로 미세튜닝 하였을 때 정확도가 얼만큼 나오는지를 보여주는 것이다.

# 표

![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6523d751ab14165941297c07/lqtQt5D_Mx6mvUxICblEC.jpeg)

# lora-llama-2-7b-food-order-understanding

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""kiyeon1221/lora-llama-2-7b-food-order-understanding"", ""author"": ""kiyeon1221"", ""sha"": ""0fd1d420193b48cd7f45c33dd4227fa75064bafa"", ""last_modified"": ""2023-12-11 14:19:37+00:00"", ""created_at"": ""2023-12-11 09:13:09+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-food-order-understanding\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-food-order-understanding"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 14:19:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-food-order-understanding\n  results: []"", ""transformersInfo"": null, ""_id"": ""6576d2a51345577b701a24ff"", ""modelId"": ""kiyeon1221/lora-llama-2-7b-food-order-understanding"", ""usedStorage"": 80517507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=kiyeon1221/lora-llama-2-7b-food-order-understanding&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bkiyeon1221%2Flora-llama-2-7b-food-order-understanding%5D(%2Fkiyeon1221%2Flora-llama-2-7b-food-order-understanding)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
haeun161/llama-2-nsmc,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-nsmc
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-nsmc

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

한국어 영화 리뷰 데이터셋(NSMC)을 해결하는 모델이 되도록 미세튜닝
한국 영화 리뷰의 긍정 또는 부정을 판단하는 모델을 학습

## Intended uses & limitations

More information needed

## Training and evaluation data

TrainOutput(global_step=363, training_loss=0.9200148254058249, metrics={'train_runtime': 1447.7436, 'train_samples_per_second': 2.072, 'train_steps_per_second': 1.036, 'total_flos': 1.4780753208410112e+16, 'train_loss': 0.9200148254058249, 'epoch': 0.24})

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""haeun161/llama-2-nsmc"", ""author"": ""haeun161"", ""sha"": ""fa113f14a0c91dde311c4095f6bb97689eecf779"", ""last_modified"": ""2023-12-11 13:19:20+00:00"", ""created_at"": ""2023-12-11 12:43:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-nsmc\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-nsmc"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 13:19:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-nsmc\n  results: []"", ""transformersInfo"": null, ""_id"": ""657703fec991ca09566c9a73"", ""modelId"": ""haeun161/llama-2-nsmc"", ""usedStorage"": 80517419}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=haeun161/llama-2-nsmc&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bhaeun161%2Fllama-2-nsmc%5D(%2Fhaeun161%2Fllama-2-nsmc)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
ChloeKa/lora-llama-2-7b-food-order-understanding,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-food-order-understanding
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# <Llama2 모델을 nsmc 데이터셋을 해결하는 모델이 되도록 미세튜닝 하기>

모델: Llama2</br>
데이터셋: nsmc</br>
https://huggingface.co/datasets/nsmc </br>
Train 데이터: 3000</br>
Test 데이터: 1000

## [테스트 결과]

**정확도: 86.10%**

**혼동행렬(Confusion Matrix)**

||정답 Positive|정답 Negative|
|:------:|:------:|:------:|
|예측 Positive|395|26|
|예측 Negative|113|466|


**평가지표**

||||
|:------:|:------:|:------:|
|정밀도(Precision)|0.938|
|재현율(Recall)|0.459|
|F1 Score|0.616|

## [성능 향상] </br>
train 데이터 수를 2000에서 2500, 3000으로 늘려가며 성능을 약 11% 정도 높였으며,
TrainingArguments의 max_steps 등의 파라미터를 조절해가며 성능을 높이고자 노력하였다. 



------------------------------------------------------------------------------------------------------------------------



# lora-llama-2-7b-food-order-understanding

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 300
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""ChloeKa/lora-llama-2-7b-food-order-understanding"", ""author"": ""ChloeKa"", ""sha"": ""974ae10d18ed4032c656e955156193122a997d4a"", ""last_modified"": ""2023-12-17 14:36:51+00:00"", ""created_at"": ""2023-12-11 12:52:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-food-order-understanding\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-food-order-understanding"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-17 14:36:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-food-order-understanding\n  results: []"", ""transformersInfo"": null, ""_id"": ""6577060d8869730b226d079f"", ""modelId"": ""ChloeKa/lora-llama-2-7b-food-order-understanding"", ""usedStorage"": 80517507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=ChloeKa/lora-llama-2-7b-food-order-understanding&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BChloeKa%2Flora-llama-2-7b-food-order-understanding%5D(%2FChloeKa%2Flora-llama-2-7b-food-order-understanding)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
RiverYou/lora-llama-2-7b-nsmc-understanding,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-llama-2-7b-nsmc-understanding
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# 데이터셋 구조

- 15만개의 train데이터와 5만개의 test데이터로 구성됐다.
- 다만 일부 데이터가 NaN인 경우가 있으며, 중복된 데이터도 존재한다.
- label이 0일 경우 부정, 1일 경우 긍정이고, document가 리뷰 텍스트다.

# 분류 결과

- 세개의 계정 모두 colob의 GPU 사용 제한이 넘어 끝까지 마무리하지 못함.
- midm 으로는 완벽한 결과를 출력하지 못했다. 

# 과제 수행 결과
완벽한 미세튜닝과 테스트를 수행하지 못하여 분석불가. 과제 결과 보고서 형태로 작성.

  ## GPU 와 메모리 관리의 중요성
  - 기존 학생 신분으로 대규모 데이터를 다뤄보지 않았었다.
  - 그 결과 프로젝트를 진행하더라도 버려지고 낭비되는 메모리를 신경쓸 만큼의 경험을 하지 못했었다.
  - 이번에 대규모 데이터셋을 수행하면서 여러번 GPU 메모리의 한계를 느꼈으며 작성한 코드와 메모리 이용량의 연관성을 더 공부해봐야겠다는 것을 느꼈다.


# lora-llama-2-7b-nsmc-understanding

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""RiverYou/lora-llama-2-7b-nsmc-understanding"", ""author"": ""RiverYou"", ""sha"": ""de6b28807f42733c5135f2808c49e8bf2fe833c0"", ""last_modified"": ""2023-12-11 14:36:19+00:00"", ""created_at"": ""2023-12-11 13:29:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-understanding\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-llama-2-7b-nsmc-understanding"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-11 14:36:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-llama-2-7b-nsmc-understanding\n  results: []"", ""transformersInfo"": null, ""_id"": ""65770ec6a1688debf18bb186"", ""modelId"": ""RiverYou/lora-llama-2-7b-nsmc-understanding"", ""usedStorage"": 80517507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=RiverYou/lora-llama-2-7b-nsmc-understanding&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BRiverYou%2Flora-llama-2-7b-nsmc-understanding%5D(%2FRiverYou%2Flora-llama-2-7b-nsmc-understanding)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
simoHamlili/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: chatbot413
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chatbot413

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.09
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.31.0
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.13.3
","{""id"": ""simoHamlili/results"", ""author"": ""simoHamlili"", ""sha"": ""3f90ccd2415343a15e9db117b14f612ff10dbf97"", ""last_modified"": ""2023-12-13 15:14:59+00:00"", ""created_at"": ""2023-12-13 15:14:08+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: chatbot413\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""chatbot413"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-13 15:14:59+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: chatbot413\n  results: []"", ""transformersInfo"": null, ""_id"": ""6579ca403db8c022afdc3e59"", ""modelId"": ""simoHamlili/results"", ""usedStorage"": 134268610}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=simoHamlili/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BsimoHamlili%2Fresults%5D(%2FsimoHamlili%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
bunbohue/zero-shot-prompting-llama2-7b-chat_readsum,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-chat_readme_summarization
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-chat_readme_summarization

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.14.7
- Tokenizers 0.14.1
","{""id"": ""bunbohue/zero-shot-prompting-llama2-7b-chat_readsum"", ""author"": ""bunbohue"", ""sha"": ""e36a776512359324614f5fa9e0695d08213402ac"", ""last_modified"": ""2023-12-15 07:44:17+00:00"", ""created_at"": ""2023-12-14 12:23:05+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat_readme_summarization\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-chat_readme_summarization"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-15 07:44:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat_readme_summarization\n  results: []"", ""transformersInfo"": null, ""_id"": ""657af3a9e37d702c1d428992"", ""modelId"": ""bunbohue/zero-shot-prompting-llama2-7b-chat_readsum"", ""usedStorage"": 320440147}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=bunbohue/zero-shot-prompting-llama2-7b-chat_readsum&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bbunbohue%2Fzero-shot-prompting-llama2-7b-chat_readsum%5D(%2Fbunbohue%2Fzero-shot-prompting-llama2-7b-chat_readsum)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
MVRL/Eco-Llama-7b,"---
base_model: meta-llama/Llama-2-7b-chat-hf
license: apache-2.0
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->
#### --- Still in development (beta-stage) ---
#### --- Only for testing ---

This is a Llama 2 7B model fine-tuned to be good at responding to queries related to species distributions, species descriptions, etc.

### Model Description

- **Developed by:** [Srikumar Sastry]
- **Shared by [optional]:** [Srikumar Sastry]
- **Language(s) (NLP):** [Fine-tuned on English language]
- **License:** [Apache 2.0]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->
Model's primary purpose is to answer queries in the ecological domain. Given below are some example prompts:

```
prompt1 = ""Example of Red birds""
prompt2 = ""Describe the appearance of a Bald Eagle""
prompt3 = ""Where can I find Elephants?""
prompt4 = ""Best place to visit to find Cherry Blossoms""
prompt5 = ""Give Examples of 4 Trees and 5 Birds""
prompt6 = ""Which species is black in color?""
```

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->
The model may produce incorrect results. Extreme care must be taken when using and interpreting the results from the model.

## Model Card Authors

Srikumar Sastry, s.sastry@wustl.edu","{""id"": ""MVRL/Eco-Llama-7b"", ""author"": ""MVRL"", ""sha"": ""03834d0a197f69bd80ad7ed5aee6bc8be7eee02d"", ""last_modified"": ""2023-12-18 20:05:00+00:00"", ""created_at"": ""2023-12-18 17:38:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2023-12-18 20:05:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0"", ""transformersInfo"": null, ""_id"": ""6580837b98aa9fcdd252284c"", ""modelId"": ""MVRL/Eco-Llama-7b"", ""usedStorage"": 33588400}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=MVRL/Eco-Llama-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMVRL%2FEco-Llama-7b%5D(%2FMVRL%2FEco-Llama-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_arithmetic,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_arithmetic
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_arithmetic

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0525

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.9153        | 0.2   | 94   | 1.6186          |
| 0.5407        | 0.4   | 188  | 2.4259          |
| 0.6913        | 0.6   | 282  | 1.3859          |
| 0.8195        | 0.8   | 376  | 1.0525          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_arithmetic"", ""author"": ""Jukaboo"", ""sha"": ""89e7c67a68c4ea430c1badbefa79e8a8ea68fd5e"", ""last_modified"": ""2024-01-17 12:41:26+00:00"", ""created_at"": ""2023-12-19 13:31:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_arithmetic"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-31-58_126fe9f2d7af/events.out.tfevents.1702992736.126fe9f2d7af.3741.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-38-36_126fe9f2d7af/events.out.tfevents.1702993121.126fe9f2d7af.3741.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-42-43_126fe9f2d7af/events.out.tfevents.1702993368.126fe9f2d7af.3741.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-48-08_126fe9f2d7af/events.out.tfevents.1702993714.126fe9f2d7af.3741.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-55-49_126fe9f2d7af/events.out.tfevents.1702994155.126fe9f2d7af.3741.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_13-55-49_126fe9f2d7af/events.out.tfevents.1702997014.126fe9f2d7af.3741.5', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_15-08-08_126fe9f2d7af/events.out.tfevents.1702998496.126fe9f2d7af.3741.6', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_15-09-14_126fe9f2d7af/events.out.tfevents.1702998561.126fe9f2d7af.3741.7', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_15-20-23_126fe9f2d7af/events.out.tfevents.1702999324.126fe9f2d7af.3741.8', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_15-28-58_126fe9f2d7af/events.out.tfevents.1702999743.126fe9f2d7af.3741.9', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec19_15-31-35_126fe9f2d7af/events.out.tfevents.1702999901.126fe9f2d7af.3741.10', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_11-57-11_2fcd920cc540/events.out.tfevents.1704801435.2fcd920cc540.2581.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_11-57-11_2fcd920cc540/events.out.tfevents.1704802007.2fcd920cc540.2581.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_13-26-12_2fcd920cc540/events.out.tfevents.1704806777.2fcd920cc540.2581.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_13-26-12_2fcd920cc540/events.out.tfevents.1704807348.2fcd920cc540.2581.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_13-50-49_2fcd920cc540/events.out.tfevents.1704808253.2fcd920cc540.2581.4', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_13-50-49_2fcd920cc540/events.out.tfevents.1704808826.2fcd920cc540.2581.5', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_14-10-11_2fcd920cc540/events.out.tfevents.1704809416.2fcd920cc540.2581.6', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_14-10-11_2fcd920cc540/events.out.tfevents.1704809988.2fcd920cc540.2581.7', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_14-42-08_a98bb6000f5e/events.out.tfevents.1704811341.a98bb6000f5e.1509.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan09_14-42-08_a98bb6000f5e/events.out.tfevents.1704811914.a98bb6000f5e.1509.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan10_11-39-52_a298e425a5a7/events.out.tfevents.1704886801.a298e425a5a7.1725.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan10_14-55-30_d9d5f1f7db2b/events.out.tfevents.1704898540.d9d5f1f7db2b.6122.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan10_14-55-30_d9d5f1f7db2b/events.out.tfevents.1704899112.d9d5f1f7db2b.6122.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan17_10-24-08_4112523af026/events.out.tfevents.1705487060.4112523af026.3935.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan17_10-24-08_4112523af026/events.out.tfevents.1705489743.4112523af026.3935.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan17_10-24-08_4112523af026/events.out.tfevents.1705492302.4112523af026.3935.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan17_10-24-08_4112523af026/events.out.tfevents.1705494872.4112523af026.3935.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-17 12:41:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic\n  results: []"", ""transformersInfo"": null, ""_id"": ""65819b43907aaef86f9f2d53"", ""modelId"": ""Jukaboo/Llama2_7B_chat_arithmetic"", ""usedStorage"": 7231930430}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_arithmetic&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_arithmetic%5D(%2FJukaboo%2FLlama2_7B_chat_arithmetic)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/abdulrahman-nuzha/finetuned-llama2-chat-5000-v2.0,N/A,N/A,1,,0,,0,,0,,0,,0
yy0514/llama2-7b-chat-qlora-lek-train-2-epochs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: vicuna-7b-qlora-lek-train
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# vicuna-7b-qlora-lek-train

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""yy0514/llama2-7b-chat-qlora-lek-train-2-epochs"", ""author"": ""yy0514"", ""sha"": ""1d6da0e261280cc411675b5aba10000a5ef9a291"", ""last_modified"": ""2024-01-01 21:01:38+00:00"", ""created_at"": ""2024-01-01 21:01:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: vicuna-7b-qlora-lek-train\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""vicuna-7b-qlora-lek-train"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-01 21:01:38+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: vicuna-7b-qlora-lek-train\n  results: []"", ""transformersInfo"": null, ""_id"": ""6593282c7fe02354739c337f"", ""modelId"": ""yy0514/llama2-7b-chat-qlora-lek-train-2-epochs"", ""usedStorage"": 160472203}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yy0514/llama2-7b-chat-qlora-lek-train-2-epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byy0514%2Fllama2-7b-chat-qlora-lek-train-2-epochs%5D(%2Fyy0514%2Fllama2-7b-chat-qlora-lek-train-2-epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_arithmetic_2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_arithmetic_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_arithmetic_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.6614

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.622         | 0.2   | 94   | 2.4674          |
| 0.9407        | 0.4   | 188  | 2.9233          |
| 1.0502        | 0.6   | 282  | 2.0151          |
| 1.2152        | 0.8   | 376  | 1.6614          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_arithmetic_2"", ""author"": ""Jukaboo"", ""sha"": ""96058f19489bc1a708e52b712d0671f7be986b49"", ""last_modified"": ""2024-01-02 10:08:47+00:00"", ""created_at"": ""2024-01-02 09:49:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_arithmetic_2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan02_09-49-50_1c9f3f1f0a74/events.out.tfevents.1704188999.1c9f3f1f0a74.2290.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-02 10:08:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_2\n  results: []"", ""transformersInfo"": null, ""_id"": ""6593dc1987944e494ebf4b95"", ""modelId"": ""Jukaboo/Llama2_7B_chat_arithmetic_2"", ""usedStorage"": 791831189}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_arithmetic_2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_arithmetic_2%5D(%2FJukaboo%2FLlama2_7B_chat_arithmetic_2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_arithmetic_nocarry,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_arithmetic_nocarry
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_arithmetic_nocarry

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1935

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.5437        | 0.2   | 94   | 1.6203          |
| 0.499         | 0.4   | 188  | 2.2858          |
| 0.6523        | 0.6   | 282  | 1.6741          |
| 0.7247        | 0.8   | 376  | 1.1935          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_arithmetic_nocarry"", ""author"": ""Jukaboo"", ""sha"": ""2a0e079e14964f63a0a2ef26fec9302ddb43ce46"", ""last_modified"": ""2024-01-02 11:42:23+00:00"", ""created_at"": ""2024-01-02 11:16:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_nocarry\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_arithmetic_nocarry"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan02_09-49-50_1c9f3f1f0a74/events.out.tfevents.1704188999.1c9f3f1f0a74.2290.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan02_11-16-36_1c9f3f1f0a74/events.out.tfevents.1704194211.1c9f3f1f0a74.2290.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan02_11-41-40_1c9f3f1f0a74/events.out.tfevents.1704195710.1c9f3f1f0a74.2290.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-02 11:42:23+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_nocarry\n  results: []"", ""transformersInfo"": null, ""_id"": ""6593f09f5b7553ca5c02c43b"", ""modelId"": ""Jukaboo/Llama2_7B_chat_arithmetic_nocarry"", ""usedStorage"": 808715635}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_arithmetic_nocarry&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_arithmetic_nocarry%5D(%2FJukaboo%2FLlama2_7B_chat_arithmetic_nocarry)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-qlora-lek-train-more-epochs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-qlora-lek-train-more-epochs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- num_epochs: 4
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run1"", ""author"": ""yy0514"", ""sha"": ""266ba28d21d49a667a949e7c849584c37fba0468"", ""last_modified"": ""2024-01-02 22:07:16+00:00"", ""created_at"": ""2024-01-02 22:06:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-qlora-lek-train-more-epochs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-qlora-lek-train-more-epochs"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-02 22:07:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-qlora-lek-train-more-epochs\n  results: []"", ""transformersInfo"": null, ""_id"": ""659488fca78a27780368f992"", ""modelId"": ""yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run1"", ""usedStorage"": 160472203}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byy0514%2Fllama2-7b-chat-qlora-lek-train-4-epochs-run1%5D(%2Fyy0514%2Fllama2-7b-chat-qlora-lek-train-4-epochs-run1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
retinol/llama-2-7b-psy-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: llama-2-7b-psy-chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-psy-chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 1000
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""retinol/llama-2-7b-psy-chat"", ""author"": ""retinol"", ""sha"": ""9df6e76e922c4d8a288947a64b91b78a669fd76f"", ""last_modified"": ""2024-01-05 10:30:00+00:00"", ""created_at"": ""2024-01-05 07:50:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-psy-chat\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama-2-7b-psy-chat"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""sep_token"": ""[SEP]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_06-52-16_66d9d1894f8d/events.out.tfevents.1704437546.66d9d1894f8d.6532.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_08-33-37_66d9d1894f8d/events.out.tfevents.1704443631.66d9d1894f8d.33734.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_08-38-36_66d9d1894f8d/events.out.tfevents.1704443927.66d9d1894f8d.35930.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-27-20_610c38096778/events.out.tfevents.1704446853.610c38096778.623.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-33-23_610c38096778/events.out.tfevents.1704447207.610c38096778.623.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-43-38_610c38096778/events.out.tfevents.1704447830.610c38096778.5765.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-48-05_610c38096778/events.out.tfevents.1704448096.610c38096778.7010.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6771970048}, ""total"": 6771970048}, ""security_repo_status"": null, ""lastModified"": ""2024-01-05 10:30:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-psy-chat\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6597b4c47f63adec59f8a3c3"", ""modelId"": ""retinol/llama-2-7b-psy-chat"", ""usedStorage"": 56323815143}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=retinol/llama-2-7b-psy-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bretinol%2Fllama-2-7b-psy-chat%5D(%2Fretinol%2Fllama-2-7b-psy-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_arithmetic_nocarry_20000,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_arithmetic_nocarry_20000
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_arithmetic_nocarry_20000

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.1910

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.4997        | 0.2   | 188  | 1.7177          |
| 0.859         | 0.4   | 376  | 1.2559          |
| 0.9954        | 0.6   | 564  | 1.0398          |
| 4.5291        | 0.8   | 752  | 2.1910          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Jukaboo/Llama2_7B_chat_arithmetic_nocarry_20000"", ""author"": ""Jukaboo"", ""sha"": ""0ab1a6ae1a3f0e7b446bbe4a0788e41677ea3f0f"", ""last_modified"": ""2024-01-08 12:42:33+00:00"", ""created_at"": ""2024-01-05 09:33:01+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_nocarry_20000\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama2_7B_chat_arithmetic_nocarry_20000"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-43-27_73c9e80c8f4b/events.out.tfevents.1704447819.73c9e80c8f4b.1689.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan05_09-45-05_73c9e80c8f4b/events.out.tfevents.1704447918.73c9e80c8f4b.1689.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan08_12-23-55_d79aa1cf30f1/events.out.tfevents.1704716646.d79aa1cf30f1.1434.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-08 12:42:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2_7B_chat_arithmetic_nocarry_20000\n  results: []"", ""transformersInfo"": null, ""_id"": ""6597cccdce76219628f549da"", ""modelId"": ""Jukaboo/Llama2_7B_chat_arithmetic_nocarry_20000"", ""usedStorage"": 1766325455}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_arithmetic_nocarry_20000&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_arithmetic_nocarry_20000%5D(%2FJukaboo%2FLlama2_7B_chat_arithmetic_nocarry_20000)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-chat-qlora-lek-train-4-epochs-recheck
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-chat-qlora-lek-train-4-epochs-recheck

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- num_epochs: 4
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run2"", ""author"": ""yy0514"", ""sha"": ""7cf34edfc9a8834a1c906a2a6d1536d417826529"", ""last_modified"": ""2024-01-07 20:44:24+00:00"", ""created_at"": ""2024-01-07 19:52:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-qlora-lek-train-4-epochs-recheck\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-chat-qlora-lek-train-4-epochs-recheck"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-07 20:44:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-qlora-lek-train-4-epochs-recheck\n  results: []"", ""transformersInfo"": null, ""_id"": ""659b00e3f6dc0afd24ccd5e8"", ""modelId"": ""yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run2"", ""usedStorage"": 160472203}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yy0514/llama2-7b-chat-qlora-lek-train-4-epochs-run2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byy0514%2Fllama2-7b-chat-qlora-lek-train-4-epochs-run2%5D(%2Fyy0514%2Fllama2-7b-chat-qlora-lek-train-4-epochs-run2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
arturolinares26/finetuned-llama-7b-chat-hf-sustainbility,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-sustainbility
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-sustainbility

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""arturolinares26/finetuned-llama-7b-chat-hf-sustainbility"", ""author"": ""arturolinares26"", ""sha"": ""ad44727cf0fa7fb25a40a8b565c488c2531a9f99"", ""last_modified"": ""2024-01-08 08:39:01+00:00"", ""created_at"": ""2024-01-08 08:11:54+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-sustainbility\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-sustainbility"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan08_07-18-28_290f0fc4a99f/events.out.tfevents.1704698318.290f0fc4a99f.1383.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-08 08:39:01+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-sustainbility\n  results: []"", ""transformersInfo"": null, ""_id"": ""659bae4aac728bc303105095"", ""modelId"": ""arturolinares26/finetuned-llama-7b-chat-hf-sustainbility"", ""usedStorage"": 33585850}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=arturolinares26/finetuned-llama-7b-chat-hf-sustainbility&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Barturolinares26%2Ffinetuned-llama-7b-chat-hf-sustainbility%5D(%2Farturolinares26%2Ffinetuned-llama-7b-chat-hf-sustainbility)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jukaboo/Llama2_7B_chat_arithmetic_withcarry_10000,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: Llama2_7B_chat_arithmetic_withcarry_10000
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2_7B_chat_arithmetic_withcarry_10000

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4131

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.6453        | 0.2   | 94   | 1.8452          |
| 0.8056        | 0.4   | 188  | 1.5600          |
| 0.9203        | 0.6   | 282  | 1.8045          |
| 1.0643        | 0.8   | 376  | 1.4131          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
",N/A,1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jukaboo/Llama2_7B_chat_arithmetic_withcarry_10000&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJukaboo%2FLlama2_7B_chat_arithmetic_withcarry_10000%5D(%2FJukaboo%2FLlama2_7B_chat_arithmetic_withcarry_10000)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: lora-fine-tuning-llama2-SQL-lora-100-dataset-size
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-fine-tuning-llama2-SQL-lora-100-dataset-size

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6509

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 5
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.3966        | 0.54  | 7    | 2.1651          |
| 1.0786        | 1.08  | 14   | 0.9747          |
| 0.6613        | 1.62  | 21   | 0.7750          |
| 0.6943        | 2.15  | 28   | 0.6982          |
| 0.4302        | 2.69  | 35   | 0.6674          |
| 0.4016        | 3.23  | 42   | 0.6505          |
| 0.624         | 3.77  | 49   | 0.6498          |
| 0.5111        | 4.31  | 56   | 0.6542          |
| 0.4381        | 4.85  | 63   | 0.6509          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size"", ""author"": ""Federic"", ""sha"": ""56ff1a6d63d66e6b1ef9e5e584925e9993fd7fa3"", ""last_modified"": ""2024-01-11 15:56:53+00:00"", ""created_at"": ""2024-01-11 14:08:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-100-dataset-size\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-fine-tuning-llama2-SQL-lora-100-dataset-size"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-11 15:56:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-100-dataset-size\n  results: []"", ""transformersInfo"": null, ""_id"": ""659ff66d58a49686b2bb2792"", ""modelId"": ""Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size"", ""usedStorage"": 1173225792}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/lora-fine-tuning-llama2-SQL-lora-100-dataset-size&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2Flora-fine-tuning-llama2-SQL-lora-100-dataset-size%5D(%2FFederic%2Flora-fine-tuning-llama2-SQL-lora-100-dataset-size)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/lora-fine-tuning-llama2-SQL-lora-10-dataset-size,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-fine-tuning-llama2-SQL-lora-10-dataset-size
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-fine-tuning-llama2-SQL-lora-10-dataset-size

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Federic/lora-fine-tuning-llama2-SQL-lora-10-dataset-size"", ""author"": ""Federic"", ""sha"": ""1597e55318df91c5206f36eeecb0c802efed9c97"", ""last_modified"": ""2024-01-12 09:43:30+00:00"", ""created_at"": ""2024-01-12 08:34:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-10-dataset-size\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-fine-tuning-llama2-SQL-lora-10-dataset-size"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-12 09:43:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-10-dataset-size\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a0f98190eb7a152424d129"", ""modelId"": ""Federic/lora-fine-tuning-llama2-SQL-lora-10-dataset-size"", ""usedStorage"": 4692903168}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/lora-fine-tuning-llama2-SQL-lora-10-dataset-size&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2Flora-fine-tuning-llama2-SQL-lora-10-dataset-size%5D(%2FFederic%2Flora-fine-tuning-llama2-SQL-lora-10-dataset-size)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Federic/lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size"", ""author"": ""Federic"", ""sha"": ""a21677664e50fe739baa12644427f3d810f0186b"", ""last_modified"": ""2024-01-12 15:53:34+00:00"", ""created_at"": ""2024-01-12 14:34:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-12 15:53:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a14e0c79a95d1a3eb4d6cc"", ""modelId"": ""Federic/lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size"", ""usedStorage"": 6842953900}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/lora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2Flora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size%5D(%2FFederic%2Flora-fine-tuning-llama2-SQL-lora-1000-2-dataset-size)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Federic/lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size"", ""author"": ""Federic"", ""sha"": ""1a82e8e75c0621569df97d6a335b408bd4ef0bdf"", ""last_modified"": ""2024-01-12 17:15:58+00:00"", ""created_at"": ""2024-01-12 15:58:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-12 17:15:58+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a161960251d2c6bfdb0f2e"", ""modelId"": ""Federic/lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size"", ""usedStorage"": 7168571860}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/lora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2Flora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size%5D(%2FFederic%2Flora-fine-tuning-llama2-SQL-lora-1000-3-dataset-size)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""Federic/lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size"", ""author"": ""Federic"", ""sha"": ""8ef03a5fd548b029a90c97fd00aad11ced534b80"", ""last_modified"": ""2024-01-15 09:03:26+00:00"", ""created_at"": ""2024-01-15 08:23:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-15 09:03:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a4eb8cea98738768235382"", ""modelId"": ""Federic/lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size"", ""usedStorage"": 2872518520}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/lora-fine-tuning-llama2-SQL-lora-100-4-dataset-size&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2Flora-fine-tuning-llama2-SQL-lora-100-4-dataset-size%5D(%2FFederic%2Flora-fine-tuning-llama2-SQL-lora-100-4-dataset-size)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-2"", ""author"": ""mojuss"", ""sha"": ""02a1bc24f488625b6ddf53463b60af962d4a6b45"", ""last_modified"": ""2024-01-16 13:10:44+00:00"", ""created_at"": ""2024-01-16 13:10:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_11-03-40_33d362183329/events.out.tfevents.1705403032.33d362183329.40249.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 13:10:44+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a68050a840ac8b3c235e40"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-2"", ""usedStorage"": 33582258}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-2%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-3"", ""author"": ""mojuss"", ""sha"": ""a5fcb0440fc58d466551cac89058c169f640f0eb"", ""last_modified"": ""2024-01-16 15:56:11+00:00"", ""created_at"": ""2024-01-16 15:56:06+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-3\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-3"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_15-15-17_33d362183329/events.out.tfevents.1705418139.33d362183329.102936.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 15:56:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-3\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6a7163bb0e70b4175a0fb"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-3"", ""usedStorage"": 33581778}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-3%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-4,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-4
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-4

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-4"", ""author"": ""mojuss"", ""sha"": ""052c84970bb93e23b2ddc3bae431e0f96a6c8439"", ""last_modified"": ""2024-01-16 16:57:03+00:00"", ""created_at"": ""2024-01-16 16:56:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-4\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-4"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_16-16-26_33d362183329/events.out.tfevents.1705421793.33d362183329.119693.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 16:57:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-4\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6b55bf1d4e7bccc4497dd"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-4"", ""usedStorage"": 33581932}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-4%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-5,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-5
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-5

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-5"", ""author"": ""mojuss"", ""sha"": ""ab0f6cd7c11eb575957d71fe7f5da674c18bc3ac"", ""last_modified"": ""2024-01-16 17:58:28+00:00"", ""created_at"": ""2024-01-16 17:58:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-5\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-5"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_17-17-51_33d362183329/events.out.tfevents.1705425478.33d362183329.135030.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 17:58:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-5\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6c3c0d9c4c62f766eb67a"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-5"", ""usedStorage"": 33581932}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-5&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-5%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-5)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-6,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-6
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-6

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-6"", ""author"": ""mojuss"", ""sha"": ""67c203f89d3c81eb98247dcd63a91be1a046c0f3"", ""last_modified"": ""2024-01-16 19:15:06+00:00"", ""created_at"": ""2024-01-16 19:15:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-6\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-6"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_18-32-53_33d362183329/events.out.tfevents.1705429980.33d362183329.154493.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 19:15:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-6\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6d5b6da9f6df1412ec8fc"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-6"", ""usedStorage"": 33581932}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-6&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-6%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-6)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-7,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-7
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-7

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-7"", ""author"": ""mojuss"", ""sha"": ""64079df6a342240b96e2d0915bbbe6136ba61336"", ""last_modified"": ""2024-01-16 20:13:32+00:00"", ""created_at"": ""2024-01-16 20:13:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-7\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-7"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_19-30-21_33d362183329/events.out.tfevents.1705433430.33d362183329.169379.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 20:13:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-7\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6e368148ef3dc39e9b27d"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-7"", ""usedStorage"": 33581932}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-7&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-7%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-7)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mojuss/finetuned-llama-7b-chat-hf-gpt-exam-8,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: finetuned-llama-7b-chat-hf-gpt-exam-8
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned-llama-7b-chat-hf-gpt-exam-8

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 3
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 9
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0
","{""id"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-8"", ""author"": ""mojuss"", ""sha"": ""2b8ee35537712a44be65dd59ab730e223550853d"", ""last_modified"": ""2024-01-16 21:04:56+00:00"", ""created_at"": ""2024-01-16 21:04:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-8\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""finetuned-llama-7b-chat-hf-gpt-exam-8"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan16_20-24-19_33d362183329/events.out.tfevents.1705436670.33d362183329.183008.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-16 21:04:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: finetuned-llama-7b-chat-hf-gpt-exam-8\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a6ef733efe2c547c404884"", ""modelId"": ""mojuss/finetuned-llama-7b-chat-hf-gpt-exam-8"", ""usedStorage"": 33582084}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mojuss/finetuned-llama-7b-chat-hf-gpt-exam-8&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-8%5D(%2Fmojuss%2Ffinetuned-llama-7b-chat-hf-gpt-exam-8)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
amit70/llama2-finetuned-squad-hf-2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- squad
model-index:
- name: llama2-finetuned-squad-hf-2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-finetuned-squad-hf-2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the squad dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1759

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.4102        | 0.96  | 18   | 1.2801          |
| 1.7776        | 1.92  | 36   | 1.2137          |
| 1.7838        | 2.88  | 54   | 1.1908          |
| 1.6322        | 3.84  | 72   | 1.1784          |
| 1.2947        | 4.8   | 90   | 1.1759          |


### Framework versions

- Transformers 4.31.0
- Pytorch 2.0.0
- Datasets 2.1.0
- Tokenizers 0.13.3
","{""id"": ""amit70/llama2-finetuned-squad-hf-2"", ""author"": ""amit70"", ""sha"": ""63d768229aa420cf4ce5fc8e82e03accbac4c8a9"", ""last_modified"": ""2024-01-18 08:32:20+00:00"", ""created_at"": ""2024-01-18 07:36:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""generated_from_trainer"", ""dataset:squad"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- squad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-finetuned-squad-hf-2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-finetuned-squad-hf-2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='.gitignore', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan18_07-36-09_12e06f5fbcae/events.out.tfevents.1705563377.12e06f5fbcae.26.2', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan18_07-36-09_12e06f5fbcae/events.out.tfevents.1705563419.12e06f5fbcae.26.3', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan18_07-38-09_12e06f5fbcae/events.out.tfevents.1705563493.12e06f5fbcae.2428.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-18 08:32:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- squad\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-finetuned-squad-hf-2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65a8d4eb9bd7d5189dd4f314"", ""modelId"": ""amit70/llama2-finetuned-squad-hf-2"", ""usedStorage"": 336607527}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=amit70/llama2-finetuned-squad-hf-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bamit70%2Fllama2-finetuned-squad-hf-2%5D(%2Famit70%2Fllama2-finetuned-squad-hf-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
shahrukh95/Llama-2-7b-Set-1-cybersecurity-layered-config,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-Set-1-cybersecurity-layered-config
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-Set-1-cybersecurity-layered-config

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0003
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 15

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""shahrukh95/Llama-2-7b-Set-1-cybersecurity-layered-config"", ""author"": ""shahrukh95"", ""sha"": ""cade5fd1d876ef882f512bc18d35b14061063c59"", ""last_modified"": ""2024-01-19 15:33:09+00:00"", ""created_at"": ""2024-01-19 15:32:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-1-cybersecurity-layered-config\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-Set-1-cybersecurity-layered-config"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-19 15:33:09+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-1-cybersecurity-layered-config\n  results: []"", ""transformersInfo"": null, ""_id"": ""65aa9619e2a2c863564a8a9b"", ""modelId"": ""shahrukh95/Llama-2-7b-Set-1-cybersecurity-layered-config"", ""usedStorage"": 201377666}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=shahrukh95/Llama-2-7b-Set-1-cybersecurity-layered-config&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bshahrukh95%2FLlama-2-7b-Set-1-cybersecurity-layered-config%5D(%2Fshahrukh95%2FLlama-2-7b-Set-1-cybersecurity-layered-config)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
shahrukh95/Llama-2-7b-Set-3-cybersecurity-layered-config,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-Set-3-cybersecurity-layered-config
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-Set-3-cybersecurity-layered-config

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0005
- train_batch_size: 10
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 12

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""shahrukh95/Llama-2-7b-Set-3-cybersecurity-layered-config"", ""author"": ""shahrukh95"", ""sha"": ""58a342712c376e232d8f083692e2135210824a4f"", ""last_modified"": ""2024-01-29 17:20:07+00:00"", ""created_at"": ""2024-01-29 17:19:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-3-cybersecurity-layered-config\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-Set-3-cybersecurity-layered-config"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-29 17:20:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-3-cybersecurity-layered-config\n  results: []"", ""transformersInfo"": null, ""_id"": ""65b7de2fd49f4330ab0031ee"", ""modelId"": ""shahrukh95/Llama-2-7b-Set-3-cybersecurity-layered-config"", ""usedStorage"": 201377666}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=shahrukh95/Llama-2-7b-Set-3-cybersecurity-layered-config&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bshahrukh95%2FLlama-2-7b-Set-3-cybersecurity-layered-config%5D(%2Fshahrukh95%2FLlama-2-7b-Set-3-cybersecurity-layered-config)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Federic/LLM-to-SQL,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: LLM-to-SQL
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# LLM-to-SQL

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 12
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 2
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.1
","{""id"": ""Federic/LLM-to-SQL"", ""author"": ""Federic"", ""sha"": ""bc118b9d9b04bb2d056dc52abe34d7609c3940fd"", ""last_modified"": ""2024-01-31 16:34:52+00:00"", ""created_at"": ""2024-01-31 15:19:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLM-to-SQL\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""LLM-to-SQL"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-01-31 16:34:52+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: LLM-to-SQL\n  results: []"", ""transformersInfo"": null, ""_id"": ""65ba64ee145d4d463b344889"", ""modelId"": ""Federic/LLM-to-SQL"", ""usedStorage"": 16659330203}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Federic/LLM-to-SQL&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFederic%2FLLM-to-SQL%5D(%2FFederic%2FLLM-to-SQL)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Gennaro22/Test-Llama2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Test-Llama2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Test-Llama2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 3
- total_train_batch_size: 12
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.1
","{""id"": ""Gennaro22/Test-Llama2"", ""author"": ""Gennaro22"", ""sha"": ""8324d93e535b94dfc41d4a025218de0905e361d7"", ""last_modified"": ""2024-02-01 15:39:14+00:00"", ""created_at"": ""2024-02-01 15:05:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Test-Llama2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Test-Llama2"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='last-checkpoint/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-01 15:39:14+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Test-Llama2\n  results: []"", ""transformersInfo"": null, ""_id"": ""65bbb33a839d72afa481376b"", ""modelId"": ""Gennaro22/Test-Llama2"", ""usedStorage"": 2992204603}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Gennaro22/Test-Llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BGennaro22%2FTest-Llama2%5D(%2FGennaro22%2FTest-Llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Lalith16/LLAMA2-10epoch-finetuned-NXAIR,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0380

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2.5e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 10

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.5462        | 0.72  | 100  | 1.5099          |
| 1.0012        | 1.45  | 200  | 1.1049          |
| 0.8908        | 2.17  | 300  | 0.9713          |
| 0.8156        | 2.9   | 400  | 0.9004          |
| 0.6787        | 3.62  | 500  | 0.8666          |
| 0.5238        | 4.35  | 600  | 0.8536          |
| 0.4373        | 5.07  | 700  | 0.8734          |
| 0.4224        | 5.8   | 800  | 0.8725          |
| 0.2836        | 6.52  | 900  | 0.9276          |
| 0.2222        | 7.25  | 1000 | 0.9700          |
| 0.2388        | 7.97  | 1100 | 0.9690          |
| 0.2064        | 8.7   | 1200 | 1.0128          |
| 0.1713        | 9.42  | 1300 | 1.0380          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.1
","{""id"": ""Lalith16/LLAMA2-10epoch-finetuned-NXAIR"", ""author"": ""Lalith16"", ""sha"": ""f3300f6c2cd423d52b90e46bdc4f71bc0064efa0"", ""last_modified"": ""2024-02-05 14:19:40+00:00"", ""created_at"": ""2024-02-05 14:18:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Feb05_12-11-50_c6f970f36a3d/events.out.tfevents.1707135116.c6f970f36a3d.403.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-05 14:19:40+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""65c0ee2d997d4ef034eb245f"", ""modelId"": ""Lalith16/LLAMA2-10epoch-finetuned-NXAIR"", ""usedStorage"": 605978907}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Lalith16/LLAMA2-10epoch-finetuned-NXAIR&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BLalith16%2FLLAMA2-10epoch-finetuned-NXAIR%5D(%2FLalith16%2FLLAMA2-10epoch-finetuned-NXAIR)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
shahrukh95/Llama-2-7b-Set-2-cybersecurity-layered-config,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: Llama-2-7b-Set-2-cybersecurity-layered-config
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-Set-2-cybersecurity-layered-config

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.00025
- train_batch_size: 6
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 20

### Training results



### Framework versions

- Transformers 4.34.1
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""shahrukh95/Llama-2-7b-Set-2-cybersecurity-layered-config"", ""author"": ""shahrukh95"", ""sha"": ""88b8bd66e3c62d595031162369e2d77554ea3466"", ""last_modified"": ""2024-02-06 23:25:27+00:00"", ""created_at"": ""2024-02-06 23:25:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-2-cybersecurity-layered-config\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""Llama-2-7b-Set-2-cybersecurity-layered-config"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-06 23:25:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-Set-2-cybersecurity-layered-config\n  results: []"", ""transformersInfo"": null, ""_id"": ""65c2bfcec75f7b0871c964a1"", ""modelId"": ""shahrukh95/Llama-2-7b-Set-2-cybersecurity-layered-config"", ""usedStorage"": 201377666}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=shahrukh95/Llama-2-7b-Set-2-cybersecurity-layered-config&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bshahrukh95%2FLlama-2-7b-Set-2-cybersecurity-layered-config%5D(%2Fshahrukh95%2FLlama-2-7b-Set-2-cybersecurity-layered-config)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
codewizardUV/NXAIR_M_12-2-2024,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: NXAIR_M_12-2-2024
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# NXAIR_M_12-2-2024

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: nan

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.00025
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- training_steps: 500

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.0           | 0.77  | 250  | nan             |
| 0.0           | 1.55  | 500  | nan             |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.17.0
- Tokenizers 0.15.1
","{""id"": ""codewizardUV/NXAIR_M_12-2-2024"", ""author"": ""codewizardUV"", ""sha"": ""ca939069dfdae33a3a4c12cd6b832d9099f5f8b2"", ""last_modified"": ""2024-02-12 05:53:50+00:00"", ""created_at"": ""2024-02-12 05:53:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: NXAIR_M_12-2-2024\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""NXAIR_M_12-2-2024"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Feb12_05-17-12_741caf8fd47e/events.out.tfevents.1707715040.741caf8fd47e.3091.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-12 05:53:50+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: NXAIR_M_12-2-2024\n  results: []"", ""transformersInfo"": null, ""_id"": ""65c9b256ec1e90b3ab1098c6"", ""modelId"": ""codewizardUV/NXAIR_M_12-2-2024"", ""usedStorage"": 605970532}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=codewizardUV/NXAIR_M_12-2-2024&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BcodewizardUV%2FNXAIR_M_12-2-2024%5D(%2FcodewizardUV%2FNXAIR_M_12-2-2024)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_350STEPS_1e5_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: 350_STEPS_TEST_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# 350_STEPS_TEST_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3260

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 350

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.4448        | 0.1   | 50   | 0.5649          |
| 0.5998        | 0.2   | 100  | 0.6260          |
| 0.5585        | 0.29  | 150  | 0.5314          |
| 0.3909        | 0.39  | 200  | 0.3844          |
| 0.3704        | 0.49  | 250  | 0.3523          |
| 0.3376        | 0.59  | 300  | 0.3305          |
| 0.3266        | 0.68  | 350  | 0.3260          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_350STEPS_1e5_SFT"", ""author"": ""tsavage68"", ""sha"": ""b2546e86e7d4cd86cd598790443c11681eefb080"", ""last_modified"": ""2024-02-13 14:09:14+00:00"", ""created_at"": ""2024-02-13 14:05:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: 350_STEPS_TEST_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""350_STEPS_TEST_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 14:09:14+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: 350_STEPS_TEST_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cb772d877f943912ad6696"", ""modelId"": ""tsavage68/chat_350STEPS_1e5_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_350STEPS_1e5_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_350STEPS_1e5_SFT%5D(%2Ftsavage68%2Fchat_350STEPS_1e5_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
m7mdal7aj/fine_tuned_llama_2_7b_chat_OKVQA,"---
license: apache-2.0
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: visual-question-answering
tags:
- multimodal
- KBVQA
- VQA
- Finetuning
datasets:
- m7mdal7aj/OK-VQA
---","{""id"": ""m7mdal7aj/fine_tuned_llama_2_7b_chat_OKVQA"", ""author"": ""m7mdal7aj"", ""sha"": ""de6abdfbfcf2a90c962cfd4020372b1b8ec470b7"", ""last_modified"": ""2024-10-15 06:37:31+00:00"", ""created_at"": ""2024-02-13 14:09:36+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""multimodal"", ""KBVQA"", ""VQA"", ""Finetuning"", ""visual-question-answering"", ""en"", ""dataset:m7mdal7aj/OK-VQA"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""visual-question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- m7mdal7aj/OK-VQA\nlanguage:\n- en\nlicense: apache-2.0\npipeline_tag: visual-question-answering\ntags:\n- multimodal\n- KBVQA\n- VQA\n- Finetuning"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [""m7mdal7aj/KB-VQA"", ""m7mdal7aj/KB-VQA-E""], ""safetensors"": {""parameters"": {""F16"": 6738472960}, ""total"": 6738472960}, ""security_repo_status"": null, ""lastModified"": ""2024-10-15 06:37:31+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- m7mdal7aj/OK-VQA\nlanguage:\n- en\nlicense: apache-2.0\npipeline_tag: visual-question-answering\ntags:\n- multimodal\n- KBVQA\n- VQA\n- Finetuning"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cb78207faf059c56e08627"", ""modelId"": ""m7mdal7aj/fine_tuned_llama_2_7b_chat_OKVQA"", ""usedStorage"": 13477479163}",1,,0,,0,,0,,0,"huggingface/InferenceSupport/discussions/new?title=m7mdal7aj/fine_tuned_llama_2_7b_chat_OKVQA&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bm7mdal7aj%2Ffine_tuned_llama_2_7b_chat_OKVQA%5D(%2Fm7mdal7aj%2Ffine_tuned_llama_2_7b_chat_OKVQA)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A, m7mdal7aj/KB-VQA, m7mdal7aj/KB-VQA-E",3
tsavage68/chat_500STEPS_1e5rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_500STEPS_1e5rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_500STEPS_1e5rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3160

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 500

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.351         | 0.1   | 50   | 1.2639          |
| 0.3961        | 0.2   | 100  | 0.3739          |
| 0.3542        | 0.29  | 150  | 0.3401          |
| 0.3308        | 0.39  | 200  | 0.3253          |
| 0.33          | 0.49  | 250  | 0.3196          |
| 0.3246        | 0.59  | 300  | 0.3175          |
| 0.3159        | 0.68  | 350  | 0.3163          |
| 0.3105        | 0.78  | 400  | 0.3160          |
| 0.2879        | 0.88  | 450  | 0.3160          |
| 0.316         | 0.98  | 500  | 0.3160          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_500STEPS_1e5rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""3e66727fa3f30d9f497b4f650afce5a5d27950d4"", ""last_modified"": ""2024-02-13 18:33:37+00:00"", ""created_at"": ""2024-02-13 15:04:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_500STEPS_1e5rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_500STEPS_1e5rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 18:33:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_500STEPS_1e5rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cb850a9a4ee02bbb8ea4ef"", ""modelId"": ""tsavage68/chat_500STEPS_1e5rate_SFT"", ""usedStorage"": 26953737942}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_500STEPS_1e5rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_500STEPS_1e5rate_SFT%5D(%2Ftsavage68%2Fchat_500STEPS_1e5rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_700STEPS_1e4rate_01beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_700STEPS_1e4rate
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_700STEPS_1e4rate

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1848
- Rewards/chosen: -4.4236
- Rewards/rejected: -4.3538
- Rewards/accuracies: 0.4000
- Rewards/margins: -0.0698
- Logps/rejected: -62.3289
- Logps/chosen: -60.9807
- Logits/rejected: -4.5000
- Logits/chosen: -4.5000

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 700

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 1.2329        | 0.1   | 50   | 1.6243          | -8.0715        | -8.0351          | 0.4176             | -0.0364         | -99.1426       | -97.4598     | -0.6161         | -0.6164       |
| 1.3399        | 0.2   | 100  | 1.2323          | -4.6270        | -4.6104          | 0.3978             | -0.0167         | -64.8951       | -63.0151     | -2.3014         | -2.3014       |
| 1.316         | 0.29  | 150  | 1.2017          | -4.3807        | -4.3323          | 0.4000             | -0.0484         | -62.1140       | -60.5517     | -2.9891         | -2.9891       |
| 1.2778        | 0.39  | 200  | 1.1891          | -4.3216        | -4.2623          | 0.4044             | -0.0593         | -61.4138       | -59.9605     | -3.4116         | -3.4116       |
| 1.0721        | 0.49  | 250  | 1.1847          | -4.3234        | -4.2565          | 0.4110             | -0.0669         | -61.3561       | -59.9788     | -3.7977         | -3.7977       |
| 1.3775        | 0.59  | 300  | 1.1896          | -4.3481        | -4.2745          | 0.4176             | -0.0737         | -61.5360       | -60.2260     | -4.0911         | -4.0911       |
| 1.3232        | 0.68  | 350  | 1.1818          | -4.3486        | -4.2846          | 0.4088             | -0.0640         | -61.6375       | -60.2310     | -4.2490         | -4.2490       |
| 1.2476        | 0.78  | 400  | 1.1789          | -4.3705        | -4.3105          | 0.4044             | -0.0600         | -61.8961       | -60.4495     | -4.1920         | -4.1920       |
| 1.3082        | 0.88  | 450  | 1.1766          | -4.3831        | -4.3296          | 0.4000             | -0.0535         | -62.0870       | -60.5753     | -4.2348         | -4.2348       |
| 1.1007        | 0.98  | 500  | 1.1762          | -4.4022        | -4.3470          | 0.4000             | -0.0552         | -62.2610       | -60.7662     | -4.2010         | -4.2010       |
| 0.8786        | 1.07  | 550  | 1.1811          | -4.4150        | -4.3527          | 0.4000             | -0.0623         | -62.3185       | -60.8948     | -4.3351         | -4.3351       |
| 1.3113        | 1.17  | 600  | 1.1842          | -4.4174        | -4.3487          | 0.4000             | -0.0687         | -62.2785       | -60.9186     | -4.4858         | -4.4858       |
| 0.9783        | 1.27  | 650  | 1.1850          | -4.4234        | -4.3533          | 0.4000             | -0.0701         | -62.3242       | -60.9783     | -4.4997         | -4.4997       |
| 1.3696        | 1.37  | 700  | 1.1848          | -4.4236        | -4.3538          | 0.4000             | -0.0698         | -62.3289       | -60.9807     | -4.5000         | -4.5000       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_700STEPS_1e4rate_01beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""c50a173a2a723ea71f01f7a831199e0135a69658"", ""last_modified"": ""2024-02-13 15:33:36+00:00"", ""created_at"": ""2024-02-13 15:29:46+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_700STEPS_1e4rate\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_700STEPS_1e4rate"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 15:33:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_700STEPS_1e4rate\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cb8aea9a4ee02bbb903749"", ""modelId"": ""tsavage68/chat_700STEPS_1e4rate_01beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_700STEPS_1e4rate_01beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_700STEPS_1e4rate_01beta_DPO%5D(%2Ftsavage68%2Fchat_700STEPS_1e4rate_01beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_500STEPS_1e7rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_500STEPS_1e7rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_500STEPS_1e7rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4297

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 500

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.6169        | 0.1   | 50   | 1.6126          |
| 1.5653        | 0.2   | 100  | 1.5784          |
| 1.524         | 0.29  | 150  | 1.5257          |
| 1.4813        | 0.39  | 200  | 1.4845          |
| 1.4608        | 0.49  | 250  | 1.4560          |
| 1.4351        | 0.59  | 300  | 1.4397          |
| 1.4317        | 0.68  | 350  | 1.4319          |
| 1.4269        | 0.78  | 400  | 1.4300          |
| 1.4167        | 0.88  | 450  | 1.4297          |
| 1.4284        | 0.98  | 500  | 1.4297          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_500STEPS_1e7rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""0ae1498f455748ec46259a53a7fe646c81242dce"", ""last_modified"": ""2024-02-13 21:06:24+00:00"", ""created_at"": ""2024-02-13 21:02:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_500STEPS_1e7rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_500STEPS_1e7rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 21:06:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_500STEPS_1e7rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cbd8e843207e438afd9dc6"", ""modelId"": ""tsavage68/chat_500STEPS_1e7rate_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_500STEPS_1e7rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_500STEPS_1e7rate_SFT%5D(%2Ftsavage68%2Fchat_500STEPS_1e7rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_300STEPS_1e7rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_300STEPS_1e7rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_300STEPS_1e7rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4992

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 300

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.6169        | 0.1   | 50   | 1.6126          |
| 1.5653        | 0.2   | 100  | 1.5784          |
| 1.5269        | 0.29  | 150  | 1.5290          |
| 1.4991        | 0.39  | 200  | 1.5046          |
| 1.5009        | 0.49  | 250  | 1.4995          |
| 1.4926        | 0.59  | 300  | 1.4992          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_300STEPS_1e7rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""a16fa8862dfccf34b2c38e0d116c10aeabf8d372"", ""last_modified"": ""2024-02-13 21:44:20+00:00"", ""created_at"": ""2024-02-13 21:40:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_300STEPS_1e7rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_300STEPS_1e7rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 21:44:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_300STEPS_1e7rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cbe1d643207e438a00de7a"", ""modelId"": ""tsavage68/chat_300STEPS_1e7rate_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_300STEPS_1e7rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_300STEPS_1e7rate_SFT%5D(%2Ftsavage68%2Fchat_300STEPS_1e7rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_400STEPS_1e6rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_400STEPS_1e6rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_400STEPS_1e6rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3202

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 400

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.351         | 0.1   | 50   | 1.2639          |
| 0.3961        | 0.2   | 100  | 0.3739          |
| 0.3545        | 0.29  | 150  | 0.3403          |
| 0.332         | 0.39  | 200  | 0.3267          |
| 0.332         | 0.49  | 250  | 0.3218          |
| 0.3278        | 0.59  | 300  | 0.3205          |
| 0.3196        | 0.68  | 350  | 0.3202          |
| 0.3146        | 0.78  | 400  | 0.3202          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_400STEPS_1e6rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""1d26194a7742c97954e1a8ebc7b529321a9506eb"", ""last_modified"": ""2024-02-13 22:29:55+00:00"", ""created_at"": ""2024-02-13 22:26:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_400STEPS_1e6rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_400STEPS_1e6rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 22:29:55+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_400STEPS_1e6rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cbec864e267fc9c574304f"", ""modelId"": ""tsavage68/chat_400STEPS_1e6rate_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_400STEPS_1e6rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_400STEPS_1e6rate_SFT%5D(%2Ftsavage68%2Fchat_400STEPS_1e6rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_150STEPS_1e6rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_150STEPS_1e6rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_150STEPS_1e6rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3523

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 150

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.351         | 0.1   | 50   | 1.2639          |
| 0.3961        | 0.2   | 100  | 0.3739          |
| 0.3651        | 0.29  | 150  | 0.3523          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_150STEPS_1e6rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""c8d6877c0f5bb5d8d0d4fd60718c9afaa69523f0"", ""last_modified"": ""2024-02-13 22:52:07+00:00"", ""created_at"": ""2024-02-13 22:48:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_150STEPS_1e6rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_150STEPS_1e6rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-13 22:52:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_150STEPS_1e6rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cbf1b2e2efb927600c5587"", ""modelId"": ""tsavage68/chat_150STEPS_1e6rate_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_150STEPS_1e6rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_150STEPS_1e6rate_SFT%5D(%2Ftsavage68%2Fchat_150STEPS_1e6rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
santiadavani/alpaca-gpt4-conversation-opt-350m,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: alpaca-gpt4-conversation-opt-350m
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# alpaca-gpt4-conversation-opt-350m

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Framework versions

- Transformers 4.37.2
- Pytorch 2.1.2+cu121
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""santiadavani/alpaca-gpt4-conversation-opt-350m"", ""author"": ""santiadavani"", ""sha"": ""90fc1a2cf5e7d337dd8f92f5775ae0d2230dcf7a"", ""last_modified"": ""2024-02-15 23:20:45+00:00"", ""created_at"": ""2024-02-13 23:04:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: alpaca-gpt4-conversation-opt-350m\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""alpaca-gpt4-conversation-opt-350m"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-15 23:20:45+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: alpaca-gpt4-conversation-opt-350m\n  results: []"", ""transformersInfo"": null, ""_id"": ""65cbf576188a42144472544d"", ""modelId"": ""santiadavani/alpaca-gpt4-conversation-opt-350m"", ""usedStorage"": 9989275}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=santiadavani/alpaca-gpt4-conversation-opt-350m&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsantiadavani%2Falpaca-gpt4-conversation-opt-350m%5D(%2Fsantiadavani%2Falpaca-gpt4-conversation-opt-350m)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_600STEPS_1e8rate_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_600STEPS_1e8rate_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_600STEPS_1e8rate_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.6169

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-08
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 600

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.6205        | 0.1   | 50   | 1.6173          |
| 1.5976        | 0.2   | 100  | 1.6169          |
| 1.6086        | 0.29  | 150  | 1.6172          |
| 1.6093        | 0.39  | 200  | 1.6169          |
| 1.618         | 0.49  | 250  | 1.6169          |
| 1.6087        | 0.59  | 300  | 1.6168          |
| 1.6166        | 0.68  | 350  | 1.6168          |
| 1.6135        | 0.78  | 400  | 1.6170          |
| 1.6054        | 0.88  | 450  | 1.6169          |
| 1.6162        | 0.98  | 500  | 1.6169          |
| 1.6052        | 1.07  | 550  | 1.6169          |
| 1.6057        | 1.17  | 600  | 1.6169          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_600STEPS_1e8rate_SFT"", ""author"": ""tsavage68"", ""sha"": ""84cd2470d3b838be43f96d1f5bd835fba6bc9561"", ""last_modified"": ""2024-02-14 00:00:46+00:00"", ""created_at"": ""2024-02-13 23:57:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_600STEPS_1e8rate_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_600STEPS_1e8rate_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-14 00:00:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_600STEPS_1e8rate_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cc01cc9b312118ded8f412"", ""modelId"": ""tsavage68/chat_600STEPS_1e8rate_SFT"", ""usedStorage"": 13476868971}",1,"https://huggingface.co/tsavage68/chat_1000_STEPS_01beta_1e5rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_05beta_1e5rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_03beta_1e5rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_01beta_1e6_rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_03beta_1e6rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_05beta_1e6rate_CDPOSFT, https://huggingface.co/tsavage68/chat_400_STEPS_05beta_1e6rate_CDPOSFT, https://huggingface.co/tsavage68/chat_550_STEPS_01beta_1e6_rate_CDPOSFT, https://huggingface.co/tsavage68/chat_700_STEPS_03beta_1e6rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_01beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_03beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_05beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_400_STEPS_05beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_550_STEPS_01beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_650_STEPS_03beta_1e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_05beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_01beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_03beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_300_STEPS_03beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_400_STEPS_01beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_600_STEPS_05beta_5e7rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_01beta_1e8rate_CDPOSFT, https://huggingface.co/tsavage68/chat_1000_STEPS_05beta_1e8rate_CDPOSFT",23,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_600STEPS_1e8rate_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_600STEPS_1e8rate_SFT%5D(%2Ftsavage68%2Fchat_600STEPS_1e8rate_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_01beta_1e5rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_01beta_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_01beta_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9140
- Rewards/chosen: -2.4513
- Rewards/rejected: -2.9029
- Rewards/accuracies: 0.4901
- Rewards/margins: 0.4516
- Logps/rejected: -47.8308
- Logps/chosen: -41.2673
- Logits/rejected: -0.3435
- Logits/chosen: -0.3434

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.7203        | 0.0977 | 50   | 0.7044          | -0.4487        | -0.4880          | 0.4527             | 0.0393          | -23.6818       | -21.2416     | -0.5577         | -0.5575       |
| 0.8621        | 0.1953 | 100  | 0.8300          | -1.1435        | -1.1120          | 0.3802             | -0.0314         | -29.9224       | -28.1895     | -0.4308         | -0.4305       |
| 0.8777        | 0.2930 | 150  | 0.8612          | -1.0764        | -1.0264          | 0.3648             | -0.0499         | -29.0661       | -27.5182     | -0.7212         | -0.7205       |
| 0.8866        | 0.3906 | 200  | 0.8505          | -0.4936        | -0.4251          | 0.3912             | -0.0684         | -23.0532       | -21.6902     | 0.8383          | 0.8386        |
| 0.8943        | 0.4883 | 250  | 0.9037          | -1.0321        | -0.9550          | 0.3736             | -0.0771         | -28.3517       | -27.0753     | 2.2593          | 2.2595        |
| 0.8969        | 0.5859 | 300  | 0.8893          | -0.9749        | -0.8979          | 0.3714             | -0.0771         | -27.7806       | -26.5040     | 1.0067          | 1.0072        |
| 0.9063        | 0.6836 | 350  | 0.9050          | -1.2039        | -1.1361          | 0.4044             | -0.0679         | -30.1626       | -28.7939     | 0.7986          | 0.7986        |
| 0.7892        | 0.7812 | 400  | 0.8418          | -1.0320        | -1.0841          | 0.4330             | 0.0521          | -29.6434       | -27.0748     | 0.2635          | 0.2646        |
| 0.7866        | 0.8789 | 450  | 0.8482          | -1.6282        | -1.6894          | 0.4242             | 0.0612          | -35.6958       | -33.0367     | 0.4838          | 0.4849        |
| 0.8563        | 0.9766 | 500  | 0.8541          | -1.6971        | -1.7380          | 0.4132             | 0.0408          | -36.1819       | -33.7261     | 0.6780          | 0.6787        |
| 0.3046        | 1.0742 | 550  | 0.8749          | -1.7613        | -1.9274          | 0.4440             | 0.1661          | -38.0758       | -34.3676     | 0.6885          | 0.6888        |
| 0.3175        | 1.1719 | 600  | 0.9081          | -2.1900        | -2.4423          | 0.4615             | 0.2523          | -43.2246       | -38.6545     | 0.2508          | 0.2509        |
| 0.2851        | 1.2695 | 650  | 0.9462          | -2.4571        | -2.7801          | 0.4505             | 0.3231          | -46.6032       | -41.3252     | -0.1026         | -0.1026       |
| 0.4224        | 1.3672 | 700  | 0.9056          | -2.0324        | -2.3887          | 0.4637             | 0.3562          | -42.6887       | -37.0789     | -0.1502         | -0.1499       |
| 0.1386        | 1.4648 | 750  | 0.9267          | -2.5222        | -2.9198          | 0.4879             | 0.3975          | -47.9997       | -41.9771     | -0.2790         | -0.2789       |
| 0.2923        | 1.5625 | 800  | 0.9171          | -2.4324        | -2.8660          | 0.4813             | 0.4336          | -47.4622       | -41.0790     | -0.3229         | -0.3228       |
| 0.3088        | 1.6602 | 850  | 0.9140          | -2.4368        | -2.8842          | 0.4879             | 0.4474          | -47.6445       | -41.1227     | -0.3429         | -0.3428       |
| 0.2886        | 1.7578 | 900  | 0.9138          | -2.4524        | -2.9047          | 0.4879             | 0.4524          | -47.8490       | -41.2781     | -0.3427         | -0.3427       |
| 0.2677        | 1.8555 | 950  | 0.9151          | -2.4517        | -2.9016          | 0.4879             | 0.4499          | -47.8176       | -41.2713     | -0.3438         | -0.3437       |
| 0.2167        | 1.9531 | 1000 | 0.9140          | -2.4513        | -2.9029          | 0.4901             | 0.4516          | -47.8308       | -41.2673     | -0.3435         | -0.3434       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_01beta_1e5rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""3814546a9fb1abbd24bc396a33974f48903d9c2f"", ""last_modified"": ""2024-05-05 19:16:44+00:00"", ""created_at"": ""2024-05-05 19:11:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_01beta_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-05 19:16:44+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6637d9d2e4156d34a43f5ab4"", ""modelId"": ""tsavage68/chat_1000_STEPS_01beta_1e5rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_01beta_1e5rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_01beta_1e5rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_01beta_1e5rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_05beta_1e5rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_05beta_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_05beta_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9591
- Rewards/chosen: -2.3309
- Rewards/rejected: -2.7467
- Rewards/accuracies: 0.4703
- Rewards/margins: 0.4158
- Logps/rejected: -24.2954
- Logps/chosen: -21.4165
- Logits/rejected: 0.7996
- Logits/chosen: 0.8002

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.8761        | 0.0977 | 50   | 0.9959          | -1.8123        | -2.0646          | 0.4703             | 0.2523          | -22.9312       | -20.3793     | -0.5909         | -0.5908       |
| 1.3595        | 0.1953 | 100  | 1.6440          | -4.1891        | -4.1878          | 0.4000             | -0.0014         | -27.1775       | -25.1329     | 0.0807          | 0.0814        |
| 2.308         | 0.2930 | 150  | 1.9206          | -2.8317        | -2.4482          | 0.3560             | -0.3835         | -23.6984       | -22.4180     | -0.3052         | -0.3058       |
| 2.1704        | 0.3906 | 200  | 2.0223          | -1.4807        | -1.0616          | 0.3692             | -0.4191         | -20.9251       | -19.7160     | 0.1414          | 0.1417        |
| 1.9751        | 0.4883 | 250  | 2.0674          | -2.7530        | -2.4864          | 0.3824             | -0.2666         | -23.7748       | -22.2607     | 0.3122          | 0.3129        |
| 2.1664        | 0.5859 | 300  | 2.1396          | -3.3148        | -2.9537          | 0.3736             | -0.3610         | -24.7095       | -23.3841     | 0.7138          | 0.7139        |
| 2.3861        | 0.6836 | 350  | 2.4133          | -3.5559        | -3.0649          | 0.3868             | -0.4910         | -24.9319       | -23.8665     | 0.8230          | 0.8231        |
| 1.6234        | 0.7812 | 400  | 1.9885          | -2.7362        | -2.6712          | 0.4198             | -0.0650         | -24.1443       | -22.2270     | 0.0176          | 0.0179        |
| 2.1754        | 0.8789 | 450  | 2.0755          | -3.9759        | -3.8057          | 0.4044             | -0.1701         | -26.4135       | -24.7063     | -1.1567         | -1.1564       |
| 2.1709        | 0.9766 | 500  | 2.0516          | -2.8482        | -2.7256          | 0.4132             | -0.1227         | -24.2531       | -22.4511     | 0.0334          | 0.0339        |
| 0.4438        | 1.0742 | 550  | 1.9671          | -2.7066        | -2.6893          | 0.4154             | -0.0173         | -24.1807       | -22.1679     | 0.5746          | 0.5752        |
| 0.4123        | 1.1719 | 600  | 2.1253          | -2.7676        | -2.8681          | 0.4396             | 0.1005          | -24.5381       | -22.2898     | 0.6889          | 0.6894        |
| 0.4884        | 1.2695 | 650  | 2.0208          | -2.9445        | -3.1160          | 0.4484             | 0.1715          | -25.0340       | -22.6437     | 0.6377          | 0.6383        |
| 0.8103        | 1.3672 | 700  | 1.9313          | -2.2016        | -2.4993          | 0.4549             | 0.2977          | -23.8006       | -21.1578     | 0.7416          | 0.7422        |
| 0.6385        | 1.4648 | 750  | 1.9420          | -2.2243        | -2.5777          | 0.4593             | 0.3533          | -23.9573       | -21.2033     | 0.7943          | 0.7949        |
| 0.708         | 1.5625 | 800  | 1.9438          | -2.1910        | -2.5802          | 0.4527             | 0.3892          | -23.9624       | -21.1367     | 0.8128          | 0.8134        |
| 0.5451        | 1.6602 | 850  | 1.9599          | -2.3369        | -2.7540          | 0.4637             | 0.4171          | -24.3099       | -21.4285     | 0.8004          | 0.8010        |
| 0.5874        | 1.7578 | 900  | 1.9604          | -2.3346        | -2.7464          | 0.4615             | 0.4117          | -24.2947       | -21.4239     | 0.7988          | 0.7994        |
| 0.4059        | 1.8555 | 950  | 1.9599          | -2.3347        | -2.7487          | 0.4615             | 0.4141          | -24.2995       | -21.4240     | 0.7998          | 0.8003        |
| 0.2957        | 1.9531 | 1000 | 1.9591          | -2.3309        | -2.7467          | 0.4703             | 0.4158          | -24.2954       | -21.4165     | 0.7996          | 0.8002        |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_05beta_1e5rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""236b0c1e1918a9b113d195ea8ff8ec3f7ed300a2"", ""last_modified"": ""2024-05-05 19:41:33+00:00"", ""created_at"": ""2024-05-05 19:38:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_05beta_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-05 19:41:33+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6637e03b9c16723fb90c243d"", ""modelId"": ""tsavage68/chat_1000_STEPS_05beta_1e5rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_05beta_1e5rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_05beta_1e5rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_05beta_1e5rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_03beta_1e5rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_03beta_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_03beta_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4273
- Rewards/chosen: -2.7830
- Rewards/rejected: -3.1629
- Rewards/accuracies: 0.4571
- Rewards/margins: 0.3799
- Logps/rejected: -29.3449
- Logps/chosen: -26.0311
- Logits/rejected: 0.7969
- Logits/chosen: 0.7973

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.8463        | 0.0977 | 50   | 0.7942          | -1.4469        | -1.5636          | 0.4703             | 0.1167          | -24.0138       | -21.5775     | -0.4872         | -0.4869       |
| 1.0828        | 0.1953 | 100  | 1.2743          | -1.3396        | -1.1843          | 0.3714             | -0.1553         | -22.7497       | -21.2200     | 0.0456          | 0.0465        |
| 1.8211        | 0.2930 | 150  | 1.4868          | -4.4773        | -4.4154          | 0.3846             | -0.0618         | -33.5200       | -31.6789     | -0.0645         | -0.0641       |
| 1.5571        | 0.3906 | 200  | 1.3347          | -1.3781        | -1.2581          | 0.3714             | -0.1200         | -22.9957       | -21.3483     | 0.1053          | 0.1057        |
| 1.4698        | 0.4883 | 250  | 1.4544          | -2.2266        | -2.0161          | 0.3692             | -0.2105         | -25.5224       | -24.1767     | 0.6310          | 0.6312        |
| 1.3848        | 0.5859 | 300  | 1.4989          | -1.5043        | -1.2708          | 0.3407             | -0.2335         | -23.0381       | -21.7690     | 1.0175          | 1.0178        |
| 1.608         | 0.6836 | 350  | 1.5122          | -1.8704        | -1.6039          | 0.3626             | -0.2666         | -24.1482       | -22.9894     | 0.2808          | 0.2809        |
| 1.3065        | 0.7812 | 400  | 1.3363          | -2.6927        | -2.7556          | 0.4044             | 0.0630          | -27.9874       | -25.7301     | -0.0631         | -0.0627       |
| 1.3358        | 0.8789 | 450  | 1.4411          | -3.3344        | -3.3172          | 0.4132             | -0.0172         | -29.8592       | -27.8692     | 0.8838          | 0.8844        |
| 1.4118        | 0.9766 | 500  | 1.4283          | -3.1040        | -3.0583          | 0.3978             | -0.0457         | -28.9962       | -27.1012     | 1.4387          | 1.4389        |
| 0.3497        | 1.0742 | 550  | 1.4828          | -2.9569        | -3.0649          | 0.4220             | 0.1081          | -29.0184       | -26.6108     | 1.1567          | 1.1568        |
| 0.3298        | 1.1719 | 600  | 1.5122          | -2.8698        | -3.2356          | 0.4396             | 0.3658          | -29.5874       | -26.3207     | 0.9066          | 0.9067        |
| 0.339         | 1.2695 | 650  | 1.5409          | -3.8192        | -4.1085          | 0.4308             | 0.2893          | -32.4970       | -29.4854     | 0.7328          | 0.7330        |
| 0.6687        | 1.3672 | 700  | 1.4092          | -2.7766        | -3.0696          | 0.4352             | 0.2930          | -29.0340       | -26.0099     | 0.8641          | 0.8644        |
| 0.2321        | 1.4648 | 750  | 1.4316          | -2.7067        | -3.0246          | 0.4484             | 0.3179          | -28.8840       | -25.7770     | 0.8522          | 0.8525        |
| 0.5027        | 1.5625 | 800  | 1.4235          | -2.7303        | -3.0988          | 0.4549             | 0.3685          | -29.1314       | -25.8558     | 0.8129          | 0.8132        |
| 0.469         | 1.6602 | 850  | 1.4289          | -2.7971        | -3.1797          | 0.4549             | 0.3825          | -29.4008       | -26.0784     | 0.7913          | 0.7917        |
| 0.4367        | 1.7578 | 900  | 1.4267          | -2.7870        | -3.1689          | 0.4593             | 0.3819          | -29.3650       | -26.0447     | 0.7954          | 0.7957        |
| 0.3005        | 1.8555 | 950  | 1.4262          | -2.7846        | -3.1662          | 0.4593             | 0.3816          | -29.3560       | -26.0368     | 0.7965          | 0.7968        |
| 0.2266        | 1.9531 | 1000 | 1.4273          | -2.7830        | -3.1629          | 0.4571             | 0.3799          | -29.3449       | -26.0311     | 0.7969          | 0.7973        |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_03beta_1e5rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""0694a48534c7a7655183b574f537387b21018d87"", ""last_modified"": ""2024-05-05 19:44:49+00:00"", ""created_at"": ""2024-05-05 19:39:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_03beta_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-05 19:44:49+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6637e05b9c16723fb90c2b24"", ""modelId"": ""tsavage68/chat_1000_STEPS_03beta_1e5rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_03beta_1e5rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_03beta_1e5rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_03beta_1e5rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_01beta_1e6_rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_01beta_1e6_rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_01beta_1e6_rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6625
- Rewards/chosen: -0.4073
- Rewards/rejected: -0.5192
- Rewards/accuracies: 0.5077
- Rewards/margins: 0.1119
- Logps/rejected: -23.9940
- Logps/chosen: -20.8276
- Logits/rejected: -0.8077
- Logits/chosen: -0.8075

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6925        | 0.0977 | 50   | 0.6917          | 0.0117         | 0.0085           | 0.4659             | 0.0031          | -18.7166       | -16.6380     | -0.6015         | -0.6013       |
| 0.6776        | 0.1953 | 100  | 0.6812          | -0.0371        | -0.0646          | 0.5253             | 0.0275          | -19.4479       | -17.1259     | -0.6242         | -0.6241       |
| 0.6915        | 0.2930 | 150  | 0.6811          | -0.0700        | -0.1026          | 0.4945             | 0.0325          | -19.8275       | -17.4549     | -0.6198         | -0.6196       |
| 0.6953        | 0.3906 | 200  | 0.6773          | -0.0987        | -0.1411          | 0.5209             | 0.0424          | -20.2134       | -17.7417     | -0.6034         | -0.6031       |
| 0.6947        | 0.4883 | 250  | 0.6764          | -0.0437        | -0.0935          | 0.5143             | 0.0498          | -19.7369       | -17.1915     | -0.5932         | -0.5930       |
| 0.6739        | 0.5859 | 300  | 0.6712          | -0.1173        | -0.1856          | 0.5077             | 0.0682          | -20.6575       | -17.9280     | -0.6422         | -0.6420       |
| 0.6648        | 0.6836 | 350  | 0.6740          | -0.1818        | -0.2444          | 0.5187             | 0.0626          | -21.2460       | -18.5724     | -0.6126         | -0.6123       |
| 0.6576        | 0.7812 | 400  | 0.6680          | -0.1865        | -0.2647          | 0.5143             | 0.0782          | -21.4489       | -18.6195     | -0.6568         | -0.6566       |
| 0.6645        | 0.8789 | 450  | 0.6702          | -0.1619        | -0.2377          | 0.5077             | 0.0759          | -21.1794       | -18.3732     | -0.6161         | -0.6159       |
| 0.6531        | 0.9766 | 500  | 0.6707          | -0.2045        | -0.2840          | 0.4989             | 0.0795          | -21.6418       | -18.7995     | -0.6466         | -0.6464       |
| 0.4839        | 1.0742 | 550  | 0.6670          | -0.2362        | -0.3265          | 0.5209             | 0.0903          | -22.0674       | -19.1168     | -0.6941         | -0.6939       |
| 0.4713        | 1.1719 | 600  | 0.6664          | -0.3234        | -0.4195          | 0.5209             | 0.0961          | -22.9967       | -19.9882     | -0.7392         | -0.7389       |
| 0.4965        | 1.2695 | 650  | 0.6644          | -0.3536        | -0.4578          | 0.5121             | 0.1043          | -23.3802       | -20.2903     | -0.7732         | -0.7730       |
| 0.4232        | 1.3672 | 700  | 0.6632          | -0.3744        | -0.4833          | 0.5187             | 0.1088          | -23.6347       | -20.4989     | -0.7902         | -0.7899       |
| 0.4192        | 1.4648 | 750  | 0.6627          | -0.3915        | -0.5018          | 0.5143             | 0.1103          | -23.8201       | -20.6699     | -0.8012         | -0.8009       |
| 0.4946        | 1.5625 | 800  | 0.6629          | -0.3997        | -0.5103          | 0.5187             | 0.1106          | -23.9054       | -20.7517     | -0.8051         | -0.8049       |
| 0.4538        | 1.6602 | 850  | 0.6630          | -0.4061        | -0.5168          | 0.5143             | 0.1107          | -23.9701       | -20.8155     | -0.8072         | -0.8069       |
| 0.4873        | 1.7578 | 900  | 0.6630          | -0.4074        | -0.5183          | 0.5121             | 0.1109          | -23.9850       | -20.8283     | -0.8080         | -0.8078       |
| 0.4596        | 1.8555 | 950  | 0.6628          | -0.4070        | -0.5181          | 0.5143             | 0.1112          | -23.9834       | -20.8241     | -0.8078         | -0.8075       |
| 0.4361        | 1.9531 | 1000 | 0.6625          | -0.4073        | -0.5192          | 0.5077             | 0.1119          | -23.9940       | -20.8276     | -0.8077         | -0.8075       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_01beta_1e6_rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""df952ed6b1121106a150b8796ee523c3a48092e7"", ""last_modified"": ""2024-05-06 01:31:39+00:00"", ""created_at"": ""2024-05-06 01:26:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e6_rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_01beta_1e6_rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 01:31:39+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e6_rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663831c236f6e89578aa4b6a"", ""modelId"": ""tsavage68/chat_1000_STEPS_01beta_1e6_rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_01beta_1e6_rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_01beta_1e6_rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_01beta_1e6_rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_03beta_1e6rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_03beta_1e6rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_03beta_1e6rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6755
- Rewards/chosen: -0.5736
- Rewards/rejected: -0.7849
- Rewards/accuracies: 0.5121
- Rewards/margins: 0.2113
- Logps/rejected: -21.4183
- Logps/chosen: -18.6666
- Logits/rejected: -0.7004
- Logits/chosen: -0.7002

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6903        | 0.0977 | 50   | 0.6898          | 0.0339         | 0.0260           | 0.4264             | 0.0078          | -18.7152       | -16.6418     | -0.6000         | -0.5999       |
| 0.6568        | 0.1953 | 100  | 0.6714          | -0.1082        | -0.1762          | 0.5099             | 0.0680          | -19.3893       | -17.1151     | -0.6152         | -0.6151       |
| 0.7127        | 0.2930 | 150  | 0.6820          | -0.1152        | -0.1845          | 0.4879             | 0.0693          | -19.4168       | -17.1385     | -0.5988         | -0.5986       |
| 0.7008        | 0.3906 | 200  | 0.6810          | -0.1658        | -0.2536          | 0.5055             | 0.0878          | -19.6473       | -17.3074     | -0.5830         | -0.5828       |
| 0.7256        | 0.4883 | 250  | 0.6858          | -0.0964        | -0.2054          | 0.4923             | 0.1090          | -19.4867       | -17.0761     | -0.5766         | -0.5764       |
| 0.6817        | 0.5859 | 300  | 0.6762          | -0.2368        | -0.3883          | 0.5187             | 0.1515          | -20.0964       | -17.5440     | -0.6063         | -0.6061       |
| 0.6486        | 0.6836 | 350  | 0.6850          | -0.3387        | -0.4688          | 0.5055             | 0.1301          | -20.3646       | -17.8836     | -0.5899         | -0.5897       |
| 0.651         | 0.7812 | 400  | 0.6734          | -0.3143        | -0.4779          | 0.5275             | 0.1636          | -20.3950       | -17.8025     | -0.6197         | -0.6195       |
| 0.6761        | 0.8789 | 450  | 0.6825          | -0.1942        | -0.3362          | 0.5011             | 0.1420          | -19.9226       | -17.4020     | -0.5790         | -0.5788       |
| 0.6615        | 0.9766 | 500  | 0.6798          | -0.2233        | -0.3810          | 0.4967             | 0.1578          | -20.0720       | -17.4988     | -0.6050         | -0.6048       |
| 0.3298        | 1.0742 | 550  | 0.6743          | -0.2860        | -0.4658          | 0.5055             | 0.1798          | -20.3546       | -17.7080     | -0.6296         | -0.6294       |
| 0.3296        | 1.1719 | 600  | 0.6753          | -0.4100        | -0.5995          | 0.5099             | 0.1894          | -20.8002       | -18.1215     | -0.6547         | -0.6545       |
| 0.3571        | 1.2695 | 650  | 0.6753          | -0.4787        | -0.6784          | 0.5143             | 0.1998          | -21.0634       | -18.3502     | -0.6784         | -0.6782       |
| 0.254         | 1.3672 | 700  | 0.6750          | -0.5165        | -0.7231          | 0.5099             | 0.2066          | -21.2124       | -18.4763     | -0.6901         | -0.6899       |
| 0.2391        | 1.4648 | 750  | 0.6754          | -0.5562        | -0.7657          | 0.5187             | 0.2095          | -21.3543       | -18.6087     | -0.6964         | -0.6962       |
| 0.3665        | 1.5625 | 800  | 0.6750          | -0.5607        | -0.7724          | 0.5055             | 0.2117          | -21.3766       | -18.6235     | -0.6992         | -0.6990       |
| 0.315         | 1.6602 | 850  | 0.6758          | -0.5717        | -0.7824          | 0.5077             | 0.2106          | -21.4099       | -18.6604     | -0.7006         | -0.7004       |
| 0.3595        | 1.7578 | 900  | 0.6761          | -0.5738        | -0.7840          | 0.5077             | 0.2101          | -21.4152       | -18.6674     | -0.7007         | -0.7005       |
| 0.3196        | 1.8555 | 950  | 0.6747          | -0.5736        | -0.7866          | 0.5077             | 0.2130          | -21.4241       | -18.6667     | -0.7012         | -0.7010       |
| 0.2841        | 1.9531 | 1000 | 0.6755          | -0.5736        | -0.7849          | 0.5121             | 0.2113          | -21.4183       | -18.6666     | -0.7004         | -0.7002       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_03beta_1e6rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""9922298423c1f0c0edd05e86debebfce699eff15"", ""last_modified"": ""2024-05-06 01:36:50+00:00"", ""created_at"": ""2024-05-06 01:31:36+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_1e6rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_03beta_1e6rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 01:36:50+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_1e6rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663832f8d37124438e64f24f"", ""modelId"": ""tsavage68/chat_1000_STEPS_03beta_1e6rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_03beta_1e6rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_03beta_1e6rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_03beta_1e6rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_05beta_1e6rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_05beta_1e6rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_05beta_1e6rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7027
- Rewards/chosen: -0.5988
- Rewards/rejected: -0.8712
- Rewards/accuracies: 0.5099
- Rewards/margins: 0.2724
- Logps/rejected: -20.5443
- Logps/chosen: -17.9521
- Logits/rejected: -0.6530
- Logits/chosen: -0.6528

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6871        | 0.0977 | 50   | 0.6897          | 0.0517         | 0.0417           | 0.4352             | 0.0100          | -18.7185       | -16.6512     | -0.6010         | -0.6009       |
| 0.6399        | 0.1953 | 100  | 0.6728          | -0.1560        | -0.2548          | 0.5099             | 0.0989          | -19.3116       | -17.0666     | -0.6090         | -0.6089       |
| 0.7472        | 0.2930 | 150  | 0.6915          | -0.1391        | -0.2449          | 0.4725             | 0.1058          | -19.2918       | -17.0328     | -0.6010         | -0.6008       |
| 0.7204        | 0.3906 | 200  | 0.7024          | -0.2454        | -0.3692          | 0.4923             | 0.1239          | -19.5404       | -17.2454     | -0.5899         | -0.5897       |
| 0.7965        | 0.4883 | 250  | 0.7140          | -0.1153        | -0.2604          | 0.4835             | 0.1451          | -19.3228       | -16.9852     | -0.5793         | -0.5791       |
| 0.7139        | 0.5859 | 300  | 0.6956          | -0.2819        | -0.4872          | 0.5121             | 0.2052          | -19.7763       | -17.3185     | -0.6031         | -0.6029       |
| 0.6559        | 0.6836 | 350  | 0.7078          | -0.3820        | -0.5600          | 0.5099             | 0.1780          | -19.9219       | -17.5186     | -0.5854         | -0.5853       |
| 0.6696        | 0.7812 | 400  | 0.6942          | -0.3456        | -0.5617          | 0.5143             | 0.2160          | -19.9254       | -17.4459     | -0.6059         | -0.6057       |
| 0.7053        | 0.8789 | 450  | 0.7006          | -0.1577        | -0.3434          | 0.5033             | 0.1857          | -19.4887       | -17.0699     | -0.5689         | -0.5687       |
| 0.7143        | 0.9766 | 500  | 0.7009          | -0.1658        | -0.3724          | 0.5055             | 0.2066          | -19.5467       | -17.0862     | -0.5925         | -0.5923       |
| 0.2701        | 1.0742 | 550  | 0.6978          | -0.2341        | -0.4595          | 0.5121             | 0.2254          | -19.7210       | -17.2229     | -0.6038         | -0.6036       |
| 0.2867        | 1.1719 | 600  | 0.6987          | -0.3718        | -0.6187          | 0.5077             | 0.2469          | -20.0393       | -17.4982     | -0.6186         | -0.6184       |
| 0.3128        | 1.2695 | 650  | 0.7018          | -0.4995        | -0.7601          | 0.5055             | 0.2605          | -20.3221       | -17.7537     | -0.6358         | -0.6356       |
| 0.1953        | 1.3672 | 700  | 0.7004          | -0.5365        | -0.8019          | 0.5165             | 0.2653          | -20.4057       | -17.8277     | -0.6439         | -0.6437       |
| 0.1831        | 1.4648 | 750  | 0.7014          | -0.5837        | -0.8544          | 0.5143             | 0.2707          | -20.5108       | -17.9220     | -0.6497         | -0.6495       |
| 0.3253        | 1.5625 | 800  | 0.7019          | -0.5862        | -0.8575          | 0.5077             | 0.2713          | -20.5169       | -17.9270     | -0.6514         | -0.6512       |
| 0.2709        | 1.6602 | 850  | 0.7043          | -0.6000        | -0.8668          | 0.5077             | 0.2668          | -20.5356       | -17.9547     | -0.6522         | -0.6520       |
| 0.3225        | 1.7578 | 900  | 0.7035          | -0.6017        | -0.8716          | 0.5033             | 0.2699          | -20.5452       | -17.9580     | -0.6530         | -0.6528       |
| 0.2806        | 1.8555 | 950  | 0.7020          | -0.5987        | -0.8741          | 0.5121             | 0.2754          | -20.5502       | -17.9521     | -0.6531         | -0.6529       |
| 0.2262        | 1.9531 | 1000 | 0.7027          | -0.5988        | -0.8712          | 0.5099             | 0.2724          | -20.5443       | -17.9521     | -0.6530         | -0.6528       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_05beta_1e6rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""d84423230484dbe9496859cffe621ddb5d249c36"", ""last_modified"": ""2024-05-06 01:35:37+00:00"", ""created_at"": ""2024-05-06 01:32:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e6rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_05beta_1e6rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 01:35:37+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e6rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66383335c78619ba637c6145"", ""modelId"": ""tsavage68/chat_1000_STEPS_05beta_1e6rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_05beta_1e6rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_05beta_1e6rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_05beta_1e6rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_400_STEPS_05beta_1e6rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_400_STEPS_05beta_1e6rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_400_STEPS_05beta_1e6rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6853
- Rewards/chosen: -0.1288
- Rewards/rejected: -0.2807
- Rewards/accuracies: 0.5143
- Rewards/margins: 0.1518
- Logps/rejected: -19.3633
- Logps/chosen: -17.0123
- Logits/rejected: -0.5890
- Logits/chosen: -0.5888

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 400

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6871        | 0.0977 | 50   | 0.6897          | 0.0517         | 0.0417           | 0.4352             | 0.0100          | -18.7185       | -16.6512     | -0.6010         | -0.6009       |
| 0.6399        | 0.1953 | 100  | 0.6728          | -0.1560        | -0.2548          | 0.5099             | 0.0989          | -19.3116       | -17.0666     | -0.6090         | -0.6089       |
| 0.752         | 0.2930 | 150  | 0.6985          | -0.1949        | -0.2845          | 0.4505             | 0.0896          | -19.3710       | -17.1445     | -0.5936         | -0.5934       |
| 0.713         | 0.3906 | 200  | 0.6945          | -0.1538        | -0.2727          | 0.4923             | 0.1188          | -19.3473       | -17.0623     | -0.5881         | -0.5879       |
| 0.7476        | 0.4883 | 250  | 0.6974          | -0.1319        | -0.2605          | 0.5165             | 0.1286          | -19.3230       | -17.0185     | -0.5854         | -0.5852       |
| 0.6906        | 0.5859 | 300  | 0.6883          | -0.1320        | -0.2782          | 0.5165             | 0.1461          | -19.3583       | -17.0187     | -0.5910         | -0.5909       |
| 0.6808        | 0.6836 | 350  | 0.6861          | -0.1290        | -0.2784          | 0.5077             | 0.1494          | -19.3587       | -17.0125     | -0.5888         | -0.5887       |
| 0.6476        | 0.7812 | 400  | 0.6853          | -0.1288        | -0.2807          | 0.5143             | 0.1518          | -19.3633       | -17.0123     | -0.5890         | -0.5888       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_400_STEPS_05beta_1e6rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""199c768038795b79e30ed05c487b77798bdeffd0"", ""last_modified"": ""2024-05-06 04:39:10+00:00"", ""created_at"": ""2024-05-06 04:35:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_05beta_1e6rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_400_STEPS_05beta_1e6rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 04:39:10+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_05beta_1e6rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66385e1c362d1be020b3b09a"", ""modelId"": ""tsavage68/chat_400_STEPS_05beta_1e6rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_400_STEPS_05beta_1e6rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_400_STEPS_05beta_1e6rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_400_STEPS_05beta_1e6rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_550_STEPS_01beta_1e6_rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_550_STEPS_01beta_1e6_rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_550_STEPS_01beta_1e6_rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6716
- Rewards/chosen: -0.1192
- Rewards/rejected: -0.1802
- Rewards/accuracies: 0.5253
- Rewards/margins: 0.0610
- Logps/rejected: -20.6044
- Logps/chosen: -17.9469
- Logits/rejected: -0.6222
- Logits/chosen: -0.6220

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 550

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6925        | 0.0977 | 50   | 0.6917          | 0.0117         | 0.0085           | 0.4659             | 0.0031          | -18.7166       | -16.6380     | -0.6015         | -0.6013       |
| 0.6776        | 0.1953 | 100  | 0.6812          | -0.0371        | -0.0646          | 0.5253             | 0.0275          | -19.4479       | -17.1259     | -0.6242         | -0.6241       |
| 0.6927        | 0.2930 | 150  | 0.6819          | -0.0802        | -0.1112          | 0.5011             | 0.0310          | -19.9140       | -17.5569     | -0.6222         | -0.6220       |
| 0.6928        | 0.3906 | 200  | 0.6776          | -0.1032        | -0.1444          | 0.5033             | 0.0412          | -20.2463       | -17.7865     | -0.6050         | -0.6048       |
| 0.6937        | 0.4883 | 250  | 0.6762          | -0.0643        | -0.1121          | 0.5121             | 0.0478          | -19.9228       | -17.3977     | -0.6013         | -0.6011       |
| 0.6758        | 0.5859 | 300  | 0.6717          | -0.1055        | -0.1663          | 0.5231             | 0.0608          | -20.4645       | -17.8094     | -0.6301         | -0.6299       |
| 0.6696        | 0.6836 | 350  | 0.6724          | -0.1144        | -0.1731          | 0.5275             | 0.0587          | -20.5330       | -17.8991     | -0.6162         | -0.6160       |
| 0.6587        | 0.7812 | 400  | 0.6711          | -0.1221        | -0.1842          | 0.5297             | 0.0621          | -20.6441       | -17.9756     | -0.6249         | -0.6247       |
| 0.6755        | 0.8789 | 450  | 0.6713          | -0.1178        | -0.1794          | 0.5341             | 0.0616          | -20.5960       | -17.9326     | -0.6214         | -0.6212       |
| 0.6637        | 0.9766 | 500  | 0.6712          | -0.1188        | -0.1808          | 0.5253             | 0.0620          | -20.6100       | -17.9427     | -0.6222         | -0.6220       |
| 0.5575        | 1.0742 | 550  | 0.6716          | -0.1192        | -0.1802          | 0.5253             | 0.0610          | -20.6044       | -17.9469     | -0.6222         | -0.6220       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_550_STEPS_01beta_1e6_rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""c1f5637d12d56905e20d5ea9db35c895aa5672c1"", ""last_modified"": ""2024-05-06 05:25:40+00:00"", ""created_at"": ""2024-05-06 05:14:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_550_STEPS_01beta_1e6_rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_550_STEPS_01beta_1e6_rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 05:25:40+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_550_STEPS_01beta_1e6_rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663867365ea4c28a8b7cda77"", ""modelId"": ""tsavage68/chat_550_STEPS_01beta_1e6_rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_550_STEPS_01beta_1e6_rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_550_STEPS_01beta_1e6_rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_550_STEPS_01beta_1e6_rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_700_STEPS_03beta_1e6rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_700_STEPS_03beta_1e6rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_700_STEPS_03beta_1e6rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6706
- Rewards/chosen: -0.2188
- Rewards/rejected: -0.3671
- Rewards/accuracies: 0.5143
- Rewards/margins: 0.1484
- Logps/rejected: -20.0258
- Logps/chosen: -17.4839
- Logits/rejected: -0.6007
- Logits/chosen: -0.6005

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 700

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6903        | 0.0977 | 50   | 0.6898          | 0.0339         | 0.0260           | 0.4264             | 0.0078          | -18.7152       | -16.6418     | -0.6000         | -0.5999       |
| 0.6568        | 0.1953 | 100  | 0.6714          | -0.1082        | -0.1762          | 0.5099             | 0.0680          | -19.3893       | -17.1151     | -0.6152         | -0.6151       |
| 0.7125        | 0.2930 | 150  | 0.6838          | -0.1101        | -0.1755          | 0.4791             | 0.0653          | -19.3869       | -17.1217     | -0.5952         | -0.5950       |
| 0.7095        | 0.3906 | 200  | 0.6820          | -0.1564        | -0.2410          | 0.5055             | 0.0846          | -19.6053       | -17.2759     | -0.5844         | -0.5842       |
| 0.7264        | 0.4883 | 250  | 0.6859          | -0.0974        | -0.1989          | 0.4967             | 0.1016          | -19.4651       | -17.0792     | -0.5778         | -0.5776       |
| 0.6767        | 0.5859 | 300  | 0.6737          | -0.2009        | -0.3435          | 0.5121             | 0.1426          | -19.9470       | -17.4243     | -0.6046         | -0.6044       |
| 0.6546        | 0.6836 | 350  | 0.6776          | -0.2753        | -0.4068          | 0.5033             | 0.1316          | -20.1581       | -17.6722     | -0.5869         | -0.5867       |
| 0.6473        | 0.7812 | 400  | 0.6697          | -0.2700        | -0.4199          | 0.5209             | 0.1499          | -20.2016       | -17.6546     | -0.6084         | -0.6082       |
| 0.68          | 0.8789 | 450  | 0.6720          | -0.2073        | -0.3505          | 0.5121             | 0.1432          | -19.9703       | -17.4455     | -0.5885         | -0.5883       |
| 0.6626        | 0.9766 | 500  | 0.6726          | -0.2140        | -0.3584          | 0.5099             | 0.1444          | -19.9967       | -17.4681     | -0.5948         | -0.5946       |
| 0.3861        | 1.0742 | 550  | 0.6702          | -0.2078        | -0.3569          | 0.5209             | 0.1492          | -19.9917       | -17.4471     | -0.5992         | -0.5990       |
| 0.4031        | 1.1719 | 600  | 0.6720          | -0.2186        | -0.3641          | 0.5121             | 0.1455          | -20.0158       | -17.4834     | -0.6004         | -0.6002       |
| 0.4139        | 1.2695 | 650  | 0.6703          | -0.2170        | -0.3648          | 0.5121             | 0.1478          | -20.0179       | -17.4778     | -0.6006         | -0.6004       |
| 0.3251        | 1.3672 | 700  | 0.6706          | -0.2188        | -0.3671          | 0.5143             | 0.1484          | -20.0258       | -17.4839     | -0.6007         | -0.6005       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_700_STEPS_03beta_1e6rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""4df1b76ee862683c80c02a9a7333c21b0975b717"", ""last_modified"": ""2024-05-06 06:01:30+00:00"", ""created_at"": ""2024-05-06 05:55:01+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_700_STEPS_03beta_1e6rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_700_STEPS_03beta_1e6rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 06:01:30+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_700_STEPS_03beta_1e6rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663870b54fe4d2b6c4676586"", ""modelId"": ""tsavage68/chat_700_STEPS_03beta_1e6rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_700_STEPS_03beta_1e6rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_700_STEPS_03beta_1e6rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_700_STEPS_03beta_1e6rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_01beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_01beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_01beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6923
- Rewards/chosen: -0.0014
- Rewards/rejected: -0.0031
- Rewards/accuracies: 0.4352
- Rewards/margins: 0.0018
- Logps/rejected: -18.8334
- Logps/chosen: -16.7684
- Logits/rejected: -0.5994
- Logits/chosen: -0.5993

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6944        | 0.0977 | 50   | 0.6937          | -0.0002        | 0.0007           | 0.3846             | -0.0010         | -18.7946       | -16.7570     | -0.5974         | -0.5972       |
| 0.6929        | 0.1953 | 100  | 0.6932          | -0.0013        | -0.0013          | 0.4352             | 0.0000          | -18.8149       | -16.7673     | -0.5987         | -0.5985       |
| 0.6937        | 0.2930 | 150  | 0.6929          | -0.0008        | -0.0013          | 0.4242             | 0.0005          | -18.8152       | -16.7631     | -0.5980         | -0.5979       |
| 0.6909        | 0.3906 | 200  | 0.6929          | -0.0011        | -0.0016          | 0.4110             | 0.0005          | -18.8177       | -16.7654     | -0.5980         | -0.5979       |
| 0.6939        | 0.4883 | 250  | 0.6925          | -0.0009        | -0.0022          | 0.4527             | 0.0013          | -18.8240       | -16.7635     | -0.5982         | -0.5981       |
| 0.6914        | 0.5859 | 300  | 0.6925          | -0.0020        | -0.0035          | 0.4308             | 0.0014          | -18.8366       | -16.7748     | -0.5990         | -0.5989       |
| 0.6922        | 0.6836 | 350  | 0.6926          | -0.0031        | -0.0043          | 0.4527             | 0.0012          | -18.8453       | -16.7857     | -0.5985         | -0.5984       |
| 0.6926        | 0.7812 | 400  | 0.6924          | -0.0021        | -0.0036          | 0.4440             | 0.0015          | -18.8380       | -16.7757     | -0.5992         | -0.5991       |
| 0.6912        | 0.8789 | 450  | 0.6922          | -0.0021        | -0.0041          | 0.4615             | 0.0021          | -18.8432       | -16.7752     | -0.5984         | -0.5982       |
| 0.6918        | 0.9766 | 500  | 0.6921          | -0.0018        | -0.0040          | 0.4418             | 0.0022          | -18.8422       | -16.7723     | -0.5986         | -0.5985       |
| 0.69          | 1.0742 | 550  | 0.6918          | -0.0017        | -0.0045          | 0.4637             | 0.0028          | -18.8469       | -16.7718     | -0.5988         | -0.5987       |
| 0.6882        | 1.1719 | 600  | 0.6923          | -0.0013        | -0.0031          | 0.4659             | 0.0018          | -18.8330       | -16.7675     | -0.5994         | -0.5993       |
| 0.6887        | 1.2695 | 650  | 0.6924          | -0.0019        | -0.0036          | 0.4308             | 0.0016          | -18.8375       | -16.7739     | -0.5988         | -0.5987       |
| 0.6886        | 1.3672 | 700  | 0.6918          | -0.0003        | -0.0030          | 0.4549             | 0.0028          | -18.8325       | -16.7572     | -0.5991         | -0.5989       |
| 0.6876        | 1.4648 | 750  | 0.6919          | -0.0005        | -0.0031          | 0.4725             | 0.0026          | -18.8327       | -16.7592     | -0.5994         | -0.5993       |
| 0.6921        | 1.5625 | 800  | 0.6914          | -0.0001        | -0.0038          | 0.4725             | 0.0037          | -18.8396       | -16.7556     | -0.5994         | -0.5992       |
| 0.6882        | 1.6602 | 850  | 0.6920          | -0.0006        | -0.0029          | 0.4945             | 0.0023          | -18.8307       | -16.7602     | -0.5996         | -0.5994       |
| 0.69          | 1.7578 | 900  | 0.6920          | -0.0010        | -0.0033          | 0.4505             | 0.0023          | -18.8350       | -16.7647     | -0.5995         | -0.5993       |
| 0.6888        | 1.8555 | 950  | 0.6923          | -0.0014        | -0.0032          | 0.4352             | 0.0018          | -18.8340       | -16.7686     | -0.5994         | -0.5993       |
| 0.6878        | 1.9531 | 1000 | 0.6923          | -0.0014        | -0.0031          | 0.4352             | 0.0018          | -18.8334       | -16.7684     | -0.5994         | -0.5993       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_01beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""8aef45c21abb9a042de1abf89cbeb219f5757145"", ""last_modified"": ""2024-05-06 18:44:26+00:00"", ""created_at"": ""2024-05-06 18:40:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_01beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 18:44:26+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639241a34d8810d10092746"", ""modelId"": ""tsavage68/chat_1000_STEPS_01beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_01beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_01beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_01beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_03beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_03beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_03beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6892
- Rewards/chosen: -0.0034
- Rewards/rejected: -0.0121
- Rewards/accuracies: 0.4725
- Rewards/margins: 0.0086
- Logps/rejected: -18.8422
- Logps/chosen: -16.7661
- Logits/rejected: -0.5986
- Logits/chosen: -0.5984

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6925        | 0.0977 | 50   | 0.6943          | 0.0013         | 0.0029           | 0.3846             | -0.0016         | -18.7922       | -16.7503     | -0.5979         | -0.5978       |
| 0.6919        | 0.1953 | 100  | 0.6932          | -0.0001        | -0.0007          | 0.4110             | 0.0005          | -18.8042       | -16.7551     | -0.5986         | -0.5985       |
| 0.6942        | 0.2930 | 150  | 0.6933          | -0.0039        | -0.0042          | 0.4176             | 0.0003          | -18.8160       | -16.7678     | -0.5979         | -0.5977       |
| 0.6964        | 0.3906 | 200  | 0.6932          | -0.0035        | -0.0040          | 0.4352             | 0.0005          | -18.8154       | -16.7662     | -0.5984         | -0.5983       |
| 0.6945        | 0.4883 | 250  | 0.6914          | -0.0028        | -0.0069          | 0.4505             | 0.0041          | -18.8249       | -16.7639     | -0.5977         | -0.5976       |
| 0.6906        | 0.5859 | 300  | 0.6920          | -0.0066        | -0.0096          | 0.4440             | 0.0031          | -18.8341       | -16.7765     | -0.5985         | -0.5984       |
| 0.6871        | 0.6836 | 350  | 0.6906          | -0.0055        | -0.0114          | 0.4440             | 0.0059          | -18.8400       | -16.7730     | -0.5982         | -0.5981       |
| 0.6889        | 0.7812 | 400  | 0.6897          | -0.0066        | -0.0143          | 0.4703             | 0.0076          | -18.8495       | -16.7768     | -0.5990         | -0.5989       |
| 0.689         | 0.8789 | 450  | 0.6905          | -0.0053        | -0.0115          | 0.4396             | 0.0063          | -18.8404       | -16.7722     | -0.5986         | -0.5984       |
| 0.6915        | 0.9766 | 500  | 0.6896          | -0.0031        | -0.0110          | 0.4681             | 0.0079          | -18.8388       | -16.7650     | -0.5990         | -0.5989       |
| 0.6834        | 1.0742 | 550  | 0.6906          | -0.0031        | -0.0091          | 0.4418             | 0.0060          | -18.8323       | -16.7650     | -0.5987         | -0.5986       |
| 0.683         | 1.1719 | 600  | 0.6894          | -0.0041        | -0.0125          | 0.4615             | 0.0084          | -18.8437       | -16.7683     | -0.5991         | -0.5990       |
| 0.6814        | 1.2695 | 650  | 0.6890          | -0.0031        | -0.0123          | 0.4681             | 0.0092          | -18.8430       | -16.7650     | -0.5992         | -0.5991       |
| 0.6811        | 1.3672 | 700  | 0.6895          | -0.0025        | -0.0108          | 0.4703             | 0.0083          | -18.8379       | -16.7630     | -0.5991         | -0.5989       |
| 0.6803        | 1.4648 | 750  | 0.6907          | -0.0024        | -0.0081          | 0.4242             | 0.0057          | -18.8289       | -16.7626     | -0.5983         | -0.5982       |
| 0.6836        | 1.5625 | 800  | 0.6911          | -0.0028        | -0.0078          | 0.4549             | 0.0050          | -18.8281       | -16.7640     | -0.5989         | -0.5987       |
| 0.6774        | 1.6602 | 850  | 0.6904          | -0.0039        | -0.0103          | 0.4484             | 0.0064          | -18.8363       | -16.7677     | -0.5988         | -0.5987       |
| 0.6866        | 1.7578 | 900  | 0.6875          | -0.0009        | -0.0130          | 0.4769             | 0.0121          | -18.8454       | -16.7576     | -0.5987         | -0.5986       |
| 0.6811        | 1.8555 | 950  | 0.6892          | -0.0034        | -0.0121          | 0.4725             | 0.0086          | -18.8422       | -16.7661     | -0.5986         | -0.5984       |
| 0.6812        | 1.9531 | 1000 | 0.6892          | -0.0034        | -0.0121          | 0.4725             | 0.0086          | -18.8422       | -16.7661     | -0.5986         | -0.5984       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_03beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""f8a8cc2f26d2bc594fb6b0022ee7f7faf8d36083"", ""last_modified"": ""2024-05-06 19:10:51+00:00"", ""created_at"": ""2024-05-06 19:05:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_03beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 19:10:51+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663929ebd5ef8849fe52c9a9"", ""modelId"": ""tsavage68/chat_1000_STEPS_03beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_03beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_03beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_03beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_05beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_05beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_05beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6899
- Rewards/chosen: -0.0048
- Rewards/rejected: -0.0138
- Rewards/accuracies: 0.4527
- Rewards/margins: 0.0090
- Logps/rejected: -18.8295
- Logps/chosen: -16.7641
- Logits/rejected: -0.5988
- Logits/chosen: -0.5987

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6929        | 0.0977 | 50   | 0.6947          | -0.0000        | 0.0016           | 0.4066             | -0.0016         | -18.7989       | -16.7547     | -0.5985         | -0.5983       |
| 0.694         | 0.1953 | 100  | 0.6903          | 0.0030         | -0.0047          | 0.4527             | 0.0076          | -18.8113       | -16.7487     | -0.5976         | -0.5975       |
| 0.6922        | 0.2930 | 150  | 0.6941          | -0.0056        | -0.0053          | 0.4044             | -0.0003         | -18.8127       | -16.7659     | -0.5978         | -0.5977       |
| 0.7012        | 0.3906 | 200  | 0.6957          | -0.0099        | -0.0065          | 0.4132             | -0.0034         | -18.8151       | -16.7744     | -0.5982         | -0.5980       |
| 0.6992        | 0.4883 | 250  | 0.6932          | -0.0081        | -0.0099          | 0.4484             | 0.0017          | -18.8217       | -16.7709     | -0.5975         | -0.5974       |
| 0.6872        | 0.5859 | 300  | 0.6918          | -0.0096        | -0.0144          | 0.4440             | 0.0048          | -18.8309       | -16.7738     | -0.5990         | -0.5989       |
| 0.6875        | 0.6836 | 350  | 0.6894          | -0.0116        | -0.0209          | 0.4484             | 0.0093          | -18.8438       | -16.7778     | -0.5985         | -0.5984       |
| 0.6918        | 0.7812 | 400  | 0.6878          | -0.0070        | -0.0200          | 0.4462             | 0.0129          | -18.8419       | -16.7687     | -0.5987         | -0.5985       |
| 0.6868        | 0.8789 | 450  | 0.6897          | -0.0052        | -0.0141          | 0.4396             | 0.0089          | -18.8302       | -16.7651     | -0.5982         | -0.5981       |
| 0.6867        | 0.9766 | 500  | 0.6904          | -0.0080        | -0.0160          | 0.4176             | 0.0080          | -18.8339       | -16.7706     | -0.5988         | -0.5987       |
| 0.6744        | 1.0742 | 550  | 0.6883          | -0.0035        | -0.0157          | 0.4527             | 0.0123          | -18.8334       | -16.7616     | -0.5985         | -0.5984       |
| 0.6791        | 1.1719 | 600  | 0.6897          | -0.0033        | -0.0127          | 0.4484             | 0.0094          | -18.8275       | -16.7612     | -0.5988         | -0.5987       |
| 0.6793        | 1.2695 | 650  | 0.6887          | -0.0077        | -0.0191          | 0.4418             | 0.0114          | -18.8402       | -16.7700     | -0.5985         | -0.5983       |
| 0.6696        | 1.3672 | 700  | 0.6863          | -0.0015        | -0.0176          | 0.4527             | 0.0161          | -18.8372       | -16.7576     | -0.5988         | -0.5986       |
| 0.6689        | 1.4648 | 750  | 0.6873          | -0.0024        | -0.0167          | 0.4593             | 0.0143          | -18.8353       | -16.7594     | -0.5983         | -0.5982       |
| 0.6808        | 1.5625 | 800  | 0.6879          | -0.0050        | -0.0179          | 0.4637             | 0.0129          | -18.8378       | -16.7646     | -0.5992         | -0.5991       |
| 0.6718        | 1.6602 | 850  | 0.6902          | -0.0058        | -0.0139          | 0.4462             | 0.0082          | -18.8299       | -16.7662     | -0.5985         | -0.5984       |
| 0.678         | 1.7578 | 900  | 0.6872          | -0.0008        | -0.0151          | 0.4571             | 0.0144          | -18.8323       | -16.7562     | -0.5989         | -0.5988       |
| 0.6745        | 1.8555 | 950  | 0.6899          | -0.0048        | -0.0138          | 0.4527             | 0.0090          | -18.8295       | -16.7641     | -0.5988         | -0.5987       |
| 0.6759        | 1.9531 | 1000 | 0.6899          | -0.0048        | -0.0138          | 0.4527             | 0.0090          | -18.8295       | -16.7641     | -0.5988         | -0.5987       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_05beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""b9bc9323255fd5b4243d433a5c96bf67a61c4644"", ""last_modified"": ""2024-05-06 19:10:42+00:00"", ""created_at"": ""2024-05-06 19:06:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 13, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_05beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 19:10:42+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66392a37bb723d459d895e11"", ""modelId"": ""tsavage68/chat_1000_STEPS_05beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_05beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_05beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_05beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_400_STEPS_05beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_400_STEPS_05beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_400_STEPS_05beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6919
- Rewards/chosen: -0.0067
- Rewards/rejected: -0.0110
- Rewards/accuracies: 0.4308
- Rewards/margins: 0.0043
- Logps/rejected: -18.8240
- Logps/chosen: -16.7680
- Logits/rejected: -0.5983
- Logits/chosen: -0.5982

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 400

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6929        | 0.0977 | 50   | 0.6947          | -0.0000        | 0.0016           | 0.4066             | -0.0016         | -18.7989       | -16.7547     | -0.5985         | -0.5983       |
| 0.694         | 0.1953 | 100  | 0.6903          | 0.0030         | -0.0047          | 0.4527             | 0.0076          | -18.8113       | -16.7487     | -0.5976         | -0.5975       |
| 0.6941        | 0.2930 | 150  | 0.6914          | -0.0038        | -0.0090          | 0.4330             | 0.0052          | -18.8200       | -16.7622     | -0.5986         | -0.5985       |
| 0.6934        | 0.3906 | 200  | 0.6941          | -0.0035        | -0.0032          | 0.4044             | -0.0003         | -18.8084       | -16.7616     | -0.5978         | -0.5977       |
| 0.6963        | 0.4883 | 250  | 0.6932          | -0.0058        | -0.0078          | 0.3890             | 0.0019          | -18.8175       | -16.7663     | -0.5978         | -0.5977       |
| 0.6898        | 0.5859 | 300  | 0.6900          | -0.0039        | -0.0122          | 0.4330             | 0.0084          | -18.8264       | -16.7624     | -0.5980         | -0.5979       |
| 0.6856        | 0.6836 | 350  | 0.6923          | -0.0075        | -0.0109          | 0.4571             | 0.0034          | -18.8237       | -16.7695     | -0.5975         | -0.5974       |
| 0.6978        | 0.7812 | 400  | 0.6919          | -0.0067        | -0.0110          | 0.4308             | 0.0043          | -18.8240       | -16.7680     | -0.5983         | -0.5982       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_400_STEPS_05beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""4ff5ca10abed115f5f76d1b81d4fd0e3c17f8f80"", ""last_modified"": ""2024-05-06 21:03:47+00:00"", ""created_at"": ""2024-05-06 21:00:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_05beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_400_STEPS_05beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 21:03:47+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_05beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663944e006b25a7ea6bfb948"", ""modelId"": ""tsavage68/chat_400_STEPS_05beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_400_STEPS_05beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_400_STEPS_05beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_400_STEPS_05beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_550_STEPS_01beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_550_STEPS_01beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_550_STEPS_01beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6924
- Rewards/chosen: -0.0020
- Rewards/rejected: -0.0034
- Rewards/accuracies: 0.4505
- Rewards/margins: 0.0015
- Logps/rejected: -18.8364
- Logps/chosen: -16.7742
- Logits/rejected: -0.5980
- Logits/chosen: -0.5979

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 550

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6944        | 0.0977 | 50   | 0.6937          | -0.0002        | 0.0007           | 0.3846             | -0.0010         | -18.7946       | -16.7570     | -0.5974         | -0.5972       |
| 0.6929        | 0.1953 | 100  | 0.6932          | -0.0013        | -0.0013          | 0.4352             | 0.0000          | -18.8149       | -16.7673     | -0.5987         | -0.5985       |
| 0.693         | 0.2930 | 150  | 0.6929          | -0.0015        | -0.0021          | 0.4264             | 0.0006          | -18.8229       | -16.7696     | -0.5983         | -0.5982       |
| 0.6939        | 0.3906 | 200  | 0.6934          | -0.0009        | -0.0005          | 0.4000             | -0.0004         | -18.8074       | -16.7637     | -0.5983         | -0.5982       |
| 0.6942        | 0.4883 | 250  | 0.6928          | -0.0014        | -0.0021          | 0.4352             | 0.0008          | -18.8233       | -16.7684     | -0.5976         | -0.5974       |
| 0.6928        | 0.5859 | 300  | 0.6929          | -0.0025        | -0.0032          | 0.4462             | 0.0006          | -18.8338       | -16.7801     | -0.5984         | -0.5983       |
| 0.6912        | 0.6836 | 350  | 0.6929          | -0.0025        | -0.0031          | 0.4198             | 0.0005          | -18.8325       | -16.7799     | -0.5977         | -0.5976       |
| 0.6928        | 0.7812 | 400  | 0.6926          | -0.0019        | -0.0030          | 0.4484             | 0.0011          | -18.8320       | -16.7736     | -0.5986         | -0.5984       |
| 0.6915        | 0.8789 | 450  | 0.6928          | -0.0022        | -0.0029          | 0.4396             | 0.0007          | -18.8311       | -16.7765     | -0.5989         | -0.5987       |
| 0.6925        | 0.9766 | 500  | 0.6928          | -0.0024        | -0.0032          | 0.4088             | 0.0008          | -18.8341       | -16.7791     | -0.5980         | -0.5978       |
| 0.6916        | 1.0742 | 550  | 0.6924          | -0.0020        | -0.0034          | 0.4505             | 0.0015          | -18.8364       | -16.7742     | -0.5980         | -0.5979       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_550_STEPS_01beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""c937215613a3e7ce5f8d560cb30d8cbf9bbe267d"", ""last_modified"": ""2024-05-06 21:15:23+00:00"", ""created_at"": ""2024-05-06 21:11:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_550_STEPS_01beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_550_STEPS_01beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 21:15:23+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_550_STEPS_01beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639478c1d39aa6896c9d6a9"", ""modelId"": ""tsavage68/chat_550_STEPS_01beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_550_STEPS_01beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_550_STEPS_01beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_550_STEPS_01beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_650_STEPS_03beta_1e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_650_STEPS_03beta_1e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_650_STEPS_03beta_1e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6935
- Rewards/chosen: -0.0079
- Rewards/rejected: -0.0079
- Rewards/accuracies: 0.4286
- Rewards/margins: -0.0000
- Logps/rejected: -18.8283
- Logps/chosen: -16.7810
- Logits/rejected: -0.5983
- Logits/chosen: -0.5982

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 650

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6925        | 0.0977 | 50   | 0.6943          | 0.0013         | 0.0029           | 0.3846             | -0.0016         | -18.7922       | -16.7503     | -0.5979         | -0.5978       |
| 0.6919        | 0.1953 | 100  | 0.6932          | -0.0001        | -0.0007          | 0.4110             | 0.0005          | -18.8042       | -16.7551     | -0.5986         | -0.5985       |
| 0.6907        | 0.2930 | 150  | 0.6939          | -0.0044        | -0.0036          | 0.4198             | -0.0008         | -18.8141       | -16.7693     | -0.5983         | -0.5982       |
| 0.6943        | 0.3906 | 200  | 0.6931          | -0.0045        | -0.0052          | 0.4198             | 0.0007          | -18.8195       | -16.7697     | -0.5976         | -0.5975       |
| 0.6956        | 0.4883 | 250  | 0.6926          | -0.0038        | -0.0056          | 0.4396             | 0.0017          | -18.8205       | -16.7673     | -0.5985         | -0.5984       |
| 0.6893        | 0.5859 | 300  | 0.6921          | -0.0055        | -0.0082          | 0.4022             | 0.0027          | -18.8295       | -16.7730     | -0.5980         | -0.5979       |
| 0.6886        | 0.6836 | 350  | 0.6908          | -0.0050        | -0.0105          | 0.4484             | 0.0054          | -18.8369       | -16.7714     | -0.5979         | -0.5978       |
| 0.6909        | 0.7812 | 400  | 0.6908          | -0.0036        | -0.0092          | 0.4198             | 0.0056          | -18.8326       | -16.7665     | -0.5984         | -0.5983       |
| 0.6882        | 0.8789 | 450  | 0.6927          | -0.0075        | -0.0091          | 0.4264             | 0.0016          | -18.8322       | -16.7795     | -0.5983         | -0.5982       |
| 0.6907        | 0.9766 | 500  | 0.6911          | -0.0053        | -0.0101          | 0.4484             | 0.0048          | -18.8357       | -16.7724     | -0.5984         | -0.5983       |
| 0.6897        | 1.0742 | 550  | 0.6932          | -0.0076        | -0.0082          | 0.4110             | 0.0005          | -18.8293       | -16.7801     | -0.5983         | -0.5982       |
| 0.6826        | 1.1719 | 600  | 0.6916          | -0.0047        | -0.0085          | 0.4593             | 0.0038          | -18.8302       | -16.7702     | -0.5981         | -0.5980       |
| 0.6857        | 1.2695 | 650  | 0.6935          | -0.0079        | -0.0079          | 0.4286             | -0.0000         | -18.8283       | -16.7810     | -0.5983         | -0.5982       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_650_STEPS_03beta_1e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""550ba9b10f887db85e6bdeecc691306c9d35c60d"", ""last_modified"": ""2024-05-06 22:05:15+00:00"", ""created_at"": ""2024-05-06 21:59:58+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_650_STEPS_03beta_1e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_650_STEPS_03beta_1e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-06 22:05:15+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_650_STEPS_03beta_1e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663952ded82673d8e49dda67"", ""modelId"": ""tsavage68/chat_650_STEPS_03beta_1e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_650_STEPS_03beta_1e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_650_STEPS_03beta_1e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_650_STEPS_03beta_1e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_05beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_05beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_05beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6616
- Rewards/chosen: -0.1436
- Rewards/rejected: -0.2746
- Rewards/accuracies: 0.5121
- Rewards/margins: 0.1310
- Logps/rejected: -19.3513
- Logps/chosen: -17.0419
- Logits/rejected: -0.6146
- Logits/chosen: -0.6144

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6903        | 0.0977 | 50   | 0.6936          | 0.0166         | 0.0155           | 0.4000             | 0.0011          | -18.7710       | -16.7214     | -0.5983         | -0.5982       |
| 0.6671        | 0.1953 | 100  | 0.6792          | -0.0508        | -0.0879          | 0.4835             | 0.0371          | -18.9777       | -16.8562     | -0.6007         | -0.6006       |
| 0.6959        | 0.2930 | 150  | 0.6832          | -0.1265        | -0.1680          | 0.4835             | 0.0414          | -19.1379       | -17.0077     | -0.6015         | -0.6014       |
| 0.6846        | 0.3906 | 200  | 0.6802          | -0.0532        | -0.1115          | 0.4945             | 0.0582          | -19.0249       | -16.8611     | -0.5963         | -0.5961       |
| 0.7093        | 0.4883 | 250  | 0.6785          | -0.0329        | -0.1015          | 0.5055             | 0.0686          | -19.0051       | -16.8204     | -0.5935         | -0.5934       |
| 0.6806        | 0.5859 | 300  | 0.6692          | -0.0525        | -0.1502          | 0.5319             | 0.0977          | -19.1024       | -16.8596     | -0.6013         | -0.6012       |
| 0.6602        | 0.6836 | 350  | 0.6687          | -0.1217        | -0.2201          | 0.5055             | 0.0984          | -19.2423       | -16.9981     | -0.5956         | -0.5955       |
| 0.6623        | 0.7812 | 400  | 0.6638          | -0.0882        | -0.2063          | 0.5187             | 0.1181          | -19.2146       | -16.9310     | -0.6041         | -0.6040       |
| 0.68          | 0.8789 | 450  | 0.6676          | -0.0466        | -0.1563          | 0.5033             | 0.1096          | -19.1145       | -16.8479     | -0.5958         | -0.5956       |
| 0.6566        | 0.9766 | 500  | 0.6673          | -0.0526        | -0.1670          | 0.5209             | 0.1143          | -19.1359       | -16.8599     | -0.6025         | -0.6024       |
| 0.4534        | 1.0742 | 550  | 0.6642          | -0.0606        | -0.1820          | 0.5165             | 0.1214          | -19.1661       | -16.8759     | -0.6045         | -0.6043       |
| 0.4636        | 1.1719 | 600  | 0.6618          | -0.1037        | -0.2295          | 0.5187             | 0.1259          | -19.2611       | -16.9619     | -0.6071         | -0.6070       |
| 0.4729        | 1.2695 | 650  | 0.6600          | -0.1190        | -0.2504          | 0.5231             | 0.1314          | -19.3028       | -16.9927     | -0.6106         | -0.6105       |
| 0.4057        | 1.3672 | 700  | 0.6601          | -0.1176        | -0.2495          | 0.5297             | 0.1320          | -19.3011       | -16.9898     | -0.6115         | -0.6114       |
| 0.3873        | 1.4648 | 750  | 0.6601          | -0.1335        | -0.2670          | 0.5187             | 0.1335          | -19.3359       | -17.0216     | -0.6135         | -0.6133       |
| 0.4769        | 1.5625 | 800  | 0.6603          | -0.1398        | -0.2738          | 0.5165             | 0.1339          | -19.3495       | -17.0343     | -0.6136         | -0.6134       |
| 0.4437        | 1.6602 | 850  | 0.6558          | -0.1370        | -0.2785          | 0.5187             | 0.1415          | -19.3589       | -17.0286     | -0.6142         | -0.6140       |
| 0.4781        | 1.7578 | 900  | 0.6587          | -0.1393        | -0.2752          | 0.5209             | 0.1359          | -19.3524       | -17.0332     | -0.6146         | -0.6145       |
| 0.4408        | 1.8555 | 950  | 0.6611          | -0.1424        | -0.2727          | 0.5121             | 0.1303          | -19.3474       | -17.0395     | -0.6146         | -0.6145       |
| 0.4387        | 1.9531 | 1000 | 0.6616          | -0.1436        | -0.2746          | 0.5121             | 0.1310          | -19.3513       | -17.0419     | -0.6146         | -0.6144       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_05beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""5ac114e680dc1f55f4c9265b4c1b444b1982c721"", ""last_modified"": ""2024-05-07 02:28:04+00:00"", ""created_at"": ""2024-05-07 02:24:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_05beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 02:28:04+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663990db37337e4147d4cb6c"", ""modelId"": ""tsavage68/chat_1000_STEPS_05beta_5e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_05beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_05beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_05beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_01beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_01beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_01beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6764
- Rewards/chosen: -0.0580
- Rewards/rejected: -0.0983
- Rewards/accuracies: 0.5209
- Rewards/margins: 0.0403
- Logps/rejected: -19.7850
- Logps/chosen: -17.3346
- Logits/rejected: -0.6386
- Logits/chosen: -0.6384

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6924        | 0.0977 | 50   | 0.6933          | 0.0017         | 0.0020           | 0.4154             | -0.0003         | -18.7815       | -16.7372     | -0.5990         | -0.5988       |
| 0.6889        | 0.1953 | 100  | 0.6896          | -0.0103        | -0.0178          | 0.4769             | 0.0075          | -18.9805       | -16.8580     | -0.6027         | -0.6025       |
| 0.6916        | 0.2930 | 150  | 0.6883          | -0.0335        | -0.0443          | 0.4945             | 0.0109          | -19.2454       | -17.0895     | -0.6042         | -0.6041       |
| 0.6896        | 0.3906 | 200  | 0.6862          | -0.0223        | -0.0382          | 0.4835             | 0.0159          | -19.1840       | -16.9772     | -0.6024         | -0.6023       |
| 0.6921        | 0.4883 | 250  | 0.6851          | -0.0229        | -0.0418          | 0.5011             | 0.0188          | -19.2195       | -16.9840     | -0.6008         | -0.6007       |
| 0.6846        | 0.5859 | 300  | 0.6811          | -0.0344        | -0.0622          | 0.5099             | 0.0277          | -19.4235       | -17.0989     | -0.6126         | -0.6125       |
| 0.6801        | 0.6836 | 350  | 0.6812          | -0.0454        | -0.0734          | 0.5275             | 0.0279          | -19.5357       | -17.2091     | -0.6080         | -0.6079       |
| 0.6757        | 0.7812 | 400  | 0.6796          | -0.0410        | -0.0732          | 0.5275             | 0.0322          | -19.5340       | -17.1644     | -0.6197         | -0.6196       |
| 0.6822        | 0.8789 | 450  | 0.6794          | -0.0353        | -0.0681          | 0.5275             | 0.0328          | -19.4827       | -17.1072     | -0.6122         | -0.6121       |
| 0.6728        | 0.9766 | 500  | 0.6787          | -0.0351        | -0.0699          | 0.5121             | 0.0348          | -19.5013       | -17.1061     | -0.6178         | -0.6176       |
| 0.6186        | 1.0742 | 550  | 0.6781          | -0.0359        | -0.0723          | 0.5209             | 0.0363          | -19.5246       | -17.1139     | -0.6238         | -0.6237       |
| 0.6205        | 1.1719 | 600  | 0.6777          | -0.0476        | -0.0850          | 0.5275             | 0.0374          | -19.6518       | -17.2306     | -0.6283         | -0.6282       |
| 0.6273        | 1.2695 | 650  | 0.6772          | -0.0533        | -0.0918          | 0.5209             | 0.0385          | -19.7197       | -17.2874     | -0.6341         | -0.6340       |
| 0.6067        | 1.3672 | 700  | 0.6762          | -0.0522        | -0.0927          | 0.5341             | 0.0405          | -19.7290       | -17.2762     | -0.6358         | -0.6356       |
| 0.5987        | 1.4648 | 750  | 0.6759          | -0.0558        | -0.0971          | 0.5363             | 0.0413          | -19.7734       | -17.3129     | -0.6378         | -0.6377       |
| 0.6302        | 1.5625 | 800  | 0.6761          | -0.0560        | -0.0969          | 0.5363             | 0.0409          | -19.7707       | -17.3142     | -0.6384         | -0.6382       |
| 0.6155        | 1.6602 | 850  | 0.6765          | -0.0572        | -0.0973          | 0.5363             | 0.0401          | -19.7749       | -17.3264     | -0.6388         | -0.6386       |
| 0.6308        | 1.7578 | 900  | 0.6760          | -0.0572        | -0.0982          | 0.5253             | 0.0411          | -19.7842       | -17.3263     | -0.6389         | -0.6388       |
| 0.6092        | 1.8555 | 950  | 0.6760          | -0.0570        | -0.0981          | 0.5407             | 0.0411          | -19.7825       | -17.3244     | -0.6387         | -0.6386       |
| 0.6166        | 1.9531 | 1000 | 0.6764          | -0.0580        | -0.0983          | 0.5209             | 0.0403          | -19.7850       | -17.3346     | -0.6386         | -0.6384       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_01beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""957e247314a681de18195de02b0247a8e6a07d3d"", ""last_modified"": ""2024-05-07 02:30:05+00:00"", ""created_at"": ""2024-05-07 02:26:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 6, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_01beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 02:30:05+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66399145b3395ea4fe2d8c21"", ""modelId"": ""tsavage68/chat_1000_STEPS_01beta_5e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_01beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_01beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_01beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_03beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_03beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_03beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6933
- Rewards/chosen: 0.0002
- Rewards/rejected: -0.0002
- Rewards/accuracies: 0.4242
- Rewards/margins: 0.0004
- Logps/rejected: -18.8025
- Logps/chosen: -16.7538
- Logits/rejected: -0.5979
- Logits/chosen: -0.5978

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-08
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6928        | 0.0977 | 50   | 0.6947          | 0.0015         | 0.0040           | 0.4264             | -0.0024         | -18.7888       | -16.7496     | -0.5974         | -0.5972       |
| 0.6895        | 0.1953 | 100  | 0.6950          | -0.0009        | 0.0022           | 0.3890             | -0.0030         | -18.7948       | -16.7576     | -0.5980         | -0.5979       |
| 0.6932        | 0.2930 | 150  | 0.6934          | 0.0018         | 0.0016           | 0.4308             | 0.0002          | -18.7966       | -16.7487     | -0.5984         | -0.5983       |
| 0.695         | 0.3906 | 200  | 0.6947          | 0.0002         | 0.0027           | 0.3868             | -0.0025         | -18.7930       | -16.7541     | -0.5982         | -0.5981       |
| 0.6971        | 0.4883 | 250  | 0.6938          | 0.0028         | 0.0034           | 0.4220             | -0.0006         | -18.7907       | -16.7453     | -0.5980         | -0.5979       |
| 0.6891        | 0.5859 | 300  | 0.6934          | -0.0003        | -0.0005          | 0.4396             | 0.0002          | -18.8036       | -16.7557     | -0.5988         | -0.5987       |
| 0.6872        | 0.6836 | 350  | 0.6939          | 0.0002         | 0.0010           | 0.4088             | -0.0009         | -18.7985       | -16.7541     | -0.5983         | -0.5982       |
| 0.6953        | 0.7812 | 400  | 0.6964          | -0.0050        | 0.0010           | 0.3846             | -0.0060         | -18.7985       | -16.7713     | -0.5980         | -0.5979       |
| 0.689         | 0.8789 | 450  | 0.6949          | 0.0002         | 0.0031           | 0.4088             | -0.0029         | -18.7915       | -16.7539     | -0.5983         | -0.5982       |
| 0.6935        | 0.9766 | 500  | 0.6920          | 0.0025         | -0.0003          | 0.4154             | 0.0028          | -18.8029       | -16.7461     | -0.5985         | -0.5983       |
| 0.6931        | 1.0742 | 550  | 0.6931          | 0.0014         | 0.0007           | 0.4220             | 0.0007          | -18.7996       | -16.7500     | -0.5979         | -0.5978       |
| 0.6919        | 1.1719 | 600  | 0.6951          | -0.0001        | 0.0032           | 0.3780             | -0.0032         | -18.7914       | -16.7548     | -0.5982         | -0.5980       |
| 0.6916        | 1.2695 | 650  | 0.6930          | 0.0027         | 0.0019           | 0.4110             | 0.0009          | -18.7958       | -16.7455     | -0.5982         | -0.5980       |
| 0.6914        | 1.3672 | 700  | 0.6936          | 0.0017         | 0.0020           | 0.4198             | -0.0003         | -18.7954       | -16.7489     | -0.5975         | -0.5974       |
| 0.6937        | 1.4648 | 750  | 0.6937          | 0.0001         | 0.0006           | 0.4352             | -0.0005         | -18.7999       | -16.7542     | -0.5977         | -0.5976       |
| 0.6953        | 1.5625 | 800  | 0.6948          | -0.0003        | 0.0025           | 0.3934             | -0.0028         | -18.7936       | -16.7556     | -0.5977         | -0.5976       |
| 0.6914        | 1.6602 | 850  | 0.6932          | 0.0004         | -0.0002          | 0.4264             | 0.0006          | -18.8026       | -16.7534     | -0.5979         | -0.5978       |
| 0.6936        | 1.7578 | 900  | 0.6933          | 0.0002         | -0.0002          | 0.4242             | 0.0004          | -18.8025       | -16.7538     | -0.5979         | -0.5978       |
| 0.6925        | 1.8555 | 950  | 0.6933          | 0.0002         | -0.0002          | 0.4242             | 0.0004          | -18.8025       | -16.7538     | -0.5979         | -0.5978       |
| 0.6919        | 1.9531 | 1000 | 0.6933          | 0.0002         | -0.0002          | 0.4242             | 0.0004          | -18.8025       | -16.7538     | -0.5979         | -0.5978       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_03beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""7f92199700e76e8889473359936daf9cc5ca1130"", ""last_modified"": ""2024-05-07 08:36:45+00:00"", ""created_at"": ""2024-05-07 02:41:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_03beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 08:36:45+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_03beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663994ef4755d70dffea3de6"", ""modelId"": ""tsavage68/chat_1000_STEPS_03beta_5e7rate_CDPOSFT"", ""usedStorage"": 26953738582}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_03beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_03beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_03beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_300_STEPS_03beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_300_STEPS_03beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_300_STEPS_03beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6793
- Rewards/chosen: -0.0623
- Rewards/rejected: -0.1009
- Rewards/accuracies: 0.4945
- Rewards/margins: 0.0386
- Logps/rejected: -19.1383
- Logps/chosen: -16.9623
- Logits/rejected: -0.5997
- Logits/chosen: -0.5996

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 300

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6938        | 0.0977 | 50   | 0.6926          | 0.0099         | 0.0081           | 0.4242             | 0.0018          | -18.7749       | -16.7215     | -0.5989         | -0.5987       |
| 0.6799        | 0.1953 | 100  | 0.6850          | -0.0283        | -0.0476          | 0.4769             | 0.0194          | -18.9607       | -16.8488     | -0.6017         | -0.6016       |
| 0.6902        | 0.2930 | 150  | 0.6833          | -0.0843        | -0.1125          | 0.4725             | 0.0281          | -19.1769       | -17.0358     | -0.6005         | -0.6004       |
| 0.6881        | 0.3906 | 200  | 0.6808          | -0.0590        | -0.0933          | 0.4835             | 0.0343          | -19.1129       | -16.9512     | -0.5994         | -0.5993       |
| 0.6919        | 0.4883 | 250  | 0.6812          | -0.0623        | -0.0971          | 0.4989             | 0.0348          | -19.1258       | -16.9623     | -0.6003         | -0.6001       |
| 0.6828        | 0.5859 | 300  | 0.6793          | -0.0623        | -0.1009          | 0.4945             | 0.0386          | -19.1383       | -16.9623     | -0.5997         | -0.5996       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_300_STEPS_03beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""e896362a141b40e4feff2284ac012e41b9a7cbb0"", ""last_modified"": ""2024-05-07 04:12:17+00:00"", ""created_at"": ""2024-05-07 04:07:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_300_STEPS_03beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_300_STEPS_03beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 04:12:17+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_300_STEPS_03beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639a90227ef2d37a7080de8"", ""modelId"": ""tsavage68/chat_300_STEPS_03beta_5e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_300_STEPS_03beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_300_STEPS_03beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_300_STEPS_03beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_400_STEPS_01beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_400_STEPS_01beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_400_STEPS_01beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6851
- Rewards/chosen: -0.0303
- Rewards/rejected: -0.0485
- Rewards/accuracies: 0.5077
- Rewards/margins: 0.0182
- Logps/rejected: -19.2868
- Logps/chosen: -17.0576
- Logits/rejected: -0.6041
- Logits/chosen: -0.6040

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 400

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6924        | 0.0977 | 50   | 0.6933          | 0.0017         | 0.0020           | 0.4154             | -0.0003         | -18.7815       | -16.7372     | -0.5990         | -0.5988       |
| 0.6889        | 0.1953 | 100  | 0.6896          | -0.0103        | -0.0178          | 0.4769             | 0.0075          | -18.9805       | -16.8580     | -0.6027         | -0.6025       |
| 0.692         | 0.2930 | 150  | 0.6885          | -0.0339        | -0.0443          | 0.4967             | 0.0104          | -19.2452       | -17.0936     | -0.6039         | -0.6038       |
| 0.6898        | 0.3906 | 200  | 0.6871          | -0.0252        | -0.0389          | 0.5033             | 0.0137          | -19.1906       | -17.0066     | -0.6024         | -0.6022       |
| 0.6911        | 0.4883 | 250  | 0.6862          | -0.0287        | -0.0445          | 0.5099             | 0.0159          | -19.2474       | -17.0415     | -0.6037         | -0.6036       |
| 0.6854        | 0.5859 | 300  | 0.6852          | -0.0303        | -0.0482          | 0.5121             | 0.0179          | -19.2838       | -17.0573     | -0.6047         | -0.6046       |
| 0.683         | 0.6836 | 350  | 0.6849          | -0.0303        | -0.0489          | 0.5231             | 0.0186          | -19.2907       | -17.0575     | -0.6039         | -0.6037       |
| 0.6853        | 0.7812 | 400  | 0.6851          | -0.0303        | -0.0485          | 0.5077             | 0.0182          | -19.2868       | -17.0576     | -0.6041         | -0.6040       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_400_STEPS_01beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""797a4613111f9f02204bed4b80510588017fd202"", ""last_modified"": ""2024-05-07 04:40:19+00:00"", ""created_at"": ""2024-05-07 04:36:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_01beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_400_STEPS_01beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 04:40:19+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_400_STEPS_01beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639afd38bf959c6e2391287"", ""modelId"": ""tsavage68/chat_400_STEPS_01beta_5e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_400_STEPS_01beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_400_STEPS_01beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_400_STEPS_01beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_600_STEPS_05beta_5e7rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_600_STEPS_05beta_5e7rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_600_STEPS_05beta_5e7rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6669
- Rewards/chosen: -0.0665
- Rewards/rejected: -0.1611
- Rewards/accuracies: 0.5275
- Rewards/margins: 0.0946
- Logps/rejected: -19.1242
- Logps/chosen: -16.8876
- Logits/rejected: -0.5967
- Logits/chosen: -0.5966

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 600

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6903        | 0.0977 | 50   | 0.6936          | 0.0166         | 0.0155           | 0.4000             | 0.0011          | -18.7710       | -16.7214     | -0.5983         | -0.5982       |
| 0.6671        | 0.1953 | 100  | 0.6792          | -0.0508        | -0.0879          | 0.4835             | 0.0371          | -18.9777       | -16.8562     | -0.6007         | -0.6006       |
| 0.6942        | 0.2930 | 150  | 0.6855          | -0.1406        | -0.1792          | 0.4791             | 0.0386          | -19.1604       | -17.0359     | -0.5997         | -0.5996       |
| 0.6826        | 0.3906 | 200  | 0.6802          | -0.0490        | -0.1057          | 0.4835             | 0.0567          | -19.0134       | -16.8527     | -0.5953         | -0.5952       |
| 0.7074        | 0.4883 | 250  | 0.6747          | -0.0391        | -0.1111          | 0.4967             | 0.0721          | -19.0242       | -16.8328     | -0.5930         | -0.5929       |
| 0.6745        | 0.5859 | 300  | 0.6694          | -0.0467        | -0.1352          | 0.5011             | 0.0885          | -19.0723       | -16.8480     | -0.5980         | -0.5979       |
| 0.6636        | 0.6836 | 350  | 0.6685          | -0.0796        | -0.1700          | 0.5253             | 0.0905          | -19.1420       | -16.9137     | -0.5947         | -0.5945       |
| 0.6607        | 0.7812 | 400  | 0.6691          | -0.0747        | -0.1648          | 0.5209             | 0.0902          | -19.1317       | -16.9040     | -0.5986         | -0.5984       |
| 0.6758        | 0.8789 | 450  | 0.6693          | -0.0676        | -0.1582          | 0.5275             | 0.0906          | -19.1183       | -16.8898     | -0.5967         | -0.5965       |
| 0.6562        | 0.9766 | 500  | 0.6686          | -0.0674        | -0.1598          | 0.5187             | 0.0924          | -19.1216       | -16.8894     | -0.5965         | -0.5964       |
| 0.5185        | 1.0742 | 550  | 0.6689          | -0.0681        | -0.1596          | 0.5077             | 0.0915          | -19.1213       | -16.8909     | -0.5971         | -0.5970       |
| 0.5392        | 1.1719 | 600  | 0.6669          | -0.0665        | -0.1611          | 0.5275             | 0.0946          | -19.1242       | -16.8876     | -0.5967         | -0.5966       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_600_STEPS_05beta_5e7rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""dabf230456fb23a3f39408b1ada4361820043dd3"", ""last_modified"": ""2024-05-07 05:27:10+00:00"", ""created_at"": ""2024-05-07 05:23:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_600_STEPS_05beta_5e7rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_600_STEPS_05beta_5e7rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 05:27:10+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_600_STEPS_05beta_5e7rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639bad27b276c7ac6e6461c"", ""modelId"": ""tsavage68/chat_600_STEPS_05beta_5e7rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_600_STEPS_05beta_5e7rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_600_STEPS_05beta_5e7rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_600_STEPS_05beta_5e7rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_01beta_1e8rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_01beta_1e8rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_01beta_1e8rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6928
- Rewards/chosen: 0.0004
- Rewards/rejected: -0.0004
- Rewards/accuracies: 0.4154
- Rewards/margins: 0.0008
- Logps/rejected: -18.8060
- Logps/chosen: -16.7510
- Logits/rejected: -0.5976
- Logits/chosen: -0.5974

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-08
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6933        | 0.0977 | 50   | 0.6938          | -0.0003        | 0.0009           | 0.3956             | -0.0012         | -18.7927       | -16.7574     | -0.5974         | -0.5973       |
| 0.6926        | 0.1953 | 100  | 0.6938          | -0.0011        | 0.0002           | 0.3890             | -0.0013         | -18.7999       | -16.7655     | -0.5978         | -0.5977       |
| 0.6939        | 0.2930 | 150  | 0.6934          | 0.0001         | 0.0006           | 0.3780             | -0.0004         | -18.7964       | -16.7533     | -0.5977         | -0.5976       |
| 0.6935        | 0.3906 | 200  | 0.6933          | -0.0003        | -0.0002          | 0.4198             | -0.0002         | -18.8035       | -16.7577     | -0.5978         | -0.5977       |
| 0.6934        | 0.4883 | 250  | 0.6931          | 0.0005         | 0.0004           | 0.4242             | 0.0001          | -18.7978       | -16.7493     | -0.5980         | -0.5979       |
| 0.6925        | 0.5859 | 300  | 0.6938          | 0.0000         | 0.0012           | 0.3912             | -0.0012         | -18.7895       | -16.7546     | -0.5982         | -0.5981       |
| 0.6919        | 0.6836 | 350  | 0.6930          | 0.0002         | -0.0002          | 0.4044             | 0.0004          | -18.8044       | -16.7530     | -0.5977         | -0.5976       |
| 0.6941        | 0.7812 | 400  | 0.6932          | 0.0001         | 0.0002           | 0.4154             | -0.0001         | -18.7997       | -16.7532     | -0.5979         | -0.5978       |
| 0.6918        | 0.8789 | 450  | 0.6933          | 0.0005         | 0.0007           | 0.4154             | -0.0002         | -18.7949       | -16.7500     | -0.5982         | -0.5981       |
| 0.694         | 0.9766 | 500  | 0.6935          | -0.0000        | 0.0006           | 0.4132             | -0.0006         | -18.7963       | -16.7548     | -0.5978         | -0.5977       |
| 0.692         | 1.0742 | 550  | 0.6934          | 0.0006         | 0.0011           | 0.4088             | -0.0005         | -18.7914       | -16.7489     | -0.5970         | -0.5969       |
| 0.6927        | 1.1719 | 600  | 0.6935          | 0.0001         | 0.0008           | 0.3978             | -0.0007         | -18.7940       | -16.7538     | -0.5973         | -0.5972       |
| 0.6942        | 1.2695 | 650  | 0.6931          | 0.0003         | 0.0001           | 0.4286             | 0.0002          | -18.8007       | -16.7513     | -0.5974         | -0.5973       |
| 0.6928        | 1.3672 | 700  | 0.6938          | -0.0001        | 0.0012           | 0.3846             | -0.0013         | -18.7896       | -16.7554     | -0.5974         | -0.5973       |
| 0.6934        | 1.4648 | 750  | 0.6938          | -0.0002        | 0.0011           | 0.3934             | -0.0013         | -18.7914       | -16.7566     | -0.5976         | -0.5975       |
| 0.6946        | 1.5625 | 800  | 0.6931          | -0.0004        | -0.0005          | 0.4132             | 0.0001          | -18.8067       | -16.7586     | -0.5976         | -0.5975       |
| 0.6928        | 1.6602 | 850  | 0.6928          | 0.0004         | -0.0004          | 0.4154             | 0.0008          | -18.8060       | -16.7510     | -0.5976         | -0.5974       |
| 0.6924        | 1.7578 | 900  | 0.6928          | 0.0004         | -0.0004          | 0.4154             | 0.0008          | -18.8060       | -16.7510     | -0.5976         | -0.5974       |
| 0.6939        | 1.8555 | 950  | 0.6928          | 0.0004         | -0.0004          | 0.4154             | 0.0008          | -18.8060       | -16.7510     | -0.5976         | -0.5974       |
| 0.6928        | 1.9531 | 1000 | 0.6928          | 0.0004         | -0.0004          | 0.4154             | 0.0008          | -18.8060       | -16.7510     | -0.5976         | -0.5974       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_01beta_1e8rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""c90ded8cedc844349a2c9761bf4c610856700994"", ""last_modified"": ""2024-05-07 09:15:54+00:00"", ""created_at"": ""2024-05-07 09:12:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e8rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_01beta_1e8rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 09:15:54+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_01beta_1e8rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6639f0694ecb5e1c18b9a336"", ""modelId"": ""tsavage68/chat_1000_STEPS_01beta_1e8rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_01beta_1e8rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_01beta_1e8rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_01beta_1e8rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000_STEPS_05beta_1e8rate_CDPOSFT,"---
base_model: tsavage68/chat_600STEPS_1e8rate_SFT
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000_STEPS_05beta_1e8rate_CDPOSFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000_STEPS_05beta_1e8rate_CDPOSFT

This model is a fine-tuned version of [tsavage68/chat_600STEPS_1e8rate_SFT](https://huggingface.co/tsavage68/chat_600STEPS_1e8rate_SFT) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6934
- Rewards/chosen: 0.0042
- Rewards/rejected: 0.0031
- Rewards/accuracies: 0.4176
- Rewards/margins: 0.0011
- Logps/rejected: -18.7959
- Logps/chosen: -16.7463
- Logits/rejected: -0.5975
- Logits/chosen: -0.5974

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-08
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:------:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6957        | 0.0977 | 50   | 0.6960          | 0.0006         | 0.0049           | 0.3648             | -0.0043         | -18.7923       | -16.7534     | -0.5974         | -0.5973       |
| 0.69          | 0.1953 | 100  | 0.6940          | 0.0010         | 0.0011           | 0.4308             | -0.0001         | -18.7998       | -16.7526     | -0.5985         | -0.5983       |
| 0.7001        | 0.2930 | 150  | 0.6953          | 0.0032         | 0.0058           | 0.4132             | -0.0026         | -18.7904       | -16.7482     | -0.5979         | -0.5978       |
| 0.6948        | 0.3906 | 200  | 0.6936          | -0.0016        | -0.0025          | 0.4352             | 0.0009          | -18.8070       | -16.7578     | -0.5978         | -0.5977       |
| 0.6978        | 0.4883 | 250  | 0.6946          | 0.0002         | 0.0016           | 0.4198             | -0.0014         | -18.7989       | -16.7543     | -0.5980         | -0.5978       |
| 0.6903        | 0.5859 | 300  | 0.6944          | 0.0032         | 0.0040           | 0.4088             | -0.0008         | -18.7940       | -16.7482     | -0.5970         | -0.5969       |
| 0.6902        | 0.6836 | 350  | 0.6956          | -0.0040        | -0.0007          | 0.3560             | -0.0033         | -18.8034       | -16.7625     | -0.5971         | -0.5970       |
| 0.6966        | 0.7812 | 400  | 0.6948          | -0.0005        | 0.0011           | 0.4198             | -0.0016         | -18.7999       | -16.7557     | -0.5978         | -0.5976       |
| 0.6891        | 0.8789 | 450  | 0.6932          | 0.0019         | 0.0000           | 0.4308             | 0.0019          | -18.8019       | -16.7508     | -0.5973         | -0.5972       |
| 0.6907        | 0.9766 | 500  | 0.6940          | 0.0028         | 0.0027           | 0.4352             | 0.0001          | -18.7966       | -16.7490     | -0.5974         | -0.5973       |
| 0.6941        | 1.0742 | 550  | 0.6916          | 0.0010         | -0.0039          | 0.4330             | 0.0049          | -18.8098       | -16.7526     | -0.5974         | -0.5972       |
| 0.6857        | 1.1719 | 600  | 0.6949          | 0.0012         | 0.0030           | 0.4132             | -0.0018         | -18.7960       | -16.7523     | -0.5975         | -0.5973       |
| 0.6927        | 1.2695 | 650  | 0.6926          | 0.0050         | 0.0020           | 0.4308             | 0.0029          | -18.7979       | -16.7447     | -0.5977         | -0.5976       |
| 0.6844        | 1.3672 | 700  | 0.6954          | -0.0029        | -0.0002          | 0.4044             | -0.0027         | -18.8024       | -16.7604     | -0.5978         | -0.5977       |
| 0.6951        | 1.4648 | 750  | 0.6940          | 0.0000         | 0.0000           | 0.4176             | -0.0000         | -18.8020       | -16.7546     | -0.5978         | -0.5977       |
| 0.6965        | 1.5625 | 800  | 0.6947          | 0.0046         | 0.0060           | 0.4132             | -0.0014         | -18.7900       | -16.7455     | -0.5975         | -0.5974       |
| 0.6894        | 1.6602 | 850  | 0.6934          | 0.0043         | 0.0031           | 0.4176             | 0.0012          | -18.7958       | -16.7461     | -0.5976         | -0.5974       |
| 0.6882        | 1.7578 | 900  | 0.6934          | 0.0042         | 0.0031           | 0.4176             | 0.0011          | -18.7959       | -16.7463     | -0.5975         | -0.5974       |
| 0.6932        | 1.8555 | 950  | 0.6934          | 0.0042         | 0.0031           | 0.4176             | 0.0011          | -18.7959       | -16.7463     | -0.5975         | -0.5974       |
| 0.6899        | 1.9531 | 1000 | 0.6934          | 0.0042         | 0.0031           | 0.4176             | 0.0011          | -18.7959       | -16.7463     | -0.5975         | -0.5974       |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.0.0+cu117
- Datasets 2.19.1
- Tokenizers 0.19.1
","{""id"": ""tsavage68/chat_1000_STEPS_05beta_1e8rate_CDPOSFT"", ""author"": ""tsavage68"", ""sha"": ""5be1eeaa01f084fcfed5722369a0d1d7bc3f3e36"", ""last_modified"": ""2024-05-07 11:56:19+00:00"", ""created_at"": ""2024-05-07 11:52:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:tsavage68/chat_600STEPS_1e8rate_SFT"", ""base_model:finetune:tsavage68/chat_600STEPS_1e8rate_SFT"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e8rate_CDPOSFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000_STEPS_05beta_1e8rate_CDPOSFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-05-07 11:56:19+00:00"", ""cardData"": ""base_model: tsavage68/chat_600STEPS_1e8rate_SFT\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000_STEPS_05beta_1e8rate_CDPOSFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""663a161095085055e9bf63aa"", ""modelId"": ""tsavage68/chat_1000_STEPS_05beta_1e8rate_CDPOSFT"", ""usedStorage"": 13476869291}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000_STEPS_05beta_1e8rate_CDPOSFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000_STEPS_05beta_1e8rate_CDPOSFT%5D(%2Ftsavage68%2Fchat_1000_STEPS_05beta_1e8rate_CDPOSFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e6rate_01beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e6rate
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e6rate

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6684
- Rewards/chosen: -0.3437
- Rewards/rejected: -0.4414
- Rewards/accuracies: 0.5055
- Rewards/margins: 0.0978
- Logps/rejected: -23.2056
- Logps/chosen: -20.1814
- Logits/rejected: -0.8363
- Logits/chosen: -0.8361

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6939        | 0.1   | 50   | 0.6917          | -0.0037        | -0.0069          | 0.4901             | 0.0032          | -18.8600       | -16.7813     | -0.5975         | -0.5973       |
| 0.6902        | 0.2   | 100  | 0.6919          | -0.1261        | -0.1323          | 0.4440             | 0.0063          | -20.1147       | -18.0054     | -0.6143         | -0.6142       |
| 0.6923        | 0.29  | 150  | 0.6796          | -0.0370        | -0.0721          | 0.4945             | 0.0351          | -19.5126       | -17.1150     | -0.6569         | -0.6568       |
| 0.6793        | 0.39  | 200  | 0.6803          | -0.0086        | -0.0473          | 0.4769             | 0.0387          | -19.2641       | -16.8305     | -0.6452         | -0.6450       |
| 0.6446        | 0.49  | 250  | 0.6790          | -0.0967        | -0.1427          | 0.4857             | 0.0460          | -20.2182       | -17.7115     | -0.6468         | -0.6466       |
| 0.6365        | 0.59  | 300  | 0.6809          | -0.1168        | -0.1650          | 0.4681             | 0.0482          | -20.4409       | -17.9127     | -0.6877         | -0.6874       |
| 0.6828        | 0.68  | 350  | 0.6765          | -0.1034        | -0.1632          | 0.4923             | 0.0599          | -20.4235       | -17.7782     | -0.6849         | -0.6847       |
| 0.6797        | 0.78  | 400  | 0.6788          | -0.0900        | -0.1511          | 0.4923             | 0.0611          | -20.3023       | -17.6445     | -0.6763         | -0.6762       |
| 0.6751        | 0.88  | 450  | 0.6772          | -0.0807        | -0.1445          | 0.4945             | 0.0638          | -20.2366       | -17.5521     | -0.6528         | -0.6526       |
| 0.6596        | 0.98  | 500  | 0.6744          | -0.1091        | -0.1779          | 0.5055             | 0.0688          | -20.5702       | -17.8358     | -0.6395         | -0.6393       |
| 0.4819        | 1.07  | 550  | 0.6714          | -0.2112        | -0.2907          | 0.5077             | 0.0795          | -21.6987       | -18.8566     | -0.7045         | -0.7043       |
| 0.4754        | 1.17  | 600  | 0.6699          | -0.2743        | -0.3603          | 0.5011             | 0.0860          | -22.3943       | -19.4880     | -0.7556         | -0.7554       |
| 0.4339        | 1.27  | 650  | 0.6694          | -0.2906        | -0.3826          | 0.5033             | 0.0920          | -22.6175       | -19.6505     | -0.8041         | -0.8039       |
| 0.4692        | 1.37  | 700  | 0.6673          | -0.3183        | -0.4163          | 0.5033             | 0.0980          | -22.9541       | -19.9276     | -0.8200         | -0.8199       |
| 0.4767        | 1.46  | 750  | 0.6681          | -0.3342        | -0.4320          | 0.5055             | 0.0978          | -23.1116       | -20.0865     | -0.8291         | -0.8289       |
| 0.4125        | 1.56  | 800  | 0.6684          | -0.3381        | -0.4355          | 0.5099             | 0.0974          | -23.1466       | -20.1256     | -0.8330         | -0.8328       |
| 0.4733        | 1.66  | 850  | 0.6681          | -0.3425        | -0.4407          | 0.5011             | 0.0983          | -23.1986       | -20.1691     | -0.8359         | -0.8357       |
| 0.4699        | 1.76  | 900  | 0.6683          | -0.3431        | -0.4412          | 0.5077             | 0.0981          | -23.2032       | -20.1758     | -0.8365         | -0.8363       |
| 0.4629        | 1.86  | 950  | 0.6682          | -0.3438        | -0.4421          | 0.5011             | 0.0984          | -23.2125       | -20.1823     | -0.8365         | -0.8363       |
| 0.4482        | 1.95  | 1000 | 0.6684          | -0.3437        | -0.4414          | 0.5055             | 0.0978          | -23.2056       | -20.1814     | -0.8363         | -0.8361       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e6rate_01beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""56014f4e50997fab52d2652266222e3a0e4d0043"", ""last_modified"": ""2024-02-14 03:47:59+00:00"", ""created_at"": ""2024-02-14 03:44:25+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6rate\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e6rate"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-14 03:47:59+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6rate\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cc371911a80579a433cee1"", ""modelId"": ""tsavage68/chat_1000STEPS_1e6rate_01beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e6rate_01beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e6rate_01beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e6rate_01beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_150STEPS_1e7rate_01beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_150STEPS_1e7rate_01beta
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_150STEPS_1e7rate_01beta

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6933
- Rewards/chosen: -0.0025
- Rewards/rejected: -0.0022
- Rewards/accuracies: 0.4022
- Rewards/margins: -0.0003
- Logps/rejected: -18.8131
- Logps/chosen: -16.7695
- Logits/rejected: -0.5968
- Logits/chosen: -0.5967

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 150

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6931        | 0.1   | 50   | 0.6934          | -0.0010        | -0.0005          | 0.4110             | -0.0005         | -18.7964       | -16.7546     | -0.5967         | -0.5965       |
| 0.6923        | 0.2   | 100  | 0.6935          | -0.0018        | -0.0012          | 0.4044             | -0.0006         | -18.8033       | -16.7622     | -0.5978         | -0.5977       |
| 0.6939        | 0.29  | 150  | 0.6933          | -0.0025        | -0.0022          | 0.4022             | -0.0003         | -18.8131       | -16.7695     | -0.5968         | -0.5967       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_150STEPS_1e7rate_01beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""7325f192e5ce6e999c25fb06ee4cea8782b24e76"", ""last_modified"": ""2024-02-14 04:57:20+00:00"", ""created_at"": ""2024-02-14 04:52:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 4, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_150STEPS_1e7rate_01beta\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_150STEPS_1e7rate_01beta"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-14 04:57:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_150STEPS_1e7rate_01beta\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cc46f211a80579a43aba87"", ""modelId"": ""tsavage68/chat_150STEPS_1e7rate_01beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_150STEPS_1e7rate_01beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_150STEPS_1e7rate_01beta_DPO%5D(%2Ftsavage68%2Fchat_150STEPS_1e7rate_01beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_200STEPS_1e6_01beta,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_200STEPS_1e6_01beta
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_200STEPS_1e6_01beta

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6840
- Rewards/chosen: -0.0632
- Rewards/rejected: -0.0877
- Rewards/accuracies: 0.4637
- Rewards/margins: 0.0245
- Logps/rejected: -19.6678
- Logps/chosen: -17.3765
- Logits/rejected: -0.6331
- Logits/chosen: -0.6330

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 200

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6939        | 0.1   | 50   | 0.6917          | -0.0037        | -0.0069          | 0.4901             | 0.0032          | -18.8600       | -16.7813     | -0.5975         | -0.5973       |
| 0.6902        | 0.2   | 100  | 0.6919          | -0.1261        | -0.1323          | 0.4440             | 0.0063          | -20.1147       | -18.0054     | -0.6143         | -0.6142       |
| 0.691         | 0.29  | 150  | 0.6846          | -0.0911        | -0.1153          | 0.4615             | 0.0242          | -19.9439       | -17.6551     | -0.6419         | -0.6418       |
| 0.6838        | 0.39  | 200  | 0.6840          | -0.0632        | -0.0877          | 0.4637             | 0.0245          | -19.6678       | -17.3765     | -0.6331         | -0.6330       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_200STEPS_1e6_01beta"", ""author"": ""tsavage68"", ""sha"": ""249c244eb440a54192e786cf30890b1cbbb2a3ce"", ""last_modified"": ""2024-02-14 10:15:07+00:00"", ""created_at"": ""2024-02-14 10:10:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_200STEPS_1e6_01beta\n  results: []"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": [{""name"": ""chat_200STEPS_1e6_01beta"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-14 10:15:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_200STEPS_1e6_01beta\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cc91853431b270161e42f1"", ""modelId"": ""tsavage68/chat_200STEPS_1e6_01beta"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_200STEPS_1e6_01beta&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_200STEPS_1e6_01beta%5D(%2Ftsavage68%2Fchat_200STEPS_1e6_01beta)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
TachyHealthResearch/Llama2-7B-Medical-Finetune_V2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: Llama2-7B-Medical-Finetune_V2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama2-7B-Medical-Finetune_V2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0369

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.00025
- train_batch_size: 26
- eval_batch_size: 26
- seed: 42
- gradient_accumulation_steps: 26
- total_train_batch_size: 676
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 1
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.2907        | 0.3   | 100  | 1.1620          |
| 1.1355        | 0.59  | 200  | 1.1211          |
| 1.1063        | 0.89  | 300  | 1.0957          |
| 1.0724        | 1.19  | 400  | 1.0780          |
| 1.0489        | 1.49  | 500  | 1.0633          |
| 1.0359        | 1.78  | 600  | 1.0512          |
| 1.0228        | 2.08  | 700  | 1.0447          |
| 0.9924        | 2.38  | 800  | 1.0401          |
| 0.9906        | 2.68  | 900  | 1.0375          |
| 0.9863        | 2.97  | 1000 | 1.0369          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.2.0+cu121
- Datasets 2.17.0
- Tokenizers 0.15.1
","{""id"": ""TachyHealthResearch/Llama2-7B-Medical-Finetune_V2"", ""author"": ""TachyHealthResearch"", ""sha"": ""54c186453b4bf2a8d5c8d1c7766d7bdbac5f6a51"", ""last_modified"": ""2024-02-15 08:11:04+00:00"", ""created_at"": ""2024-02-15 08:02:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 14, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2-7B-Medical-Finetune_V2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama2-7B-Medical-Finetune_V2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-15 08:11:04+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama2-7B-Medical-Finetune_V2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cdc52346a98b4598db4d29"", ""modelId"": ""TachyHealthResearch/Llama2-7B-Medical-Finetune_V2"", ""usedStorage"": 13637336891}",1,,0,,0,https://huggingface.co/mradermacher/Llama2-7B-Medical-Finetune_V2-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=TachyHealthResearch/Llama2-7B-Medical-Finetune_V2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTachyHealthResearch%2FLlama2-7B-Medical-Finetune_V2%5D(%2FTachyHealthResearch%2FLlama2-7B-Medical-Finetune_V2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e5rate_01beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e7rate_01beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e7rate_01beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9688
- Rewards/chosen: -2.8329
- Rewards/rejected: -3.3687
- Rewards/accuracies: 0.4989
- Rewards/margins: 0.5358
- Logps/rejected: -52.4786
- Logps/chosen: -45.0740
- Logits/rejected: -0.2885
- Logits/chosen: -0.2875

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.8129        | 0.2   | 100  | 0.7825          | -1.1957        | -1.1981          | 0.3934             | 0.0024          | -30.7728       | -28.7020     | -0.0569         | -0.0566       |
| 0.8136        | 0.39  | 200  | 0.8828          | -0.9245        | -0.8916          | 0.4044             | -0.0329         | -27.7071       | -25.9900     | 0.2762          | 0.2769        |
| 0.7535        | 0.59  | 300  | 0.8597          | -1.3930        | -1.4515          | 0.4000             | 0.0585          | -33.3058       | -30.6746     | 1.0803          | 1.0813        |
| 0.9558        | 0.78  | 400  | 0.8896          | -0.8319        | -0.7033          | 0.3604             | -0.1285         | -25.8247       | -25.0635     | 0.4421          | 0.4425        |
| 0.7839        | 0.98  | 500  | 0.7987          | -0.8948        | -1.0616          | 0.4264             | 0.1667          | -29.4069       | -25.6928     | 0.6877          | 0.6886        |
| 0.2401        | 1.17  | 600  | 0.9002          | -2.8266        | -3.2238          | 0.4725             | 0.3972          | -51.0296       | -45.0107     | -0.0174         | -0.0164       |
| 0.2852        | 1.37  | 700  | 0.9362          | -2.6553        | -3.0787          | 0.4769             | 0.4234          | -49.5784       | -43.2978     | -0.1079         | -0.1069       |
| 0.2151        | 1.56  | 800  | 0.9663          | -2.5826        | -3.1268          | 0.5011             | 0.5443          | -50.0594       | -42.5702     | -0.1730         | -0.1719       |
| 0.2376        | 1.76  | 900  | 0.9701          | -2.8346        | -3.3672          | 0.4945             | 0.5326          | -52.4633       | -45.0905     | -0.2881         | -0.2870       |
| 0.2943        | 1.95  | 1000 | 0.9688          | -2.8329        | -3.3687          | 0.4989             | 0.5358          | -52.4786       | -45.0740     | -0.2885         | -0.2875       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e5rate_01beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""5885a6176d25190b6cafcfd672128826a440f751"", ""last_modified"": ""2024-02-15 19:48:18+00:00"", ""created_at"": ""2024-02-15 10:15:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_01beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e7rate_01beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-15 19:48:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_01beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cde450e283ef392a3ca595"", ""modelId"": ""tsavage68/chat_1000STEPS_1e5rate_01beta_DPO"", ""usedStorage"": 26953737942}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e5rate_01beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e5rate_01beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e5rate_01beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e6_03beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e6_03beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e6_03beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6804
- Rewards/chosen: -0.5183
- Rewards/rejected: -0.7327
- Rewards/accuracies: 0.5363
- Rewards/margins: 0.2144
- Logps/rejected: -21.2336
- Logps/chosen: -18.4723
- Logits/rejected: -0.6767
- Logits/chosen: -0.6766

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6885        | 0.2   | 100  | 0.6933          | -0.2467        | -0.2660          | 0.4637             | 0.0193          | -19.6779       | -17.5670     | -0.6067         | -0.6066       |
| 0.683         | 0.39  | 200  | 0.6859          | 0.0215         | -0.0664          | 0.4923             | 0.0879          | -19.0127       | -16.6730     | -0.6150         | -0.6148       |
| 0.6033        | 0.59  | 300  | 0.6999          | -0.1969        | -0.2977          | 0.4791             | 0.1009          | -19.7837       | -17.4008     | -0.6311         | -0.6309       |
| 0.6812        | 0.78  | 400  | 0.6942          | -0.0785        | -0.2126          | 0.4813             | 0.1340          | -19.4998       | -17.0064     | -0.6041         | -0.6039       |
| 0.6633        | 0.98  | 500  | 0.6789          | -0.1266        | -0.2799          | 0.5077             | 0.1533          | -19.7242       | -17.1665     | -0.5557         | -0.5555       |
| 0.2615        | 1.17  | 600  | 0.6788          | -0.4082        | -0.6084          | 0.5253             | 0.2002          | -20.8192       | -18.1052     | -0.6281         | -0.6279       |
| 0.3175        | 1.37  | 700  | 0.6809          | -0.4980        | -0.7087          | 0.5297             | 0.2107          | -21.1536       | -18.4046     | -0.6655         | -0.6653       |
| 0.2805        | 1.56  | 800  | 0.6794          | -0.5125        | -0.7293          | 0.5341             | 0.2169          | -21.2224       | -18.4529     | -0.6754         | -0.6753       |
| 0.3255        | 1.76  | 900  | 0.6807          | -0.5148        | -0.7297          | 0.5385             | 0.2149          | -21.2235       | -18.4605     | -0.6768         | -0.6766       |
| 0.2966        | 1.95  | 1000 | 0.6804          | -0.5183        | -0.7327          | 0.5363             | 0.2144          | -21.2336       | -18.4723     | -0.6767         | -0.6766       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e6_03beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""a48d225ffe76383244a77f27036a5c25f55fa343"", ""last_modified"": ""2024-02-15 10:42:57+00:00"", ""created_at"": ""2024-02-15 10:39:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6_03beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e6_03beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-15 10:42:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6_03beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cde9d29b3d4aaef6760814"", ""modelId"": ""tsavage68/chat_1000STEPS_1e6_03beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e6_03beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e6_03beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e6_03beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e7_03beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e7_03beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e7_03beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6902
- Rewards/chosen: -0.0000
- Rewards/rejected: -0.0069
- Rewards/accuracies: 0.4681
- Rewards/margins: 0.0069
- Logps/rejected: -18.8144
- Logps/chosen: -16.7447
- Logits/rejected: -0.5973
- Logits/chosen: -0.5972

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6935        | 0.2   | 100  | 0.6925          | -0.0035        | -0.0055          | 0.4286             | 0.0019          | -18.8094       | -16.7564     | -0.5969         | -0.5967       |
| 0.6934        | 0.39  | 200  | 0.6911          | 0.0022         | -0.0027          | 0.4615             | 0.0049          | -18.8003       | -16.7374     | -0.5979         | -0.5977       |
| 0.6882        | 0.59  | 300  | 0.6929          | -0.0047        | -0.0060          | 0.4330             | 0.0013          | -18.8112       | -16.7601     | -0.5973         | -0.5972       |
| 0.6896        | 0.78  | 400  | 0.6907          | -0.0013        | -0.0070          | 0.4615             | 0.0057          | -18.8147       | -16.7490     | -0.5982         | -0.5981       |
| 0.6877        | 0.98  | 500  | 0.6904          | 0.0012         | -0.0051          | 0.4923             | 0.0063          | -18.8082       | -16.7405     | -0.5972         | -0.5971       |
| 0.6829        | 1.17  | 600  | 0.6903          | -0.0020        | -0.0085          | 0.4703             | 0.0066          | -18.8198       | -16.7511     | -0.5976         | -0.5975       |
| 0.6832        | 1.37  | 700  | 0.6904          | -0.0032        | -0.0097          | 0.4593             | 0.0064          | -18.8236       | -16.7554     | -0.5971         | -0.5970       |
| 0.6802        | 1.56  | 800  | 0.6889          | -0.0010        | -0.0105          | 0.4923             | 0.0096          | -18.8263       | -16.7478     | -0.5979         | -0.5978       |
| 0.6826        | 1.76  | 900  | 0.6897          | -0.0009        | -0.0088          | 0.4769             | 0.0079          | -18.8206       | -16.7475     | -0.5972         | -0.5971       |
| 0.6761        | 1.95  | 1000 | 0.6902          | -0.0000        | -0.0069          | 0.4681             | 0.0069          | -18.8144       | -16.7447     | -0.5973         | -0.5972       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e7_03beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""4f4302ad8af5d6b6a3a511949c332746aa1a4ff7"", ""last_modified"": ""2024-02-15 19:58:08+00:00"", ""created_at"": ""2024-02-15 19:54:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7_03beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e7_03beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-15 19:58:08+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7_03beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65ce6bef7c497672350b7f0a"", ""modelId"": ""tsavage68/chat_1000STEPS_1e7_03beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e7_03beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e7_03beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e7_03beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e7rate_01beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e7rate_01beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e7rate_01beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6919
- Rewards/chosen: -0.0000
- Rewards/rejected: -0.0027
- Rewards/accuracies: 0.4637
- Rewards/margins: 0.0027
- Logps/rejected: -18.8181
- Logps/chosen: -16.7447
- Logits/rejected: -0.5977
- Logits/chosen: -0.5976

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6923        | 0.2   | 100  | 0.6935          | -0.0018        | -0.0012          | 0.4044             | -0.0006         | -18.8033       | -16.7622     | -0.5978         | -0.5977       |
| 0.6937        | 0.39  | 200  | 0.6928          | -0.0003        | -0.0010          | 0.4505             | 0.0007          | -18.8010       | -16.7472     | -0.5978         | -0.5977       |
| 0.6901        | 0.59  | 300  | 0.6923          | -0.0008        | -0.0025          | 0.4527             | 0.0018          | -18.8166       | -16.7523     | -0.5969         | -0.5968       |
| 0.6912        | 0.78  | 400  | 0.6922          | 0.0001         | -0.0020          | 0.4549             | 0.0020          | -18.8109       | -16.7440     | -0.5982         | -0.5981       |
| 0.6912        | 0.98  | 500  | 0.6922          | 0.0001         | -0.0020          | 0.4813             | 0.0020          | -18.8108       | -16.7437     | -0.5979         | -0.5978       |
| 0.689         | 1.17  | 600  | 0.6920          | -0.0008        | -0.0033          | 0.4637             | 0.0025          | -18.8240       | -16.7525     | -0.5979         | -0.5978       |
| 0.6898        | 1.37  | 700  | 0.6916          | 0.0003         | -0.0029          | 0.5055             | 0.0032          | -18.8205       | -16.7416     | -0.5979         | -0.5977       |
| 0.6876        | 1.56  | 800  | 0.6921          | -0.0011        | -0.0033          | 0.4593             | 0.0022          | -18.8246       | -16.7559     | -0.5981         | -0.5979       |
| 0.6902        | 1.76  | 900  | 0.6917          | -0.0000        | -0.0030          | 0.4637             | 0.0030          | -18.8217       | -16.7450     | -0.5974         | -0.5973       |
| 0.6883        | 1.95  | 1000 | 0.6919          | -0.0000        | -0.0027          | 0.4637             | 0.0027          | -18.8181       | -16.7447     | -0.5977         | -0.5976       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e7rate_01beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""c9b5b21dd7e2cccfb4d9feb309c092e27044267e"", ""last_modified"": ""2024-02-16 01:03:41+00:00"", ""created_at"": ""2024-02-16 00:59:58+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_01beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e7rate_01beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 01:03:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_01beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65ceb38e167be8a5f321dd61"", ""modelId"": ""tsavage68/chat_1000STEPS_1e7rate_01beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e7rate_01beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e7rate_01beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e7rate_01beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e7_05beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e7_05beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e7_05beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6864
- Rewards/chosen: 0.0033
- Rewards/rejected: -0.0130
- Rewards/accuracies: 0.4571
- Rewards/margins: 0.0163
- Logps/rejected: -18.8173
- Logps/chosen: -16.7381
- Logits/rejected: -0.5974
- Logits/chosen: -0.5973

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6957        | 0.2   | 100  | 0.6926          | -0.0030        | -0.0058          | 0.4132             | 0.0028          | -18.8028       | -16.7506     | -0.5972         | -0.5971       |
| 0.6931        | 0.39  | 200  | 0.6899          | 0.0035         | -0.0050          | 0.4835             | 0.0085          | -18.8013       | -16.7376     | -0.5981         | -0.5980       |
| 0.6783        | 0.59  | 300  | 0.6915          | -0.0059        | -0.0111          | 0.4593             | 0.0052          | -18.8135       | -16.7564     | -0.5978         | -0.5977       |
| 0.6952        | 0.78  | 400  | 0.6904          | 0.0004         | -0.0075          | 0.4615             | 0.0079          | -18.8063       | -16.7439     | -0.5975         | -0.5973       |
| 0.6927        | 0.98  | 500  | 0.6904          | -0.0036        | -0.0115          | 0.4396             | 0.0080          | -18.8144       | -16.7518     | -0.5981         | -0.5980       |
| 0.6701        | 1.17  | 600  | 0.6878          | -0.0038        | -0.0170          | 0.4681             | 0.0132          | -18.8254       | -16.7522     | -0.5978         | -0.5977       |
| 0.6796        | 1.37  | 700  | 0.6886          | -0.0031        | -0.0150          | 0.4725             | 0.0119          | -18.8213       | -16.7508     | -0.5970         | -0.5969       |
| 0.6686        | 1.56  | 800  | 0.6881          | -0.0031        | -0.0158          | 0.4813             | 0.0127          | -18.8228       | -16.7508     | -0.5973         | -0.5972       |
| 0.6767        | 1.76  | 900  | 0.6901          | -0.0033        | -0.0123          | 0.4440             | 0.0091          | -18.8159       | -16.7511     | -0.5972         | -0.5971       |
| 0.6702        | 1.95  | 1000 | 0.6864          | 0.0033         | -0.0130          | 0.4571             | 0.0163          | -18.8173       | -16.7381     | -0.5974         | -0.5973       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e7_05beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""e4f969ee27d84d946cd53b56b4ef8de1b255f0b6"", ""last_modified"": ""2024-02-16 01:36:11+00:00"", ""created_at"": ""2024-02-16 01:32:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7_05beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e7_05beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 01:36:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7_05beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cebb21e1c8a3d33cf57714"", ""modelId"": ""tsavage68/chat_1000STEPS_1e7_05beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e7_05beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e7_05beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e7_05beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e7rate_SFT_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e7rate_SFT_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e7rate_SFT_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2866

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-07
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.5641        | 0.2   | 100  | 1.5784          |
| 1.475         | 0.39  | 200  | 1.4776          |
| 1.3992        | 0.59  | 300  | 1.4008          |
| 1.3461        | 0.78  | 400  | 1.3476          |
| 1.3148        | 0.98  | 500  | 1.3150          |
| 1.3           | 1.17  | 600  | 1.2964          |
| 1.2906        | 1.37  | 700  | 1.2886          |
| 1.2711        | 1.56  | 800  | 1.2865          |
| 1.3078        | 1.76  | 900  | 1.2864          |
| 1.2906        | 1.95  | 1000 | 1.2866          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e7rate_SFT_SFT"", ""author"": ""tsavage68"", ""sha"": ""47c51d2512403acdb8d7a85d46c9a0e3903e2f7b"", ""last_modified"": ""2024-02-16 03:10:18+00:00"", ""created_at"": ""2024-02-16 03:06:58+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_SFT_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e7rate_SFT_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 03:10:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e7rate_SFT_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65ced152167be8a5f328bf7c"", ""modelId"": ""tsavage68/chat_1000STEPS_1e7rate_SFT_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e7rate_SFT_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e7rate_SFT_SFT%5D(%2Ftsavage68%2Fchat_1000STEPS_1e7rate_SFT_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e6rate_SFT_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e6rate_SFT_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e6rate_SFT_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3054

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.3957        | 0.2   | 100  | 0.3739          |
| 0.3295        | 0.39  | 200  | 0.3239          |
| 0.3211        | 0.59  | 300  | 0.3141          |
| 0.3047        | 0.78  | 400  | 0.3095          |
| 0.3072        | 0.98  | 500  | 0.3072          |
| 0.3006        | 1.17  | 600  | 0.3060          |
| 0.3109        | 1.37  | 700  | 0.3055          |
| 0.2994        | 1.56  | 800  | 0.3054          |
| 0.3219        | 1.76  | 900  | 0.3054          |
| 0.3016        | 1.95  | 1000 | 0.3054          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e6rate_SFT_SFT"", ""author"": ""tsavage68"", ""sha"": ""607e8754af0e81f6763102393cd701df5048ec61"", ""last_modified"": ""2024-02-16 04:51:33+00:00"", ""created_at"": ""2024-02-16 04:48:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6rate_SFT_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e6rate_SFT_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 04:51:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6rate_SFT_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cee91120c54d9349f6c621"", ""modelId"": ""tsavage68/chat_1000STEPS_1e6rate_SFT_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e6rate_SFT_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e6rate_SFT_SFT%5D(%2Ftsavage68%2Fchat_1000STEPS_1e6rate_SFT_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e6_05beta_DPO,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- dpo
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e6_05beta_DPO
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e6_05beta_DPO

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7047
- Rewards/chosen: -0.5484
- Rewards/rejected: -0.8442
- Rewards/accuracies: 0.5319
- Rewards/margins: 0.2958
- Logps/rejected: -20.4796
- Logps/chosen: -17.8414
- Logits/rejected: -0.6334
- Logits/chosen: -0.6333

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6923        | 0.2   | 100  | 0.6978          | -0.3692        | -0.4056          | 0.4549             | 0.0364          | -19.6025       | -17.4830     | -0.6054         | -0.6052       |
| 0.7106        | 0.39  | 200  | 0.7053          | 0.1136         | -0.0026          | 0.4791             | 0.1161          | -18.7964       | -16.5175     | -0.6058         | -0.6056       |
| 0.5991        | 0.59  | 300  | 0.7229          | -0.2199        | -0.3741          | 0.4879             | 0.1541          | -19.5394       | -17.1845     | -0.6117         | -0.6115       |
| 0.7082        | 0.78  | 400  | 0.7221          | -0.0056        | -0.1904          | 0.5033             | 0.1848          | -19.1721       | -16.7559     | -0.5870         | -0.5868       |
| 0.6684        | 0.98  | 500  | 0.7010          | -0.1029        | -0.3043          | 0.5275             | 0.2014          | -19.3998       | -16.9504     | -0.5454         | -0.5452       |
| 0.2004        | 1.17  | 600  | 0.6974          | -0.4104        | -0.6928          | 0.5341             | 0.2824          | -20.1768       | -17.5654     | -0.6005         | -0.6004       |
| 0.2715        | 1.37  | 700  | 0.7012          | -0.5147        | -0.8128          | 0.5429             | 0.2981          | -20.4169       | -17.7741     | -0.6258         | -0.6257       |
| 0.2303        | 1.56  | 800  | 0.7031          | -0.5366        | -0.8347          | 0.5341             | 0.2981          | -20.4606       | -17.8177     | -0.6321         | -0.6320       |
| 0.2729        | 1.76  | 900  | 0.7052          | -0.5480        | -0.8437          | 0.5341             | 0.2957          | -20.4787       | -17.8406     | -0.6333         | -0.6331       |
| 0.2621        | 1.95  | 1000 | 0.7047          | -0.5484        | -0.8442          | 0.5319             | 0.2958          | -20.4796       | -17.8414     | -0.6334         | -0.6333       |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e6_05beta_DPO"", ""author"": ""tsavage68"", ""sha"": ""5ea58e84a6f303611f34365808946798444ca002"", ""last_modified"": ""2024-02-16 06:36:41+00:00"", ""created_at"": ""2024-02-16 06:33:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6_05beta_DPO\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e6_05beta_DPO"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 06:36:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e6_05beta_DPO\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cf019cf35d963bd8f3073f"", ""modelId"": ""tsavage68/chat_1000STEPS_1e6_05beta_DPO"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e6_05beta_DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e6_05beta_DPO%5D(%2Ftsavage68%2Fchat_1000STEPS_1e6_05beta_DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
andreasnaoum/CounselLlama7b,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: CounselLlama7b
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# CounselLlama7b

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0039

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 1.0301        | 1.0   | 450  | 1.0757          |
| 1.1803        | 2.0   | 900  | 1.0411          |
| 1.2021        | 3.0   | 1350 | 1.0039          |


### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""andreasnaoum/CounselLlama7b"", ""author"": ""andreasnaoum"", ""sha"": ""cdc60233155b304bba7d3fbc7947479afdfe1f58"", ""last_modified"": ""2024-02-16 11:35:06+00:00"", ""created_at"": ""2024-02-16 11:34:50+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7b\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""CounselLlama7b"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 11:35:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: CounselLlama7b\n  results: []"", ""transformersInfo"": null, ""_id"": ""65cf485ab104eb084a35229a"", ""modelId"": ""andreasnaoum/CounselLlama7b"", ""usedStorage"": 134739307}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=andreasnaoum/CounselLlama7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bandreasnaoum%2FCounselLlama7b%5D(%2Fandreasnaoum%2FCounselLlama7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tsavage68/chat_1000STEPS_1e5rate_SFT_SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: chat_1000STEPS_1e5rate_SFT_SFT
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chat_1000STEPS_1e5rate_SFT_SFT

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2871

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 1
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1000

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.3537        | 0.2   | 100  | 0.3273          |
| 0.316         | 0.39  | 200  | 0.3060          |
| 0.3025        | 0.59  | 300  | 0.2980          |
| 0.2896        | 0.78  | 400  | 0.2924          |
| 0.2881        | 0.98  | 500  | 0.2859          |
| 0.2164        | 1.17  | 600  | 0.2897          |
| 0.2211        | 1.37  | 700  | 0.2882          |
| 0.2047        | 1.56  | 800  | 0.2882          |
| 0.223         | 1.76  | 900  | 0.2871          |
| 0.2098        | 1.95  | 1000 | 0.2871          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.0.0+cu117
- Datasets 2.17.0
- Tokenizers 0.15.2
","{""id"": ""tsavage68/chat_1000STEPS_1e5rate_SFT_SFT"", ""author"": ""tsavage68"", ""sha"": ""77a9facd6366b8fb0d9ff0c8ee1b32bd70890f21"", ""last_modified"": ""2024-02-16 15:24:14+00:00"", ""created_at"": ""2024-02-16 15:20:46+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e5rate_SFT_SFT\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""chat_1000STEPS_1e5rate_SFT_SFT"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-02-16 15:24:14+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: chat_1000STEPS_1e5rate_SFT_SFT\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65cf7d4e0954f06e47dc5ed5"", ""modelId"": ""tsavage68/chat_1000STEPS_1e5rate_SFT_SFT"", ""usedStorage"": 13476868971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tsavage68/chat_1000STEPS_1e5rate_SFT_SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2Fchat_1000STEPS_1e5rate_SFT_SFT%5D(%2Ftsavage68%2Fchat_1000STEPS_1e5rate_SFT_SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
myra/broadening_llama_chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: broadening_llama_chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# broadening_llama_chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- total_train_batch_size: 4
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.37.2
- Pytorch 2.2.0+cu121
- Tokenizers 0.15.1
","{""id"": ""myra/broadening_llama_chat"", ""author"": ""myra"", ""sha"": ""e5ff8dc92db9e01dac0f4f14b38f10aa41586b4c"", ""last_modified"": ""2024-02-19 20:29:56+00:00"", ""created_at"": ""2024-02-17 18:30:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: broadening_llama_chat\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""broadening_llama_chat"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738423808}, ""total"": 6738423808}, ""security_repo_status"": null, ""lastModified"": ""2024-02-19 20:29:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: broadening_llama_chat\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65d0fb3588d13d81280ac462"", ""modelId"": ""myra/broadening_llama_chat"", ""usedStorage"": 26954233571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=myra/broadening_llama_chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmyra%2Fbroadening_llama_chat%5D(%2Fmyra%2Fbroadening_llama_chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
myra/counterexamples_llama_chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: counterexamples_llama_chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# counterexamples_llama_chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- total_train_batch_size: 4
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.37.2
- Pytorch 2.2.0+cu121
- Tokenizers 0.15.1
","{""id"": ""myra/counterexamples_llama_chat"", ""author"": ""myra"", ""sha"": ""746613e63ccebaaa90dc299f18077e048c0b7d62"", ""last_modified"": ""2024-02-20 00:26:43+00:00"", ""created_at"": ""2024-02-17 23:28:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: counterexamples_llama_chat\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""counterexamples_llama_chat"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738423808}, ""total"": 6738423808}, ""security_repo_status"": null, ""lastModified"": ""2024-02-20 00:26:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: counterexamples_llama_chat\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65d14116c4d2b2e402a421e3"", ""modelId"": ""myra/counterexamples_llama_chat"", ""usedStorage"": 26954233571}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=myra/counterexamples_llama_chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmyra%2Fcounterexamples_llama_chat%5D(%2Fmyra%2Fcounterexamples_llama_chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
myra/negation_llama_chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: negation_llama_chat
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# negation_llama_chat

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.37.2
- Pytorch 2.2.0+cu121
- Tokenizers 0.15.1
","{""id"": ""myra/negation_llama_chat"", ""author"": ""myra"", ""sha"": ""023efd85822fc40ed3725149bddb517498ed2bc9"", ""last_modified"": ""2024-02-19 17:18:19+00:00"", ""created_at"": ""2024-02-18 08:29:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: negation_llama_chat\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""negation_llama_chat"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738423808}, ""total"": 6738423808}, ""security_repo_status"": null, ""lastModified"": ""2024-02-19 17:18:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: negation_llama_chat\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65d1bff32383296176ca1fb5"", ""modelId"": ""myra/negation_llama_chat"", ""usedStorage"": 26954233507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=myra/negation_llama_chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmyra%2Fnegation_llama_chat%5D(%2Fmyra%2Fnegation_llama_chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yy0514/llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 2
- num_epochs: 2
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.17.1
- Tokenizers 0.15.2
","{""id"": ""yy0514/llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs"", ""author"": ""yy0514"", ""sha"": ""674fd3714948b7baf4c7aeb15cd15e293330b6bc"", ""last_modified"": ""2024-02-19 14:30:51+00:00"", ""created_at"": ""2024-02-19 13:28:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-19 14:30:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs\n  results: []"", ""transformersInfo"": null, ""_id"": ""65d357863d686f49a1bdc0b1"", ""modelId"": ""yy0514/llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs"", ""usedStorage"": 160472203}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yy0514/llama2-7b-chat-qlora-lek-train-for-medqa-2-epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byy0514%2Fllama2-7b-chat-qlora-lek-train-for-medqa-2-epochs%5D(%2Fyy0514%2Fllama2-7b-chat-qlora-lek-train-for-medqa-2-epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
SaiSiddhanth/llama-2-test,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama-2-test
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-test

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu121
- Datasets 2.17.1
- Tokenizers 0.15.2
","{""id"": ""SaiSiddhanth/llama-2-test"", ""author"": ""SaiSiddhanth"", ""sha"": ""6ade144cbce22c5f13f2f499e544d40360594fba"", ""last_modified"": ""2024-02-20 06:29:32+00:00"", ""created_at"": ""2024-02-20 06:20:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-test\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-test"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Feb20_06-27-02_c6a7be2cde27/events.out.tfevents.1708410423.c6a7be2cde27.417.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-02-20 06:29:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama-2-test\n  results: []"", ""transformersInfo"": null, ""_id"": ""65d444c3492611d68f284387"", ""modelId"": ""SaiSiddhanth/llama-2-test"", ""usedStorage"": 8914675}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=SaiSiddhanth/llama-2-test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSaiSiddhanth%2Fllama-2-test%5D(%2FSaiSiddhanth%2Fllama-2-test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dilip025/llama-2-7b,"---
language:
- en
license: llama2
tags:
- facebook
- meta
- pytorch
- llama
- llama-2
model_name: Llama 2 7B Chat
arxiv: 2307.09288
base_model: meta-llama/Llama-2-7b-chat-hf
inference: false
model_creator: Meta Llama 2
model_type: llama
pipeline_tag: text-generation
prompt_template: '[INST] <<SYS>>
  You are NutriLife chatbot, you are going to get questions related to food, nutrition, health, and diet by the users from Nepal. Answer them very shortly and accurately if the message is only about food, nutrition, and diet. Otherwise, ignore.
  <</SYS>>
  {prompt}[/INST]
  '
quantized_by: Dilip Pokhrel
---

<hr style=""margin-top: 1.0em; margin-bottom: 1.0em;"">
<!-- header end -->
# Llama 2 7B Chat -- Food and Nutrition
<br>
- Model creator: [Meta Llama 2]
<br>
- Original model: [Llama 2 7B Chat] <a href=""https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"">Original Model</a>
<br>
- Fine Tuned by: [Dilip Pokhrel] <a href=""https://dilippokhrel.com.np"">Profile</a>


#### Simple example code to load one of these GGUF models

```python
# Load model directly or use qunatization technique if you have low gpu ram

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(""dilip025/llama-2-7b"")
model = AutoModelForCausalLM.from_pretrained(""dilip025/llama-2-7b"")
system_message = 'You are NutriLife chatbot, you are going to get questions related to food, nutrition, health, and diet by the users from Nepal. Answer them very shortly and accurately if the message is only about food, nutrition, and diet. Otherwise, ignore.'

prompt = f""[INST] <<SYS>>\n{system_message}\n<</SYS>>\n\n Tell me some of the famous Nepali food recipes [/INST]""
num_new_tokens = 200  # Change to the number of new tokens you want to generate

# Count the number of tokens in the prompt
num_prompt_tokens = len(tokenizer(prompt)['input_ids'])

# Calculate the maximum length for the generation
max_length = num_prompt_tokens + num_new_tokens

gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)
result = gen(prompt)
print(result[0]['generated_text'].replace(prompt, ''))
```

## Ethical Considerations and Limitations
Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.
Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)","{""id"": ""dilip025/llama-2-7b"", ""author"": ""dilip025"", ""sha"": ""66366d3dc44518288f26037d96f5e9ff2a98ef80"", ""last_modified"": ""2024-03-10 14:39:26+00:00"", ""created_at"": ""2024-03-02 17:03:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""facebook"", ""meta"", ""llama-2"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama2\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_type: llama\nprompt_template: '[INST] <<SYS>> You are NutriLife chatbot, you are going to get questions\n  related to food, nutrition, health, and diet by the users from Nepal. Answer them\n  very shortly and accurately if the message is only about food, nutrition, and diet.\n  Otherwise, ignore. <</SYS>> {prompt}[/INST] '\nquantized_by: Dilip Pokhrel"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-03-10 14:39:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama2\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_type: llama\nprompt_template: '[INST] <<SYS>> You are NutriLife chatbot, you are going to get questions\n  related to food, nutrition, health, and diet by the users from Nepal. Answer them\n  very shortly and accurately if the message is only about food, nutrition, and diet.\n  Otherwise, ignore. <</SYS>> {prompt}[/INST] '\nquantized_by: Dilip Pokhrel"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65e35be175b43b925eafa5cc"", ""modelId"": ""dilip025/llama-2-7b"", ""usedStorage"": 13477450782}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dilip025/llama-2-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdilip025%2Fllama-2-7b%5D(%2Fdilip025%2Fllama-2-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
sh0men/autotrain-0pm1h-neolk,"---
tags:
- autotrain
- text-generation
- pytorch
- llama
- llama-2
widget:
- text: 'I love AutoTrain because '
license: other
datasets:
- sh0men/TestLSY
pipeline_tag: text-generation
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""sh0men/autotrain-0pm1h-neolk"", ""author"": ""sh0men"", ""sha"": ""f9ae4acf2599f62a687a55ec473c746b2ff2f836"", ""last_modified"": ""2024-03-09 17:34:49+00:00"", ""created_at"": ""2024-03-09 11:35:33+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""autotrain"", ""text-generation"", ""pytorch"", ""llama"", ""llama-2"", ""conversational"", ""dataset:sh0men/TestLSY"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sh0men/TestLSY\nlibrary_name: transformers\nlicense: other\npipeline_tag: text-generation\ntags:\n- autotrain\n- text-generation\n- pytorch\n- llama\n- llama-2\nwidget:\n- text: 'I love AutoTrain because '"", ""widget_data"": [{""text"": ""I love AutoTrain because ""}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-36/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='handler.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='requirements.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar09_11-33-19_r-sh0men-lsytrain-8psyttfi-6224a-l5gug/events.out.tfevents.1709984000.r-sh0men-lsytrain-8psyttfi-6224a-l5gug.54.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-03-09 17:34:49+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sh0men/TestLSY\nlibrary_name: transformers\nlicense: other\npipeline_tag: text-generation\ntags:\n- autotrain\n- text-generation\n- pytorch\n- llama\n- llama-2\nwidget:\n- text: 'I love AutoTrain because '"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""65ec49850c9227e967d4736e"", ""modelId"": ""sh0men/autotrain-0pm1h-neolk"", ""usedStorage"": 480687472}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=sh0men/autotrain-0pm1h-neolk&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsh0men%2Fautotrain-0pm1h-neolk%5D(%2Fsh0men%2Fautotrain-0pm1h-neolk)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
seanmemery/MLP-FinLLM-7b-it,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- unsloth
- generated_from_trainer
datasets:
- generator
model-index:
- name: MLP-FinLLM-7b-it
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# MLP-FinLLM-7b-it

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.4445

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0025177606136092684
- train_batch_size: 32
- eval_batch_size: 1
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: polynomial
- num_epochs: 4

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.5449        | 1.22  | 50   | 0.7534          |
| 0.3165        | 2.44  | 100  | 0.5170          |
| 0.2331        | 3.66  | 150  | 0.4445          |


### Framework versions

- Transformers 4.38.2
- Pytorch 2.1.2+cu121
- Datasets 2.18.0
- Tokenizers 0.15.2
","{""id"": ""seanmemery/MLP-FinLLM-7b-it"", ""author"": ""seanmemery"", ""sha"": ""5fa1c6564c6c0710381d94f93fe01dffd65ab584"", ""last_modified"": ""2024-03-15 16:09:12+00:00"", ""created_at"": ""2024-03-13 12:55:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""unsloth"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- unsloth\n- generated_from_trainer\nmodel-index:\n- name: MLP-FinLLM-7b-it\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""MLP-FinLLM-7b-it"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-15 16:09:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- unsloth\n- generated_from_trainer\nmodel-index:\n- name: MLP-FinLLM-7b-it\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65f1a22e85cad1062a6fcfce"", ""modelId"": ""seanmemery/MLP-FinLLM-7b-it"", ""usedStorage"": 17067863555}",1,https://huggingface.co/seanmemery/MLP-FinLLM-dpo-7b,1,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=seanmemery/MLP-FinLLM-7b-it&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bseanmemery%2FMLP-FinLLM-7b-it%5D(%2Fseanmemery%2FMLP-FinLLM-7b-it)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
seanmemery/MLP-FinLLM-dpo-7b,"---
base_model: seanmemery/MLP-FinLLM-7b-it
tags:
- trl
- dpo
- unsloth
- generated_from_trainer
model-index:
- name: MLP-FinLLM-dpo-7b
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# MLP-FinLLM-dpo-7b

This model is a fine-tuned version of [seanmemery/MLP-FinLLM-7b-it](https://huggingface.co/seanmemery/MLP-FinLLM-7b-it) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6707
- Rewards/chosen: 0.0674
- Rewards/rejected: -0.0381
- Rewards/accuracies: 0.2975
- Rewards/margins: 0.1055
- Logps/rejected: -22.125
- Logps/chosen: -29.875
- Logits/rejected: -5.8438
- Logits/chosen: -5.875

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-06
- train_batch_size: 32
- eval_batch_size: 1
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- num_epochs: 1

### Training results

| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |
|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|
| 0.6367        | 0.69  | 50   | 0.6707          | 0.0674         | -0.0381          | 0.2975             | 0.1055          | -22.125        | -29.875      | -5.8438         | -5.875        |


### Framework versions

- Transformers 4.38.2
- Pytorch 2.1.2+cu121
- Datasets 2.18.0
- Tokenizers 0.15.2
","{""id"": ""seanmemery/MLP-FinLLM-dpo-7b"", ""author"": ""seanmemery"", ""sha"": ""4b2e31d9b0e0a29ecc0b3c7479a53a8bcc487434"", ""last_modified"": ""2024-03-21 21:05:43+00:00"", ""created_at"": ""2024-03-13 19:55:57+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""dpo"", ""unsloth"", ""generated_from_trainer"", ""conversational"", ""base_model:seanmemery/MLP-FinLLM-7b-it"", ""base_model:finetune:seanmemery/MLP-FinLLM-7b-it"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: seanmemery/MLP-FinLLM-7b-it\ntags:\n- trl\n- dpo\n- unsloth\n- generated_from_trainer\nmodel-index:\n- name: MLP-FinLLM-dpo-7b\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""MLP-FinLLM-dpo-7b"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-21 21:05:43+00:00"", ""cardData"": ""base_model: seanmemery/MLP-FinLLM-7b-it\ntags:\n- trl\n- dpo\n- unsloth\n- generated_from_trainer\nmodel-index:\n- name: MLP-FinLLM-dpo-7b\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""65f204cd04834414fa8b59bf"", ""modelId"": ""seanmemery/MLP-FinLLM-dpo-7b"", ""usedStorage"": 175199810139}",2,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=seanmemery/MLP-FinLLM-dpo-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bseanmemery%2FMLP-FinLLM-dpo-7b%5D(%2Fseanmemery%2FMLP-FinLLM-dpo-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
calibration-tuning/Llama-2-7b-chat-hf-ct-choice,"---
library_name: transformers
tags:
- transformers
- peft
- arxiv:2406.08391
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
datasets:
- calibration-tuning/Llama-2-7b-chat-hf-20k-choice
---

# Model Card

**Llama 7B Chat CT-Choice** is a fine-tuned [Llama 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model that provides well-calibrated confidence estimates for multiple-choice question answering.

The model is fine-tuned (calibration-tuned) using a [dataset](https://huggingface.co/datasets/calibration-tuning/Llama-2-7b-chat-hf-20k-choice) of *multiple-choice* generations from `meta-llama/Llama-2-7b-chat-hf`, labeled for correctness. 
At test/inference time, the probability of correctness defines the confidence of the model in its answer. 
For full details, please see our [paper](https://arxiv.org/abs/2406.08391) and supporting [code](https://github.com/activatedgeek/calibration-tuning).

**Other Models**: We also release a broader collection of [Multiple-Choice CT Models](https://huggingface.co/collections/calibration-tuning/multiple-choice-ct-models-66043dedebf973d639090821).

## Usage

This adapter model is meant to be used on top of `meta-llama/Llama-2-7b-chat-hf` model generations.

The confidence estimation pipeline follows these steps,
1. Load base model and PEFT adapter.
2. Disable adapter and generate answer.
3. Enable adapter and generate confidence.

All standard guidelines for the base model's generation apply.

For a complete example, see [play.py](https://github.com/activatedgeek/calibration-tuning/blob/main/experiments/play.py) at the supporting code repository.

**NOTE**: Using the adapter for generations may hurt downstream task accuracy and confidence estimates. We recommend using the adapter to estimate *only* confidence.

## License

The model is released under the original model's Llama 2 Community License Agreement.","{""id"": ""calibration-tuning/Llama-2-7b-chat-hf-ct-choice"", ""author"": ""calibration-tuning"", ""sha"": ""575094e2b0c5b6447132ddceaff181fea8cc4dde"", ""last_modified"": ""2024-06-13 15:16:45+00:00"", ""created_at"": ""2024-03-19 04:25:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""peft"", ""arxiv:2406.08391"", ""dataset:calibration-tuning/Llama-2-7b-chat-hf-20k-choice"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- calibration-tuning/Llama-2-7b-chat-hf-20k-choice\nlibrary_name: transformers\nlicense: llama2\ntags:\n- transformers\n- peft\n- arxiv:2406.08391"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='temperature_model.pt', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-13 15:16:45+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- calibration-tuning/Llama-2-7b-chat-hf-20k-choice\nlibrary_name: transformers\nlicense: llama2\ntags:\n- transformers\n- peft\n- arxiv:2406.08391"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""65f913b7d2ce5aec40ca404a"", ""modelId"": ""calibration-tuning/Llama-2-7b-chat-hf-ct-choice"", ""usedStorage"": 532712007}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=calibration-tuning/Llama-2-7b-chat-hf-ct-choice&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bcalibration-tuning%2FLlama-2-7b-chat-hf-ct-choice%5D(%2Fcalibration-tuning%2FLlama-2-7b-chat-hf-ct-choice)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
calibration-tuning/Llama-2-7b-chat-hf-ct-oe,"---
library_name: transformers
tags:
- transformers
- peft
- arxiv:2406.08391
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
datasets:
- calibration-tuning/Llama-2-7b-chat-hf-20k-oe
---

# Model Card

**Llama 2 7B Chat CT-OE** is a fine-tuned [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model that provides well-calibrated confidence estimates for open-ended question answering.

The model is fine-tuned (calibration-tuned) using a [dataset](https://huggingface.co/datasets/calibration-tuning/Llama-2-7b-chat-hf-20k-oe) of *open-ended* generations from `meta-llama/Llama-2-7b-chat-hf`, labeled for correctness. 
At test/inference time, the probability of correctness defines the confidence of the model in its answer. 
For full details, please see our [paper](https://arxiv.org/abs/2406.08391) and supporting [code](https://github.com/activatedgeek/calibration-tuning).

**Other Models**: We also release a broader collection of [Open-Ended CT Models](https://huggingface.co/collections/calibration-tuning/open-ended-ct-models-66043b12c7902115c826a20e).

## Usage

This adapter model is meant to be used on top of `meta-llama/Llama-2-7b-chat-hf` model generations.

The confidence estimation pipeline follows these steps,
1. Load base model and PEFT adapter.
2. Disable adapter and generate answer.
3. Enable adapter and generate confidence.

All standard guidelines for the base model's generation apply.

For a complete example, see [play.py](https://github.com/activatedgeek/calibration-tuning/blob/main/experiments/play.py) at the supporting code repository.

**NOTE**: Using the adapter for generations may hurt downstream task accuracy and confidence estimates. We recommend using the adapter to estimate *only* confidence.

## License

The model is released under the original model's Llama 2 Community License Agreement.","{""id"": ""calibration-tuning/Llama-2-7b-chat-hf-ct-oe"", ""author"": ""calibration-tuning"", ""sha"": ""c992b5dfab03e9cf0c18157b8f5045484d208dbb"", ""last_modified"": ""2024-06-13 15:13:46+00:00"", ""created_at"": ""2024-03-19 04:31:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""peft"", ""arxiv:2406.08391"", ""dataset:calibration-tuning/Llama-2-7b-chat-hf-20k-oe"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- calibration-tuning/Llama-2-7b-chat-hf-20k-oe\nlibrary_name: transformers\nlicense: llama2\ntags:\n- transformers\n- peft\n- arxiv:2406.08391"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='temperature_model.pt', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-13 15:13:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- calibration-tuning/Llama-2-7b-chat-hf-20k-oe\nlibrary_name: transformers\nlicense: llama2\ntags:\n- transformers\n- peft\n- arxiv:2406.08391"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""65f9150eca387c9d458cca4e"", ""modelId"": ""calibration-tuning/Llama-2-7b-chat-hf-ct-oe"", ""usedStorage"": 532712007}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=calibration-tuning/Llama-2-7b-chat-hf-ct-oe&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bcalibration-tuning%2FLlama-2-7b-chat-hf-ct-oe%5D(%2Fcalibration-tuning%2FLlama-2-7b-chat-hf-ct-oe)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
thrunlab/sparse_llama_7b_refined_web_90p_2024-03-21,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: sparse_llama_7b_refined_web_90p_2024-03-21
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sparse_llama_7b_refined_web_90p_2024-03-21

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 3.6918

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 0
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 100

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 5.2955        | 0.01  | 25   | 5.1918          |
| 4.6783        | 0.02  | 50   | 4.6078          |
| 3.9425        | 0.02  | 75   | 3.9603          |
| 3.455         | 0.03  | 100  | 3.5515          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.1.1+cu121
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-21"", ""author"": ""thrunlab"", ""sha"": ""1db64b5750e29352733506eec610b04af550ba8b"", ""last_modified"": ""2024-03-22 03:17:05+00:00"", ""created_at"": ""2024-03-22 02:00:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""sparse_llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""custom_code"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-21\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""sparse_llama_7b_refined_web_90p_2024-03-21"", ""results"": []}], ""config"": {""architectures"": [""SparseLlamaForCausalLM""], ""auto_map"": {""AutoConfig"": ""ugly_utils.SparseLlamaConfig"", ""AutoModelForCausalLM"": ""ugly_utils.SparseLlamaForCausalLM""}, ""model_type"": ""sparse_llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='ugly_utils.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-22 03:17:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-21\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""_id"": ""65fce62a7a009ec44c107ae9"", ""modelId"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-21"", ""usedStorage"": 13982371632}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=thrunlab/sparse_llama_7b_refined_web_90p_2024-03-21&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-21%5D(%2Fthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-21)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
thrunlab/sparse_llama_7b_refined_web_90p_2024-03-22,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: sparse_llama_7b_refined_web_90p_2024-03-22
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sparse_llama_7b_refined_web_90p_2024-03-22

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.9625

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 0
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 200

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 5.3089        | 0.01  | 25   | 5.2010          |
| 4.692         | 0.02  | 50   | 4.5938          |
| 3.9452        | 0.02  | 75   | 3.9591          |
| 3.4555        | 0.03  | 100  | 3.5625          |
| 3.2557        | 0.04  | 125  | 3.3530          |
| 3.0322        | 0.05  | 150  | 3.2484          |
| 2.9015        | 0.06  | 175  | 3.1688          |
| 2.9955        | 0.06  | 200  | 3.1071          |


### Framework versions

- Transformers 4.37.2
- Pytorch 2.1.1+cu121
- Datasets 2.15.0
- Tokenizers 0.15.0
","{""id"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-22"", ""author"": ""thrunlab"", ""sha"": ""522703d6d17150ca73b1e18cefd0751eb17b853c"", ""last_modified"": ""2024-03-22 20:30:19+00:00"", ""created_at"": ""2024-03-22 10:41:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""sparse_llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""custom_code"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-22\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""sparse_llama_7b_refined_web_90p_2024-03-22"", ""results"": []}], ""config"": {""architectures"": [""SparseLlamaForCausalLM""], ""auto_map"": {""AutoConfig"": ""ugly_utils.SparseLlamaConfig"", ""AutoModelForCausalLM"": ""ugly_utils.SparseLlamaForCausalLM""}, ""model_type"": ""sparse_llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='ugly_utils.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-22 20:30:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-22\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""_id"": ""65fd605c5e35ae4c8e01fdc9"", ""modelId"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-22"", ""usedStorage"": 54665716808}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=thrunlab/sparse_llama_7b_refined_web_90p_2024-03-22&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-22%5D(%2Fthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-22)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
thrunlab/sparse_llama_7b_refined_web_90p_2024-03-23,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: sparse_llama_7b_refined_web_90p_2024-03-23
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sparse_llama_7b_refined_web_90p_2024-03-23

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.9593

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 0
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 200

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 5.2575        | 0.01  | 25   | 5.1578          |
| 4.636         | 0.02  | 50   | 4.5777          |
| 3.9255        | 0.02  | 75   | 3.9336          |
| 3.4456        | 0.03  | 100  | 3.5406          |
| 3.2456        | 0.04  | 125  | 3.3417          |
| 3.0263        | 0.05  | 150  | 3.2372          |
| 2.898         | 0.06  | 175  | 3.1641          |
| 2.9902        | 0.06  | 200  | 3.0936          |


### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.1+cu121
- Datasets 2.15.0
- Tokenizers 0.15.2
","{""id"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-23"", ""author"": ""thrunlab"", ""sha"": ""cfbe1f6e016ed09cc93255901a946c3779042d48"", ""last_modified"": ""2024-03-24 03:58:16+00:00"", ""created_at"": ""2024-03-24 01:53:55+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""sparse_llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""custom_code"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-23\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""sparse_llama_7b_refined_web_90p_2024-03-23"", ""results"": []}], ""config"": {""architectures"": [""SparseLlamaForCausalLM""], ""auto_map"": {""AutoConfig"": ""ugly_utils.SparseLlamaConfig"", ""AutoModelForCausalLM"": ""ugly_utils.SparseLlamaForCausalLM""}, ""model_type"": ""sparse_llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='ugly_utils.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-24 03:58:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_90p_2024-03-23\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""_id"": ""65ff87b3502b0336ffa653eb"", ""modelId"": ""thrunlab/sparse_llama_7b_refined_web_90p_2024-03-23"", ""usedStorage"": 121544541920}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=thrunlab/sparse_llama_7b_refined_web_90p_2024-03-23&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-23%5D(%2Fthrunlab%2Fsparse_llama_7b_refined_web_90p_2024-03-23)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: sparse_llama_7b_refined_web_50p_2024-03-24
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sparse_llama_7b_refined_web_50p_2024-03-24

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 2.1950

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 1
- eval_batch_size: 1
- seed: 0
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- total_eval_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- training_steps: 800

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.4969        | 0.01  | 25   | 2.7682          |
| 2.4532        | 0.02  | 50   | 2.7136          |
| 2.4855        | 0.02  | 75   | 2.6372          |
| 2.4368        | 0.03  | 100  | 2.6029          |
| 2.4952        | 0.04  | 125  | 2.5761          |
| 2.3209        | 0.05  | 150  | 2.5665          |
| 2.2798        | 0.06  | 175  | 2.5517          |
| 2.4447        | 0.06  | 200  | 2.5399          |
| 2.4008        | 0.07  | 225  | 2.5317          |
| 2.3508        | 0.08  | 250  | 2.5271          |
| 2.2851        | 0.09  | 275  | 2.5222          |
| 2.3171        | 0.1   | 300  | 2.5151          |
| 2.3594        | 0.1   | 325  | 2.5102          |
| 2.3233        | 0.11  | 350  | 2.5063          |
| 2.2479        | 0.12  | 375  | 2.5039          |
| 2.3484        | 0.13  | 400  | 2.5004          |
| 2.3252        | 0.14  | 425  | 2.4961          |
| 2.2819        | 0.14  | 450  | 2.4951          |
| 2.3504        | 0.15  | 475  | 2.4907          |
| 2.3745        | 0.16  | 500  | 2.4860          |
| 2.2705        | 0.17  | 525  | 2.4860          |
| 2.271         | 0.18  | 550  | 2.4836          |
| 2.3821        | 0.18  | 575  | 2.4820          |
| 2.2663        | 0.19  | 600  | 2.4795          |
| 2.2919        | 0.2   | 625  | 2.4764          |
| 2.3755        | 0.21  | 650  | 2.4718          |
| 2.2654        | 0.22  | 675  | 2.4745          |
| 2.2857        | 0.22  | 700  | 2.4723          |
| 2.3063        | 0.23  | 725  | 2.4716          |
| 2.2062        | 0.24  | 750  | 2.4698          |
| 2.2921        | 0.25  | 775  | 2.4664          |
| 2.3404        | 0.26  | 800  | 2.4676          |


### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.1+cu121
- Datasets 2.15.0
- Tokenizers 0.15.2
","{""id"": ""thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24"", ""author"": ""thrunlab"", ""sha"": ""49f8e175103d0184bbbd7b16af742dd8200a6930"", ""last_modified"": ""2024-03-24 18:20:27+00:00"", ""created_at"": ""2024-03-24 12:27:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""sparse_llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""custom_code"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_50p_2024-03-24\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""sparse_llama_7b_refined_web_50p_2024-03-24"", ""results"": []}], ""config"": {""architectures"": [""SparseLlamaForCausalLM""], ""auto_map"": {""AutoConfig"": ""ugly_utils.SparseLlamaConfig"", ""AutoModelForCausalLM"": ""ugly_utils.SparseLlamaForCausalLM""}, ""model_type"": ""sparse_llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='ugly_utils.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-03-24 18:20:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: sparse_llama_7b_refined_web_50p_2024-03-24\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": ""ugly_utils.SparseLlamaForCausalLM"", ""pipeline_tag"": ""text-generation"", ""processor"": null}, ""_id"": ""66001c39d2a378a163f44737"", ""modelId"": ""thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24"", ""usedStorage"": 82502755288}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthrunlab%2Fsparse_llama_7b_refined_web_50p_2024-03-24%5D(%2Fthrunlab%2Fsparse_llama_7b_refined_web_50p_2024-03-24)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
FriendliAI/Llama-2-7b-chat-hf-fp8,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
inference: false
language:
  - en
model_creator: Meta Llama 2
model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
model_name: Llama 2 7B Chat
model_type: llama
pipeline_tag: text-generation
quantized_by: FriendliAI
tags:
- facebook
- meta
- pytorch
- llama
- llama-2
arxiv: 2307.09288
---

<!-- header start -->
<p align=""center"">
  <img src=""https://i.imgur.com/mNM6Cai.png"" width=""100%"" alt=""Friendli Logo"">
</p>
<!-- header end -->

# Llama 2 7B Chat - FP8

- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)
- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

## Description

This repo contains the Llama 2 7B chat model quantized to FP8 by FriendliAI, significantly enhancing its inference efficiency while maintaining high accuracy.
Note that FP8 is only supported by NVIDIA Ada, Hopper, and Blackwell GPU architectures.
Check out [FriendliAI documentation](https://docs.friendli.ai/) for more details.

## License

Refer to the license of the original model card.

## Compatibility

This model is compatible with **[Friendli Container](https://friendli.ai/products/container/)**.

## Prerequisites

- Before you begin, make sure you have signed up for [Friendli Suite](https://suite.friendli.ai/). **You can use Friendli Containers free of charge for four weeks.**
- Prepare a Personal Access Token following [this guide](#preparing-personal-access-token).
- Prepare a Friendli Container Secret following [this guide](#preparing-container-secret).

### Preparing Personal Access Token

PAT (Personal Access Token) is the user credential for for logging into our container registry.

1. Sign in [Friendli Suite](https://suite.friendli.ai/).
2. Go to **[User Settings > Tokens](https://suite.friendli.ai/user-settings/tokens)** and click **'Create new token'**.
3. Save your created token value.

### Preparing Container Secret

Container secret is a credential to launch our Friendli Container images.
You should pass the container secret as an environment variable to run the container image.

1. Sign in [Friendli Suite](https://suite.friendli.ai/).
2. Go to **Container > Container Secrets** and click **'Create secret'**.
3. Save your created secret value.

### Pulling Friendli Container Image

1. Log in to the Docker client using the personal access token created as outlined in [this guide](#preparing-personal-access-token).

  ```sh
  export FRIENDLI_PAT=""YOUR PAT""
  docker login registry.friendli.ai -u $YOUR_EMAIL -p $FRIENDLI_PAT
  ```

2. Pull image

  ```sh
  docker pull registry.friendli.ai/trial
  ```

## Running Friendli Container

Once you've prepared the image of Friendli Container, you can launch it to create a serving endpoint.

```sh
docker run \
  --gpus '""device=0""' \
  -p 8000:8000 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -e FRIENDLI_CONTAINER_SECRET=""YOUR CONTAINER SECRET"" \
  registry.friendli.ai/trial \
    --web-server-port 8000 \
    --hf-model-name FriendliAI/Llama-2-7b-chat-hf-fp8
```

---

# Original model card: Meta Llama 2's Llama 2 7B Chat

# **Llama 2**
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.

## Model Details
*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*

Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.

**Model Developers** Meta

**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.

**Input** Models input text only.

**Output** Models generate text only.

**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.


||Training Data|Params|Content Length|GQA|Tokens|LR|
|---|---|---|---|---|---|---|
|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|
|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|

*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.

**Model Dates** Llama 2 was trained between January 2023 and July 2023.

**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.

**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

**Research Paper** [""Llama-2: Open Foundation and Fine-tuned Chat Models""](https://arxiv.org/abs/2307.09288)

## Intended Use
**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.

To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).

**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.

## Hardware and Software
**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.

**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.

||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|
|---|---|---|---|
|Llama 2 7B|184320|400|31.22|
|Llama 2 13B|368640|400|62.44|
|Llama 2 70B|1720320|400|291.42|
|Total|3311616||539.00|

**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.

## Training Data
**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.

**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.

## Evaluation Results

In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.

|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|
|---|---|---|---|---|---|---|---|---|---|
|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|
|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|
|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|
|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|
|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|
|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|
|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|

**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.

|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama 1|7B|27.42|23.00|
|Llama 1|13B|41.74|23.08|
|Llama 1|33B|44.19|22.57|
|Llama 1|65B|48.71|21.77|
|Llama 2|7B|33.29|**21.25**|
|Llama 2|13B|41.86|26.10|
|Llama 2|70B|**50.18**|24.60|

**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).


|||TruthfulQA|Toxigen|
|---|---|---|---|
|Llama-2-Chat|7B|57.04|**0.00**|
|Llama-2-Chat|13B|62.18|**0.00**|
|Llama-2-Chat|70B|**64.14**|0.01|

**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.

## Ethical Considerations and Limitations
Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.

Please see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)

## Reporting Issues
Please report any software “bug,” or other problems with the models through one of the following means:
- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)
- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)

## Llama Model Index
|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|
|---|---|---|---|---|
|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|
|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|
|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|
","{""id"": ""FriendliAI/Llama-2-7b-chat-hf-fp8"", ""author"": ""FriendliAI"", ""sha"": ""f04be4e09ed44511b208308c8e3dcffde4b89820"", ""last_modified"": ""2024-04-19 08:27:34+00:00"", ""created_at"": ""2024-03-28 10:26:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 21, ""downloads_all_time"": null, ""likes"": 10, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""facebook"", ""meta"", ""pytorch"", ""llama-2"", ""conversational"", ""en"", ""arxiv:2307.09288"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""8-bit"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama2\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: FriendliAI\narxiv: 2307.09288"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 262410240, ""I8"": 6476005376}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-19 08:27:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama2\nmodel_name: Llama 2 7B Chat\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\nquantized_by: FriendliAI\narxiv: 2307.09288"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""660545e5893cf5035f72d030"", ""modelId"": ""FriendliAI/Llama-2-7b-chat-hf-fp8"", ""usedStorage"": 7000904208}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=FriendliAI/Llama-2-7b-chat-hf-fp8&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFriendliAI%2FLlama-2-7b-chat-hf-fp8%5D(%2FFriendliAI%2FLlama-2-7b-chat-hf-fp8)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-100k_50_50,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-100k_50_50
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-100k_50_50

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-100k_50_50"", ""author"": ""Niyantha23M"", ""sha"": ""5b27428b750ce5aa96615a207ab5658d9b7904a6"", ""last_modified"": ""2024-04-11 09:16:03+00:00"", ""created_at"": ""2024-04-11 09:15:57+00:00"", ""private"": false, ""gated"": ""manual"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-100k_50_50\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-100k_50_50"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-11 09:16:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-100k_50_50\n  results: []"", ""transformersInfo"": null, ""_id"": ""6617aa4d40a96eda66d10def"", ""modelId"": ""Niyantha23M/llama-7b-chat-100k_50_50"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-100k_50_50&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-100k_50_50%5D(%2FNiyantha23M%2Fllama-7b-chat-100k_50_50)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-100k_65_35,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-100k_65_35
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-100k_65_35

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-100k_65_35"", ""author"": ""Niyantha23M"", ""sha"": ""3a0df20d34ffd4f2f5103aca50b7d8c93604e548"", ""last_modified"": ""2024-04-11 10:34:53+00:00"", ""created_at"": ""2024-04-11 10:34:46+00:00"", ""private"": false, ""gated"": ""manual"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-100k_65_35\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-100k_65_35"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-11 10:34:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-100k_65_35\n  results: []"", ""transformersInfo"": null, ""_id"": ""6617bcc66976218a013ff679"", ""modelId"": ""Niyantha23M/llama-7b-chat-100k_65_35"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-100k_65_35&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-100k_65_35%5D(%2FNiyantha23M%2Fllama-7b-chat-100k_65_35)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-25k_50_50,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-25k_50_50
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-25k_50_50

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-25k_50_50"", ""author"": ""Niyantha23M"", ""sha"": ""d0e961b728010085ebc667494879fb69631ec61b"", ""last_modified"": ""2024-04-12 00:42:58+00:00"", ""created_at"": ""2024-04-12 00:42:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25k_50_50\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-25k_50_50"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 00:42:58+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25k_50_50\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618838b685fb848bc4c7780"", ""modelId"": ""Niyantha23M/llama-7b-chat-25k_50_50"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-25k_50_50&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-25k_50_50%5D(%2FNiyantha23M%2Fllama-7b-chat-25k_50_50)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_v1,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_v1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 4
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2
- Datasets 2.16.0
- Tokenizers 0.15.0
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_v1"", ""author"": ""yzhuang"", ""sha"": ""ca2b66bf1a7b00dddaa2a6155b5ecde659c0d781"", ""last_modified"": ""2024-04-12 21:19:51+00:00"", ""created_at"": ""2024-04-12 01:15:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_v1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_v1"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 21:19:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_v1\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66188b40a7c93dc43f60bc7b"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_v1"", ""usedStorage"": 17877585971}",1,,0,,0,https://huggingface.co/mradermacher/Llama-2-7b-chat-hf_fictional_v1-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_v1%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-dummy,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-dummy
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-dummy

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-dummy"", ""author"": ""Niyantha23M"", ""sha"": ""78865e1fc497ac4ed8357f3832fa9b01f235ac9b"", ""last_modified"": ""2024-04-12 01:44:39+00:00"", ""created_at"": ""2024-04-12 01:44:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-dummy\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-dummy"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 01:44:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-dummy\n  results: []"", ""transformersInfo"": null, ""_id"": ""661891f8a9e939c58464498d"", ""modelId"": ""Niyantha23M/llama-7b-chat-dummy"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-dummy&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-dummy%5D(%2FNiyantha23M%2Fllama-7b-chat-dummy)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_v2,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_v2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_v2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results



### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.2
- Datasets 2.16.0
- Tokenizers 0.15.0
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_v2"", ""author"": ""yzhuang"", ""sha"": ""ce163815abbb0e4ae521c86624a85ebd5a959df4"", ""last_modified"": ""2024-04-12 19:21:53+00:00"", ""created_at"": ""2024-04-12 04:31:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_v2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_v2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 19:21:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_v2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6618b92ede161bdcb0fdd2de"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_v2"", ""usedStorage"": 17877585971}",1,,0,,0,https://huggingface.co/mradermacher/Llama-2-7b-chat-hf_fictional_v2-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_v2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_v2%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_v2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-25000-50-50-L,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-25000-50-50-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-25000-50-50-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-25000-50-50-L"", ""author"": ""Niyantha23M"", ""sha"": ""6fb349f84e9da7130a592161f31b7042d3389606"", ""last_modified"": ""2024-04-12 04:54:32+00:00"", ""created_at"": ""2024-04-12 04:54:25+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-50-50-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-25000-50-50-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 04:54:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-50-50-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618be8173248041b8734433"", ""modelId"": ""Niyantha23M/llama-7b-chat-25000-50-50-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-25000-50-50-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-25000-50-50-L%5D(%2FNiyantha23M%2Fllama-7b-chat-25000-50-50-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-25000-25-75-L,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-25000-25-75-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-25000-25-75-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-25000-25-75-L"", ""author"": ""Niyantha23M"", ""sha"": ""c9a75dc241f3cecfa49ba368c1e3d43de30c978f"", ""last_modified"": ""2024-04-12 06:57:35+00:00"", ""created_at"": ""2024-04-12 06:57:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-25-75-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-25000-25-75-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 06:57:35+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-25-75-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618db5908ef6c5b4e7a85b7"", ""modelId"": ""Niyantha23M/llama-7b-chat-25000-25-75-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-25000-25-75-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-25000-25-75-L%5D(%2FNiyantha23M%2Fllama-7b-chat-25000-25-75-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-25000-75-25-L,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-25000-75-25-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-25000-75-25-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-25000-75-25-L"", ""author"": ""Niyantha23M"", ""sha"": ""6cfa6bf45a15ebb50f346cc14b10b7908e2d77b8"", ""last_modified"": ""2024-04-12 06:57:50+00:00"", ""created_at"": ""2024-04-12 06:57:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-75-25-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-25000-75-25-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 06:57:50+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-25000-75-25-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618db6662884d657d99d108"", ""modelId"": ""Niyantha23M/llama-7b-chat-25000-75-25-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-25000-75-25-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-25000-75-25-L%5D(%2FNiyantha23M%2Fllama-7b-chat-25000-75-25-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-75000-25-75-L,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-75000-25-75-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-75000-25-75-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-75000-25-75-L"", ""author"": ""Niyantha23M"", ""sha"": ""ce143a6b286a2737bfaef210d290d1273a466892"", ""last_modified"": ""2024-04-12 09:09:29+00:00"", ""created_at"": ""2024-04-12 09:09:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-75000-25-75-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-75000-25-75-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 09:09:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-75000-25-75-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618fa442546edcbdce41dbd"", ""modelId"": ""Niyantha23M/llama-7b-chat-75000-25-75-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-75000-25-75-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-75000-25-75-L%5D(%2FNiyantha23M%2Fllama-7b-chat-75000-25-75-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-75000-50-50-L,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-75000-50-50-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-75000-50-50-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-75000-50-50-L"", ""author"": ""Niyantha23M"", ""sha"": ""824e57eca665587d57be99aa9d07500e6531ccaf"", ""last_modified"": ""2024-04-12 09:30:25+00:00"", ""created_at"": ""2024-04-12 09:30:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-75000-50-50-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-75000-50-50-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-12 09:30:25+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-75000-50-50-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6618ff296b786b742e0bcd36"", ""modelId"": ""Niyantha23M/llama-7b-chat-75000-50-50-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-75000-50-50-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-75000-50-50-L%5D(%2FNiyantha23M%2Fllama-7b-chat-75000-50-50-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-Non-Toxic-143k,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-Non-Toxic-143k
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-Non-Toxic-143k

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-Non-Toxic-143k"", ""author"": ""Niyantha23M"", ""sha"": ""f10f0d7b7a6d4f5a1e10e4d296a686b510a6f5a9"", ""last_modified"": ""2024-04-13 02:21:26+00:00"", ""created_at"": ""2024-04-13 02:21:19+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-Non-Toxic-143k\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-Non-Toxic-143k"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-13 02:21:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-Non-Toxic-143k\n  results: []"", ""transformersInfo"": null, ""_id"": ""6619ec1fd7c07238c2a1605f"", ""modelId"": ""Niyantha23M/llama-7b-chat-Non-Toxic-143k"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-Non-Toxic-143k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-Non-Toxic-143k%5D(%2FNiyantha23M%2Fllama-7b-chat-Non-Toxic-143k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-Toxic-50k,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-Toxic-50k
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-Toxic-50k

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-Toxic-50k"", ""author"": ""Niyantha23M"", ""sha"": ""986ca99b0346b98311b89e1fcbd020ca94e406d6"", ""last_modified"": ""2024-04-13 09:34:50+00:00"", ""created_at"": ""2024-04-13 09:34:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-Toxic-50k\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-Toxic-50k"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-13 09:34:50+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-Toxic-50k\n  results: []"", ""transformersInfo"": null, ""_id"": ""661a51b4cb3c7aacef8eb8db"", ""modelId"": ""Niyantha23M/llama-7b-chat-Toxic-50k"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-Toxic-50k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-Toxic-50k%5D(%2FNiyantha23M%2Fllama-7b-chat-Toxic-50k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jfo150/llama-2-brainstems-chat,"---
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 1

### Framework versions

- Transformers 4.39.3
- Pytorch 2.2.2
- Datasets 2.18.0
- Tokenizers 0.15.2
","{""id"": ""jfo150/llama-2-brainstems-chat"", ""author"": ""jfo150"", ""sha"": ""5bad5555f90ccaabfaa23ff6d0896a9d0c1a6fbb"", ""last_modified"": ""2024-04-16 05:24:32+00:00"", ""created_at"": ""2024-04-16 03:41:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama""}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-16 05:24:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""661df377bca423783d556d92"", ""modelId"": ""jfo150/llama-2-brainstems-chat"", ""usedStorage"": 26953701016}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jfo150/llama-2-brainstems-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjfo150%2Fllama-2-brainstems-chat%5D(%2Fjfo150%2Fllama-2-brainstems-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-10000-75-25-L,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-10000-75-25-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-10000-75-25-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-10000-75-25-L"", ""author"": ""Niyantha23M"", ""sha"": ""b1bafb7b82ca55e99c92c49a10967b7cbebd59a8"", ""last_modified"": ""2024-04-18 05:07:06+00:00"", ""created_at"": ""2024-04-18 05:07:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-10000-75-25-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-10000-75-25-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-18 05:07:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-10000-75-25-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6620aa7411561bf97910c5e1"", ""modelId"": ""Niyantha23M/llama-7b-chat-10000-75-25-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-10000-75-25-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-10000-75-25-L%5D(%2FNiyantha23M%2Fllama-7b-chat-10000-75-25-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Niyantha23M/llama-7b-chat-10000-25-75-L,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-7b-chat-10000-25-75-L
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-7b-chat-10000-25-75-L

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2200
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 4400
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.33.3
- Pytorch 2.2.1
- Datasets 2.18.0
- Tokenizers 0.13.3
","{""id"": ""Niyantha23M/llama-7b-chat-10000-25-75-L"", ""author"": ""Niyantha23M"", ""sha"": ""997e20b213e77924ab6717ab4c85ede36cc76e9b"", ""last_modified"": ""2024-04-18 06:35:41+00:00"", ""created_at"": ""2024-04-18 06:35:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-10000-25-75-L\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-7b-chat-10000-25-75-L"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-18 06:35:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-7b-chat-10000-25-75-L\n  results: []"", ""transformersInfo"": null, ""_id"": ""6620bf375d61dc5d2136c31c"", ""modelId"": ""Niyantha23M/llama-7b-chat-10000-25-75-L"", ""usedStorage"": 67659597}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Niyantha23M/llama-7b-chat-10000-25-75-L&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNiyantha23M%2Fllama-7b-chat-10000-25-75-L%5D(%2FNiyantha23M%2Fllama-7b-chat-10000-25-75-L)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_chinese_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_chinese_v1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 10

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1"", ""author"": ""yzhuang"", ""sha"": ""7d708194ce40d1157d0ecb420511bf6b821818f5"", ""last_modified"": ""2024-04-22 18:48:00+00:00"", ""created_at"": ""2024-04-21 18:18:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_chinese_v1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_chinese_v1"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-22 18:48:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_chinese_v1\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66255879ea4f4ed066afc47d"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1"", ""usedStorage"": 17877586291}",1,https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v2,1,,0,https://huggingface.co/mradermacher/Llama-2-7b-chat-hf_fictional_chinese_v1-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_chinese_v1%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_chinese_v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v2,"---
base_model: yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_chinese_v2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_chinese_v2

This model is a fine-tuned version of [yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1](https://huggingface.co/yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 12

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v2"", ""author"": ""yzhuang"", ""sha"": ""d5c92d8434c28e46aa22495b46f76f5f508e602c"", ""last_modified"": ""2024-04-22 18:49:17+00:00"", ""created_at"": ""2024-04-22 04:13:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1"", ""base_model:finetune:yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_chinese_v2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_chinese_v2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-22 18:49:17+00:00"", ""cardData"": ""base_model: yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v1\ndatasets:\n- generator\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_chinese_v2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6625e3e3b642e29cdf299eb3"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v2"", ""usedStorage"": 13477369755}",2,,0,,0,https://huggingface.co/mradermacher/Llama-2-7b-chat-hf_fictional_chinese_v2-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_chinese_v2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_chinese_v2%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_chinese_v2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_Korean_v1,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_Korean_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_Korean_v1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 12

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_Korean_v1"", ""author"": ""yzhuang"", ""sha"": ""7db9ed3c07ce3dd1851869cc6078659c24004d97"", ""last_modified"": ""2024-04-23 10:55:42+00:00"", ""created_at"": ""2024-04-23 00:12:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 7, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_Korean_v1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_Korean_v1"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-23 10:55:42+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_Korean_v1\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6626fce1c657fc6df9e5411a"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_Korean_v1"", ""usedStorage"": 13477369819}",1,,0,,0,https://huggingface.co/mradermacher/Llama-2-7b-chat-hf_fictional_Korean_v1-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_Korean_v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_Korean_v1%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_Korean_v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
sohamslc5/new_llama_new,"---
datasets:
- sohamslc5/curr1
language:
- en
metrics:
- accuracy
pipeline_tag: text-generation
base_model: ""meta-llama/Llama-2-7b-chat-hf""
---","{""id"": ""sohamslc5/new_llama_new"", ""author"": ""sohamslc5"", ""sha"": ""68b5325bb0b32d3f494feffa108beedf10af6947"", ""last_modified"": ""2024-04-25 21:37:54+00:00"", ""created_at"": ""2024-04-24 11:56:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 8, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""en"", ""dataset:sohamslc5/curr1"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sohamslc5/curr1\nlanguage:\n- en\nmetrics:\n- accuracy\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-25 21:37:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sohamslc5/curr1\nlanguage:\n- en\nmetrics:\n- accuracy\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6628f38430936f0d8b11f106"", ""modelId"": ""sohamslc5/new_llama_new"", ""usedStorage"": 13476954513}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=sohamslc5/new_llama_new&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsohamslc5%2Fnew_llama_new%5D(%2Fsohamslc5%2Fnew_llama_new)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
sohamslc5/IIITA-Chatbot,"---
language:
- en
metrics:
- accuracy
library_name: transformers
pipeline_tag: text-generation
base_model: meta-llama/Llama-2-7b-chat-hf
datasets:
- sohamslc5/curr1
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""sohamslc5/IIITA-Chatbot"", ""author"": ""sohamslc5"", ""sha"": ""0c34cf41618f928c739e8e2bf70c514a9139d974"", ""last_modified"": ""2024-04-24 19:38:57+00:00"", ""created_at"": ""2024-04-24 12:49:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""text-generation"", ""en"", ""dataset:sohamslc5/curr1"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sohamslc5/curr1\nlanguage:\n- en\nlibrary_name: transformers\nmetrics:\n- accuracy\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-24 19:38:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sohamslc5/curr1\nlanguage:\n- en\nlibrary_name: transformers\nmetrics:\n- accuracy\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6628ffd1f33c0e63b542d10a"", ""modelId"": ""sohamslc5/IIITA-Chatbot"", ""usedStorage"": 16794200}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=sohamslc5/IIITA-Chatbot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsohamslc5%2FIIITA-Chatbot%5D(%2Fsohamslc5%2FIIITA-Chatbot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v1,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_arc_easy_english_v1

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 24

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v1"", ""author"": ""yzhuang"", ""sha"": ""ebf32779fef098bfdf90825d459a31bdbd6692f6"", ""last_modified"": ""2024-04-25 08:29:34+00:00"", ""created_at"": ""2024-04-25 07:10:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v1\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_arc_easy_english_v1"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-25 08:29:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v1\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""662a01e221e410e67c604494"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v1"", ""usedStorage"": 13477369819}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v1%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v2,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_arc_easy_english_v2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 18

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v2"", ""author"": ""yzhuang"", ""sha"": ""1b366199d541bc27ea5d92b75cd2e78834a3f97a"", ""last_modified"": ""2024-04-26 20:21:41+00:00"", ""created_at"": ""2024-04-25 19:12:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_arc_easy_english_v2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-26 20:21:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""662aab2d4a5b49e81b4627d7"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v2"", ""usedStorage"": 26954239915}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v2%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v3,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# Llama-2-7b-chat-hf_fictional_arc_easy_english_v3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 18

### Training results



### Framework versions

- Transformers 4.40.0
- Pytorch 2.1.2
- Datasets 2.19.0
- Tokenizers 0.19.1
","{""id"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v3"", ""author"": ""yzhuang"", ""sha"": ""9e1de238ad0437e0e313852cba3545fb67ba88aa"", ""last_modified"": ""2024-04-26 23:14:19+00:00"", ""created_at"": ""2024-04-26 23:00:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 4, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v3\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""Llama-2-7b-chat-hf_fictional_arc_easy_english_v3"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-04-26 23:14:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: Llama-2-7b-chat-hf_fictional_arc_easy_english_v3\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""662c322b01e4fa6f01592669"", ""modelId"": ""yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v3"", ""usedStorage"": 13477369819}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yzhuang/Llama-2-7b-chat-hf_fictional_arc_easy_english_v3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v3%5D(%2Fyzhuang%2FLlama-2-7b-chat-hf_fictional_arc_easy_english_v3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
armanbabayan/Llama2_Immigration_Low_Chat,"---
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
base_model: meta-llama/Llama-2-7b-chat-hf
---

# Uploaded  model

- **Developed by:** armanbabayan
- **License:** apache-2.0
- **Finetuned from model :** meta-llama/Llama-2-7b-chat-hf

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.","{""id"": ""armanbabayan/Llama2_Immigration_Low_Chat"", ""author"": ""armanbabayan"", ""sha"": ""ce94aac6b82867b2e6bc4cd50dd71591a4ba9757"", ""last_modified"": ""2024-04-28 19:31:30+00:00"", ""created_at"": ""2024-04-28 17:15:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""text-generation-inference"", ""unsloth"", ""llama"", ""trl"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-04-28 19:31:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""662e84442b1b529a43dea221"", ""modelId"": ""armanbabayan/Llama2_Immigration_Low_Chat"", ""usedStorage"": 639691872}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=armanbabayan/Llama2_Immigration_Low_Chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Barmanbabayan%2FLlama2_Immigration_Low_Chat%5D(%2Farmanbabayan%2FLlama2_Immigration_Low_Chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3
metrics:
- accuracy
model-index:
- name: lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3
      type: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.6022051282051282
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3 dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9008
- Accuracy: 0.6022

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 10.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 1.4745        | 1.0   | 529  | 1.3418          | 0.6122   |
| 1.414         | 2.0   | 1058 | 1.3398          | 0.5886   |
| 1.3204        | 3.0   | 1587 | 1.3659          | 0.6158   |
| 1.1963        | 4.0   | 2116 | 1.4242          | 0.61     |
| 1.0807        | 5.0   | 2645 | 1.5381          | 0.608    |
| 0.9652        | 6.0   | 3174 | 1.6063          | 0.5807   |
| 0.8552        | 7.0   | 3703 | 1.6981          | 0.6037   |
| 0.759         | 8.0   | 4232 | 1.7846          | 0.6042   |
| 0.6433        | 9.0   | 4761 | 1.8386          | 0.6028   |
| 0.5475        | 10.0  | 5290 | 1.9008          | 0.6022   |


### Framework versions

- Transformers 4.34.0
- Pytorch 2.1.0+cu121
- Datasets 2.18.0
- Tokenizers 0.14.1
","{""id"": ""tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2"", ""author"": ""tyzhu"", ""sha"": ""8f29e4867bda6f81f3f78116597b0eb8eab8a3db"", ""last_modified"": ""2024-06-03 12:01:51+00:00"", ""created_at"": ""2024-06-03 05:09:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""dataset:tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""model-index"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\nlicense: llama2\nmetrics:\n- accuracy\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2\n  results:\n  - task:\n      type: text-generation\n      name: Causal Language Modeling\n    dataset:\n      name: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\n      type: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\n    metrics:\n    - type: accuracy\n      value: 0.6022051282051282\n      name: Accuracy\n      verified: false"", ""widget_data"": null, ""model_index"": [{""name"": ""lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2"", ""results"": [{""task"": {""name"": ""Causal Language Modeling"", ""type"": ""text-generation""}, ""dataset"": {""name"": ""tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3"", ""type"": ""tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3""}, ""metrics"": [{""name"": ""Accuracy"", ""type"": ""accuracy"", ""value"": 0.6022051282051282, ""verified"": false}]}]}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-03 12:01:51+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\nlicense: llama2\nmetrics:\n- accuracy\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2\n  results:\n  - task:\n      type: text-generation\n      name: Causal Language Modeling\n    dataset:\n      name: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\n      type: tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3\n    metrics:\n    - type: accuracy\n      value: 0.6022051282051282\n      name: Accuracy\n      verified: false"", ""transformersInfo"": null, ""_id"": ""665d4ffb386b4ea7be6c1f32"", ""modelId"": ""tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2"", ""usedStorage"": 1433198311}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tyzhu/lmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btyzhu%2Flmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2%5D(%2Ftyzhu%2Flmind_nq_train6000_eval6489_v1_doc_qa_v3_meta-llama_Llama-2-7b-chat-hf_lora2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
FemkeBakker/AmsterdamDocClassificationLlama200T2Epochs,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: AmsterdamDocClassificationLlama200T2Epochs
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# AmsterdamDocClassificationLlama200T2Epochs

As part of the Assessing Large Language Models for Document Classification project by the Municipality of Amsterdam, we fine-tune Mistral, Llama, and GEITje for document classification. 
The fine-tuning is performed using the [AmsterdamBalancedFirst200Tokens](https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens) dataset, which consists of documents truncated to the first 200 tokens. 
In our research, we evaluate the fine-tuning of these LLMs across one, two, and three epochs. 
This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and has been fine-tuned for two epochs.

It achieves the following results on the evaluation set:
- Loss: 0.8173


## Training and evaluation data

- The training data consists of 9900 documents and their labels formatted into conversations. 
- The evaluation data consists of 1100 documents and their labels formatted into conversations. 

## Training procedure

See the [GitHub](https://github.com/Amsterdam-Internships/document-classification-using-large-language-models) for specifics about the training and the code.

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 2

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 1.0345        | 0.1988 | 123  | 0.9800          |
| 0.8537        | 0.3976 | 246  | 0.8808          |
| 0.5807        | 0.5964 | 369  | 0.8503          |
| 0.7419        | 0.7952 | 492  | 0.8413          |
| 0.9967        | 0.9939 | 615  | 0.8406          |
| 0.7252        | 1.1939 | 738  | 0.8301          |
| 0.9605        | 1.3927 | 861  | 0.8214          |
| 0.7785        | 1.5915 | 984  | 0.8186          |
| 0.7233        | 1.7903 | 1107 | 0.8178          |
| 0.8389        | 1.9891 | 1230 | 0.8173          |

Training time: it took 80 minutes to fine-tune the model for two epochs.

### Framework versions

- Transformers 4.41.1
- Pytorch 2.3.0+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1

### Acknowledgements
This model was trained as part of [insert thesis info] in collaboration with Amsterdam Intelligence for the City of Amsterdam.","{""id"": ""FemkeBakker/AmsterdamDocClassificationLlama200T2Epochs"", ""author"": ""FemkeBakker"", ""sha"": ""d7c4fa2a97ace786a23ff5d84ce34887d406830d"", ""last_modified"": ""2024-07-12 13:28:09+00:00"", ""created_at"": ""2024-06-03 06:49:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: AmsterdamDocClassificationLlama200T2Epochs\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""AmsterdamDocClassificationLlama200T2Epochs"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun03_06-52-31_femke-gpu-24cores-220ram/events.out.tfevents.1717397689.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/May29_14-44-46_femke-gpu-24cores-220ram/events.out.tfevents.1716993891.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/May29_15-32-33_femke-gpu-24cores-220ram/events.out.tfevents.1716996856.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-07-12 13:28:09+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: AmsterdamDocClassificationLlama200T2Epochs\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""665d67790f35c005de8a230d"", ""modelId"": ""FemkeBakker/AmsterdamDocClassificationLlama200T2Epochs"", ""usedStorage"": 13477650307}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=FemkeBakker/AmsterdamDocClassificationLlama200T2Epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFemkeBakker%2FAmsterdamDocClassificationLlama200T2Epochs%5D(%2FFemkeBakker%2FAmsterdamDocClassificationLlama200T2Epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
tyzhu/lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
datasets:
- tyzhu/lmind_nq_train6000_eval6489_v1_qa
metrics:
- accuracy
model-index:
- name: lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2
  results:
  - task:
      name: Causal Language Modeling
      type: text-generation
    dataset:
      name: tyzhu/lmind_nq_train6000_eval6489_v1_qa
      type: tyzhu/lmind_nq_train6000_eval6489_v1_qa
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.5974358974358974
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the tyzhu/lmind_nq_train6000_eval6489_v1_qa dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9837
- Accuracy: 0.5974

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 10.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 1.8687        | 1.0   | 187  | 1.3245          | 0.6109   |
| 1.2052        | 2.0   | 375  | 1.3271          | 0.6131   |
| 0.9568        | 3.0   | 562  | 1.4014          | 0.6095   |
| 0.7696        | 4.0   | 750  | 1.5195          | 0.6054   |
| 0.6348        | 5.0   | 937  | 1.6407          | 0.6016   |
| 0.5592        | 6.0   | 1125 | 1.7334          | 0.5997   |
| 0.5166        | 7.0   | 1312 | 1.8043          | 0.5997   |
| 0.4911        | 8.0   | 1500 | 1.9042          | 0.5991   |
| 0.4494        | 9.0   | 1687 | 1.9244          | 0.5984   |
| 0.4399        | 9.97  | 1870 | 1.9837          | 0.5974   |


### Framework versions

- Transformers 4.34.0
- Pytorch 2.1.0+cu121
- Datasets 2.18.0
- Tokenizers 0.14.1
","{""id"": ""tyzhu/lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2"", ""author"": ""tyzhu"", ""sha"": ""ff30a8ccf9a9ff454da48a98f881a17c9cf3437a"", ""last_modified"": ""2024-06-03 11:40:01+00:00"", ""created_at"": ""2024-06-03 09:14:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""dataset:tyzhu/lmind_nq_train6000_eval6489_v1_qa"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""model-index"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tyzhu/lmind_nq_train6000_eval6489_v1_qa\nlicense: llama2\nmetrics:\n- accuracy\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2\n  results:\n  - task:\n      type: text-generation\n      name: Causal Language Modeling\n    dataset:\n      name: tyzhu/lmind_nq_train6000_eval6489_v1_qa\n      type: tyzhu/lmind_nq_train6000_eval6489_v1_qa\n    metrics:\n    - type: accuracy\n      value: 0.5974358974358974\n      name: Accuracy\n      verified: false"", ""widget_data"": null, ""model_index"": [{""name"": ""lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2"", ""results"": [{""task"": {""name"": ""Causal Language Modeling"", ""type"": ""text-generation""}, ""dataset"": {""name"": ""tyzhu/lmind_nq_train6000_eval6489_v1_qa"", ""type"": ""tyzhu/lmind_nq_train6000_eval6489_v1_qa""}, ""metrics"": [{""name"": ""Accuracy"", ""type"": ""accuracy"", ""value"": 0.5974358974358974, ""verified"": false}]}]}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-03 11:40:01+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tyzhu/lmind_nq_train6000_eval6489_v1_qa\nlicense: llama2\nmetrics:\n- accuracy\ntags:\n- generated_from_trainer\nmodel-index:\n- name: lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2\n  results:\n  - task:\n      type: text-generation\n      name: Causal Language Modeling\n    dataset:\n      name: tyzhu/lmind_nq_train6000_eval6489_v1_qa\n      type: tyzhu/lmind_nq_train6000_eval6489_v1_qa\n    metrics:\n    - type: accuracy\n      value: 0.5974358974358974\n      name: Accuracy\n      verified: false"", ""transformersInfo"": null, ""_id"": ""665d898b7d73ed1a16c8da97"", ""modelId"": ""tyzhu/lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2"", ""usedStorage"": 1433198311}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=tyzhu/lmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btyzhu%2Flmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2%5D(%2Ftyzhu%2Flmind_nq_train6000_eval6489_v1_qa_meta-llama_Llama-2-7b-chat-hf_lora2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: AmsterdamDocClassificationLlama200T3Epochs
  results: []
datasets:
- FemkeBakker/AmsterdamBalancedFirst200Tokens
language:
- nl
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# AmsterdamDocClassificationLlama200T3Epochs

As part of the Assessing Large Language Models for Document Classification project by the Municipality of Amsterdam, we fine-tune Mistral, Llama, and GEITje for document classification. 
The fine-tuning is performed using the [AmsterdamBalancedFirst200Tokens](https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens) dataset, which consists of documents truncated to the first 200 tokens. 
In our research, we evaluate the fine-tuning of these LLMs across one, two, and three epochs. 
This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and has been fine-tuned for three epochs.

It achieves the following results on the evaluation set:
- Loss: 0.8116


## Training and evaluation data

- The training data consists of 9900 documents and their labels formatted into conversations. 
- The evaluation data consists of 1100 documents and their labels formatted into conversations. 

## Training procedure

See the [GitHub](https://github.com/Amsterdam-Internships/document-classification-using-large-language-models) for specifics about the training and the code.

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 1.0345        | 0.1988 | 123  | 0.9800          |
| 0.8537        | 0.3976 | 246  | 0.8808          |
| 0.5807        | 0.5964 | 369  | 0.8503          |
| 0.7419        | 0.7952 | 492  | 0.8413          |
| 0.9967        | 0.9939 | 615  | 0.8406          |
| 0.7252        | 1.1939 | 738  | 0.8301          |
| 0.9605        | 1.3927 | 861  | 0.8214          |
| 0.7785        | 1.5915 | 984  | 0.8186          |
| 0.7233        | 1.7903 | 1107 | 0.8178          |
| 0.8389        | 1.9891 | 1230 | 0.8173          |
| 0.976         | 2.1891 | 1353 | 0.8148          |
| 0.6826        | 2.3879 | 1476 | 0.8127          |
| 0.7712        | 2.5867 | 1599 | 0.8117          |
| 0.9744        | 2.7855 | 1722 | 0.8116          |
| 1.0399        | 2.9842 | 1845 | 0.8116          |

Training time: in total it took 2 hours and 3 minutes to fine-tune the model for three epochs.


### Framework versions

- Transformers 4.41.1
- Pytorch 2.3.0+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1



### Acknowledgements
This model was trained as part of [insert thesis info] in collaboration with Amsterdam Intelligence for the City of Amsterdam.","{""id"": ""FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs"", ""author"": ""FemkeBakker"", ""sha"": ""61c1cc790e36d8cac4afed61a745836289a29193"", ""last_modified"": ""2024-07-12 13:30:04+00:00"", ""created_at"": ""2024-06-03 12:37:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""nl"", ""dataset:FemkeBakker/AmsterdamBalancedFirst200Tokens"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- FemkeBakker/AmsterdamBalancedFirst200Tokens\nlanguage:\n- nl\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: AmsterdamDocClassificationLlama200T3Epochs\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""AmsterdamDocClassificationLlama200T3Epochs"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/optimizer.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/rng_state.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-1854/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun03_06-52-31_femke-gpu-24cores-220ram/events.out.tfevents.1717397689.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun03_14-59-26_femke-gpu-24cores-220ram/events.out.tfevents.1717426829.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/May29_14-44-46_femke-gpu-24cores-220ram/events.out.tfevents.1716993891.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/May29_15-32-33_femke-gpu-24cores-220ram/events.out.tfevents.1716996856.femke-gpu-24cores-220ram', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-07-12 13:30:04+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- FemkeBakker/AmsterdamBalancedFirst200Tokens\nlanguage:\n- nl\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: AmsterdamDocClassificationLlama200T3Epochs\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""665db8fabcbb98f60d3dada1"", ""modelId"": ""FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs"", ""usedStorage"": 40458933124}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=FemkeBakker/AmsterdamDocClassificationLlama200T3Epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFemkeBakker%2FAmsterdamDocClassificationLlama200T3Epochs%5D(%2FFemkeBakker%2FAmsterdamDocClassificationLlama200T3Epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
FemkeBakker/AmsterdamDocClassificationLlama200T1Epochs,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: AmsterdamDocClassificationLlama200T1Epochs
  results: []
datasets:
- FemkeBakker/AmsterdamBalancedFirst200Tokens
language:
- nl
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# AmsterdamDocClassificationLlama200T1Epochs

As part of the Assessing Large Language Models for Document Classification project by the Municipality of Amsterdam, we fine-tune Mistral, Llama, and GEITje for document classification. 
The fine-tuning is performed using the [AmsterdamBalancedFirst200Tokens](https://huggingface.co/datasets/FemkeBakker/AmsterdamBalancedFirst200Tokens) dataset, which consists of documents truncated to the first 200 tokens. 
In our research, we evaluate the fine-tuning of these LLMs across one, two, and three epochs. 
This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and has been fine-tuned for one epoch.

It achieves the following results on the evaluation set:
- Loss: 0.8403


## Training and evaluation data

- The training data consists of 9900 documents and their labels formatted into conversations. 
- The evaluation data consists of 1100 documents and their labels formatted into conversations. 

## Training procedure

See the [GitHub](https://github.com/Amsterdam-Internships/document-classification-using-large-language-models) for specifics about the training and the code.

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 1.0328        | 0.1988 | 123  | 0.9794          |
| 0.8533        | 0.3976 | 246  | 0.8803          |
| 0.5802        | 0.5964 | 369  | 0.8492          |
| 0.7408        | 0.7952 | 492  | 0.8413          |
| 0.996         | 0.9939 | 615  | 0.8403          |

Training time: in total it took 39 minutes to fine-tune the model for one epoch.

### Framework versions

- Transformers 4.41.1
- Pytorch 2.3.0+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1


### Acknowledgements
This model was trained as part of [insert thesis info] in collaboration with Amsterdam Intelligence for the City of Amsterdam.",N/A,1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=FemkeBakker/AmsterdamDocClassificationLlama200T1Epochs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFemkeBakker%2FAmsterdamDocClassificationLlama200T1Epochs%5D(%2FFemkeBakker%2FAmsterdamDocClassificationLlama200T1Epochs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mperestoronin/llama2-v11-chat,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""mperestoronin/llama2-v11-chat"", ""author"": ""mperestoronin"", ""sha"": ""4336438c2cde3593a0b7c75f762482037b7e7c1b"", ""last_modified"": ""2024-06-06 14:01:36+00:00"", ""created_at"": ""2024-06-06 13:30:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""autotrain"", ""text-generation-inference"", ""peft"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-06-06 14:01:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6661b9f60d1be84ebeb20613"", ""modelId"": ""mperestoronin/llama2-v11-chat"", ""usedStorage"": 13477369587}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mperestoronin/llama2-v11-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmperestoronin%2Fllama2-v11-chat%5D(%2Fmperestoronin%2Fllama2-v11-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
abhayesian/llama2-7b-sft-lora,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: llama2-7b-sft-lora
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2-7b-sft-lora

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 8
- seed: 4
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 4
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- num_epochs: 1

### Framework versions

- Transformers 4.35.0
- Pytorch 2.1.0+cu121
- Datasets 2.14.6
- Tokenizers 0.14.1
","{""id"": ""abhayesian/llama2-7b-sft-lora"", ""author"": ""abhayesian"", ""sha"": ""b4a17921f327f5205a585d02a9ebf16b547e987f"", ""last_modified"": ""2024-06-11 20:00:42+00:00"", ""created_at"": ""2024-06-10 18:52:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-sft-lora\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama2-7b-sft-lora"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun10_18-53-29_353f74b94daf/events.out.tfevents.1718045696.353f74b94daf.13309.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun11_07-26-18_353f74b94daf/events.out.tfevents.1718090868.353f74b94daf.35973.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jun11_07-31-15_353f74b94daf/events.out.tfevents.1718091166.353f74b94daf.37548.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_0/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1200/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_1600/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_2000/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_400/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='step_800/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-11 20:00:42+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: llama2-7b-sft-lora\n  results: []"", ""transformersInfo"": null, ""_id"": ""66674b6b0ba069d8275f8515"", ""modelId"": ""abhayesian/llama2-7b-sft-lora"", ""usedStorage"": 2838697447}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=abhayesian/llama2-7b-sft-lora&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Babhayesian%2Fllama2-7b-sft-lora%5D(%2Fabhayesian%2Fllama2-7b-sft-lora)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
datafreak/results,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: results
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.33.1
- Pytorch 2.1.2
- Datasets 2.19.2
- Tokenizers 0.13.3
","{""id"": ""datafreak/results"", ""author"": ""datafreak"", ""sha"": ""b419502cac3084cb37ad231a639cb7b980016816"", ""last_modified"": ""2024-06-17 14:17:16+00:00"", ""created_at"": ""2024-06-17 14:17:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""results"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""use_default_system_prompt"": true}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-17 14:17:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: results\n  results: []"", ""transformersInfo"": null, ""_id"": ""66704569713bc068bb6fddb5"", ""modelId"": ""datafreak/results"", ""usedStorage"": 67659533}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=datafreak/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdatafreak%2Fresults%5D(%2Fdatafreak%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed,"---
thumbnail: ""https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg""
base_model: meta-llama/Llama-2-7b-chat-hf
metrics:
- memory_disk
- memory_inference
- inference_latency
- inference_throughput
- inference_CO2_emissions
- inference_energy_consumption
tags:
- pruna-ai
---
<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
    <a href=""https://www.pruna.ai/"" target=""_blank"" rel=""noopener noreferrer"">
        <img src=""https://i.imgur.com/eDAlcgk.png"" alt=""PrunaAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
    </a>
</div>
<!-- header end -->

[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)
[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)
[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)

# Simply make AI models cheaper, smaller, faster, and greener!

- Give a thumbs up if you like this model!
- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)
- Join Pruna AI community on Discord [here](https://discord.gg/rskEr4BZJx) to share feedback/suggestions or get help.

## Results

![image info](./plots.png)

**Frequently Asked Questions**
- ***How does the compression work?*** The model is compressed with quanto.
- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.
- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.
- ***What is the model format?*** We use safetensors.
- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.
- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append ""turbo"", ""tiny"", or ""green"" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.
- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- ***What are ""first"" metrics?*** Results mentioning ""first"" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.
- ***What are ""Sync"" and ""Async"" metrics?*** ""Sync"" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. ""Async"" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.

## Setup

You can run the smashed model with these steps:

0. Check requirements from the original repo meta-llama/Llama-2-7b-chat-hf installed. In particular, check python, cuda, and transformers versions.
1. Make sure that you have installed quantization related packages.
    ```bash
    pip install quanto
    ```
2. Load & run the model.
    ```python 
   from transformers import AutoModelForCausalLM, AutoTokenizer
   IMPORTS

   model = AutoModelForCausalLM.from_pretrained(""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed"", trust_remote_code=True, device_map='auto')
   tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")
    
   input_ids = tokenizer(""What is the color of prunes?,"", return_tensors='pt').to(model.device)[""input_ids""]
    
   outputs = model.generate(input_ids, max_new_tokens=216)
   tokenizer.decode(outputs[0])
    ```

## Configurations

The configuration info are in `smash_config.json`.

## Credits & License

The license of the smashed model follows the license of the original model. Please check the license of the original model meta-llama/Llama-2-7b-chat-hf before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.

## Want to compress other models?

- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).","{""id"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed"", ""author"": ""PrunaAI"", ""sha"": ""67952730591c151cc4546958089418461ebffc0f"", ""last_modified"": ""2024-08-02 16:04:12+00:00"", ""created_at"": ""2024-06-17 22:52:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pruna-ai"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-02 16:04:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6670be3d3f3043b7311200ce"", ""modelId"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed"", ""usedStorage"": 13544805581}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed%5D(%2FPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-QUANTO-int4bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed,"---
thumbnail: ""https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg""
base_model: meta-llama/Llama-2-7b-chat-hf
metrics:
- memory_disk
- memory_inference
- inference_latency
- inference_throughput
- inference_CO2_emissions
- inference_energy_consumption
tags:
- pruna-ai
---
<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
    <a href=""https://www.pruna.ai/"" target=""_blank"" rel=""noopener noreferrer"">
        <img src=""https://i.imgur.com/eDAlcgk.png"" alt=""PrunaAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
    </a>
</div>
<!-- header end -->

[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)
[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)
[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)

# Simply make AI models cheaper, smaller, faster, and greener!

- Give a thumbs up if you like this model!
- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)
- Join Pruna AI community on Discord [here](https://discord.gg/rskEr4BZJx) to share feedback/suggestions or get help.

## Results

![image info](./plots.png)

**Frequently Asked Questions**
- ***How does the compression work?*** The model is compressed with quanto.
- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.
- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.
- ***What is the model format?*** We use safetensors.
- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.
- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append ""turbo"", ""tiny"", or ""green"" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.
- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- ***What are ""first"" metrics?*** Results mentioning ""first"" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.
- ***What are ""Sync"" and ""Async"" metrics?*** ""Sync"" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. ""Async"" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.

## Setup

You can run the smashed model with these steps:

0. Check requirements from the original repo meta-llama/Llama-2-7b-chat-hf installed. In particular, check python, cuda, and transformers versions.
1. Make sure that you have installed quantization related packages.
    ```bash
    pip install quanto
    ```
2. Load & run the model.
    ```python 
   from transformers import AutoModelForCausalLM, AutoTokenizer
   IMPORTS

   model = AutoModelForCausalLM.from_pretrained(""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed"", trust_remote_code=True, device_map='auto')
   tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")
    
   input_ids = tokenizer(""What is the color of prunes?,"", return_tensors='pt').to(model.device)[""input_ids""]
    
   outputs = model.generate(input_ids, max_new_tokens=216)
   tokenizer.decode(outputs[0])
    ```

## Configurations

The configuration info are in `smash_config.json`.

## Credits & License

The license of the smashed model follows the license of the original model. Please check the license of the original model meta-llama/Llama-2-7b-chat-hf before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.

## Want to compress other models?

- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).","{""id"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed"", ""author"": ""PrunaAI"", ""sha"": ""7312b781d22d3fe672578e12a36ab4fea08e49c3"", ""last_modified"": ""2024-08-02 16:04:14+00:00"", ""created_at"": ""2024-06-17 22:52:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pruna-ai"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-02 16:04:14+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6670be40f69ec2d1e9f4b6a3"", ""modelId"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed"", ""usedStorage"": 13544805581}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PrunaAI/meta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed%5D(%2FPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-QUANTO-int2bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed,"---
thumbnail: ""https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg""
base_model: meta-llama/Llama-2-7b-chat-hf
metrics:
- memory_disk
- memory_inference
- inference_latency
- inference_throughput
- inference_CO2_emissions
- inference_energy_consumption
tags:
- pruna-ai
---
<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
    <a href=""https://www.pruna.ai/"" target=""_blank"" rel=""noopener noreferrer"">
        <img src=""https://i.imgur.com/eDAlcgk.png"" alt=""PrunaAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
    </a>
</div>
<!-- header end -->

[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)
[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)
[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)

# Simply make AI models cheaper, smaller, faster, and greener!

- Give a thumbs up if you like this model!
- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)
- Join Pruna AI community on Discord [here](https://discord.gg/rskEr4BZJx) to share feedback/suggestions or get help.

## Results

![image info](./plots.png)

**Frequently Asked Questions**
- ***How does the compression work?*** The model is compressed with hqq.
- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.
- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.
- ***What is the model format?*** We use safetensors.
- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.
- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append ""turbo"", ""tiny"", or ""green"" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.
- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).
- ***What are ""first"" metrics?*** Results mentioning ""first"" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.
- ***What are ""Sync"" and ""Async"" metrics?*** ""Sync"" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. ""Async"" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.

## Setup

You can run the smashed model with these steps:

0. Check requirements from the original repo meta-llama/Llama-2-7b-chat-hf installed. In particular, check python, cuda, and transformers versions.
1. Make sure that you have installed quantization related packages.
    ```bash
    pip install hqq
    ```
2. Load & run the model.
    ```python 
   from transformers import AutoModelForCausalLM, AutoTokenizer
    from hqq.engine.hf import HQQModelForCausalLM
 from hqq.models.hf.base import AutoHQQHFModel

   try:
     model = HQQModelForCausalLM.from_quantized(""PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed"", device_map='auto')
    except: 
     model = AutoHQQHFModel.from_quantized(""PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed"")
   tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")
    
   input_ids = tokenizer(""What is the color of prunes?,"", return_tensors='pt').to(model.device)[""input_ids""]
    
   outputs = model.generate(input_ids, max_new_tokens=216)
   tokenizer.decode(outputs[0])
    ```

## Configurations

The configuration info are in `smash_config.json`.

## Credits & License

The license of the smashed model follows the license of the original model. Please check the license of the original model meta-llama/Llama-2-7b-chat-hf before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.

## Want to compress other models?

- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).
- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).","{""id"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed"", ""author"": ""PrunaAI"", ""sha"": ""de686ea65364ab2ce203a6e3685e5230325e3ce8"", ""last_modified"": ""2024-08-02 16:04:18+00:00"", ""created_at"": ""2024-06-17 22:53:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""llama"", ""text-generation"", ""pruna-ai"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='qmodel.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-02 16:04:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6670be743dd872e666c1043e"", ""modelId"": ""PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed"", ""usedStorage"": 2296481611}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=PrunaAI/meta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed%5D(%2FPrunaAI%2Fmeta-llama-Llama-2-7b-chat-hf-HQQ-2bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wenzhy7/int-llama2,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
datasets:
- wenzhy7/llama2_sft_int
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""wenzhy7/int-llama2"", ""author"": ""wenzhy7"", ""sha"": ""36ed9078d7f2a929f14d5393a7b24380d9988443"", ""last_modified"": ""2024-06-22 15:32:32+00:00"", ""created_at"": ""2024-06-22 15:21:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""autotrain"", ""text-generation-inference"", ""text-generation"", ""peft"", ""conversational"", ""dataset:wenzhy7/llama2_sft_int"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wenzhy7/llama2_sft_int\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-06-22 15:32:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wenzhy7/llama2_sft_int\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6676ebdc8edbae2088df2d51"", ""modelId"": ""wenzhy7/int-llama2"", ""usedStorage"": 160472971}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=wenzhy7/int-llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwenzhy7%2Fint-llama2%5D(%2Fwenzhy7%2Fint-llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wadhma/Critique-L2-FT-DCR,"---
library_name: transformers
license: mit
datasets:
- wadhma/dcr_data
language:
- en
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
---

Given a document and a factually inconsistent claim, this model generates an explanation for why the claim is inconsistent with the document along with fine-grained span with the inconsistency.



Repository: https://github.com/ManyaWadhwa/DCR
Paper: https://arxiv.org/pdf/2407.02397","{""id"": ""wadhma/Critique-L2-FT-DCR"", ""author"": ""wadhma"", ""sha"": ""25701d0622958ba48c3f0185b3a5c6cc8655a7e8"", ""last_modified"": ""2024-09-09 17:55:48+00:00"", ""created_at"": ""2024-07-01 18:03:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""conversational"", ""en"", ""dataset:wadhma/dcr_data"", ""arxiv:2407.02397"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wadhma/dcr_data\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-09 17:55:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wadhma/dcr_data\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6682ef6c224478a1f6e2310f"", ""modelId"": ""wadhma/Critique-L2-FT-DCR"", ""usedStorage"": 26953696096}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=wadhma/Critique-L2-FT-DCR&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwadhma%2FCritique-L2-FT-DCR%5D(%2Fwadhma%2FCritique-L2-FT-DCR)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wadhma/Refine-L2-FT-DCR,"---
library_name: transformers
license: mit
datasets:
- wadhma/dcr_data
language:
- en
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
---

Given a document and a factually inconsistent summary and a natural language feedback, this model generates a minimally edited refinement based on the feedback.

Repository: https://github.com/ManyaWadhwa/DCR
Paper: https://arxiv.org/pdf/2407.02397","{""id"": ""wadhma/Refine-L2-FT-DCR"", ""author"": ""wadhma"", ""sha"": ""326c452e8df149c0792d5db4986d9c5f4dc533ed"", ""last_modified"": ""2024-09-09 17:55:03+00:00"", ""created_at"": ""2024-07-01 18:21:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""conversational"", ""en"", ""dataset:wadhma/dcr_data"", ""arxiv:2407.02397"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wadhma/dcr_data\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""[PAD]"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-09 17:55:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wadhma/dcr_data\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6682f3b9d0e580699e62fb9f"", ""modelId"": ""wadhma/Refine-L2-FT-DCR"", ""usedStorage"": 26953696096}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=wadhma/Refine-L2-FT-DCR&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwadhma%2FRefine-L2-FT-DCR%5D(%2Fwadhma%2FRefine-L2-FT-DCR)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ogamon/llama2_inst_truth_model,"---
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: train_2024-07-11-09-30-54_llama2_inst_truth
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# train_2024-07-11-09-30-54_llama2_inst_truth

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the truth_train dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-06
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 256
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 600
- num_epochs: 5.0

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.0a0+ebedce2
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Ogamon/llama2_inst_truth_model"", ""author"": ""Ogamon"", ""sha"": ""3290319ee10d17c615b13dd1c936cdb5131a7181"", ""last_modified"": ""2024-07-11 14:13:03+00:00"", ""created_at"": ""2024-07-11 13:43:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-11-09-30-54_llama2_inst_truth\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""train_2024-07-11-09-30-54_llama2_inst_truth"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if loop.index0 == 0 and system_message is defined %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/global_step385/mp_rank_00_model_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/latest', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_0.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_1.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_2.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_3.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_4.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_5.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_6.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/rng_state_7.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-385/zero_to_fp32.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generated_predictions.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llamaboard_config.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='predict_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='running_log.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_log.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_loss.png', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-07-11 14:13:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-11-09-30-54_llama2_inst_truth\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""668fe17a5cb4c07caae5c1d5"", ""modelId"": ""Ogamon/llama2_inst_truth_model"", ""usedStorage"": 107815470299}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ogamon/llama2_inst_truth_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOgamon%2Fllama2_inst_truth_model%5D(%2FOgamon%2Fllama2_inst_truth_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
bhadauriaupendra062/Llama,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: result
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# result

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.2
- Datasets 2.20.0
- Tokenizers 0.15.2
","{""id"": ""bhadauriaupendra062/Llama"", ""author"": ""bhadauriaupendra062"", ""sha"": ""86f0599b17f074a3c4384079a0e586b09f0b68b8"", ""last_modified"": ""2024-07-13 08:20:47+00:00"", ""created_at"": ""2024-07-13 08:20:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: result\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""result"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jul13_07-52-28_f7e1fd172dce/events.out.tfevents.1720857215.f7e1fd172dce.34.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-07-13 08:20:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: result\n  results: []"", ""transformersInfo"": null, ""_id"": ""669238db4f911aa1af8c995a"", ""modelId"": ""bhadauriaupendra062/Llama"", ""usedStorage"": 134745019}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=bhadauriaupendra062/Llama&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bbhadauriaupendra062%2FLlama%5D(%2Fbhadauriaupendra062%2FLlama)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ogamon/llama2_inst_truthbench1_model,"---
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: train_2024-07-16-09-05-28_llama2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# train_2024-07-16-09-05-28_llama2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the truth_train_0716 dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-06
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- num_epochs: 5.0

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.0a0+ebedce2
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Ogamon/llama2_inst_truthbench1_model"", ""author"": ""Ogamon"", ""sha"": ""5e18707df80fe752be9ea057418c837c3a6e7adf"", ""last_modified"": ""2024-07-16 16:22:56+00:00"", ""created_at"": ""2024-07-16 15:24:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-16-09-05-28_llama2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""train_2024-07-16-09-05-28_llama2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if loop.index0 == 0 and system_message is defined %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/mp_rank_00_model_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/latest', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_0.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_1.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_2.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_3.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_4.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_5.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_6.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_7.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/zero_to_fp32.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generated_predictions.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llamaboard_config.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='predict_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='running_log.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_log.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_loss.png', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-07-16 16:22:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-16-09-05-28_llama2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""669690b192a30cfde5abda40"", ""modelId"": ""Ogamon/llama2_inst_truthbench1_model"", ""usedStorage"": 107815470299}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ogamon/llama2_inst_truthbench1_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOgamon%2Fllama2_inst_truthbench1_model%5D(%2FOgamon%2Fllama2_inst_truthbench1_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ogamon/llama2_inst_truthbench2_model,"---
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: train_2024-07-16-16-48-49_llama2_2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# train_2024-07-16-16-48-49_llama2_2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the truth_train_0716_2 dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-06
- train_batch_size: 2
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- gradient_accumulation_steps: 8
- total_train_batch_size: 128
- total_eval_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- num_epochs: 5.0

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.0a0+ebedce2
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Ogamon/llama2_inst_truthbench2_model"", ""author"": ""Ogamon"", ""sha"": ""8b9ccfd468041e506a7d104a284c74efa74e7dbf"", ""last_modified"": ""2024-07-16 17:53:50+00:00"", ""created_at"": ""2024-07-16 17:33:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-16-16-48-49_llama2_2\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""train_2024-07-16-16-48-49_llama2_2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if loop.index0 == 0 and system_message is defined %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '<s>' + '[INST] ' + content + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/global_step190/mp_rank_00_model_states.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/latest', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_0.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_1.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_2.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_3.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_4.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_5.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_6.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/rng_state_7.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/scheduler.pt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='checkpoint-190/zero_to_fp32.py', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generated_predictions.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='llamaboard_config.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='predict_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='running_log.txt', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_log.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.yaml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_loss.png', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-07-16 17:53:50+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: train_2024-07-16-16-48-49_llama2_2\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6696aee83f8813f7b130e5c0"", ""modelId"": ""Ogamon/llama2_inst_truthbench2_model"", ""usedStorage"": 107815470299}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ogamon/llama2_inst_truthbench2_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOgamon%2Fllama2_inst_truthbench2_model%5D(%2FOgamon%2Fllama2_inst_truthbench2_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
FrancescoPeriti/Llama2Dictionary,"---
license: cc-by-sa-4.0
language:
- en
library_name: transformers
pipeline_tag: text2text-generation
tags:
- text-generation-inference
base_model:
- meta-llama/Llama-2-7b-chat-hf
---
# Llama2Dictionary

<!-- Provide a quick summary of what the model is/does. -->
```FrancescoPeriti/Llama2Dictionary``` is a fine-tuned version of the ```meta-llama/Llama-2-7b-chat-hf```. 
Thus, to use it, visit the AI at Meta website, accept the Meta License, and submit the [form](https://llama.meta.com/llama-downloads/).

You will need to login with your hugginface token (```[HF-TOKEN]```, in the following).


### Model Description
This model is fine-tuned on English datasets of sense definitions. Given a target word and a usage example, the model generates a sense definition for the target word in-context.

You can find more details in the paper [Automatically Generated Definitions and their utility for Modeling Word Meaning](https://aclanthology.org/2024.emnlp-main.776/) by Francesco Periti, David Alfter, Nina Tahmasebi.
The repository of our project is [https://github.com/FrancescoPeriti/LlamaDictionary](https://github.com/FrancescoPeriti/LlamaDictionary).

## Uses
The model is designed for research purposes and is conceived to work like a dictionary. 
However, given a word and an example usage, users don't choose from a list of definitions (as in a traditional dictionary); instead, the model directly provides the sense definition for the word in-context.

<!-- ### Direct Use -->
<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

<!-- ### Downstream Use [optional]-->
<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

## Bias, Risks, and Limitations
The fine-tuning datasets were limited to English, and generated definitions may reflect biases and stereotypes inherent in the underlying language model.

## How to Get Started with the Model
```python
import torch
import warnings
from peft import PeftModel # parameter-efficient fine-tuning
from datasets import Dataset
from huggingface_hub import login
from typing import (Literal, Sequence,TypedDict)
from transformers import AutoTokenizer, AutoModelForCausalLM

login([HF-TOKEN]) # e.g., hf_aGPI...ELal

model_name = ""meta-llama/Llama-2-7b-chat-hf"" # chat model
ft_model_name = ""FrancescoPeriti/Llama2Dictionary"" # fine-tuned model

# load models
chat_model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')
lama2dictionary = PeftModel.from_pretrained(chat_model, ft_model_name)
lama2dictionary.eval()

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    padding_side=""left"",
    add_eos_token=True,
    add_bos_token=True,
)
tokenizer.pad_token = tokenizer.eos_token

# end of sequence for stop condition
eos_tokens = [tokenizer.encode(token, add_special_tokens=False)[0] 
              for token in [';', ' ;', '.', ' .']]
eos_tokens.append(tokenizer.eos_token_id)

# chat format
Role = Literal[""system"", ""user""]

class Message(TypedDict):
    role: Role
    content: str

Dialog = Sequence[Message]

# load dataset
examples = [{'target': 'jam', 'example': 'The traffic jam on the highway made everyone late for work.'},
            {'target': 'jam', 'example': 'I spread a generous layer of strawberry jam on my toast this morning'}]
dataset = Dataset.from_list(examples)

# apply template
def apply_chat_template(tokenizer, dataset):
    system_message = ""You are a lexicographer familiar with providing concise definitions of word meanings.""
    template = 'Please provide a concise definition for the meaning of the word ""{}"" in the following sentence: {}'
                
    def apply_chat_template_func(record):
        dialog: Dialog = (Message(role='system', content=system_message),
                          Message(role='user', content=template.format(record['target'], record['example'])))
        prompt = tokenizer.decode(tokenizer.apply_chat_template(dialog, add_generation_prompt=True))
        return {'text': prompt}
    
    return dataset.map(apply_chat_template_func)

dataset = apply_chat_template(tokenizer, dataset)

# tokenization
max_length = 512

def formatting_func(record):
    return record['text']

def tokenization(dataset):
    result = tokenizer(formatting_func(dataset),
                       truncation=True,
                       max_length=max_length,
                       padding=""max_length"",
                       add_special_tokens=False)
    return result

tokenized_dataset = dataset.map(tokenization)

# definition generation
batch_size = 32
max_time = 4.5 # sec

sense_definitions = list()
with torch.no_grad():
    for i in range(0, len(tokenized_dataset), batch_size):
        batch = tokenized_dataset[i:i + batch_size]

        model_input = dict()
        for k in ['input_ids', 'attention_mask']:
            model_input[k] = torch.tensor(batch[k]).to('cuda')

        output_ids = lama2dictionary.generate(**model_input,
                                       max_length = max_length,
                                       forced_eos_token_id = eos_tokens,
                                       max_time = max_time * batch_size,
                                       eos_token_id = eos_tokens,
                                       temperature = 0.00001,
                                       pad_token_id = tokenizer.eos_token_id)

        answers = tokenizer.batch_decode(output_ids, skip_special_tokens=True)

        for j, answer in enumerate(answers):
            answer = answer.split('[/INST]')[-1].strip("" .,;:"")
            if 'SYS>>' in answer:
                answer=''
                warnings.warn(""Something went wrong. The input example might be too long; try reducing it."")
            sense_definitions.append(answer.replace('\n', ' ') + '\n')

# output
dataset = dataset.add_column('definition', sense_definitions)
for row in dataset:
    print(f""Target: {row['target']}\nExample: {row['example']}\nSense definition: {row['definition']}"")
```

## Citation

Francesco Periti, David Alfter, and Nina Tahmasebi. 2024. [Automatically Generated Definitions and their utility for Modeling Word Meaning](https://aclanthology.org/2024.emnlp-main.776/). In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 14008–14026, Miami, Florida, USA. Association for Computational Linguistics.

**BibTeX:**
```
@inproceedings{periti2024automatically,
    title = {{Automatically Generated Definitions and their utility for Modeling Word Meaning}},
    author = ""Periti, Francesco  and Alfter, David  and Tahmasebi, Nina"",
    editor = ""Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.776"",
    pages = ""14008--14026"",
    abstract = ""Modeling lexical semantics is a challenging task, often suffering from interpretability pitfalls. In this paper, we delve into the generation of dictionary-like sense definitions and explore their utility for modeling word meaning. We fine-tuned two Llama models and include an existing T5-based model in our evaluation. Firstly, we evaluate the quality of the generated definitions on existing English benchmarks, setting new state-of-the-art results for the Definition Generation task. Next, we explore the use of definitions generated by our models as intermediate representations subsequently encoded as sentence embeddings. We evaluate this approach on lexical semantics tasks such as the Word-in-Context, Word Sense Induction, and Lexical Semantic Change, setting new state-of-the-art results in all three tasks when compared to unsupervised baselines."",
}
```","{""id"": ""FrancescoPeriti/Llama2Dictionary"", ""author"": ""FrancescoPeriti"", ""sha"": ""122bbcd1749e783803c36662b1fd0c60c00ede01"", ""last_modified"": ""2024-12-06 12:43:07+00:00"", ""created_at"": ""2024-07-24 13:14:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 15, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""text-generation-inference"", ""text2text-generation"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:cc-by-sa-4.0"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text2text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\nlicense: cc-by-sa-4.0\npipeline_tag: text2text-generation\ntags:\n- text-generation-inference"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama""}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='requirements.txt', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-06 12:43:07+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\nlicense: cc-by-sa-4.0\npipeline_tag: text2text-generation\ntags:\n- text-generation-inference"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66a0fe40240cca17e8164b16"", ""modelId"": ""FrancescoPeriti/Llama2Dictionary"", ""usedStorage"": 10906301832}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=FrancescoPeriti/Llama2Dictionary&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFrancescoPeriti%2FLlama2Dictionary%5D(%2FFrancescoPeriti%2FLlama2Dictionary)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
minkhantycc/Llama-2-7b-chat-finetune-quantized,"---
license: mit
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
---","{""id"": ""minkhantycc/Llama-2-7b-chat-finetune-quantized"", ""author"": ""minkhantycc"", ""sha"": ""555429ffc4beddfe689e5fe7007010a09a22ec2c"", ""last_modified"": ""2024-08-30 08:02:08+00:00"", ""created_at"": ""2024-08-25 09:47:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""text-generation"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: mit\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-30 08:02:08+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: mit\npipeline_tag: text-generation"", ""transformersInfo"": null, ""_id"": ""66cafda93864174f35057a5b"", ""modelId"": ""minkhantycc/Llama-2-7b-chat-finetune-quantized"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=minkhantycc/Llama-2-7b-chat-finetune-quantized&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bminkhantycc%2FLlama-2-7b-chat-finetune-quantized%5D(%2Fminkhantycc%2FLlama-2-7b-chat-finetune-quantized)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Gandretty/efcc,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""Gandretty/efcc"", ""author"": ""Gandretty"", ""sha"": ""a2472a6a65990d63f8512f44ecb2b490197af63e"", ""last_modified"": ""2024-08-27 15:45:09+00:00"", ""created_at"": ""2024-08-27 14:55:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-27 15:45:09+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2"", ""transformersInfo"": null, ""_id"": ""66cde8d352674816f8c579c7"", ""modelId"": ""Gandretty/efcc"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Gandretty/efcc&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BGandretty%2Fefcc%5D(%2FGandretty%2Fefcc)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
subhrokomol/hindi-tokenizer,"---
license: apache-2.0
language:
- hi
- en
metrics:
- perplexity
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
library_name: transformers
---","{""id"": ""subhrokomol/hindi-tokenizer"", ""author"": ""subhrokomol"", ""sha"": ""2bc5768323632b3d20d49f9de85f56ded3e581f8"", ""last_modified"": ""2024-08-31 04:50:13+00:00"", ""created_at"": ""2024-08-31 03:57:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""text-generation"", ""hi"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- hi\n- en\nlibrary_name: transformers\nlicense: apache-2.0\nmetrics:\n- perplexity\npipeline_tag: text-generation"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='hindi-english-sentencepiece-tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-31 04:50:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- hi\n- en\nlibrary_name: transformers\nlicense: apache-2.0\nmetrics:\n- perplexity\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""66d2948e1dbd78057425d460"", ""modelId"": ""subhrokomol/hindi-tokenizer"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=subhrokomol/hindi-tokenizer&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsubhrokomol%2Fhindi-tokenizer%5D(%2Fsubhrokomol%2Fhindi-tokenizer)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Arjs/Llama-2-7b-chatbot-finetune,"---
license: apache-2.0
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
tags:
- text-generation-inference
- transformers
- llama
- casual-llm
---","{""id"": ""Arjs/Llama-2-7b-chatbot-finetune"", ""author"": ""Arjs"", ""sha"": ""0270d1438fbe00a5df70aee1a16fb3c431f87fc0"", ""last_modified"": ""2024-08-31 08:09:00+00:00"", ""created_at"": ""2024-08-31 05:55:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 5, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""text-generation-inference"", ""casual-llm"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- text-generation-inference\n- transformers\n- llama\n- casual-llm"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [""Arjs/Chat_bot""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-08-31 08:09:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- text-generation-inference\n- transformers\n- llama\n- casual-llm"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66d2b05d974c5c6902d4f768"", ""modelId"": ""Arjs/Llama-2-7b-chatbot-finetune"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,"Arjs/Chat_bot, huggingface/InferenceSupport/discussions/new?title=Arjs/Llama-2-7b-chatbot-finetune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BArjs%2FLlama-2-7b-chatbot-finetune%5D(%2FArjs%2FLlama-2-7b-chatbot-finetune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",2
Olivia1400/Yui,"---
datasets:
- HuggingFaceTB/everyday-conversations-llama3.1-2k
- lmms-lab/LLaVA-OneVision-Data
language:
- en
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text2text-generation
library_name: fastai
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""Olivia1400/Yui"", ""author"": ""Olivia1400"", ""sha"": ""fc5132b59dd523717ae27b4b09489b2b0b70369a"", ""last_modified"": ""2024-09-03 18:22:25+00:00"", ""created_at"": ""2024-09-03 18:19:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""fastai"", ""gguf"": null, ""inference"": null, ""tags"": [""fastai"", ""text2text-generation"", ""en"", ""dataset:HuggingFaceTB/everyday-conversations-llama3.1-2k"", ""dataset:lmms-lab/LLaVA-OneVision-Data"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": ""text2text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- HuggingFaceTB/everyday-conversations-llama3.1-2k\n- lmms-lab/LLaVA-OneVision-Data\nlanguage:\n- en\nlibrary_name: fastai\npipeline_tag: text2text-generation"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-09-03 18:22:25+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- HuggingFaceTB/everyday-conversations-llama3.1-2k\n- lmms-lab/LLaVA-OneVision-Data\nlanguage:\n- en\nlibrary_name: fastai\npipeline_tag: text2text-generation"", ""transformersInfo"": null, ""_id"": ""66d75329c8c857729c9bc992"", ""modelId"": ""Olivia1400/Yui"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Olivia1400/Yui&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOlivia1400%2FYui%5D(%2FOlivia1400%2FYui)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Sohaibsoussi/llama-2-7b-miniDoctor,"---
license: llama2
datasets:
- Sohaibsoussi/small_patient_doctor_llama2_chatbot
language:
- en
base_model: meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
tags:
- medical
---","{""id"": ""Sohaibsoussi/llama-2-7b-miniDoctor"", ""author"": ""Sohaibsoussi"", ""sha"": ""2b278aa2ff97db737d58c59a20195fd4d8a0df8d"", ""last_modified"": ""2024-09-06 23:23:26+00:00"", ""created_at"": ""2024-09-06 18:43:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""medical"", ""text-generation"", ""conversational"", ""en"", ""dataset:Sohaibsoussi/small_patient_doctor_llama2_chatbot"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Sohaibsoussi/small_patient_doctor_llama2_chatbot\nlanguage:\n- en\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- medical"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-09-06 23:23:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Sohaibsoussi/small_patient_doctor_llama2_chatbot\nlanguage:\n- en\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- medical"", ""transformersInfo"": null, ""_id"": ""66db4d36c35391da43ee9a3b"", ""modelId"": ""Sohaibsoussi/llama-2-7b-miniDoctor"", ""usedStorage"": 26954331470}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Sohaibsoussi/llama-2-7b-miniDoctor&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSohaibsoussi%2Fllama-2-7b-miniDoctor%5D(%2FSohaibsoussi%2Fllama-2-7b-miniDoctor)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
zjunlp/OneGen-EntityLinking-Llama2-7B,"---
license: mit
datasets:
- zjunlp/OneGen-TrainDataset-EntityLinking
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
tags:
- Entity Linking
- OneGen
- LLMs
---","{""id"": ""zjunlp/OneGen-EntityLinking-Llama2-7B"", ""author"": ""zjunlp"", ""sha"": ""a3c44977ebcd93cd7ff592a9f8ec0d292f6c3186"", ""last_modified"": ""2024-10-14 05:37:28+00:00"", ""created_at"": ""2024-09-09 23:36:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 6, ""downloads_all_time"": null, ""likes"": 3, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""Entity Linking"", ""OneGen"", ""LLMs"", ""en"", ""dataset:zjunlp/OneGen-TrainDataset-EntityLinking"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-EntityLinking\nlanguage:\n- en\nlicense: mit\ntags:\n- Entity Linking\n- OneGen\n- LLMs"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""UniModel""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738440192}, ""total"": 6738440192}, ""security_repo_status"": null, ""lastModified"": ""2024-10-14 05:37:28+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-EntityLinking\nlanguage:\n- en\nlicense: mit\ntags:\n- Entity Linking\n- OneGen\n- LLMs"", ""transformersInfo"": null, ""_id"": ""66df869b62d6ab4f11ca92c6"", ""modelId"": ""zjunlp/OneGen-EntityLinking-Llama2-7B"", ""usedStorage"": 13477413923}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=zjunlp/OneGen-EntityLinking-Llama2-7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bzjunlp%2FOneGen-EntityLinking-Llama2-7B%5D(%2Fzjunlp%2FOneGen-EntityLinking-Llama2-7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
zjunlp/OneGen-MultiHop-Llama2-7B,"---
license: mit
datasets:
- zjunlp/OneGen-TrainDataset-MultiHopQA
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
tags:
- RAG
- MultiHopRAG
- 2WIKI
- HotpotQA
- OneGen
- Efficient
- LLMs
---","{""id"": ""zjunlp/OneGen-MultiHop-Llama2-7B"", ""author"": ""zjunlp"", ""sha"": ""59250c757e840deda3550871884087f502e039d2"", ""last_modified"": ""2024-10-14 05:40:29+00:00"", ""created_at"": ""2024-09-09 23:37:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 6, ""downloads_all_time"": null, ""likes"": 3, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""RAG"", ""MultiHopRAG"", ""2WIKI"", ""HotpotQA"", ""OneGen"", ""Efficient"", ""LLMs"", ""en"", ""dataset:zjunlp/OneGen-TrainDataset-MultiHopQA"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-MultiHopQA\nlanguage:\n- en\nlicense: mit\ntags:\n- RAG\n- MultiHopRAG\n- 2WIKI\n- HotpotQA\n- OneGen\n- Efficient\n- LLMs"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""UniModel""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738448384}, ""total"": 6738448384}, ""security_repo_status"": null, ""lastModified"": ""2024-10-14 05:40:29+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-MultiHopQA\nlanguage:\n- en\nlicense: mit\ntags:\n- RAG\n- MultiHopRAG\n- 2WIKI\n- HotpotQA\n- OneGen\n- Efficient\n- LLMs"", ""transformersInfo"": null, ""_id"": ""66df86c15a0c5910d669efb6"", ""modelId"": ""zjunlp/OneGen-MultiHop-Llama2-7B"", ""usedStorage"": 13477430307}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=zjunlp/OneGen-MultiHop-Llama2-7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bzjunlp%2FOneGen-MultiHop-Llama2-7B%5D(%2Fzjunlp%2FOneGen-MultiHop-Llama2-7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
zjunlp/OneGen-SelfRAG-Llama2-7B,"---
license: mit
datasets:
- zjunlp/OneGen-TrainDataset-SelfRAG
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
tags:
- RAG
- Self-RAG
- OneGen
- Efficient
- LLMs
---","{""id"": ""zjunlp/OneGen-SelfRAG-Llama2-7B"", ""author"": ""zjunlp"", ""sha"": ""93b322ac0ce3cebd810aacd4827a277dcb891786"", ""last_modified"": ""2024-10-14 05:39:09+00:00"", ""created_at"": ""2024-09-09 23:38:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 6, ""downloads_all_time"": null, ""likes"": 2, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""safetensors"", ""llama"", ""RAG"", ""Self-RAG"", ""OneGen"", ""Efficient"", ""LLMs"", ""en"", ""dataset:zjunlp/OneGen-TrainDataset-SelfRAG"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-SelfRAG\nlanguage:\n- en\nlicense: mit\ntags:\n- RAG\n- Self-RAG\n- OneGen\n- Efficient\n- LLMs"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738554880}, ""total"": 6738554880}, ""security_repo_status"": null, ""lastModified"": ""2024-10-14 05:39:09+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- zjunlp/OneGen-TrainDataset-SelfRAG\nlanguage:\n- en\nlicense: mit\ntags:\n- RAG\n- Self-RAG\n- OneGen\n- Efficient\n- LLMs"", ""transformersInfo"": null, ""_id"": ""66df86dbef097233a5526ae8"", ""modelId"": ""zjunlp/OneGen-SelfRAG-Llama2-7B"", ""usedStorage"": 13478264433}",1,,0,,0,https://huggingface.co/DevQuasar/zjunlp.OneGen-SelfRAG-Llama2-7B-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=zjunlp/OneGen-SelfRAG-Llama2-7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bzjunlp%2FOneGen-SelfRAG-Llama2-7B%5D(%2Fzjunlp%2FOneGen-SelfRAG-Llama2-7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mathqa,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mathqa
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mathqa

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 36

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mathqa"", ""author"": ""Vivian12300"", ""sha"": ""a30f33312c527684c46a27248d181d88bfde67fd"", ""last_modified"": ""2024-09-10 13:09:17+00:00"", ""created_at"": ""2024-09-10 12:09:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 8, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mathqa"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-10 13:09:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e0370914d7a7711c604841"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mathqa"", ""usedStorage"": 13477370139}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mathqa&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mathqa%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mathqa)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mathqa-formula,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mathqa-formula
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mathqa-formula

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 36

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-formula"", ""author"": ""Vivian12300"", ""sha"": ""9759ef46e38841401ba1fc05f0ee4a843a017775"", ""last_modified"": ""2024-09-10 17:14:05+00:00"", ""created_at"": ""2024-09-10 15:39:05+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-formula\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mathqa-formula"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-10 17:14:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-formula\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e0681947e1fb9c5a583d57"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-formula"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mathqa-formula&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mathqa-formula%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mathqa-formula)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mathqa-formula-chinese,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mathqa-formula-chinese
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mathqa-formula-chinese

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 36

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-formula-chinese"", ""author"": ""Vivian12300"", ""sha"": ""ecf2dc784b27b9de7ba2118c95afe9077f067527"", ""last_modified"": ""2024-09-10 17:54:05+00:00"", ""created_at"": ""2024-09-10 15:47:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-formula-chinese\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mathqa-formula-chinese"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-10 17:54:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-formula-chinese\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e06a219ac9bce4baa4a00c"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-formula-chinese"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mathqa-formula-chinese&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mathqa-formula-chinese%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mathqa-formula-chinese)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mathqa-chinese,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mathqa-chinese
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mathqa-chinese

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 36

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-chinese"", ""author"": ""Vivian12300"", ""sha"": ""60b15f747fc73840445165bf36ec5d8671590684"", ""last_modified"": ""2024-09-10 20:06:27+00:00"", ""created_at"": ""2024-09-10 18:32:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-chinese\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mathqa-chinese"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-10 20:06:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-chinese\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e090b79e2940182530366d"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-chinese"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mathqa-chinese&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mathqa-chinese%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mathqa-chinese)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mathqa-rationale-2,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mathqa-rationale-2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mathqa-rationale-2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 36

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-rationale-2"", ""author"": ""Vivian12300"", ""sha"": ""bf21b05648854d4d76f08bf070c902c87b768404"", ""last_modified"": ""2024-09-12 15:27:53+00:00"", ""created_at"": ""2024-09-12 13:10:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-rationale-2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mathqa-rationale-2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-12 15:27:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mathqa-rationale-2\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e2e83dd0a1d5cd1b341964"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mathqa-rationale-2"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mathqa-rationale-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mathqa-rationale-2%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mathqa-rationale-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mmlu-zh,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mmlu-zh
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mmlu-zh

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mmlu-zh"", ""author"": ""Vivian12300"", ""sha"": ""0f508b1c788655d4f3f52d55e61fa57e8c6ef0a3"", ""last_modified"": ""2024-09-12 13:48:22+00:00"", ""created_at"": ""2024-09-12 13:30:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu-zh\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mmlu-zh"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-12 13:48:22+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu-zh\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e2ece645edc836763a9e90"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mmlu-zh"", ""usedStorage"": 29537926515}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mmlu-zh&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mmlu-zh%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mmlu-zh)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mmlu,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mmlu
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mmlu

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mmlu"", ""author"": ""Vivian12300"", ""sha"": ""7761605ec9df32823534699d3549ed17b68af5c8"", ""last_modified"": ""2024-09-12 15:08:20+00:00"", ""created_at"": ""2024-09-12 14:56:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mmlu"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-12 15:08:20+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e30138fa7c50759b0867a7"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mmlu"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mmlu&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mmlu%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mmlu)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/llama-2-7b-chat-hf-mmlu-full,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama-2-7b-chat-hf-mmlu-full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-hf-mmlu-full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/llama-2-7b-chat-hf-mmlu-full"", ""author"": ""Vivian12300"", ""sha"": ""1634d4565152599afbf06eec680701250a66d011"", ""last_modified"": ""2024-09-12 16:02:06+00:00"", ""created_at"": ""2024-09-12 15:49:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu-full\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-hf-mmlu-full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-12 16:02:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-hf-mmlu-full\n  results: []"", ""transformersInfo"": null, ""_id"": ""66e30d913be36ad4007d465f"", ""modelId"": ""Vivian12300/llama-2-7b-chat-hf-mmlu-full"", ""usedStorage"": 26954235187}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/llama-2-7b-chat-hf-mmlu-full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fllama-2-7b-chat-hf-mmlu-full%5D(%2FVivian12300%2Fllama-2-7b-chat-hf-mmlu-full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Vivian12300/mmlu_same_f_llama2,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: mmlu_same_f_llama2
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mmlu_same_f_llama2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 1
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 30

### Training results



### Framework versions

- Transformers 4.42.3
- Pytorch 2.3.1+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""Vivian12300/mmlu_same_f_llama2"", ""author"": ""Vivian12300"", ""sha"": ""9ae066e102327aa5e3c23d84adb9b4f8e1683537"", ""last_modified"": ""2024-09-18 17:43:02+00:00"", ""created_at"": ""2024-09-18 17:30:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: mmlu_same_f_llama2\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""mmlu_same_f_llama2"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-09-18 17:43:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: mmlu_same_f_llama2\n  results: []"", ""transformersInfo"": null, ""_id"": ""66eb0e43e465d6302b75d25c"", ""modelId"": ""Vivian12300/mmlu_same_f_llama2"", ""usedStorage"": 26954235123}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Vivian12300/mmlu_same_f_llama2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVivian12300%2Fmmlu_same_f_llama2%5D(%2FVivian12300%2Fmmlu_same_f_llama2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wentao-yuan/robopoint-v1-llama-2-7b-lora,"---
license: apache-2.0
datasets:
- wentao-yuan/robopoint-data
base_model:
- meta-llama/Llama-2-7b-chat-hf
---
# RoboPoint-v1-Llama2-7B-LoRA
RoboPoint is an open-source vision-language model instruction-tuned on a mix of robotics and VQA data. Given an image with language instructions, it outputs precise action guidance as points.

## Primary Use Cases
RoboPoint can predict spatial affordances—where actions should be taken in relation to other entities—based on instructions. For example, it can identify free space on a shelf in front of the rightmost object.

## Model Details
This model was fine-tuned using [LoRA](https://arxiv.org/abs/2106.09685) from [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and has 7 billion parameters.

## Date
This model was trained in June 2024.

## Resources for More Information

- Paper: https://arxiv.org/pdf/2406.10721
- Code: https://github.com/wentaoyuan/RoboPoint
- Website: https://robo-point.github.io

## Training dataset
See [wentao-yuan/robopoint-data](https://huggingface.co/datasets/wentao-yuan/robopoint-data).

## Citation
If you find our work helpful, please consider citing our paper.
```
@article{yuan2024robopoint,
  title={RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics},
  author={Yuan, Wentao and Duan, Jiafei and Blukis, Valts and Pumacay, Wilbert and Krishna, Ranjay and Murali, Adithyavairavan and Mousavian, Arsalan and Fox, Dieter},
  journal={arXiv preprint arXiv:2406.10721},
  year={2024}
}
```","{""id"": ""wentao-yuan/robopoint-v1-llama-2-7b-lora"", ""author"": ""wentao-yuan"", ""sha"": ""1688448935b497a7bb5405e0559e55ba3595c562"", ""last_modified"": ""2024-09-22 05:44:23+00:00"", ""created_at"": ""2024-09-21 06:43:49+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 8, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llava_llama"", ""dataset:wentao-yuan/robopoint-data"", ""arxiv:2106.09685"", ""arxiv:2406.10721"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wentao-yuan/robopoint-data\nlicense: apache-2.0"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llava_llama""}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='non_lora_trainables.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-09-22 05:44:23+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wentao-yuan/robopoint-data\nlicense: apache-2.0"", ""transformersInfo"": null, ""_id"": ""66ee6b252754b5db61e41939"", ""modelId"": ""wentao-yuan/robopoint-v1-llama-2-7b-lora"", ""usedStorage"": 648091200}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=wentao-yuan/robopoint-v1-llama-2-7b-lora&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwentao-yuan%2Frobopoint-v1-llama-2-7b-lora%5D(%2Fwentao-yuan%2Frobopoint-v1-llama-2-7b-lora)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
quarkymatter/Llama-2-7b-chat-PolicyPro,"---
license: llama3.1
datasets:
- quarkymatter/PolicyPro_dataset
language:
- en
library_name: transformers
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/66daa160fef91c84d186e338/NkHU_g1iWQd5BWwV89D8S.jpeg)

## PolicyPro [ B E T A ]

### Model Description
PolicyPro is a factual language model trained on PolicyPro handbook documents and everyday conversation data.
It can be used to generate formal and structured policy texts, edit/modify existing texts, search, and summarize information.

    **Developed by:** Brandon Cotton and Whitney Osborn
    
    **Model type:** Text-to-Text Generation
    
    **Language(s) (NLP):** English


## Uses


  ### Direct Use
  PolicyPro can be used to:
  
    * Update/modify/edit existing policies
    * Get summaries of policies
    * Ask questions about specific policies and get answers
    * Generate different creative text formats of policy content, such as paraphrases and key concepts.
  
  **Note:** PolicyPro is still under development, and its outputs should never be taken as legal advice. 

  
  ### Downstream Use
  PolicyPro will prospectively be integrated via website or chatbot to provide easy access to policy documents and information.
  
  
  ### Out-of-Scope Use
  PolicyPro is not intended for:
  
    * Generating legal documents without human evaluation
    * Providing legal advice
    * Creating misleading or false information about university policies

## Bias, Risks, and Limitations
**Bias:**
  
  * PolicyPro is trained on a dataset of university policy documents, which may reflect institutional biases.
  * The model may not be accurate for all university policies or situations.

**Risks:**
  
  * PolicyPro could be used to generate misleading or false information about university policies.
  * Users may rely on PolicyPro's outputs as legal advice, which could lead to negative consequences.

**Limitations:**
  
  * PolicyPro is a factual language model and cannot understand the nuances of legal language.
  * The model may not be able to answer all questions about university policies accurately.
  * Accurate document editing is still under construction. 

## Recommendations
  * The model should be continuously monitored and updated to address any biases or inaccuracies.
  * Libraries and datasets must be refined to provide the best model training.



## How to Get Started with the Model
*(chat coming soon)*

## Training Details

**Note:** Information is not publicly available due to client confidentiality.

The model was trained on the following custom datasets:
  * quarkymatter/PolicyPro_dataset (contains policy texts and documents)



## Contact
For questions and/or concerns regarding this model, please contact Whitney at [whitneydmosborn@gmail.com](mailto:whitneydmosborn@gmail.com).","{""id"": ""quarkymatter/Llama-2-7b-chat-PolicyPro"", ""author"": ""quarkymatter"", ""sha"": ""3f5f4dd797534846e6d8bc8d32947a3fc9015122"", ""last_modified"": ""2024-09-30 17:09:36+00:00"", ""created_at"": ""2024-09-30 16:47:00+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""pytorch"", ""llama"", ""text-generation"", ""conversational"", ""en"", ""dataset:quarkymatter/PolicyPro_dataset"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama3.1"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- quarkymatter/PolicyPro_dataset\nlanguage:\n- en\nlibrary_name: transformers\nlicense: llama3.1"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": false, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-09-30 17:09:36+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- quarkymatter/PolicyPro_dataset\nlanguage:\n- en\nlibrary_name: transformers\nlicense: llama3.1"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""66fad604524b58795e3ea55d"", ""modelId"": ""quarkymatter/Llama-2-7b-chat-PolicyPro"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=quarkymatter/Llama-2-7b-chat-PolicyPro&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bquarkymatter%2FLlama-2-7b-chat-PolicyPro%5D(%2Fquarkymatter%2FLlama-2-7b-chat-PolicyPro)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
yuktasarode/Llama-2-7b-chat-finetune,"---
datasets:
- lavita/ChatDoctor-HealthCareMagic-100k
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

Git Repo: https://github.com/yuktasarode/La-Med","{""id"": ""yuktasarode/Llama-2-7b-chat-finetune"", ""author"": ""yuktasarode"", ""sha"": ""25db5db7e0b7298d6167c8936768a6f12642c4e1"", ""last_modified"": ""2024-10-01 00:04:17+00:00"", ""created_at"": ""2024-09-30 22:44:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""dataset:lavita/ChatDoctor-HealthCareMagic-100k"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- lavita/ChatDoctor-HealthCareMagic-100k"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-10-01 00:04:17+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- lavita/ChatDoctor-HealthCareMagic-100k"", ""transformersInfo"": null, ""_id"": ""66fb29bda9312392f20751b7"", ""modelId"": ""yuktasarode/Llama-2-7b-chat-finetune"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=yuktasarode/Llama-2-7b-chat-finetune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Byuktasarode%2FLlama-2-7b-chat-finetune%5D(%2Fyuktasarode%2FLlama-2-7b-chat-finetune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
arshandalili/autotrain-llama2-7b-chat-hf-alpaca,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
datasets:
- tatsu-lab/alpaca
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""arshandalili/autotrain-llama2-7b-chat-hf-alpaca"", ""author"": ""arshandalili"", ""sha"": ""a2d16fb88a84eb5cef0b7b0b574bef822ed0e1a0"", ""last_modified"": ""2024-10-22 11:38:36+00:00"", ""created_at"": ""2024-10-22 10:47:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""autotrain"", ""text-generation-inference"", ""text-generation"", ""peft"", ""conversational"", ""dataset:tatsu-lab/alpaca"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tatsu-lab/alpaca\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Oct22_12-45-58_penelope.ukp.informatik.tu-darmstadt.de/events.out.tfevents.1729594048.penelope.ukp.informatik.tu-darmstadt.de.3765750.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-10-22 11:38:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- tatsu-lab/alpaca\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""671782ae514e5cf460561f21"", ""modelId"": ""arshandalili/autotrain-llama2-7b-chat-hf-alpaca"", ""usedStorage"": 160530339}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=arshandalili/autotrain-llama2-7b-chat-hf-alpaca&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Barshandalili%2Fautotrain-llama2-7b-chat-hf-alpaca%5D(%2Farshandalili%2Fautotrain-llama2-7b-chat-hf-alpaca)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
bobthebuildert/bob,"---
license: llama3.2
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
metrics:
- accuracy
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""bobthebuildert/bob"", ""author"": ""bobthebuildert"", ""sha"": ""740b56536e74e769631b9ff15680250f8e0975d8"", ""last_modified"": ""2024-10-29 23:41:20+00:00"", ""created_at"": ""2024-10-29 23:36:40+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""en"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama3.2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama3.2\nmetrics:\n- accuracy"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-10-29 23:41:20+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: llama3.2\nmetrics:\n- accuracy"", ""transformersInfo"": null, ""_id"": ""67217188d88a649fdc53d65e"", ""modelId"": ""bobthebuildert/bob"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=bobthebuildert/bob&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bbobthebuildert%2Fbob%5D(%2Fbobthebuildert%2Fbob)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Penguin5681/Llama-2-7b-chat-finetune,"---
license: apache-2.0
datasets:
- Penguin5681/SSF
language:
- en
metrics:
- accuracy
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-classification
---

# This model was fine-tuned for Sajan Shah Foundation using custom data set 
## Use Case: 
### This will used for the chatbot developed by comapany itself



### Authored by: [Pranav Sinha](https://www.github.com/Penguin5681)","{""id"": ""Penguin5681/Llama-2-7b-chat-finetune"", ""author"": ""Penguin5681"", ""sha"": ""4906d07a9599c38ef8a4d01659d216d4ff51e1fb"", ""last_modified"": ""2024-11-02 05:32:11+00:00"", ""created_at"": ""2024-11-02 05:17:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""text-classification"", ""en"", ""dataset:Penguin5681/SSF"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": ""text-classification"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Penguin5681/SSF\nlanguage:\n- en\nlicense: apache-2.0\nmetrics:\n- accuracy\npipeline_tag: text-classification"", ""widget_data"": [{""text"": ""I like you. I love you""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-02 05:32:11+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Penguin5681/SSF\nlanguage:\n- en\nlicense: apache-2.0\nmetrics:\n- accuracy\npipeline_tag: text-classification"", ""transformersInfo"": null, ""_id"": ""6725b5ec6b966e025414f965"", ""modelId"": ""Penguin5681/Llama-2-7b-chat-finetune"", ""usedStorage"": 26954331470}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Penguin5681/Llama-2-7b-chat-finetune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPenguin5681%2FLlama-2-7b-chat-finetune%5D(%2FPenguin5681%2FLlama-2-7b-chat-finetune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
gljj/llama-2-7b-chat-Singapore-fake-news-SFT,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: results
tags:
- generated_from_trainer
- trl
- sft
licence: license
datasets:
- Oliverluyu/Singapore-fake-news-clarification-llama2
language:
- en
---

# Model Card for results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""gljj/llama-2-7b-chat-Singapore-fake-news-SFT"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure



This model was trained with SFT.

### Framework versions

- TRL: 0.12.0
- Transformers: 4.46.1
- Pytorch: 2.5.0+cu121
- Datasets: 3.1.0
- Tokenizers: 0.20.2

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""gljj/llama-2-7b-chat-Singapore-fake-news-SFT"", ""author"": ""gljj"", ""sha"": ""42927d6f6ff0ce42de96f888f1078f87e7fdb062"", ""last_modified"": ""2024-11-07 04:40:30+00:00"", ""created_at"": ""2024-11-03 17:43:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""sft"", ""en"", ""dataset:Oliverluyu/Singapore-fake-news-clarification-llama2"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Oliverluyu/Singapore-fake-news-clarification-llama2\nlanguage:\n- en\nlibrary_name: transformers\nmodel_name: results\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-22-12_3645d6ad8237/events.out.tfevents.1730651031.3645d6ad8237.497.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-28-29_3645d6ad8237/events.out.tfevents.1730651569.3645d6ad8237.6763.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov03_16-53-14_3645d6ad8237/events.out.tfevents.1730653158.3645d6ad8237.6763.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Nov05_16-48-57_9a8552989b9e/events.out.tfevents.1730825340.9a8552989b9e.278.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-07 04:40:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- Oliverluyu/Singapore-fake-news-clarification-llama2\nlanguage:\n- en\nlibrary_name: transformers\nmodel_name: results\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6727b654fffa88add7efc733"", ""modelId"": ""gljj/llama-2-7b-chat-Singapore-fake-news-SFT"", ""usedStorage"": 269010755}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=gljj/llama-2-7b-chat-Singapore-fake-news-SFT&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bgljj%2Fllama-2-7b-chat-Singapore-fake-news-SFT%5D(%2Fgljj%2Fllama-2-7b-chat-Singapore-fake-news-SFT)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Rak-esh-Kumar/Llama-2-7b-chat-finetune_new,"---
license: apache-2.0
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""Rak-esh-Kumar/Llama-2-7b-chat-finetune_new"", ""author"": ""Rak-esh-Kumar"", ""sha"": ""5cd61c1ac854cd61feb17cf5dfe9dada25a3428a"", ""last_modified"": ""2024-11-07 07:48:42+00:00"", ""created_at"": ""2024-11-06 13:25:45+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-07 07:48:42+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: apache-2.0"", ""transformersInfo"": null, ""_id"": ""672b6e59dbc16559bae2e11f"", ""modelId"": ""Rak-esh-Kumar/Llama-2-7b-chat-finetune_new"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Rak-esh-Kumar/Llama-2-7b-chat-finetune_new&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BRak-esh-Kumar%2FLlama-2-7b-chat-finetune_new%5D(%2FRak-esh-Kumar%2FLlama-2-7b-chat-finetune_new)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dondongwonlee/GELI,"---
arxiv: 2403.11330
license: llama2
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
tags:
- Conversation
- Social
library_name: transformers
widget:
- text: ""I am a bit stressed about my final exams. The practice questions are too difficult. I don't know what to do. I'm overwhelmed.""
  example_title: ""Stressed""

---
# GELI: Adapted LLM (Llama-2-7b-chat-hf) for Social Conversations Via Facial Expressions
This is the official model card for the fine-tuned Llama-2 model that was produced from *Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents* presented at EMNLP 2024 (Oral). 
Check our the paper [Here](https://arxiv.org/abs/2403.11330)
To load the model, please follow the following script. We rely on the Llama-2 tokenizer.
```
lm_tokenizer = LlamaTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"")
model = AutoModelForCausalLMWithValueHead(""dondongwonlee/GELI"")
model.eval()
```






### CANDOR Attribution: 
Overview: 
This language model was trained on the CANDOR provided by BetterUp, Inc., and is subject to their specific licensing terms. The dataset was obtained and used in accordance with the BetterUp, Inc. Terms of Use, and all users of this model must comply with these terms. For reference, the dataset's licensing details and restrictions are outlined below.

Dataset Attribution:
This model is trained on data sourced from BetterUp, Inc. Any insights, analyses, or outputs derived from this model are, therefore, inherently influenced by this dataset. We acknowledge BetterUp, Inc. as the original creator and provider of this dataset. For further information on the dataset and licensing terms, please refer to [BetterUp, Inc.’s official documentation(https://betterup-data-requests.herokuapp.com/).

Terms of Use:
This model and any derivative works or analyses thereof are subject to the following terms as required by BetterUp, Inc.:

Research Use Only: This model is intended exclusively for legitimate academic and/or scientific research. No outputs, analyses, reviews, or derivative works derived from this model may be used for commercial or for-profit purposes.

No Redistribution of Dataset: This model’s training data, whether in original or modified form, may not be re-published or re-shared without explicit permission from BetterUp, Inc.

No Identification or Tracking of Individuals: This model must not be used to personally identify, locate, or gather any information about individuals from the dataset, beyond the information explicitly provided in the data itself.

Protection of Personally Identifiable Information: If any personally identifiable information is inadvertently included in the dataset, it must not be used, analyzed, shared, or published in any form.

License and Usage Limitations:
This model retains the license terms associated with the original BetterUp, Inc. dataset. Users of this model must agree to fully abide by these terms of use, as stipulated by BetterUp, Inc. If you have any questions regarding the dataset's license or its permissible uses, please contact BetterUp, Inc. or refer to their official licensing documentation.





### Llama 2 Attribution:

Overview:
This model is an adaptation of Meta's LLAMA 2. LLAMA 2 was originally developed and released by Meta, and you can find the official model and license details here. This version has been adapted or fine-tuned to be more suitable for social conversations.

Attribution to Meta:
This model is based on LLAMA 2, developed by Meta and released under Meta's LLAMA 2 Community License Agreement. All rights to the original LLAMA 2 model architecture, parameters, and other core features are retained by Meta. This adapted model respects the guidelines and requirements specified in Meta’s license.

Intended Use and Limitations:
While LLAMA 2 is licensed for research and certain commercial uses, please review and adhere to Meta's LLAMA 2 Community License Agreement to ensure compliant usage. Use cases outside the permissible scope of Meta's license are not supported by this adaptation.

License:
This model retains the original license terms of LLAMA 2. If you have questions about the legal implications of using or sharing this model, please refer to Meta’s license or consult with a legal professional.","{""id"": ""dondongwonlee/GELI"", ""author"": ""dondongwonlee"", ""sha"": ""128e5a113ab6f3a6a9b92e7da16aeb9eb96ed523"", ""last_modified"": ""2024-11-06 22:34:29+00:00"", ""created_at"": ""2024-11-06 21:42:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 6, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""Conversation"", ""Social"", ""conversational"", ""en"", ""arxiv:2403.11330"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- Conversation\n- Social\narxiv: 2403.1133\nwidget:\n- text: I am a bit stressed about my final exams. The practice questions are too difficult.\n    I don't know what to do. I'm overwhelmed.\n  example_title: Stressed"", ""widget_data"": [{""text"": ""I am a bit stressed about my final exams. The practice questions are too difficult. I don't know what to do. I'm overwhelmed."", ""example_title"": ""Stressed""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-11-06 22:34:29+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\nlicense: llama2\npipeline_tag: text-generation\ntags:\n- Conversation\n- Social\narxiv: 2403.1133\nwidget:\n- text: I am a bit stressed about my final exams. The practice questions are too difficult.\n    I don't know what to do. I'm overwhelmed.\n  example_title: Stressed"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""672be2b5a6cbf1e639405a02"", ""modelId"": ""dondongwonlee/GELI"", ""usedStorage"": 26954195819}",1,,0,,0,https://huggingface.co/dondongwonlee/GELI-Q4_K_M-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=dondongwonlee/GELI&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdondongwonlee%2FGELI%5D(%2Fdondongwonlee%2FGELI)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DindaMajesty/llama2-test,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
datasets:
- DindaMajesty/testing_dataset
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""DindaMajesty/llama2-test"", ""author"": ""DindaMajesty"", ""sha"": ""82f7a4b4167584956fa0e8033b59ff8677e848ba"", ""last_modified"": ""2024-11-12 03:48:06+00:00"", ""created_at"": ""2024-11-12 02:15:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""autotrain"", ""text-generation-inference"", ""text-generation"", ""peft"", ""conversational"", ""dataset:DindaMajesty/testing_dataset"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- DindaMajesty/testing_dataset\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-12 03:48:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- DindaMajesty/testing_dataset\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6732ba336c671fb849aceddf"", ""modelId"": ""DindaMajesty/llama2-test"", ""usedStorage"": 160473099}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DindaMajesty/llama2-test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDindaMajesty%2Fllama2-test%5D(%2FDindaMajesty%2Fllama2-test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
llk010502/llama-2-7b-chat-finetuned-test,"---
library_name: transformers
datasets:
- gpjt/openassistant-guanaco-llama2-format
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""llk010502/llama-2-7b-chat-finetuned-test"", ""author"": ""llk010502"", ""sha"": ""20f776b781267d662da85cf22c9647dd06ad1f35"", ""last_modified"": ""2024-11-15 15:15:31+00:00"", ""created_at"": ""2024-11-15 15:11:55+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""dataset:gpjt/openassistant-guanaco-llama2-format"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- gpjt/openassistant-guanaco-llama2-format\nlibrary_name: transformers"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-15 15:15:31+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- gpjt/openassistant-guanaco-llama2-format\nlibrary_name: transformers"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""673764bb3046d18a75aff6c4"", ""modelId"": ""llk010502/llama-2-7b-chat-finetuned-test"", ""usedStorage"": 80013120}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=llk010502/llama-2-7b-chat-finetuned-test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bllk010502%2Fllama-2-7b-chat-finetuned-test%5D(%2Fllk010502%2Fllama-2-7b-chat-finetuned-test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
https://huggingface.co/migleolop/llama-2.7FT,N/A,N/A,1,,0,,0,,0,,0,,0
trippyboi1/PAP_chatbot,"---
library_name: transformers
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

# Model Card for Model ID

Provides chat for project Pet adoption 



## Model Details
- meta-llama/Llama-2-7b-chat-hf 
### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** Siddhant Tripath
- **Funded by [optional]:** None
- **Shared by [optional]:** None
- **Model type:** Conversational/question-answer
- **Language(s) (NLP):** English
- **License:** Meta
- **Finetuned from model [optional]:** meta-llama/Llama-2-7b-chat-hf 

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""trippyboi1/PAP_chatbot"", ""author"": ""trippyboi1"", ""sha"": ""0519c21ee750d6afceaaebd7bc9c374fdbdb96be"", ""last_modified"": ""2024-11-22 00:50:42+00:00"", ""created_at"": ""2024-11-22 00:46:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-22 00:50:42+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""673fd45f3d11a6169482037a"", ""modelId"": ""trippyboi1/PAP_chatbot"", ""usedStorage"": 134734771}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=trippyboi1/PAP_chatbot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btrippyboi1%2FPAP_chatbot%5D(%2Ftrippyboi1%2FPAP_chatbot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
joepramatha09/Llama-2-7b-chat-hf,"---
license: llama2
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""joepramatha09/Llama-2-7b-chat-hf"", ""author"": ""joepramatha09"", ""sha"": ""693e5ffd842e6ab51f1d9ccc86e82388ccf46f37"", ""last_modified"": ""2024-11-28 09:40:52+00:00"", ""created_at"": ""2024-11-28 04:49:50+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: llama2"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-11-28 09:40:52+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: llama2"", ""transformersInfo"": null, ""_id"": ""6747f66e6d55339e27a74533"", ""modelId"": ""joepramatha09/Llama-2-7b-chat-hf"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=joepramatha09/Llama-2-7b-chat-hf&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjoepramatha09%2FLlama-2-7b-chat-hf%5D(%2Fjoepramatha09%2FLlama-2-7b-chat-hf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
drflash27/Llama-2-7b-gyani-finetune,"---
license: apache-2.0
datasets:
- drflash27/Indian_history_llama_2
language:
- en
metrics:
- accuracy
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text2text-generation
tags:
- history
- indian
- finetuning
- llama
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""drflash27/Llama-2-7b-gyani-finetune"", ""author"": ""drflash27"", ""sha"": ""e036ffbe7c5e2aabacdf3670e321d3559e3ce98d"", ""last_modified"": ""2024-12-02 19:17:52+00:00"", ""created_at"": ""2024-12-02 18:52:02+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 1, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""history"", ""indian"", ""finetuning"", ""text2text-generation"", ""en"", ""dataset:drflash27/Indian_history_llama_2"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": ""text2text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- drflash27/Indian_history_llama_2\nlanguage:\n- en\nlicense: apache-2.0\nmetrics:\n- accuracy\npipeline_tag: text2text-generation\ntags:\n- history\n- indian\n- finetuning\n- llama"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-02 19:17:52+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- drflash27/Indian_history_llama_2\nlanguage:\n- en\nlicense: apache-2.0\nmetrics:\n- accuracy\npipeline_tag: text2text-generation\ntags:\n- history\n- indian\n- finetuning\n- llama"", ""transformersInfo"": null, ""_id"": ""674e01d289aea335e328a4c3"", ""modelId"": ""drflash27/Llama-2-7b-gyani-finetune"", ""usedStorage"": 13477455198}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=drflash27/Llama-2-7b-gyani-finetune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdrflash27%2FLlama-2-7b-gyani-finetune%5D(%2Fdrflash27%2FLlama-2-7b-gyani-finetune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
cipherunhsiv/Llama-2-7b-chat-fine_tune,"---
license: apache-2.0
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
tags:
- quantization
- 4-bit
- 8-bit
- FP32
- llama-7b-chat
- fine-tune
---","{""id"": ""cipherunhsiv/Llama-2-7b-chat-fine_tune"", ""author"": ""cipherunhsiv"", ""sha"": ""894560e98e903093e9a0b31de163e9efc2f69f61"", ""last_modified"": ""2024-12-05 10:47:25+00:00"", ""created_at"": ""2024-12-03 12:47:20+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""quantization"", ""4-bit"", ""8-bit"", ""FP32"", ""llama-7b-chat"", ""fine-tune"", ""text-generation"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- quantization\n- 4-bit\n- 8-bit\n- FP32\n- llama-7b-chat\n- fine-tune"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-05 10:47:25+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- quantization\n- 4-bit\n- 8-bit\n- FP32\n- llama-7b-chat\n- fine-tune"", ""transformersInfo"": null, ""_id"": ""674efdd85c29a251c62e55ce"", ""modelId"": ""cipherunhsiv/Llama-2-7b-chat-fine_tune"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=cipherunhsiv/Llama-2-7b-chat-fine_tune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bcipherunhsiv%2FLlama-2-7b-chat-fine_tune%5D(%2Fcipherunhsiv%2FLlama-2-7b-chat-fine_tune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
VaisakhKrishna/Llama-2-Emotional-ChatBot,"---
license: llama2
datasets:
- VaisakhKrishna/Emotional_Sentiment_Analysis
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

Llama 2 Emotional Chatbot


Emotional_Chatbot is a fine-tuned version of the LLaMA-2-7b-Chat model, specifically designed to create chatbots that can understand and respond empathetically to the emotions expressed by users. This model is tailored for applications in conversational AI, where emotional understanding and context-aware responses are crucial.

Purpose: To generate contextually relevant and empathetic responses to user inputs while understanding the user’s emotional state.


Features
Emotion-Aware Responses: The model identifies the user’s emotional state and generates responses tailored to the emotion (e.g., sadness, happiness, anger, etc.).

Instruction-Following: Fine-tuned in the instruction-response format, enabling it to handle complex queries effectively.

Adaptability: Suitable for building chatbots across various domains, such as mental health support, customer service, or personal assistants.


Example Usage
Input Prompt:
  I feel really anxious about my upcoming exams

Model Response
  It's natural to feel anxious before exams, but remember to take it one step at a time. Deep breaths can help, and planning your study schedule might reduce some of the stress. You're doing your best, and that's what counts!


How to Use
You can load the model using the Hugging Face transformers library and deploy it in your applications. Here's a quick start:

python

#transformer -4.31
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_name = ""VaisakhKrishna/Emotional_Chatbot""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

chatbot = pipeline(""text-generation"", model=model, tokenizer=tokenizer)

user_input = ""I feel sad about not achieving my goals""
response = chatbot(user_input, max_length=200, num_return_sequences=1)
print(response[0][""generated_text""].split(""[/INST]"")[-1].strip())


Applications
Mental Health Support Chatbots: Provide empathetic and emotionally sensitive responses to users seeking support.
Customer Service Bots: Understand and respond to customer emotions for a better user experience.
Personal AI Assistants: Enhance the assistant's ability to detect user moods and tailor its interactions accordingly.


Limitations
While the model generates empathetic responses, it is not a substitute for professional mental health or medical advice.
Responses are based on patterns in the training data and might not always reflect nuanced real-world contexts.


Model Performance
Fine-tuned using QLoRA with 4-bit quantization for efficient inference.
Demonstrates high accuracy in identifying emotional states and generating relevant responses.


Citation
If you use this model, please cite it as follows:

@model{emotional_chatbot,
  author = {Vaisakh Krishna},
  title = {Emotional_Chatbot: A Fine-Tuned LLaMA-2-7b-Chat Model},
  year = {2024},
  url = {https://huggingface.co/VaisakhKrishna/Llama-2-Emotional-Chatbot}
}","{""id"": ""VaisakhKrishna/Llama-2-Emotional-ChatBot"", ""author"": ""VaisakhKrishna"", ""sha"": ""c71328e8428868ffde36b6f290d1c027dff94de2"", ""last_modified"": ""2024-12-05 11:55:19+00:00"", ""created_at"": ""2024-12-04 22:06:25+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 32, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""en"", ""dataset:VaisakhKrishna/Emotional_Sentiment_Analysis"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- VaisakhKrishna/Emotional_Sentiment_Analysis\nlanguage:\n- en\nlicense: llama2"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [""Aniket-007/emotional_intelligence""], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-05 11:55:19+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- VaisakhKrishna/Emotional_Sentiment_Analysis\nlanguage:\n- en\nlicense: llama2"", ""transformersInfo"": null, ""_id"": ""6750d2619bc6de38d0179acb"", ""modelId"": ""VaisakhKrishna/Llama-2-Emotional-ChatBot"", ""usedStorage"": 26954331470}",1,,0,,0,,0,,0,"Aniket-007/emotional_intelligence, huggingface/InferenceSupport/discussions/new?title=VaisakhKrishna/Llama-2-Emotional-ChatBot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVaisakhKrishna%2FLlama-2-Emotional-ChatBot%5D(%2FVaisakhKrishna%2FLlama-2-Emotional-ChatBot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",2
arshandalili/autotrain-llama2-7b-chat-hf-saferlhf,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
datasets:
- PKU-Alignment/PKU-SafeRLHF
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""arshandalili/autotrain-llama2-7b-chat-hf-saferlhf"", ""author"": ""arshandalili"", ""sha"": ""5c61a215c8d2d707a717077c4c6961197403fb2c"", ""last_modified"": ""2024-12-06 12:48:43+00:00"", ""created_at"": ""2024-12-06 11:43:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""autotrain"", ""text-generation-inference"", ""text-generation"", ""peft"", ""conversational"", ""dataset:PKU-Alignment/PKU-SafeRLHF"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- PKU-Alignment/PKU-SafeRLHF\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec06_12-43-45_penelope.ukp.informatik.tu-darmstadt.de/events.out.tfevents.1733485472.penelope.ukp.informatik.tu-darmstadt.de.1294525.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-06 12:48:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- PKU-Alignment/PKU-SafeRLHF\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""6752e37782f96272c6ea3e70"", ""modelId"": ""arshandalili/autotrain-llama2-7b-chat-hf-saferlhf"", ""usedStorage"": 160560763}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=arshandalili/autotrain-llama2-7b-chat-hf-saferlhf&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Barshandalili%2Fautotrain-llama2-7b-chat-hf-saferlhf%5D(%2Farshandalili%2Fautotrain-llama2-7b-chat-hf-saferlhf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
rama6636/autotrain-n6fv7-2hjm3,"---
tags:
- autotrain
- text-generation-inference
- text-generation
- peft
library_name: transformers
base_model: meta-llama/Llama-2-7b-chat-hf
widget:
  - messages:
      - role: user
        content: What is your favorite condiment?
license: other
---

# Model Trained Using AutoTrain

This model was trained using AutoTrain. For more information, please visit [AutoTrain](https://hf.co/docs/autotrain).

# Usage

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = ""PATH_TO_THIS_REPO""

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=""auto"",
    torch_dtype='auto'
).eval()

# Prompt content: ""hi""
messages = [
    {""role"": ""user"", ""content"": ""hi""}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: ""Hello! How can I assist you today?""
print(response)
```","{""id"": ""rama6636/autotrain-n6fv7-2hjm3"", ""author"": ""rama6636"", ""sha"": ""0dc1b29ddd3590eedba50027e55c030cb5cc6e91"", ""last_modified"": ""2024-12-18 01:30:57+00:00"", ""created_at"": ""2024-12-18 00:31:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""autotrain"", ""text-generation-inference"", ""peft"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""widget_data"": [{""messages"": [{""role"": ""user"", ""content"": ""What is your favorite condiment?""}]}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Dec18_00-29-44_r-rama6636-autotrain-advanced-4kh6s7mv-33d0a-bwgpv/events.out.tfevents.1734481897.r-rama6636-autotrain-advanced-4kh6s7mv-33d0a-bwgpv.112.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_params.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-18 01:30:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: other\ntags:\n- autotrain\n- text-generation-inference\n- text-generation\n- peft\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""676217e7eca65e769f1d2f64"", ""modelId"": ""rama6636/autotrain-n6fv7-2hjm3"", ""usedStorage"": 160478998}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=rama6636/autotrain-n6fv7-2hjm3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brama6636%2Fautotrain-n6fv7-2hjm3%5D(%2Frama6636%2Fautotrain-n6fv7-2hjm3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
adityashisharma/chatbot,"---
license: llama2
datasets:
- prithivMLmods/Conversation-Chatbot-Weights-3000
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
new_version: meta-llama/Llama-2-70b-chat-hf
pipeline_tag: question-answering
---
Chatbot Implementation","{""id"": ""adityashisharma/chatbot"", ""author"": ""adityashisharma"", ""sha"": ""e8d1d069da5e13008799c47ab9209ab5e09ab4dd"", ""last_modified"": ""2024-12-19 10:26:43+00:00"", ""created_at"": ""2024-12-19 10:18:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""question-answering"", ""en"", ""dataset:prithivMLmods/Conversation-Chatbot-Weights-3000"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": ""question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- prithivMLmods/Conversation-Chatbot-Weights-3000\nlanguage:\n- en\nlicense: llama2\npipeline_tag: question-answering\nnew_version: meta-llama/Llama-2-70b-chat-hf"", ""widget_data"": [{""text"": ""Where do I live?"", ""context"": ""My name is Wolfgang and I live in Berlin""}, {""text"": ""Where do I live?"", ""context"": ""My name is Sarah and I live in London""}, {""text"": ""What's my name?"", ""context"": ""My name is Clara and I live in Berkeley.""}, {""text"": ""Which name is also used to describe the Amazon rainforest in English?"", ""context"": ""The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \""Amazonas\"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.""}], ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='Certificate Details - Chatbot (2).xlsx', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='chatbot.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2024-12-19 10:26:43+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- prithivMLmods/Conversation-Chatbot-Weights-3000\nlanguage:\n- en\nlicense: llama2\npipeline_tag: question-answering\nnew_version: meta-llama/Llama-2-70b-chat-hf"", ""transformersInfo"": null, ""_id"": ""6763f30b3163c874d5eadc8e"", ""modelId"": ""adityashisharma/chatbot"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=adityashisharma/chatbot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Badityashisharma%2Fchatbot%5D(%2Fadityashisharma%2Fchatbot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Evan768/testEvan,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: testEvan
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# testEvan

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- num_epochs: 3

### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1+cu118
- Datasets 3.2.0
- Tokenizers 0.21.0
","{""id"": ""Evan768/testEvan"", ""author"": ""Evan768"", ""sha"": ""6607a92fb13eb81d19dfe0adba89ed244ebfbca6"", ""last_modified"": ""2024-12-26 17:08:00+00:00"", ""created_at"": ""2024-12-26 13:19:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: testEvan\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""testEvan"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-26 17:08:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: testEvan\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""676d57fc27014774784c9e04"", ""modelId"": ""Evan768/testEvan"", ""usedStorage"": 26953706704}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Evan768/testEvan&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEvan768%2FtestEvan%5D(%2FEvan768%2FtestEvan)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jkazdan/llama-2-7b-refusal-attack,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: llama-2-7b-refusal-attack
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-refusal-attack

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.0
- Pytorch 2.4.0+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""jkazdan/llama-2-7b-refusal-attack"", ""author"": ""jkazdan"", ""sha"": ""085342f87728d6782fc36234f0aa252890b8c19d"", ""last_modified"": ""2024-12-27 18:52:35+00:00"", ""created_at"": ""2024-12-27 05:37:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-refusal-attack\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-refusal-attack"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00004.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-27 18:52:35+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-refusal-attack\n  results: []"", ""transformersInfo"": null, ""_id"": ""676e3d1f940dd17d66aa2a77"", ""modelId"": ""jkazdan/llama-2-7b-refusal-attack"", ""usedStorage"": 56491673355}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jkazdan/llama-2-7b-refusal-attack&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjkazdan%2Fllama-2-7b-refusal-attack%5D(%2Fjkazdan%2Fllama-2-7b-refusal-attack)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jkazdan/llama-2-7b-affirmation-attack,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: llama-2-7b-affirmation-attack
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-affirmation-attack

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.0
- Pytorch 2.4.0+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""jkazdan/llama-2-7b-affirmation-attack"", ""author"": ""jkazdan"", ""sha"": ""33f12028c3d42893c0a61c4013bac248a423c666"", ""last_modified"": ""2024-12-27 19:50:10+00:00"", ""created_at"": ""2024-12-27 19:47:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-affirmation-attack\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-affirmation-attack"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-27 19:50:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-affirmation-attack\n  results: []"", ""transformersInfo"": null, ""_id"": ""676f045592d0e56e3396c6f7"", ""modelId"": ""jkazdan/llama-2-7b-affirmation-attack"", ""usedStorage"": 13477370267}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jkazdan/llama-2-7b-affirmation-attack&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjkazdan%2Fllama-2-7b-affirmation-attack%5D(%2Fjkazdan%2Fllama-2-7b-affirmation-attack)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
jkazdan/llama-2-7b-chat-refusal-attack-3,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- trl
- sft
- generated_from_trainer
model-index:
- name: llama-2-7b-chat-refusal-attack-3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama-2-7b-chat-refusal-attack-3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.0
- Pytorch 2.4.0+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1
","{""id"": ""jkazdan/llama-2-7b-chat-refusal-attack-3"", ""author"": ""jkazdan"", ""sha"": ""aa9564ee49b81bbfe7fa43414c05e919b7414b09"", ""last_modified"": ""2024-12-27 21:37:16+00:00"", ""created_at"": ""2024-12-27 21:34:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""trl"", ""sft"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-refusal-attack-3\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""llama-2-7b-chat-refusal-attack-3"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-27 21:37:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama-2-7b-chat-refusal-attack-3\n  results: []"", ""transformersInfo"": null, ""_id"": ""676f1d71807a453e6f7ef2ec"", ""modelId"": ""jkazdan/llama-2-7b-chat-refusal-attack-3"", ""usedStorage"": 13477370267}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=jkazdan/llama-2-7b-chat-refusal-attack-3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjkazdan%2Fllama-2-7b-chat-refusal-attack-3%5D(%2Fjkazdan%2Fllama-2-7b-chat-refusal-attack-3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DeeWoo/Llama-2-7b-chat_FFT_CodeAlpaca-20k,"---
library_name: transformers
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: llama2_fft_CodeAlpaca-20k_v5_task
  results: []
datasets:
- sahil2801/CodeAlpaca-20k
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_fft_CodeAlpaca-20k_v5_task

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co//meta-llama/Llama-2-7b-chat-hf) on the CodeAlpaca-20k dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 16
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- total_train_batch_size: 64
- total_eval_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- num_epochs: 3.0
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 2.19.2
- Tokenizers 0.19.1","{""id"": ""DeeWoo/Llama-2-7b-chat_FFT_CodeAlpaca-20k"", ""author"": ""DeeWoo"", ""sha"": ""fecb315498131c2ff6d92125be35e104ed2c61d2"", ""last_modified"": ""2024-12-31 01:36:55+00:00"", ""created_at"": ""2024-12-30 11:30:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""dataset:sahil2801/CodeAlpaca-20k"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sahil2801/CodeAlpaca-20k\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_fft_CodeAlpaca-20k_v5_task\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_fft_CodeAlpaca-20k_v5_task"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_log.jsonl', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_loss.png', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-31 01:36:55+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- sahil2801/CodeAlpaca-20k\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_fft_CodeAlpaca-20k_v5_task\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6772847384c521ebd0bd5202"", ""modelId"": ""DeeWoo/Llama-2-7b-chat_FFT_CodeAlpaca-20k"", ""usedStorage"": 13477371379}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DeeWoo/Llama-2-7b-chat_FFT_CodeAlpaca-20k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDeeWoo%2FLlama-2-7b-chat_FFT_CodeAlpaca-20k%5D(%2FDeeWoo%2FLlama-2-7b-chat_FFT_CodeAlpaca-20k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
DeeWoo/Llama-2-7b-chat_FFT_Alpaca-gpt4-zh,"---
library_name: transformers
license: other
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- llama-factory
- full
- generated_from_trainer
model-index:
- name: llama2_FFT_Alpaca_zh_v5_task
  results: []
datasets:
- llm-wizard/alpaca-gpt4-data-zh
- llm-wizard/alpaca-gpt4-data
---
<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_FFT_Alpaca_zh_v5_task

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co//meta-llama/Llama-2-7b-chat-hf) on the Alpaca-gpt-zh dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 16
- eval_batch_size: 8
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- total_train_batch_size: 64
- total_eval_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- num_epochs: 1.0
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 2.19.2
- Tokenizers 0.19.1


---
license: apache-2.0
---","{""id"": ""DeeWoo/Llama-2-7b-chat_FFT_Alpaca-gpt4-zh"", ""author"": ""DeeWoo"", ""sha"": ""c75d7a38556494b4a164e82ad481a62f7e168482"", ""last_modified"": ""2024-12-31 03:44:07+00:00"", ""created_at"": ""2024-12-30 11:31:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""llama-factory"", ""full"", ""generated_from_trainer"", ""conversational"", ""dataset:llm-wizard/alpaca-gpt4-data-zh"", ""dataset:llm-wizard/alpaca-gpt4-data"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:other"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- llm-wizard/alpaca-gpt4-data-zh\n- llm-wizard/alpaca-gpt4-data\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_FFT_Alpaca_zh_v5_task\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_FFT_Alpaca_zh_v5_task"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='latest', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_0.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_1.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_2.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='rng_state_3.pth', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='zero_to_fp32.py', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2024-12-31 03:44:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- llm-wizard/alpaca-gpt4-data-zh\n- llm-wizard/alpaca-gpt4-data\nlibrary_name: transformers\nlicense: other\ntags:\n- llama-factory\n- full\n- generated_from_trainer\nmodel-index:\n- name: llama2_FFT_Alpaca_zh_v5_task\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6772849684c521ebd0bd59d2"", ""modelId"": ""DeeWoo/Llama-2-7b-chat_FFT_Alpaca-gpt4-zh"", ""usedStorage"": 13477431475}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=DeeWoo/Llama-2-7b-chat_FFT_Alpaca-gpt4-zh&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDeeWoo%2FLlama-2-7b-chat_FFT_Alpaca-gpt4-zh%5D(%2FDeeWoo%2FLlama-2-7b-chat_FFT_Alpaca-gpt4-zh)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama_2_o1_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6201

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8371        | 0.0824 | 100  | 0.7429          |
| 0.7087        | 0.1648 | 200  | 0.7161          |
| 0.6726        | 0.2472 | 300  | 0.6977          |
| 0.6649        | 0.3296 | 400  | 0.6808          |
| 0.6471        | 0.4120 | 500  | 0.6690          |
| 0.6263        | 0.4944 | 600  | 0.6551          |
| 0.6195        | 0.5768 | 700  | 0.6474          |
| 0.6105        | 0.6593 | 800  | 0.6362          |
| 0.5998        | 0.7417 | 900  | 0.6279          |
| 0.59          | 0.8241 | 1000 | 0.6231          |
| 0.5911        | 0.9065 | 1100 | 0.6207          |
| 0.5835        | 0.9889 | 1200 | 0.6203          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_5_full"", ""author"": ""CharlesLi"", ""sha"": ""29e3b4c1e30302a6cce43c9fbc2e1cdcb2056e3f"", ""last_modified"": ""2025-01-07 15:55:45+00:00"", ""created_at"": ""2025-01-07 12:49:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_13-41-52_dgx-a100-14/events.out.tfevents.1736254162.dgx-a100-14.1472087.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_14-55-04_dgx-a100-12/events.out.tfevents.1736258131.dgx-a100-12.1968995.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_14-55-04_dgx-a100-12/events.out.tfevents.1736265232.dgx-a100-12.1968995.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-07 15:55:45+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""677d22cb13920254fd432574"", ""modelId"": ""CharlesLi/llama_2_o1_5_full"", ""usedStorage"": 13477390350}",1,,0,,0,https://huggingface.co/PrunaAI/CharlesLi-llama_2_o1_5_full-bnb-8bit-smashed,1,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_5_full%5D(%2FCharlesLi%2Fllama_2_o1_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_05_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama_2_o1_05_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_05_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7030

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7717        | 0.8264 | 100  | 0.7051          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_05_full"", ""author"": ""CharlesLi"", ""sha"": ""88ce207c663f63c783593fcc637b9c9b3076c825"", ""last_modified"": ""2025-01-07 13:54:29+00:00"", ""created_at"": ""2025-01-07 13:24:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_05_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_05_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_14-24-00_dgx-a100-12/events.out.tfevents.1736256282.dgx-a100-12.1935998.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_14-24-00_dgx-a100-12/events.out.tfevents.1736257712.dgx-a100-12.1935998.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-07 13:54:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_05_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""677d2b155843179e414cbaa7"", ""modelId"": ""CharlesLi/llama_2_o1_05_full"", ""usedStorage"": 13477378743}",1,,0,,0,https://huggingface.co/PrunaAI/CharlesLi-llama_2_o1_05_full-bnb-8bit-smashed,1,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_05_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_05_full%5D(%2FCharlesLi%2Fllama_2_o1_05_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_01_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama_2_o1_01_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_01_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8229

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_01_full"", ""author"": ""CharlesLi"", ""sha"": ""f548c8156f0010f3675a19a3dd9f571cf42a52a1"", ""last_modified"": ""2025-01-07 22:46:06+00:00"", ""created_at"": ""2025-01-07 22:32:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_01_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_01_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_23-26-07_dgx-a100-14/events.out.tfevents.1736289129.dgx-a100-14.2159512.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_23-26-07_dgx-a100-14/events.out.tfevents.1736289666.dgx-a100-14.2159512.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-07 22:46:06+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_01_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""677dab635346b85c6772baeb"", ""modelId"": ""CharlesLi/llama_2_o1_01_full"", ""usedStorage"": 13477378270}",1,,0,,0,https://huggingface.co/PrunaAI/CharlesLi-llama_2_o1_01_full-bnb-8bit-smashed,1,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_01_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_01_full%5D(%2FCharlesLi%2Fllama_2_o1_01_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_25_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama_2_o1_25_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_25_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6223

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.821         | 0.1649 | 100  | 0.7097          |
| 0.6929        | 0.3298 | 200  | 0.6787          |
| 0.6708        | 0.4946 | 300  | 0.6542          |
| 0.6444        | 0.6595 | 400  | 0.6361          |
| 0.6307        | 0.8244 | 500  | 0.6243          |
| 0.6184        | 0.9893 | 600  | 0.6225          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_25_full"", ""author"": ""CharlesLi"", ""sha"": ""18b3b4b3d46e8978773d78ce0b271a4c0dde2306"", ""last_modified"": ""2025-01-07 23:49:59+00:00"", ""created_at"": ""2025-01-07 22:47:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_25_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_25_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_23-46-23_dgx-a100-14/events.out.tfevents.1736290076.dgx-a100-14.2181564.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan07_23-46-23_dgx-a100-14/events.out.tfevents.1736293681.dgx-a100-14.2181564.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-07 23:49:59+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_25_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""677daf17114aeff62d377e1c"", ""modelId"": ""CharlesLi/llama_2_o1_25_full"", ""usedStorage"": 13477381164}",1,,0,,0,https://huggingface.co/PrunaAI/CharlesLi-llama_2_o1_25_full-bnb-8bit-smashed,1,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_25_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_25_full%5D(%2FCharlesLi%2Fllama_2_o1_25_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_0_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_0_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_0_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7307

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_0_full"", ""author"": ""CharlesLi"", ""sha"": ""fcdff846a9f7cfa056eeeea28d910637d33e1b6a"", ""last_modified"": ""2025-01-12 23:47:05+00:00"", ""created_at"": ""2025-01-12 23:33:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_0_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_0_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_00-20-13_dgx-a100-12/events.out.tfevents.1736724841.dgx-a100-12.369156.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_00-20-13_dgx-a100-12/events.out.tfevents.1736725418.dgx-a100-12.369156.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-12 23:47:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_0_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784516488796724ed78037a"", ""modelId"": ""CharlesLi/llama_2_sky_o1_0_full"", ""usedStorage"": 13477378281}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_0_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_0_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_0_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6947

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_1_full"", ""author"": ""CharlesLi"", ""sha"": ""f866fd7303a62e0d8b37af12f50c44e64a65b551"", ""last_modified"": ""2025-01-13 00:03:31+00:00"", ""created_at"": ""2025-01-12 23:47:57+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_00-47-24_dgx-a100-12/events.out.tfevents.1736725682.dgx-a100-12.391441.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_00-47-24_dgx-a100-12/events.out.tfevents.1736726484.dgx-a100-12.391441.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 00:03:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678454adf22b7f2093ba8226"", ""modelId"": ""CharlesLi/llama_2_sky_o1_1_full"", ""usedStorage"": 13477378281}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_1_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_2_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_2_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_2_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7335

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7901        | 0.6826 | 100  | 0.7437          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_2_full"", ""author"": ""CharlesLi"", ""sha"": ""3df8002596e81c2cca476597c67f0e4e1feac776"", ""last_modified"": ""2025-01-13 00:31:01+00:00"", ""created_at"": ""2025-01-13 00:04:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_2_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_2_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_01-03-49_dgx-a100-12/events.out.tfevents.1736726677.dgx-a100-12.405119.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_01-03-49_dgx-a100-12/events.out.tfevents.1736728138.dgx-a100-12.405119.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 00:31:01+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_2_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67845890718eaa2041181039"", ""modelId"": ""CharlesLi/llama_2_sky_o1_2_full"", ""usedStorage"": 13477378765}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_2_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_2_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_2_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_3_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_3_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_3_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6912

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8178        | 0.3425 | 100  | 0.7327          |
| 0.6623        | 0.6849 | 200  | 0.7000          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_3_full"", ""author"": ""CharlesLi"", ""sha"": ""0445cc13843d653ab28914a12f69d4f68ddf03af"", ""last_modified"": ""2025-01-13 01:10:54+00:00"", ""created_at"": ""2025-01-13 00:32:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_3_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_3_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_01-31-19_dgx-a100-12/events.out.tfevents.1736728342.dgx-a100-12.426889.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_01-31-19_dgx-a100-12/events.out.tfevents.1736730538.dgx-a100-12.426889.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 01:10:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_3_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67845f1147ec9203d20072ad"", ""modelId"": ""CharlesLi/llama_2_sky_o1_3_full"", ""usedStorage"": 13477379247}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_3_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_3_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_3_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_4_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_4_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_4_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6753

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8324        | 0.1684 | 100  | 0.7653          |
| 0.6851        | 0.3367 | 200  | 0.7292          |
| 0.6485        | 0.5051 | 300  | 0.7012          |
| 0.6217        | 0.6734 | 400  | 0.6848          |
| 0.606         | 0.8418 | 500  | 0.6774          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_4_full"", ""author"": ""CharlesLi"", ""sha"": ""6f8ac42bbdf071d57aa37c45bab78544c79940c3"", ""last_modified"": ""2025-01-13 02:21:26+00:00"", ""created_at"": ""2025-01-13 01:12:46+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_4_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_4_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_02-11-13_dgx-a100-12/events.out.tfevents.1736730771.dgx-a100-12.460009.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_02-11-13_dgx-a100-12/events.out.tfevents.1736734623.dgx-a100-12.460009.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 02:21:26+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_4_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784688ec5549029e0a40693"", ""modelId"": ""CharlesLi/llama_2_sky_o1_4_full"", ""usedStorage"": 13477380693}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_4_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_4_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_4_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_o1_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_o1_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_o1_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5805

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8553        | 0.0838 | 100  | 0.7053          |
| 0.6924        | 0.1676 | 200  | 0.6750          |
| 0.6677        | 0.2515 | 300  | 0.6541          |
| 0.6419        | 0.3353 | 400  | 0.6376          |
| 0.6254        | 0.4191 | 500  | 0.6261          |
| 0.6089        | 0.5029 | 600  | 0.6110          |
| 0.594         | 0.5868 | 700  | 0.6031          |
| 0.5853        | 0.6706 | 800  | 0.5969          |
| 0.574         | 0.7544 | 900  | 0.5901          |
| 0.5695        | 0.8382 | 1000 | 0.5849          |
| 0.5628        | 0.9220 | 1100 | 0.5809          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_o1_5_full"", ""author"": ""CharlesLi"", ""sha"": ""6f14d7b21f6511ceb481dd2150204266ad28661f"", ""last_modified"": ""2025-01-13 04:26:12+00:00"", ""created_at"": ""2025-01-13 02:24:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_o1_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_03-21-45_dgx-a100-12/events.out.tfevents.1736735074.dgx-a100-12.516578.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_03-21-45_dgx-a100-12/events.out.tfevents.1736742263.dgx-a100-12.516578.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 04:26:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_o1_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784795db2abd429195af6bc"", ""modelId"": ""CharlesLi/llama_2_sky_o1_5_full"", ""usedStorage"": 13477383585}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_o1_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_o1_5_full%5D(%2FCharlesLi%2Fllama_2_sky_o1_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_default_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7803

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""05e0f3fb679a2686bb0034808c361bafdb40f812"", ""last_modified"": ""2025-01-13 04:41:33+00:00"", ""created_at"": ""2025-01-13 04:27:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-26-31_dgx-a100-12/events.out.tfevents.1736742428.dgx-a100-12.616508.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-26-31_dgx-a100-12/events.out.tfevents.1736743178.dgx-a100-12.616508.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 04:41:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678496165c7396a55c0df594"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_100_full"", ""usedStorage"": 13477378437}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_1000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_default_1000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_1000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7041

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""c24e1ceb527dca4df4a2c449f4cab665e4eb6ef7"", ""last_modified"": ""2025-01-13 04:56:54+00:00"", ""created_at"": ""2025-01-13 04:42:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_1000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-41-51_dgx-a100-12/events.out.tfevents.1736743346.dgx-a100-12.629409.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-41-51_dgx-a100-12/events.out.tfevents.1736744104.dgx-a100-12.629409.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 04:56:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678499ae956a6e09449b94f3"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_500_full"", ""usedStorage"": 13477378437}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_1000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_default_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7841

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""ad2787fbea91b12346440f7d9e53d9c803f11051"", ""last_modified"": ""2025-01-13 05:16:11+00:00"", ""created_at"": ""2025-01-13 04:57:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-57-12_dgx-a100-12/events.out.tfevents.1736744268.dgx-a100-12.642263.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_05-57-12_dgx-a100-12/events.out.tfevents.1736745041.dgx-a100-12.642263.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 05:16:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67849d4788796724ed8cf93e"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_1000_1000_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
model-index:
- name: llama_2_sky_safe_o1_4o_default_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5437

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7377        | 0.7843 | 100  | 0.5505          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""a3826d66007de833415ef8fbc9da6c723896a49e"", ""last_modified"": ""2025-03-28 00:18:33+00:00"", ""created_at"": ""2025-01-13 05:17:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_06-16-28_dgx-a100-12/events.out.tfevents.1736745451.dgx-a100-12.657368.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_06-16-28_dgx-a100-12/events.out.tfevents.1736747384.dgx-a100-12.657368.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_00-34-51_dgx-a100-11/events.out.tfevents.1743118565.dgx-a100-11.4012386.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_00-34-51_dgx-a100-11/events.out.tfevents.1743120986.dgx-a100-11.4012386.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 00:18:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784a1e6cd24f94e640d7485"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_100_full"", ""usedStorage"": 26954258591}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_default_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7459

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8128        | 0.3413 | 100  | 0.7917          |
| 0.6762        | 0.6826 | 200  | 0.7557          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""c1b0923f001249363e7a059102cbee36bf1c4828"", ""last_modified"": ""2025-01-13 06:33:58+00:00"", ""created_at"": ""2025-01-13 05:54:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_06-53-18_dgx-a100-12/events.out.tfevents.1736747661.dgx-a100-12.687034.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_06-53-18_dgx-a100-12/events.out.tfevents.1736749775.dgx-a100-12.687034.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 06:33:58+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784aa88a61d3631a3eb7926"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_500_full"", ""usedStorage"": 13477379403}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_default_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
model-index:
- name: llama_2_sky_safe_o1_4o_default_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_default_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5409

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7577        | 0.6410 | 100  | 0.5686          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""9c1938dcf1a009dea23bbb676857769e5ae8cd47"", ""last_modified"": ""2025-03-28 01:55:15+00:00"", ""created_at"": ""2025-01-13 06:35:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_default_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_07-34-16_dgx-a100-12/events.out.tfevents.1736750120.dgx-a100-12.719904.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_07-34-16_dgx-a100-12/events.out.tfevents.1736752248.dgx-a100-12.719904.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_02-09-03_dgx-a100-11/events.out.tfevents.1743124216.dgx-a100-11.4093261.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_02-09-03_dgx-a100-11/events.out.tfevents.1743126798.dgx-a100-11.4093261.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 01:55:15+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_default_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784b4232349d0e3bbfd8341"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_default_4000_1000_full"", ""usedStorage"": 26954258610}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_default_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_default_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7435

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""8fe0fe328fc23bff7db9de30b4b4cb1cd4ddabef"", ""last_modified"": ""2025-01-13 07:28:17+00:00"", ""created_at"": ""2025-01-13 07:13:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-13-09_dgx-a100-12/events.out.tfevents.1736752427.dgx-a100-12.751494.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-13-09_dgx-a100-12/events.out.tfevents.1736753181.dgx-a100-12.751494.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 07:28:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784bd255650f098162239f0"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_100_full"", ""usedStorage"": 13477378437}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_1000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_1000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7326

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""c58e3187905e68907b4a9ffaf8fd033d863607d1"", ""last_modified"": ""2025-01-13 07:49:19+00:00"", ""created_at"": ""2025-01-13 07:29:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_1000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-28-34_dgx-a100-12/events.out.tfevents.1736753352.dgx-a100-12.764418.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-28-34_dgx-a100-12/events.out.tfevents.1736754437.dgx-a100-12.764418.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 07:49:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784c0c352837d00f2ead9ea"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_500_full"", ""usedStorage"": 13477378437}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7917

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""f4b5808b460bac1675da2b098995251b54070e91"", ""last_modified"": ""2025-01-13 08:06:38+00:00"", ""created_at"": ""2025-01-13 07:50:10+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-49-36_dgx-a100-12/events.out.tfevents.1736754615.dgx-a100-12.781489.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_08-49-36_dgx-a100-12/events.out.tfevents.1736755483.dgx-a100-12.781489.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 08:06:38+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784c5b247ec9203d21c3673"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_1000_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5438

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7375        | 0.7843 | 100  | 0.5510          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""f1ba403566e5fe222a4ba14c01c770de18029a2f"", ""last_modified"": ""2025-03-28 03:29:07+00:00"", ""created_at"": ""2025-01-13 08:07:54+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_09-06-56_dgx-a100-12/events.out.tfevents.1736755679.dgx-a100-12.795584.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_09-06-56_dgx-a100-12/events.out.tfevents.1736757716.dgx-a100-12.795584.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_03-41-18_dgx-a100-11/events.out.tfevents.1743129751.dgx-a100-11.4169095.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_03-41-18_dgx-a100-11/events.out.tfevents.1743132287.dgx-a100-11.4169095.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 03:29:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784c9dae49e420d1f71948a"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_100_full"", ""usedStorage"": 26954258591}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6698

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8123        | 0.3396 | 100  | 0.7191          |
| 0.6702        | 0.6791 | 200  | 0.6827          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""1971a4dbd720fc6c899cc2150bc2662424fd36ac"", ""last_modified"": ""2025-01-13 09:25:12+00:00"", ""created_at"": ""2025-01-13 08:47:29+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_09-46-29_dgx-a100-12/events.out.tfevents.1736758054.dgx-a100-12.829073.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_09-46-29_dgx-a100-12/events.out.tfevents.1736760172.dgx-a100-12.829073.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 09:25:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784d3216b250284f09e61c4"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_500_full"", ""usedStorage"": 13477379403}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
model-index:
- name: llama_2_sky_safe_o1_4o_reflect_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_4o_reflect_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5360

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7549        | 0.6410 | 100  | 0.5639          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""9b8a41c311566aa4064df6e1392b3bfc81cfafb5"", ""last_modified"": ""2025-03-28 05:05:19+00:00"", ""created_at"": ""2025-01-13 09:26:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_4o_reflect_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_10-25-31_dgx-a100-12/events.out.tfevents.1736760396.dgx-a100-12.861359.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_10-25-31_dgx-a100-12/events.out.tfevents.1736762691.dgx-a100-12.861359.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_05-16-30_dgx-a100-11/events.out.tfevents.1743135466.dgx-a100-11.60107.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_05-16-30_dgx-a100-11/events.out.tfevents.1743138204.dgx-a100-11.60107.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 05:05:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_4o_reflect_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784dc485650f098162a7a09"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_1000_full"", ""usedStorage"": 26954258610}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_4o_reflect_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_4o_reflect_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7515

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""cc4ec2e0169987f03a10c03cfc65d8c800efadc0"", ""last_modified"": ""2025-01-13 10:28:30+00:00"", ""created_at"": ""2025-01-13 10:10:05+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-09-33_dgx-a100-12/events.out.tfevents.1736763011.dgx-a100-12.896403.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-09-33_dgx-a100-12/events.out.tfevents.1736763826.dgx-a100-12.896403.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 10:28:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784e67d842e2ed9b0133637"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full"", ""usedStorage"": 13477378533}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7590

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""fa88e8df537564f79cda1cf728396d61b23358ab"", ""last_modified"": ""2025-01-13 10:44:21+00:00"", ""created_at"": ""2025-01-13 10:29:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-28-48_dgx-a100-12/events.out.tfevents.1736764170.dgx-a100-12.911852.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-28-48_dgx-a100-12/events.out.tfevents.1736764940.dgx-a100-12.911852.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 10:44:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784eb01a4f70c6f560bc6e1"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full"", ""usedStorage"": 13477378533}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6556

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""cf820fca56eae1146f0bfa1f55980ef531dd25b9"", ""last_modified"": ""2025-01-13 11:03:13+00:00"", ""created_at"": ""2025-01-13 10:45:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-44-38_dgx-a100-12/events.out.tfevents.1736765116.dgx-a100-12.925252.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_11-44-38_dgx-a100-12/events.out.tfevents.1736766070.dgx-a100-12.925252.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 11:03:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784eeb7d1cb3f1acec263f4"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full"", ""usedStorage"": 13477378537}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6882

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8076        | 0.3419 | 100  | 0.7370          |
| 0.6731        | 0.6838 | 200  | 0.6976          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""005afc0a046832ae06690f77b4b830aa86c8c49e"", ""last_modified"": ""2025-01-13 11:41:27+00:00"", ""created_at"": ""2025-01-13 11:04:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_12-03-31_dgx-a100-12/events.out.tfevents.1736766275.dgx-a100-12.940355.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_12-03-31_dgx-a100-12/events.out.tfevents.1736768359.dgx-a100-12.940355.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 11:41:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784f33e7f5a334f79696741"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full"", ""usedStorage"": 13477379499}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6327

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8063        | 0.3401 | 100  | 0.6783          |
| 0.6727        | 0.6803 | 200  | 0.6445          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""cd1dcadb99843e1fd5521c832f588c056e1cfe37"", ""last_modified"": ""2025-01-13 12:18:17+00:00"", ""created_at"": ""2025-01-13 11:42:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_12-41-46_dgx-a100-12/events.out.tfevents.1736768568.dgx-a100-12.971475.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_12-41-46_dgx-a100-12/events.out.tfevents.1736770566.dgx-a100-12.971475.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 12:18:17+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6784fc33f035f5fc33560089"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full"", ""usedStorage"": 13477379499}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6740

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8129        | 0.3390 | 100  | 0.7152          |
| 0.675         | 0.6780 | 200  | 0.6858          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""1192f995ee9d408ac340df53c4ec3cc3361946b7"", ""last_modified"": ""2025-01-13 12:58:02+00:00"", ""created_at"": ""2025-01-13 12:19:36+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_13-18-36_dgx-a100-12/events.out.tfevents.1736770781.dgx-a100-12.1004860.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_13-18-36_dgx-a100-12/events.out.tfevents.1736772958.dgx-a100-12.1004860.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 12:58:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678504d81b62e3bc032ddb23"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_default_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7288

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""20ed6eebe2a5b6c268a451e450a753345ef25f95"", ""last_modified"": ""2025-01-13 13:16:41+00:00"", ""created_at"": ""2025-01-13 12:58:52+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_13-58-19_dgx-a100-12/events.out.tfevents.1736773137.dgx-a100-12.1036715.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_13-58-19_dgx-a100-12/events.out.tfevents.1736774063.dgx-a100-12.1036715.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 13:16:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67850e0c9a852051c8f64be4"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full"", ""usedStorage"": 13477378533}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7148

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""41dd20f07f3c1ee2bf5d2ab3ca4c77e21b88c73c"", ""last_modified"": ""2025-01-13 13:36:13+00:00"", ""created_at"": ""2025-01-13 13:17:30+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-17-00_dgx-a100-12/events.out.tfevents.1736774256.dgx-a100-12.1052592.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-17-00_dgx-a100-12/events.out.tfevents.1736775185.dgx-a100-12.1052592.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 13:36:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785126ad529f09b04e96399"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full"", ""usedStorage"": 13477378533}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7889

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""14d8011d09da88f707bb961c2a440e52ec5f6e93"", ""last_modified"": ""2025-01-13 13:58:23+00:00"", ""created_at"": ""2025-01-13 13:37:01+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-36-30_dgx-a100-12/events.out.tfevents.1736775426.dgx-a100-12.1068909.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-36-30_dgx-a100-12/events.out.tfevents.1736776575.dgx-a100-12.1068909.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 13:58:23+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678516fd609833baf1d6ac52"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full"", ""usedStorage"": 13477378537}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7183

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8038        | 0.3425 | 100  | 0.7690          |
| 0.6684        | 0.6849 | 200  | 0.7280          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""b2bfed6b4014675c2b47627c1423bc1a489d60e1"", ""last_modified"": ""2025-01-13 14:37:18+00:00"", ""created_at"": ""2025-01-13 13:59:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-58-40_dgx-a100-12/events.out.tfevents.1736776782.dgx-a100-12.1087633.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_14-58-40_dgx-a100-12/events.out.tfevents.1736778916.dgx-a100-12.1087633.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 14:37:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67851c4ae1226951bc505fb5"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full"", ""usedStorage"": 13477379499}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5898

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8108        | 0.3396 | 100  | 0.6223          |
| 0.6753        | 0.6791 | 200  | 0.5975          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""cbd43f51a1ce2c78038eabe43d85897defe4a1e6"", ""last_modified"": ""2025-01-13 15:20:29+00:00"", ""created_at"": ""2025-01-13 14:38:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_15-37-36_dgx-a100-12/events.out.tfevents.1736779121.dgx-a100-12.1120671.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_15-37-36_dgx-a100-12/events.out.tfevents.1736781507.dgx-a100-12.1120671.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 15:20:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785256d05efa4e2f4a8c382"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full"", ""usedStorage"": 13477379499}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6559

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8082        | 0.3361 | 100  | 0.6996          |
| 0.6681        | 0.6723 | 200  | 0.6644          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""2ce2aa505b3eabd6958d515b743e7c9ded4933b7"", ""last_modified"": ""2025-01-13 16:01:27+00:00"", ""created_at"": ""2025-01-13 15:21:47+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_16-20-49_dgx-a100-12/events.out.tfevents.1736781712.dgx-a100-12.1154534.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_16-20-49_dgx-a100-12/events.out.tfevents.1736783748.dgx-a100-12.1154534.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 16:01:27+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67852f8b7efe5e02a53d4de5"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_8B_reflect_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8223

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""4e826e722dfa07a043ef7ac0468be958c59db378"", ""last_modified"": ""2025-01-13 16:19:47+00:00"", ""created_at"": ""2025-01-13 16:02:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-01-46_dgx-a100-12/events.out.tfevents.1736784143.dgx-a100-12.1186656.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-01-46_dgx-a100-12/events.out.tfevents.1736785068.dgx-a100-12.1186656.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 16:19:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785390a9f4d958260c9cc8c"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full"", ""usedStorage"": 13477378537}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7615

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""3ffa3dcf2888cf0c003d3d8a3ed9843e8e13a7a5"", ""last_modified"": ""2025-01-13 16:38:32+00:00"", ""created_at"": ""2025-01-13 16:20:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-20-06_dgx-a100-12/events.out.tfevents.1736785243.dgx-a100-12.1201757.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-20-06_dgx-a100-12/events.out.tfevents.1736786183.dgx-a100-12.1201757.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 16:38:32+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67853d5728014d20f345f60a"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full"", ""usedStorage"": 13477378537}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_default_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_default_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_default_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 2.5724

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_default_100_full"", ""author"": ""CharlesLi"", ""sha"": ""f744352489761e7a69e7ea9a41694a88064c1a0b"", ""last_modified"": ""2025-01-13 16:49:54+00:00"", ""created_at"": ""2025-01-13 16:38:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_default_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-33-04_dgx-a100-13/events.out.tfevents.1736786328.dgx-a100-13.2652820.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-33-04_dgx-a100-13/events.out.tfevents.1736786868.dgx-a100-13.2652820.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 16:49:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67854193572d269825061f70"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_default_100_full"", ""usedStorage"": 13477378356}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_default_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_default_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_default_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6967

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""2adb59c875c9d87d37e0a0504ae47ed3de654662"", ""last_modified"": ""2025-01-13 16:57:24+00:00"", ""created_at"": ""2025-01-13 16:39:35+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-38-49_dgx-a100-12/events.out.tfevents.1736786387.dgx-a100-12.1216418.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-38-49_dgx-a100-12/events.out.tfevents.1736787310.dgx-a100-12.1216418.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 16:57:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678541c7aa0e72cad928fc72"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full"", ""usedStorage"": 13477378541}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_default_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_default_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_default_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.9297

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_default_500_full"", ""author"": ""CharlesLi"", ""sha"": ""58bede4f25d6414a7a70a11a7ba516ade39186d7"", ""last_modified"": ""2025-01-13 17:03:03+00:00"", ""created_at"": ""2025-01-13 16:50:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_default_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-50-12_dgx-a100-13/events.out.tfevents.1736787039.dgx-a100-13.2666935.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-50-12_dgx-a100-13/events.out.tfevents.1736787567.dgx-a100-13.2666935.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 17:03:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785445a3e93cbae2fdab8ce"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_default_500_full"", ""usedStorage"": 13477378409}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_default_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_default_500_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_default_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6551

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8001        | 0.3419 | 100  | 0.6975          |
| 0.6758        | 0.6838 | 200  | 0.6629          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""7a5708a20534592571435bba8dffde92e60f0eaf"", ""last_modified"": ""2025-01-13 17:41:56+00:00"", ""created_at"": ""2025-01-13 16:58:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-57-42_dgx-a100-12/events.out.tfevents.1736787525.dgx-a100-12.1231885.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_17-57-42_dgx-a100-12/events.out.tfevents.1736789801.dgx-a100-12.1231885.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 17:41:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678546414cd9f683b0e5f6d0"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_default_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_default_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_default_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.3804

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_default_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""a98899f1a944c29c7720bb994245d5437ddc69c3"", ""last_modified"": ""2025-01-13 17:21:19+00:00"", ""created_at"": ""2025-01-13 17:03:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_default_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-03-20_dgx-a100-13/events.out.tfevents.1736787826.dgx-a100-13.2679131.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-03-20_dgx-a100-13/events.out.tfevents.1736788409.dgx-a100-13.2679131.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 17:21:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_default_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785476e09a36c1b53aaf303"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_default_1000_full"", ""usedStorage"": 13477378413}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_default_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_default_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_default_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_reflect_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_reflect_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_reflect_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 2.0096

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_100_full"", ""author"": ""CharlesLi"", ""sha"": ""426ef4fa5be79af5765a859954c55d23ac52b160"", ""last_modified"": ""2025-01-13 17:35:34+00:00"", ""created_at"": ""2025-01-13 17:22:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_reflect_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-21-37_dgx-a100-13/events.out.tfevents.1736788925.dgx-a100-13.2693195.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-21-37_dgx-a100-13/events.out.tfevents.1736789620.dgx-a100-13.2693195.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 17:35:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67854bb845a575221db5238c"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_100_full"", ""usedStorage"": 13477378356}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_reflect_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_reflect_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_reflect_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_reflect_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2095

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_500_full"", ""author"": ""CharlesLi"", ""sha"": ""723ac6669464eea5f1853e1bdeddad3c64a66848"", ""last_modified"": ""2025-01-13 17:50:13+00:00"", ""created_at"": ""2025-01-13 17:36:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_reflect_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-35-52_dgx-a100-13/events.out.tfevents.1736789778.dgx-a100-13.2704872.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-35-52_dgx-a100-13/events.out.tfevents.1736790352.dgx-a100-13.2704872.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 17:50:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67854f0ddb9a0895b1348b29"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_500_full"", ""usedStorage"": 13477378409}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_reflect_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_500_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5942

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8104        | 0.3401 | 100  | 0.6373          |
| 0.6702        | 0.6803 | 200  | 0.6017          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""9c72b9a1835eede17b8e997c8908b54edd17f960"", ""last_modified"": ""2025-01-13 18:23:56+00:00"", ""created_at"": ""2025-01-13 17:43:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-42-15_dgx-a100-12/events.out.tfevents.1736790198.dgx-a100-12.1266814.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-42-15_dgx-a100-12/events.out.tfevents.1736792323.dgx-a100-12.1266814.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 18:23:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678550b1d529f09b04fd3422"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_4o_reflect_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_4o_reflect_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_4o_reflect_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0361

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""4ee2af4c9a6659880f4e6d9db5560d582ffc607b"", ""last_modified"": ""2025-01-13 18:01:46+00:00"", ""created_at"": ""2025-01-13 17:50:53+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_4o_reflect_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-50-30_dgx-a100-13/events.out.tfevents.1736790658.dgx-a100-13.2716703.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_18-50-30_dgx-a100-13/events.out.tfevents.1736791161.dgx-a100-13.2716703.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 18:01:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_4o_reflect_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785527db6c7739f73d2c30f"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_4o_reflect_1000_full"", ""usedStorage"": 13477378413}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_4o_reflect_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_4o_reflect_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_8B_default_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_8B_default_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 2.1777

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_100_full"", ""author"": ""CharlesLi"", ""sha"": ""aef02d4bf2a1eeb1965e3e17a1db6f7ddbb126ac"", ""last_modified"": ""2025-01-13 18:14:19+00:00"", ""created_at"": ""2025-01-13 18:02:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_default_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_8B_default_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-02-02_dgx-a100-13/events.out.tfevents.1736791350.dgx-a100-13.2726381.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-02-02_dgx-a100-13/events.out.tfevents.1736791926.dgx-a100-13.2726381.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 18:14:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_default_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785553039e86178e6dda11a"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_100_full"", ""usedStorage"": 13477378388}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_default_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_default_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6372

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8125        | 0.3373 | 100  | 0.6780          |
| 0.674         | 0.6745 | 200  | 0.6461          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""59dac480f475bafcc93168956d62c64315312851"", ""last_modified"": ""2025-01-13 19:07:19+00:00"", ""created_at"": ""2025-01-13 18:25:17+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-24-15_dgx-a100-12/events.out.tfevents.1736792732.dgx-a100-12.1300324.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-24-15_dgx-a100-12/events.out.tfevents.1736794970.dgx-a100-12.1300324.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:07:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67855a8dc732d5b62f35ec9f"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full"", ""usedStorage"": 13477379507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_default_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_8B_default_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_8B_default_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9608

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""ebb1dc57b66e0818cdf96b94feac126e2b2f0db9"", ""last_modified"": ""2025-01-13 18:43:28+00:00"", ""created_at"": ""2025-01-13 18:32:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_default_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_8B_default_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-32-27_dgx-a100-13/events.out.tfevents.1736793173.dgx-a100-13.2753773.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-32-27_dgx-a100-13/events.out.tfevents.1736793692.dgx-a100-13.2753773.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 18:43:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_default_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67855c50d529f09b0400cd23"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_1000_full"", ""usedStorage"": 13477378445}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_8B_default_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_default_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_default_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_8B_reflect_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_8B_reflect_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.6293

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_100_full"", ""author"": ""CharlesLi"", ""sha"": ""72e3ef49048b17ce4e4c3f8546ae5c4a0c433bed"", ""last_modified"": ""2025-01-13 18:52:07+00:00"", ""created_at"": ""2025-01-13 18:44:07+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_8B_reflect_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-43-45_dgx-a100-13/events.out.tfevents.1736793853.dgx-a100-13.2764851.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-43-45_dgx-a100-13/events.out.tfevents.1736794211.dgx-a100-13.2764851.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 18:52:07+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67855ef7477f3dbe2ba8a28e"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_100_full"", ""usedStorage"": 13477378388}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_8B_reflect_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_8B_reflect_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8959

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_500_full"", ""author"": ""CharlesLi"", ""sha"": ""d9f1dfeba22c00602b846fb3c7ed1a3480d647f4"", ""last_modified"": ""2025-01-13 19:01:11+00:00"", ""created_at"": ""2025-01-13 18:52:44+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_8B_reflect_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-52-24_dgx-a100-13/events.out.tfevents.1736794369.dgx-a100-13.2775911.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_19-52-24_dgx-a100-13/events.out.tfevents.1736794756.dgx-a100-13.2775911.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:01:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678560fc9d6c198fe867d8b0"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_500_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_500_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_8B_reflect_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_8B_reflect_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7626

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""c9372d6c8271d7eabeebb784b871d4aa4ad8e80b"", ""last_modified"": ""2025-01-13 19:12:55+00:00"", ""created_at"": ""2025-01-13 19:01:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_8B_reflect_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-01-28_dgx-a100-13/events.out.tfevents.1736794916.dgx-a100-13.2784999.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-01-28_dgx-a100-13/events.out.tfevents.1736795463.dgx-a100-13.2784999.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:12:55+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_8B_reflect_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785631f2b4c41010fefa5de"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_1000_full"", ""usedStorage"": 13477378445}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_8B_reflect_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_8B_reflect_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7303

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""26aee4b10871072f6fa6c8fcd0142f2f1cc69fea"", ""last_modified"": ""2025-01-13 19:23:14+00:00"", ""created_at"": ""2025-01-13 19:08:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-07-39_dgx-a100-12/events.out.tfevents.1736795298.dgx-a100-12.1333673.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-07-39_dgx-a100-12/events.out.tfevents.1736796080.dgx-a100-12.1333673.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:23:14+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785649d2c9c42ed5acf33be"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full"", ""usedStorage"": 13477378537}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_1000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_default_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_default_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.6289

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_100_full"", ""author"": ""CharlesLi"", ""sha"": ""42512ae53408825e05ccfa5bf5e35890374cd279"", ""last_modified"": ""2025-01-13 19:21:47+00:00"", ""created_at"": ""2025-01-13 19:13:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_default_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-13-12_dgx-a100-13/events.out.tfevents.1736795618.dgx-a100-13.2796373.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-13-12_dgx-a100-13/events.out.tfevents.1736795994.dgx-a100-13.2796373.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:21:47+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678565dd1f861473a27e97c9"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_100_full"", ""usedStorage"": 13477378392}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_default_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_default_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9015

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_500_full"", ""author"": ""CharlesLi"", ""sha"": ""601c312cf468b6e26ca83775d79bd0055f24824b"", ""last_modified"": ""2025-01-13 19:33:25+00:00"", ""created_at"": ""2025-01-13 19:22:26+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_default_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-22-05_dgx-a100-13/events.out.tfevents.1736796151.dgx-a100-13.2805126.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-22-05_dgx-a100-13/events.out.tfevents.1736796651.dgx-a100-13.2805126.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:33:25+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678567f2c732d5b62f399ae6"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_500_full"", ""usedStorage"": 13477378445}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_500_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_default_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_default_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8687

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""cfc7224d4e81eaea90a1b79ea4168b1b6ce7734f"", ""last_modified"": ""2025-01-13 19:46:25+00:00"", ""created_at"": ""2025-01-13 19:34:05+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_default_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-33-42_dgx-a100-13/events.out.tfevents.1736796850.dgx-a100-13.2819466.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-33-42_dgx-a100-13/events.out.tfevents.1736797475.dgx-a100-13.2819466.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:46:25+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_default_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67856aadd8cd7d5bc3c18741"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_1000_full"", ""usedStorage"": 13477378449}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_default_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_default_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8677

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""e59ea442f38a013758bb55e66fa991f4befeb5b0"", ""last_modified"": ""2025-01-13 19:59:18+00:00"", ""created_at"": ""2025-01-13 19:42:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-42-05_dgx-a100-12/events.out.tfevents.1736797364.dgx-a100-12.1362169.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-42-05_dgx-a100-12/events.out.tfevents.1736798145.dgx-a100-12.1362169.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:59:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67856cafd84043852cb8a55f"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full"", ""usedStorage"": 13477378541}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_1000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_reflect_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_reflect_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.4599

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_100_full"", ""author"": ""CharlesLi"", ""sha"": ""186bf61702499624fef68be4b4d3d56455df76b2"", ""last_modified"": ""2025-01-13 19:55:10+00:00"", ""created_at"": ""2025-01-13 19:47:03+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_reflect_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-46-42_dgx-a100-13/events.out.tfevents.1736797628.dgx-a100-13.2830551.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-46-42_dgx-a100-13/events.out.tfevents.1736797997.dgx-a100-13.2830551.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 19:55:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67856db7cd2391fb740ee5e0"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_100_full"", ""usedStorage"": 13477378392}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_100_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_reflect_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_reflect_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7526

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_500_full"", ""author"": ""CharlesLi"", ""sha"": ""1094c7dfe3ec68ea57500de29cd360d6113ebd34"", ""last_modified"": ""2025-01-13 20:05:54+00:00"", ""created_at"": ""2025-01-13 19:55:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_reflect_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-55-29_dgx-a100-13/events.out.tfevents.1736798156.dgx-a100-13.2838005.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-55-29_dgx-a100-13/events.out.tfevents.1736798641.dgx-a100-13.2838005.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 20:05:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67856fc7b6c7739f73dbe7b2"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_500_full"", ""usedStorage"": 13477378445}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_500_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6853

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8071        | 0.3419 | 100  | 0.7289          |
| 0.6685        | 0.6838 | 200  | 0.6944          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""d654e66d21127d60be7795098715991da86cd65e"", ""last_modified"": ""2025-01-13 20:36:24+00:00"", ""created_at"": ""2025-01-13 20:00:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-59-38_dgx-a100-12/events.out.tfevents.1736798442.dgx-a100-12.1376513.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_20-59-38_dgx-a100-12/events.out.tfevents.1736800457.dgx-a100-12.1376513.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 20:36:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678570e5477f3dbe2bae1f96"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_rlhf_safe_llama_3_70B_reflect_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_rlhf_safe_llama_3_70B_reflect_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6304

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""36ca0ab0123d4ea233bc5e7ef9ca5aa3d7885afd"", ""last_modified"": ""2025-01-13 20:15:29+00:00"", ""created_at"": ""2025-01-13 20:06:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_rlhf_safe_llama_3_70B_reflect_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_21-06-11_dgx-a100-13/events.out.tfevents.1736798798.dgx-a100-13.2848685.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_21-06-11_dgx-a100-13/events.out.tfevents.1736799217.dgx-a100-13.2848685.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 20:15:29+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_rlhf_safe_llama_3_70B_reflect_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67857249395483364291982f"", ""modelId"": ""CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_1000_full"", ""usedStorage"": 13477378449}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_rlhf_safe_llama_3_70B_reflect_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_1000_full%5D(%2FCharlesLi%2Fllama_2_rlhf_safe_llama_3_70B_reflect_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6591

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8117        | 0.3390 | 100  | 0.7151          |
| 0.6685        | 0.6780 | 200  | 0.6687          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full"", ""author"": ""CharlesLi"", ""sha"": ""fdbda17f49509bca7ccc036d9a856e0ae3ab0826"", ""last_modified"": ""2025-01-13 21:15:18+00:00"", ""created_at"": ""2025-01-13 20:37:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_21-36-44_dgx-a100-12/events.out.tfevents.1736800668.dgx-a100-12.1408562.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_21-36-44_dgx-a100-12/events.out.tfevents.1736802676.dgx-a100-12.1408562.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 21:15:18+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678579977561fac156b2e596"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full"", ""usedStorage"": 13477379503}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_500_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7477

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8027        | 0.3361 | 100  | 0.7989          |
| 0.6659        | 0.6723 | 200  | 0.7594          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""902a2b317b901776f818e079cc6b212946818cd8"", ""last_modified"": ""2025-01-13 21:53:02+00:00"", ""created_at"": ""2025-01-13 21:16:36+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_22-15-39_dgx-a100-12/events.out.tfevents.1736803001.dgx-a100-12.1439532.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan13_22-15-39_dgx-a100-12/events.out.tfevents.1736805067.dgx-a100-12.1439532.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-13 21:53:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678582b4f5b24159e9792d22"", ""modelId"": ""CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full"", ""usedStorage"": 13477379507}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full%5D(%2FCharlesLi%2Fllama_2_sky_safe_o1_llama_3_70B_reflect_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
RizkyAnanda/finetuned-llama-2-7b-chat,"---
library_name: transformers
license: llama2
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""RizkyAnanda/finetuned-llama-2-7b-chat"", ""author"": ""RizkyAnanda"", ""sha"": ""df85d40ba8e1183513bd63160b44c9d8c5c25682"", ""last_modified"": ""2025-01-14 02:39:53+00:00"", ""created_at"": ""2025-01-14 02:22:51+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""conversational"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-14 02:39:53+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6785ca7b806abc109d4e6586"", ""modelId"": ""RizkyAnanda/finetuned-llama-2-7b-chat"", ""usedStorage"": 26954195819}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=RizkyAnanda/finetuned-llama-2-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BRizkyAnanda%2Ffinetuned-llama-2-7b-chat%5D(%2FRizkyAnanda%2Ffinetuned-llama-2-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
rachmanino/Llama-2-7B-chat-Trump-v1,"---
license: mit
datasets:
- pookie3000/trump-interviews
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: question-answering
---","{""id"": ""rachmanino/Llama-2-7B-chat-Trump-v1"", ""author"": ""rachmanino"", ""sha"": ""bdbe0333a23c1692b2679e56dd1d1bd3f918fc45"", ""last_modified"": ""2025-01-17 09:50:20+00:00"", ""created_at"": ""2025-01-17 04:52:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""question-answering"", ""en"", ""dataset:pookie3000/trump-interviews"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": ""question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- pookie3000/trump-interviews\nlanguage:\n- en\nlicense: mit\npipeline_tag: question-answering"", ""widget_data"": [{""text"": ""Where do I live?"", ""context"": ""My name is Wolfgang and I live in Berlin""}, {""text"": ""Where do I live?"", ""context"": ""My name is Sarah and I live in London""}, {""text"": ""What's my name?"", ""context"": ""My name is Clara and I live in Berkeley.""}, {""text"": ""Which name is also used to describe the Amazon rainforest in English?"", ""context"": ""The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \""Amazonas\"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-17 09:50:20+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- pookie3000/trump-interviews\nlanguage:\n- en\nlicense: mit\npipeline_tag: question-answering"", ""transformersInfo"": null, ""_id"": ""6789e1f0f6220901c865b315"", ""modelId"": ""rachmanino/Llama-2-7B-chat-Trump-v1"", ""usedStorage"": 13477364475}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=rachmanino/Llama-2-7B-chat-Trump-v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brachmanino%2FLlama-2-7B-chat-Trump-v1%5D(%2Frachmanino%2FLlama-2-7B-chat-Trump-v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
rathodj080898/Llama-2-7b-chat-finetune,"---
license: apache-2.0
datasets:
- timdettmers/openassistant-guanaco
metrics:
- bleu
- accuracy
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: table-question-answering
size: 9.98GB
---
# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

This modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->



- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""rathodj080898/Llama-2-7b-chat-finetune"", ""author"": ""rathodj080898"", ""sha"": ""86cfe291e75b17c07ed8a41c5388712a311e7691"", ""last_modified"": ""2025-01-26 13:19:57+00:00"", ""created_at"": ""2025-01-17 10:40:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""pytorch"", ""llama"", ""table-question-answering"", ""dataset:timdettmers/openassistant-guanaco"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": ""table-question-answering"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- timdettmers/openassistant-guanaco\nlicense: apache-2.0\nmetrics:\n- bleu\n- accuracy\npipeline_tag: table-question-answering\nsize: 9.98GB"", ""widget_data"": [{""text"": ""How many stars does the transformers repository have?"", ""table"": {""Repository"": [""Transformers"", ""Datasets"", ""Tokenizers""], ""Stars"": [36542, 4512, 3934], ""Contributors"": [651, 77, 34], ""Programming language"": [""Python"", ""Python"", ""Rust, Python and NodeJS""]}}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""</s>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<unk>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-01-26 13:19:57+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- timdettmers/openassistant-guanaco\nlicense: apache-2.0\nmetrics:\n- bleu\n- accuracy\npipeline_tag: table-question-answering\nsize: 9.98GB"", ""transformersInfo"": null, ""_id"": ""678a3380e38d2bed893ac0f1"", ""modelId"": ""rathodj080898/Llama-2-7b-chat-finetune"", ""usedStorage"": 26954331470}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=rathodj080898/Llama-2-7b-chat-finetune&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brathodj080898%2FLlama-2-7b-chat-finetune%5D(%2Frathodj080898%2FLlama-2-7b-chat-finetune)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_0_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_0_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_0_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8369

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_0_full"", ""author"": ""CharlesLi"", ""sha"": ""50397754b0c4cebd1a9689c4ef9b3bf5260826b6"", ""last_modified"": ""2025-01-20 13:48:56+00:00"", ""created_at"": ""2025-01-19 21:39:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_0_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_0_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-33-28_dgx-a100-14/events.out.tfevents.1737322766.dgx-a100-14.499270.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-33-28_dgx-a100-14/events.out.tfevents.1737323297.dgx-a100-14.499270.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-24-43_dgx-a100-13/events.out.tfevents.1737333022.dgx-a100-13.1035717.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-24-43_dgx-a100-13/events.out.tfevents.1737333423.dgx-a100-13.1035717.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-21-02_dgx-a100-12/events.out.tfevents.1737354087.dgx-a100-12.1083453.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-21-02_dgx-a100-12/events.out.tfevents.1737354466.dgx-a100-12.1083453.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-40-39_dgx-a100-16/events.out.tfevents.1737380464.dgx-a100-16.3117027.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-40-39_dgx-a100-16/events.out.tfevents.1737380823.dgx-a100-16.3117027.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:48:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_0_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d7109481c85a8a9571306"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_0_full"", ""usedStorage"": 53908014387}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_0_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_0_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_0_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8356

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_1_full"", ""author"": ""CharlesLi"", ""sha"": ""8dd40acb9d5c732f4b4de1713fa410cf9cf3611d"", ""last_modified"": ""2025-01-20 13:57:53+00:00"", ""created_at"": ""2025-01-19 21:50:48+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-50-26_dgx-a100-14/events.out.tfevents.1737323454.dgx-a100-14.512743.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-50-26_dgx-a100-14/events.out.tfevents.1737323826.dgx-a100-14.512743.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-39-20_dgx-a100-13/events.out.tfevents.1737333584.dgx-a100-13.1048645.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-39-20_dgx-a100-13/events.out.tfevents.1737333976.dgx-a100-13.1048645.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-29-52_dgx-a100-12/events.out.tfevents.1737354615.dgx-a100-12.1090407.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-29-52_dgx-a100-12/events.out.tfevents.1737355008.dgx-a100-12.1090407.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-49-13_dgx-a100-16/events.out.tfevents.1737380977.dgx-a100-16.3124001.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-49-13_dgx-a100-16/events.out.tfevents.1737381360.dgx-a100-16.3124001.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:57:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d73b845bd8acb24ff9d4e"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_1_full"", ""usedStorage"": 53908014387}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_1_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_2_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_2_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_2_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6197

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_2_full"", ""author"": ""CharlesLi"", ""sha"": ""ea06c379e8b1466511f48bf5781dd30bbd54528c"", ""last_modified"": ""2025-01-20 14:07:21+00:00"", ""created_at"": ""2025-01-19 21:59:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_2_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_2_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-59-18_dgx-a100-14/events.out.tfevents.1737323986.dgx-a100-14.520574.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_22-59-18_dgx-a100-14/events.out.tfevents.1737324397.dgx-a100-14.520574.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-48-28_dgx-a100-13/events.out.tfevents.1737334131.dgx-a100-13.1056545.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-48-28_dgx-a100-13/events.out.tfevents.1737334548.dgx-a100-13.1056545.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-38-55_dgx-a100-12/events.out.tfevents.1737355159.dgx-a100-12.1097821.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-38-55_dgx-a100-12/events.out.tfevents.1737355565.dgx-a100-12.1097821.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-58-10_dgx-a100-16/events.out.tfevents.1737381515.dgx-a100-16.3131827.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-58-10_dgx-a100-16/events.out.tfevents.1737381928.dgx-a100-16.3131827.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 14:07:21+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_2_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d75cdc74dea831b371064"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_2_full"", ""usedStorage"": 53908014387}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_2_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_2_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_2_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_3_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_3_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_3_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5628

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_3_full"", ""author"": ""CharlesLi"", ""sha"": ""02cbc1ffb422217232b6300abc691082345c3137"", ""last_modified"": ""2025-01-20 14:18:03+00:00"", ""created_at"": ""2025-01-19 22:11:12+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_3_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_3_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-09-45_dgx-a100-14/events.out.tfevents.1737324677.dgx-a100-14.529522.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-09-45_dgx-a100-14/events.out.tfevents.1737325379.dgx-a100-14.529522.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-58-04_dgx-a100-13/events.out.tfevents.1737334709.dgx-a100-13.1064354.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_01-58-04_dgx-a100-13/events.out.tfevents.1737335288.dgx-a100-13.1064354.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-48-11_dgx-a100-12/events.out.tfevents.1737355715.dgx-a100-12.1105223.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-48-11_dgx-a100-12/events.out.tfevents.1737356180.dgx-a100-12.1105223.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-07-38_dgx-a100-16/events.out.tfevents.1737382083.dgx-a100-16.3139806.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-07-38_dgx-a100-16/events.out.tfevents.1737382568.dgx-a100-16.3139806.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 14:18:03+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_3_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d788090dfd20e4d8a8a03"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_3_full"", ""usedStorage"": 53908014387}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_3_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_3_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_3_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_4_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_4_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_4_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6615

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_4_full"", ""author"": ""CharlesLi"", ""sha"": ""a0755e4aa5cbedb4133cd5649fcd927bd0c8e103"", ""last_modified"": ""2025-01-20 14:31:01+00:00"", ""created_at"": ""2025-01-19 22:25:32+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_4_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_4_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-25-03_dgx-a100-14/events.out.tfevents.1737325537.dgx-a100-14.541675.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-25-03_dgx-a100-14/events.out.tfevents.1737326134.dgx-a100-14.541675.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_02-10-22_dgx-a100-13/events.out.tfevents.1737335446.dgx-a100-13.1074915.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_02-10-22_dgx-a100-13/events.out.tfevents.1737336060.dgx-a100-13.1074915.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-58-37_dgx-a100-12/events.out.tfevents.1737356341.dgx-a100-12.1113634.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-58-37_dgx-a100-12/events.out.tfevents.1737356930.dgx-a100-12.1113634.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-18-19_dgx-a100-16/events.out.tfevents.1737382724.dgx-a100-16.3148419.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-18-19_dgx-a100-16/events.out.tfevents.1737383336.dgx-a100-16.3148419.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 14:31:01+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_4_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d7bdc09ce2030b01882ee"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_4_full"", ""usedStorage"": 53908014387}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_4_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_4_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_4_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_code_math_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_code_math_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_code_math_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5808

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.6223        | 0.8969 | 100  | 0.5814          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_code_math_5_full"", ""author"": ""CharlesLi"", ""sha"": ""857433cbca9737050c172d953274d8ef05c4f379"", ""last_modified"": ""2025-01-20 14:49:33+00:00"", ""created_at"": ""2025-01-19 22:38:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_code_math_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-37-39_dgx-a100-14/events.out.tfevents.1737326301.dgx-a100-14.551972.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan19_23-37-39_dgx-a100-14/events.out.tfevents.1737327242.dgx-a100-14.551972.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_02-23-09_dgx-a100-13/events.out.tfevents.1737336214.dgx-a100-13.1085210.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_02-23-09_dgx-a100-13/events.out.tfevents.1737337175.dgx-a100-13.1085210.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_08-10-54_dgx-a100-12/events.out.tfevents.1737357078.dgx-a100-12.1123784.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_08-10-54_dgx-a100-12/events.out.tfevents.1737358030.dgx-a100-12.1123784.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-31-19_dgx-a100-16/events.out.tfevents.1737383504.dgx-a100-16.3159106.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_15-31-19_dgx-a100-16/events.out.tfevents.1737384436.dgx-a100-16.3159106.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 14:49:33+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_code_math_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678d7ed7e9acb2d7aa4ed616"", ""modelId"": ""CharlesLi/llama_2_llama_2_code_math_5_full"", ""usedStorage"": 53908016279}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_code_math_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_code_math_5_full%5D(%2FCharlesLi%2Fllama_2_llama_2_code_math_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_0_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_0_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_0_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1711

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_0_full"", ""author"": ""CharlesLi"", ""sha"": ""4e2352f20669f37a9e1a4117f091ad4833be3e70"", ""last_modified"": ""2025-01-20 10:29:53+00:00"", ""created_at"": ""2025-01-20 02:57:57+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_0_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_0_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_03-52-52_dgx-a100-12/events.out.tfevents.1737341882.dgx-a100-12.905346.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_03-52-52_dgx-a100-12/events.out.tfevents.1737342246.dgx-a100-12.905346.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-14-09_dgx-a100-16/events.out.tfevents.1737368526.dgx-a100-16.2942567.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-14-09_dgx-a100-16/events.out.tfevents.1737368885.dgx-a100-16.2942567.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 10:29:53+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_0_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dbbb581ba0ef5a0e5c6b6"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_0_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_0_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_0_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_0_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2264

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_1_full"", ""author"": ""CharlesLi"", ""sha"": ""231b8d9eee354aa225498491e28916b14bced227"", ""last_modified"": ""2025-01-20 10:38:37+00:00"", ""created_at"": ""2025-01-20 03:06:33+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-06-10_dgx-a100-12/events.out.tfevents.1737342398.dgx-a100-12.916425.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-06-10_dgx-a100-12/events.out.tfevents.1737342777.dgx-a100-12.916425.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-30-10_dgx-a100-16/events.out.tfevents.1737369035.dgx-a100-16.2955556.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-30-10_dgx-a100-16/events.out.tfevents.1737369410.dgx-a100-16.2955556.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 10:38:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dbdb91c0169c0bc6e138b"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_1_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_1_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_2_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_2_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_2_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9462

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_2_full"", ""author"": ""CharlesLi"", ""sha"": ""1d94396e9cf73f9aa00a01c7ec535ef40af8af8b"", ""last_modified"": ""2025-01-20 10:47:56+00:00"", ""created_at"": ""2025-01-20 03:15:23+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_2_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_2_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-15-01_dgx-a100-12/events.out.tfevents.1737342929.dgx-a100-12.923864.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-15-01_dgx-a100-12/events.out.tfevents.1737343352.dgx-a100-12.923864.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-38-58_dgx-a100-16/events.out.tfevents.1737369570.dgx-a100-16.2963957.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-38-58_dgx-a100-16/events.out.tfevents.1737369968.dgx-a100-16.2963957.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 10:47:56+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_2_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dbfcb94e4992bb9e70ee9"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_2_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_2_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_2_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_2_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_3_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_3_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_3_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9216

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_3_full"", ""author"": ""CharlesLi"", ""sha"": ""fe9f76c3968edd7e339687147cc6bbc2fb660a57"", ""last_modified"": ""2025-01-20 11:00:41+00:00"", ""created_at"": ""2025-01-20 03:24:59+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_3_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_3_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-24-35_dgx-a100-12/events.out.tfevents.1737343504.dgx-a100-12.932254.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-24-35_dgx-a100-12/events.out.tfevents.1737343962.dgx-a100-12.932254.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-48-13_dgx-a100-16/events.out.tfevents.1737370119.dgx-a100-16.2972263.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_11-48-13_dgx-a100-16/events.out.tfevents.1737370732.dgx-a100-16.2972263.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:00:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_3_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dc20b148f7a067d9cf6ef"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_3_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_3_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_3_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_3_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_4_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_4_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_4_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9264

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_4_full"", ""author"": ""CharlesLi"", ""sha"": ""5dc7c914965fd74bdddf3402add338709917610f"", ""last_modified"": ""2025-01-20 11:12:45+00:00"", ""created_at"": ""2025-01-20 03:35:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_4_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_4_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-34-46_dgx-a100-12/events.out.tfevents.1737344119.dgx-a100-12.940575.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-34-46_dgx-a100-12/events.out.tfevents.1737344687.dgx-a100-12.940575.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-00-59_dgx-a100-16/events.out.tfevents.1737370883.dgx-a100-16.2983664.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-00-59_dgx-a100-16/events.out.tfevents.1737371457.dgx-a100-16.2983664.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:12:45+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_4_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dc473d898663b31a77bab"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_4_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_4_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_4_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_4_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7572

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_5_full"", ""author"": ""CharlesLi"", ""sha"": ""a3b5a55d875525d458f179552d86997db2be1689"", ""last_modified"": ""2025-01-20 11:28:31+00:00"", ""created_at"": ""2025-01-20 03:47:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-46-54_dgx-a100-12/events.out.tfevents.1737344853.dgx-a100-12.951987.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_04-46-54_dgx-a100-12/events.out.tfevents.1737345640.dgx-a100-12.951987.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-13-07_dgx-a100-16/events.out.tfevents.1737371612.dgx-a100-16.2993897.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-13-07_dgx-a100-16/events.out.tfevents.1737372402.dgx-a100-16.2993897.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:28:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dc74fae97bf4ca7454e1b"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_5_full"", ""usedStorage"": 26954257071}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_5_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_0_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_0_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_0_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8119

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_0_full"", ""author"": ""CharlesLi"", ""sha"": ""9baf180b0ee72850b1d01e90ae030a13246731e8"", ""last_modified"": ""2025-01-20 11:37:00+00:00"", ""created_at"": ""2025-01-20 04:03:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_0_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_0_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-02-47_dgx-a100-12/events.out.tfevents.1737345793.dgx-a100-12.965119.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-02-47_dgx-a100-12/events.out.tfevents.1737346215.dgx-a100-12.965119.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-28-48_dgx-a100-16/events.out.tfevents.1737372552.dgx-a100-16.3006990.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-28-48_dgx-a100-16/events.out.tfevents.1737372911.dgx-a100-16.3006990.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:37:00+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_0_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dcafdd4a7a158a8e8429d"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_0_full"", ""usedStorage"": 26954257095}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_0_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_0_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_0_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7902

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_1_full"", ""author"": ""CharlesLi"", ""sha"": ""5e6527916913740c5efcda62d4df4cb7e2de8930"", ""last_modified"": ""2025-01-20 11:45:44+00:00"", ""created_at"": ""2025-01-20 04:12:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-12-19_dgx-a100-12/events.out.tfevents.1737346366.dgx-a100-12.973407.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-12-19_dgx-a100-12/events.out.tfevents.1737346800.dgx-a100-12.973407.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-37-18_dgx-a100-16/events.out.tfevents.1737373062.dgx-a100-16.3013885.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-37-18_dgx-a100-16/events.out.tfevents.1737373434.dgx-a100-16.3013885.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:45:44+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dcd39e135e30253d77015"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_1_full"", ""usedStorage"": 26954257095}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_1_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_2_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_2_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_2_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5958

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_2_full"", ""author"": ""CharlesLi"", ""sha"": ""5cbe9e84bb2fb506b5628af1cd5124ab5ad5c700"", ""last_modified"": ""2025-01-20 11:54:46+00:00"", ""created_at"": ""2025-01-20 04:22:39+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_2_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_2_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-22-16_dgx-a100-12/events.out.tfevents.1737346965.dgx-a100-12.982045.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-22-16_dgx-a100-12/events.out.tfevents.1737347425.dgx-a100-12.982045.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-46-00_dgx-a100-16/events.out.tfevents.1737373585.dgx-a100-16.3020951.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-46-00_dgx-a100-16/events.out.tfevents.1737373980.dgx-a100-16.3020951.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 11:54:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_2_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dcf8f1c0169c0bc73b938"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_2_full"", ""usedStorage"": 26954257095}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_2_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_2_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_2_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_3_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_3_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_3_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5078

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_3_full"", ""author"": ""CharlesLi"", ""sha"": ""46baf2555b671515b679310ab3db26d78b79b878"", ""last_modified"": ""2025-01-20 12:05:15+00:00"", ""created_at"": ""2025-01-20 04:32:53+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_3_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_3_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-32-29_dgx-a100-12/events.out.tfevents.1737347579.dgx-a100-12.990550.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-32-29_dgx-a100-12/events.out.tfevents.1737348064.dgx-a100-12.990550.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-55-03_dgx-a100-16/events.out.tfevents.1737374128.dgx-a100-16.3029212.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_12-55-03_dgx-a100-16/events.out.tfevents.1737374606.dgx-a100-16.3029212.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 12:05:15+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_3_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dd1f57c5598fd170bea5c"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_3_full"", ""usedStorage"": 26954257095}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_3_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_3_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_3_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_4_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_4_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_4_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6062

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_4_full"", ""author"": ""CharlesLi"", ""sha"": ""f730d58cb13c95b8a5a71ab1673188fdd1ec3570"", ""last_modified"": ""2025-01-20 12:17:57+00:00"", ""created_at"": ""2025-01-20 04:43:36+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_4_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_4_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-43-09_dgx-a100-12/events.out.tfevents.1737348220.dgx-a100-12.999642.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-43-09_dgx-a100-12/events.out.tfevents.1737348834.dgx-a100-12.999642.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-05-32_dgx-a100-16/events.out.tfevents.1737374757.dgx-a100-16.3037652.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-05-32_dgx-a100-16/events.out.tfevents.1737375370.dgx-a100-16.3037652.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 12:17:57+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_4_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dd4785990171c80009362"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_4_full"", ""usedStorage"": 26954257095}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_4_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_4_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_4_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5425

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.6119        | 0.8333 | 100  | 0.5445          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_5_full"", ""author"": ""CharlesLi"", ""sha"": ""3f74fc243e82c3b7a5032f6a2c7dcac8095d6793"", ""last_modified"": ""2025-01-20 12:37:02+00:00"", ""created_at"": ""2025-01-20 04:56:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-55-59_dgx-a100-12/events.out.tfevents.1737349002.dgx-a100-12.1010682.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_05-55-59_dgx-a100-12/events.out.tfevents.1737350011.dgx-a100-12.1010682.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-18-14_dgx-a100-16/events.out.tfevents.1737375519.dgx-a100-16.3048174.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-18-14_dgx-a100-16/events.out.tfevents.1737376509.dgx-a100-16.3048174.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 12:37:02+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dd7859413f089e8c7455d"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_5_full"", ""usedStorage"": 26954258041}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_5_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_0_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_0_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_0_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.1953

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_0_full"", ""author"": ""CharlesLi"", ""sha"": ""4c31436aa9c76d251005555df3834dd761b6c426"", ""last_modified"": ""2025-01-20 12:45:35+00:00"", ""created_at"": ""2025-01-20 05:16:01+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_0_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_0_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-15-40_dgx-a100-12/events.out.tfevents.1737350166.dgx-a100-12.1027084.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-15-40_dgx-a100-12/events.out.tfevents.1737350540.dgx-a100-12.1027084.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-37-18_dgx-a100-16/events.out.tfevents.1737376663.dgx-a100-16.3065238.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-37-18_dgx-a100-16/events.out.tfevents.1737377023.dgx-a100-16.3065238.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 12:45:35+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_0_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678ddc11cc7d2ac663eadf03"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_0_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_0_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_0_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_0_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.3132

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_1_full"", ""author"": ""CharlesLi"", ""sha"": ""fece7a6e7f46cbe78e927af6464c0f78ab6cb139"", ""last_modified"": ""2025-01-20 12:54:10+00:00"", ""created_at"": ""2025-01-20 05:25:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-24-38_dgx-a100-12/events.out.tfevents.1737350705.dgx-a100-12.1034790.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-24-38_dgx-a100-12/events.out.tfevents.1737351090.dgx-a100-12.1034790.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-45-51_dgx-a100-16/events.out.tfevents.1737377176.dgx-a100-16.3072236.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-45-51_dgx-a100-16/events.out.tfevents.1737377541.dgx-a100-16.3072236.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 12:54:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678dde2c8b0ce22ae8964bb9"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_1_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_1_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_2_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_2_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_2_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9404

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_2_full"", ""author"": ""CharlesLi"", ""sha"": ""65638010d13d30e8ff0b7a3f8bc3027a88b0b28c"", ""last_modified"": ""2025-01-20 13:03:19+00:00"", ""created_at"": ""2025-01-20 05:34:00+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_2_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_2_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-33-37_dgx-a100-12/events.out.tfevents.1737351246.dgx-a100-12.1042225.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-33-37_dgx-a100-12/events.out.tfevents.1737351663.dgx-a100-12.1042225.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-54-27_dgx-a100-16/events.out.tfevents.1737377692.dgx-a100-16.3079468.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_13-54-27_dgx-a100-16/events.out.tfevents.1737378086.dgx-a100-16.3079468.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:03:19+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_2_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678de048736603ddc8f577cb"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_2_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_2_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_2_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_2_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_3_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_3_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_3_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9706

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_3_full"", ""author"": ""CharlesLi"", ""sha"": ""f064d89ddb8c50a7e577454e5a62ddc2a2db581d"", ""last_modified"": ""2025-01-20 13:13:28+00:00"", ""created_at"": ""2025-01-20 05:43:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_3_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_3_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-43-10_dgx-a100-12/events.out.tfevents.1737351819.dgx-a100-12.1050624.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-43-10_dgx-a100-12/events.out.tfevents.1737352268.dgx-a100-12.1050624.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-03-36_dgx-a100-16/events.out.tfevents.1737378241.dgx-a100-16.3086404.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-03-36_dgx-a100-16/events.out.tfevents.1737378696.dgx-a100-16.3086404.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:13:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_3_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678de286d46485a01abaab68"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_3_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_3_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_3_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_3_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_4_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_4_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_4_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9157

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_4_full"", ""author"": ""CharlesLi"", ""sha"": ""e5938c74bf8a96a11e6a4138e63f1007b690532f"", ""last_modified"": ""2025-01-20 13:25:28+00:00"", ""created_at"": ""2025-01-20 05:54:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_4_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_4_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-54-00_dgx-a100-12/events.out.tfevents.1737352472.dgx-a100-12.1060358.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_06-54-00_dgx-a100-12/events.out.tfevents.1737353013.dgx-a100-12.1060358.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-13-46_dgx-a100-16/events.out.tfevents.1737378851.dgx-a100-16.3094748.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-13-46_dgx-a100-16/events.out.tfevents.1737379389.dgx-a100-16.3094748.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:25:28+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_4_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678de513167c9100a216fe1f"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_4_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_4_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_4_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_4_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_llama_2_alpaca_5_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- alignment-handbook
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_llama_2_alpaca_5_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_llama_2_alpaca_5_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7509

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_llama_2_alpaca_5_full"", ""author"": ""CharlesLi"", ""sha"": ""ee5306ebe90e37cc230244a8409e80d241aa5d44"", ""last_modified"": ""2025-01-20 13:40:23+00:00"", ""created_at"": ""2025-01-20 06:06:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_5_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_llama_2_alpaca_5_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-05-41_dgx-a100-12/events.out.tfevents.1737353179.dgx-a100-12.1070109.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_07-05-41_dgx-a100-12/events.out.tfevents.1737353937.dgx-a100-12.1070109.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-25-44_dgx-a100-16/events.out.tfevents.1737379570.dgx-a100-16.3104978.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan20_14-25-44_dgx-a100-16/events.out.tfevents.1737380312.dgx-a100-16.3104978.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-20 13:40:23+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_llama_2_alpaca_5_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678de7d681ba0ef5a0f30c3e"", ""modelId"": ""CharlesLi/llama_2_llama_2_alpaca_5_full"", ""usedStorage"": 26954257031}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_llama_2_alpaca_5_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_llama_2_alpaca_5_full%5D(%2FCharlesLi%2Fllama_2_llama_2_alpaca_5_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_0_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_0_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_0_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0214

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_0_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""0f00c34e298f3ed739ebf9d6aa1b7c22f008f106"", ""last_modified"": ""2025-01-21 09:02:48+00:00"", ""created_at"": ""2025-01-21 08:54:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_0_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_0_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_09-48-46_dgx-a100-12/events.out.tfevents.1737449660.dgx-a100-12.2397968.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_09-48-46_dgx-a100-12/events.out.tfevents.1737450056.dgx-a100-12.2397968.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 09:02:48+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_0_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f60b66c337bdf151e8be6"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_0_3_epoch_full"", ""usedStorage"": 13477378429}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_0_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_0_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_0_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_1_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_1_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_1_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0663

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_1_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""c6a2fc60bcdda2bcddb691a2f709f316ba9181dd"", ""last_modified"": ""2025-01-21 09:13:12+00:00"", ""created_at"": ""2025-01-21 09:03:28+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_1_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_1_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-03-05_dgx-a100-12/events.out.tfevents.1737450212.dgx-a100-12.2409290.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-03-05_dgx-a100-12/events.out.tfevents.1737450664.dgx-a100-12.2409290.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 09:13:12+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_1_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f62e0e5737ea89883aaa1"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_1_3_epoch_full"", ""usedStorage"": 13477378429}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_1_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_1_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_1_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_2_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_2_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_2_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8581

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_2_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""b3f005a7b92421156c9c8c0ed9991e4f213234eb"", ""last_modified"": ""2025-01-21 09:24:52+00:00"", ""created_at"": ""2025-01-21 09:13:53+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_2_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_2_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-13-33_dgx-a100-12/events.out.tfevents.1737450839.dgx-a100-12.2418535.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-13-33_dgx-a100-12/events.out.tfevents.1737451376.dgx-a100-12.2418535.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 09:24:52+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_2_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f6551c93add0d86765d30"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_2_3_epoch_full"", ""usedStorage"": 13477378429}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_2_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_2_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_2_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_3_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_3_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_3_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9498

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_3_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""98fa3c45a99db61241dd8dc255cb43f4bb5be9c5"", ""last_modified"": ""2025-01-21 09:38:54+00:00"", ""created_at"": ""2025-01-21 09:25:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_3_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_3_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-25-10_dgx-a100-12/events.out.tfevents.1737451536.dgx-a100-12.2427945.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-25-10_dgx-a100-12/events.out.tfevents.1737452224.dgx-a100-12.2427945.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 09:38:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_3_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f680b158146b8b39e8406"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_3_3_epoch_full"", ""usedStorage"": 13477378429}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_3_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_3_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_3_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_4_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_4_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_4_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0400

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7381        | 2.4390 | 100  | 1.0590          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_4_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""f90deb8a98134139eb1408f4c2f7ad43bd64786d"", ""last_modified"": ""2025-01-21 09:58:37+00:00"", ""created_at"": ""2025-01-21 09:39:34+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_4_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_4_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-39-12_dgx-a100-12/events.out.tfevents.1737452379.dgx-a100-12.2438882.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-39-12_dgx-a100-12/events.out.tfevents.1737453404.dgx-a100-12.2438882.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 09:58:37+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_4_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f6b5667acbb6e0700b7d4"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_4_3_epoch_full"", ""usedStorage"": 13477378902}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_4_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_4_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_4_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_alpaca_5_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_alpaca_5_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_alpaca_5_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8417

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.9057        | 1.2048 | 100  | 0.7864          |
| 0.4976        | 2.4096 | 200  | 0.8496          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_alpaca_5_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""d37c0fe12c07d18109fe5f79cfcabed1b7ad6c6a"", ""last_modified"": ""2025-01-21 10:29:30+00:00"", ""created_at"": ""2025-01-21 09:59:16+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_5_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_alpaca_5_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-58-56_dgx-a100-12/events.out.tfevents.1737453561.dgx-a100-12.2454351.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_10-58-56_dgx-a100-12/events.out.tfevents.1737455260.dgx-a100-12.2454351.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 10:29:30+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_alpaca_5_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f6ff49ad566c9022a74d8"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_alpaca_5_3_epoch_full"", ""usedStorage"": 13477379395}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_alpaca_5_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_alpaca_5_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_alpaca_5_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_0_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_0_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_0_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6199

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_0_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""237d6402695f84829b5d59314ea5efbeb057b01e"", ""last_modified"": ""2025-01-21 10:39:05+00:00"", ""created_at"": ""2025-01-21 10:30:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_0_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_0_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-29-49_dgx-a100-12/events.out.tfevents.1737455414.dgx-a100-12.2478800.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-29-49_dgx-a100-12/events.out.tfevents.1737455836.dgx-a100-12.2478800.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 10:39:05+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_0_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f7731a376aac2e4cab1d3"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_0_3_epoch_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_0_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_0_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_0_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_1_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_1_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_1_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6809

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_1_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""1468683affc08b30791d8dae3f995eaa5838d139"", ""last_modified"": ""2025-01-21 10:49:24+00:00"", ""created_at"": ""2025-01-21 10:39:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_1_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_1_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-39-24_dgx-a100-12/events.out.tfevents.1737455989.dgx-a100-12.2486491.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-39-24_dgx-a100-12/events.out.tfevents.1737456451.dgx-a100-12.2486491.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 10:49:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_1_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f796ffb926b07261e59c1"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_1_3_epoch_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_1_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_1_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_1_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_2_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_2_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_2_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5266

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_2_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""9fa2a291a04d7ad5a483305aed340a9ad3327c1c"", ""last_modified"": ""2025-01-21 11:03:24+00:00"", ""created_at"": ""2025-01-21 10:50:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_2_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_2_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-49-41_dgx-a100-12/events.out.tfevents.1737456607.dgx-a100-12.2494682.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_11-49-41_dgx-a100-12/events.out.tfevents.1737457201.dgx-a100-12.2494682.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 11:03:24+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_2_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f7bdaba32ef1413b5f114"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_2_3_epoch_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_2_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_2_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_2_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_3_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_3_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_3_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.4985

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_3_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""541a99e74485343c2ff68f61694091f9b74c4025"", ""last_modified"": ""2025-01-21 11:18:43+00:00"", ""created_at"": ""2025-01-21 11:04:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_3_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_3_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-03-42_dgx-a100-12/events.out.tfevents.1737457447.dgx-a100-12.2508324.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-03-42_dgx-a100-12/events.out.tfevents.1737458214.dgx-a100-12.2508324.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 11:18:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_3_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f7f22fb926b0726202b1b"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_3_3_epoch_full"", ""usedStorage"": 13477378441}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_3_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_3_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_3_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_4_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_4_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_4_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6812

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.5583        | 1.9417 | 100  | 0.5909          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_4_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""4eb1178d06f64bc96f10df7b2c4977748e064c75"", ""last_modified"": ""2025-01-21 11:44:44+00:00"", ""created_at"": ""2025-01-21 11:19:20+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_4_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_4_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-19-00_dgx-a100-12/events.out.tfevents.1737458365.dgx-a100-12.2520636.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-19-00_dgx-a100-12/events.out.tfevents.1737459769.dgx-a100-12.2520636.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 11:44:44+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_4_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f82b8da47f04b2d991b9f"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_4_3_epoch_full"", ""usedStorage"": 13477378925}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_4_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_4_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_4_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_cot_simplest_code_math_5_3_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_cot_simplest_code_math_5_3_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_cot_simplest_code_math_5_3_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5930

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.6463        | 0.8333 | 100  | 0.5472          |
| 0.4158        | 1.6667 | 200  | 0.5442          |
| 0.3015        | 2.5    | 300  | 0.5952          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_cot_simplest_code_math_5_3_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""84a12d0fca5e2d8d8ad65476e84e0d5ba75e67f5"", ""last_modified"": ""2025-01-21 12:25:43+00:00"", ""created_at"": ""2025-01-21 11:45:22+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_5_3_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_cot_simplest_code_math_5_3_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-45-02_dgx-a100-12/events.out.tfevents.1737459927.dgx-a100-12.2541347.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan21_12-45-02_dgx-a100-12/events.out.tfevents.1737462226.dgx-a100-12.2541347.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-21 12:25:43+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_cot_simplest_code_math_5_3_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""678f88d2da757153c4e5b121"", ""modelId"": ""CharlesLi/llama_2_cot_simplest_code_math_5_3_epoch_full"", ""usedStorage"": 13477379889}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_cot_simplest_code_math_5_3_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_cot_simplest_code_math_5_3_epoch_full%5D(%2FCharlesLi%2Fllama_2_cot_simplest_code_math_5_3_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
ALIN-LLM/finetune-llama-2-7b-chat-gsm8k,"---
library_name: transformers
datasets:
- openai/gsm8k
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->

https://wandb.ai/seunghyukoh-kaist/star/runs/05f09b2d-13c4-4cc3-ba50-95f36005d055

## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""ALIN-LLM/finetune-llama-2-7b-chat-gsm8k"", ""author"": ""ALIN-LLM"", ""sha"": ""935da398b1d3d79bc6cf888817884a006fa0033f"", ""last_modified"": ""2025-01-27 10:44:05+00:00"", ""created_at"": ""2025-01-27 10:39:21+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""conversational"", ""en"", ""dataset:openai/gsm8k"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlanguage:\n- en\nlibrary_name: transformers"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-27 10:44:05+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- openai/gsm8k\nlanguage:\n- en\nlibrary_name: transformers"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67976259b5e7135099f59bf4"", ""modelId"": ""ALIN-LLM/finetune-llama-2-7b-chat-gsm8k"", ""usedStorage"": 13477364771}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=ALIN-LLM/finetune-llama-2-7b-chat-gsm8k&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BALIN-LLM%2Ffinetune-llama-2-7b-chat-gsm8k%5D(%2FALIN-LLM%2Ffinetune-llama-2-7b-chat-gsm8k)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_4o_cot_sky_o1_0_1_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_4o_cot_sky_o1_0_1_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_4o_cot_sky_o1_0_1_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 1.0126

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_4o_cot_sky_o1_0_1_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""f59a80d1c6d20dd01ab63c9fe8ee5172ec3ec5f2"", ""last_modified"": ""2025-01-27 22:43:49+00:00"", ""created_at"": ""2025-01-27 22:17:56+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_4o_cot_sky_o1_0_1_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_4o_cot_sky_o1_0_1_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan27_23-12-36_dgx-a100-12/events.out.tfevents.1738016281.dgx-a100-12.4014541.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan27_23-12-36_dgx-a100-12/events.out.tfevents.1738017722.dgx-a100-12.4014541.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-27 22:43:49+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_4o_cot_sky_o1_0_1_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""679806147bb3855234ad3dc9"", ""modelId"": ""CharlesLi/llama_2_4o_cot_sky_o1_0_1_epoch_full"", ""usedStorage"": 13477378405}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_4o_cot_sky_o1_0_1_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_4o_cot_sky_o1_0_1_epoch_full%5D(%2FCharlesLi%2Fllama_2_4o_cot_sky_o1_0_1_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_4o_cot_sky_o1_1_1_epoch_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_4o_cot_sky_o1_1_1_epoch_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_4o_cot_sky_o1_1_1_epoch_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.9667

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results



### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_4o_cot_sky_o1_1_1_epoch_full"", ""author"": ""CharlesLi"", ""sha"": ""b0925d25a1f72f0b89fc3be926ebd1a9d6ee5d45"", ""last_modified"": ""2025-01-27 23:12:54+00:00"", ""created_at"": ""2025-01-27 22:44:27+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_4o_cot_sky_o1_1_1_epoch_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_4o_cot_sky_o1_1_1_epoch_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan27_23-44-07_dgx-a100-12/events.out.tfevents.1738017872.dgx-a100-12.4040046.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan27_23-44-07_dgx-a100-12/events.out.tfevents.1738019471.dgx-a100-12.4040046.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-27 23:12:54+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_4o_cot_sky_o1_1_1_epoch_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67980c4b05822244e09e2543"", ""modelId"": ""CharlesLi/llama_2_4o_cot_sky_o1_1_1_epoch_full"", ""usedStorage"": 13477378405}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_4o_cot_sky_o1_1_1_epoch_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_4o_cot_sky_o1_1_1_epoch_full%5D(%2FCharlesLi%2Fllama_2_4o_cot_sky_o1_1_1_epoch_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_1_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_o1_1_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_1_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7174

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8253        | 0.5333 | 100  | 0.7402          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_1_full"", ""author"": ""CharlesLi"", ""sha"": ""cf251746516c1dd1370cf5a54671f66871375bf6"", ""last_modified"": ""2025-01-29 08:18:16+00:00"", ""created_at"": ""2025-01-29 07:26:24+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_1_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_1_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan29_08-19-29_dgx-a100-11/events.out.tfevents.1738135588.dgx-a100-11.2154919.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan29_08-19-29_dgx-a100-11/events.out.tfevents.1738138130.dgx-a100-11.2154919.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-29 08:18:16+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_1_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6799d820e5338048c5e502f4"", ""modelId"": ""CharlesLi/llama_2_o1_1_full"", ""usedStorage"": 13477378749}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_1_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_1_full%5D(%2FCharlesLi%2Fllama_2_o1_1_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama_2_o1_10_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
datasets:
- generator
model-index:
- name: llama_2_o1_10_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama_2_o1_10_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the generator dataset.
It achieves the following results on the evaluation set:
- Loss: 0.6120

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.9433        | 0.0615 | 100  | 0.7648          |
| 0.7768        | 0.1230 | 200  | 0.7311          |
| 0.7448        | 0.1844 | 300  | 0.7125          |
| 0.7336        | 0.2459 | 400  | 0.6952          |
| 0.7224        | 0.3074 | 500  | 0.6859          |
| 0.714         | 0.3689 | 600  | 0.6708          |
| 0.6965        | 0.4304 | 700  | 0.6594          |
| 0.6883        | 0.4919 | 800  | 0.6585          |
| 0.6738        | 0.5533 | 900  | 0.6418          |
| 0.6712        | 0.6148 | 1000 | 0.6328          |
| 0.6633        | 0.6763 | 1100 | 0.6239          |
| 0.6641        | 0.7378 | 1200 | 0.6183          |
| 0.6578        | 0.7993 | 1300 | 0.6167          |
| 0.6519        | 0.8607 | 1400 | 0.6141          |
| 0.652         | 0.9222 | 1500 | 0.6122          |
| 0.655         | 0.9837 | 1600 | 0.6117          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama_2_o1_10_full"", ""author"": ""CharlesLi"", ""sha"": ""48640b80ce9a2b1e6146d6159226cec11bb43e78"", ""last_modified"": ""2025-01-29 11:16:35+00:00"", ""created_at"": ""2025-01-29 08:22:46+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""dataset:generator"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_10_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama_2_o1_10_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token + 'System: ' + (messages[0]['content'] | trim + '\n\n' if messages[0]['role'] == 'system' else '') }}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') if (message['role'] == 'user') != (loop.index0 % 2 == 0) else '' }}{{ '[INST] ' + message['content'] | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan29_09-18-36_dgx-a100-11/events.out.tfevents.1738138971.dgx-a100-11.2209504.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Jan29_09-18-36_dgx-a100-11/events.out.tfevents.1738149285.dgx-a100-11.2209504.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-01-29 11:16:35+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- generator\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama_2_o1_10_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""6799e55664b2880124744f48"", ""modelId"": ""CharlesLi/llama_2_o1_10_full"", ""usedStorage"": 13477385983}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama_2_o1_10_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama_2_o1_10_full%5D(%2FCharlesLi%2Fllama_2_o1_10_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
puyol917/classification_yelp,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: classification_yelp
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# classification_yelp

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.
It achieves the following results on the evaluation set:
- eval_loss: 0.0894
- eval_accuracy: 0.9807
- eval_runtime: 9047.9094
- eval_samples_per_second: 4.2
- eval_steps_per_second: 2.1
- epoch: 1.0
- step: 50000

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- num_epochs: 1

### Framework versions

- Transformers 4.47.1
- Pytorch 2.5.1+cu124
- Datasets 3.2.0
- Tokenizers 0.21.0
","{""id"": ""puyol917/classification_yelp"", ""author"": ""puyol917"", ""sha"": ""2290bc27118858cd5d1746bacfd925701c903248"", ""last_modified"": ""2025-02-02 04:33:46+00:00"", ""created_at"": ""2025-02-01 04:56:18+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: classification_yelp\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""classification_yelp"", ""results"": []}], ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Feb01_18-00-18_b968f81bdfdf/events.out.tfevents.1738432829.b968f81bdfdf.5093.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-02 04:33:46+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: classification_yelp\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""679da97226726f21021ff033"", ""modelId"": ""puyol917/classification_yelp"", ""usedStorage"": 537447076}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=puyol917/classification_yelp&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bpuyol917%2Fclassification_yelp%5D(%2Fpuyol917%2Fclassification_yelp)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151
tags:
- generated_from_trainer
- trl
- sft
licence: license
---

# Model Card for s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/saching007/inferencescale/runs/noemi55j) 


This model was trained with SFT.

### Framework versions

- TRL: 0.14.0
- Transformers: 4.46.3
- Pytorch: 2.5.1
- Datasets: 3.2.0
- Tokenizers: 0.20.3

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151"", ""author"": ""saching0071"", ""sha"": ""5158981b24c62c7a10a2cf7145d2d81311ca9cd5"", ""last_modified"": ""2025-02-05 02:22:36+00:00"", ""created_at"": ""2025-02-05 02:02:38+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""trl"", ""sft"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-02-05 02:22:36+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67a2c6be592d6c613177feb1"", ""modelId"": ""saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151"", ""usedStorage"": 13477372059}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=saching0071/s1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsaching0071%2Fs1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151%5D(%2Fsaching0071%2Fs1K_bs8_lr1e-5_epoch5_wd1e-4_20250205_020151)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122
tags:
- generated_from_trainer
- trl
- sft
licence: license
---

# Model Card for s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/saching007/inferencescale/runs/xkaz2bm6) 


This model was trained with SFT.

### Framework versions

- TRL: 0.14.0
- Transformers: 4.46.3
- Pytorch: 2.5.1
- Datasets: 3.2.0
- Tokenizers: 0.20.3

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122"", ""author"": ""saching0071"", ""sha"": ""08ff1763e69e881df5db3f69be3c49b4dc0d050a"", ""last_modified"": ""2025-02-05 02:46:39+00:00"", ""created_at"": ""2025-02-05 02:13:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""generated_from_trainer"", ""trl"", ""sft"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-02-05 02:46:39+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67a2c95744b3cf83ebb142b0"", ""modelId"": ""saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122"", ""usedStorage"": 13477372059}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=saching0071/s1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsaching0071%2Fs1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122%5D(%2Fsaching0071%2Fs1K_bs8_lr1e-5_epoch10_wd1e-4_20250205_021122)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ousso1117/SFT-meta-Llama-2-7B-mrd3,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: SFT-meta-Llama-2-7B-mrd3
tags:
- generated_from_trainer
- trl
- sft
licence: license
---

# Model Card for SFT-meta-Llama-2-7B-mrd3

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""Ousso1117/SFT-meta-Llama-2-7B-mrd3"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/mnlp_osy/huggingface/runs/gs2z9m4x) 


This model was trained with SFT.

### Framework versions

- TRL: 0.14.0
- Transformers: 4.48.2
- Pytorch: 2.5.1
- Datasets: 3.0.1
- Tokenizers: 0.21.0

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""Ousso1117/SFT-meta-Llama-2-7B-mrd3"", ""author"": ""Ousso1117"", ""sha"": ""9598c23e1d51c8cf4ca7f44e536b1ee182136f8d"", ""last_modified"": ""2025-02-07 16:34:40+00:00"", ""created_at"": ""2025-02-07 16:01:43+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""sft"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: SFT-meta-Llama-2-7B-mrd3\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% if messages[1]['role'] == 'user' %}{{ bos_token + '[INST] <<SYS>>\\n' + messages[0]['content'] + '\\n<</SYS>>\\n\\n' + messages[1]['content'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ bos_token + '[INST] ' + messages[0]['content'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'].strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-07 16:34:40+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: SFT-meta-Llama-2-7B-mrd3\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67a62e679f2863033db14a41"", ""modelId"": ""Ousso1117/SFT-meta-Llama-2-7B-mrd3"", ""usedStorage"": 2559513603}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ousso1117/SFT-meta-Llama-2-7B-mrd3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOusso1117%2FSFT-meta-Llama-2-7B-mrd3%5D(%2FOusso1117%2FSFT-meta-Llama-2-7B-mrd3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
AjayMukundS/Llama2_7B_fine_tuned,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: SFT_FineTuned_LLaMA2-7B-v2
tags:
- generated_from_trainer
- trl
- sft
licence: license
---

# Model Card for SFT_FineTuned_LLaMA2-7B-v2

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""AjayMukundS/Llama2_7B_fine_tuned"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/ajaymukund1998-anna-university/SFT_Llama2_7B/runs/ykz95ye1) 


This model was trained with SFT.

### Framework versions

- TRL: 0.14.0
- Transformers: 4.48.3
- Pytorch: 2.5.1+cu124
- Datasets: 3.2.0
- Tokenizers: 0.21.0

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""AjayMukundS/Llama2_7B_fine_tuned"", ""author"": ""AjayMukundS"", ""sha"": ""326f580e4882925946dc6351c0cf803caab56919"", ""last_modified"": ""2025-02-13 16:38:10+00:00"", ""created_at"": ""2025-02-10 08:40:30+00:00"", ""private"": false, ""gated"": ""auto"", ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""sft"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: SFT_FineTuned_LLaMA2-7B-v2\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-13 16:38:10+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: SFT_FineTuned_LLaMA2-7B-v2\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67a9bb7e8a919ad008b8ce55"", ""modelId"": ""AjayMukundS/Llama2_7B_fine_tuned"", ""usedStorage"": 2737417899}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=AjayMukundS/Llama2_7B_fine_tuned&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAjayMukundS%2FLlama2_7B_fine_tuned%5D(%2FAjayMukundS%2FLlama2_7B_fine_tuned)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum
tags:
- generated_from_trainer
- trl
- grpo
licence: license
---

# Model Card for GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/mnlp_osy/huggingface/runs/08zwpne5) 


This model was trained with GRPO, a method introduced in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

### Framework versions

- TRL: 0.14.0
- Transformers: 4.48.2
- Pytorch: 2.5.1
- Datasets: 3.0.1
- Tokenizers: 0.21.0

## Citations

Cite GRPO as:

```bibtex
@article{zhihong2024deepseekmath,
    title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
    author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
    year         = 2024,
    eprint       = {arXiv:2402.03300},
}

```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", ""author"": ""Ousso1117"", ""sha"": ""9853e6d71780a5bfa3bdaf7c4ffca08a9ef0431e"", ""last_modified"": ""2025-02-12 03:48:11+00:00"", ""created_at"": ""2025-02-10 18:42:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""grpo"", ""arxiv:2402.03300"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum\ntags:\n- generated_from_trainer\n- trl\n- grpo\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% if messages[1]['role'] == 'user' %}{{ bos_token + '[INST] <<SYS>>\\n' + messages[0]['content'] + '\\n<</SYS>>\\n\\n' + messages[1]['content'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ bos_token + '[INST] ' + messages[0]['content'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'].strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-12 03:48:11+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum\ntags:\n- generated_from_trainer\n- trl\n- grpo\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67aa48a24f88ebf6d2519af7"", ""modelId"": ""Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", ""usedStorage"": 160474635}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ousso1117/GRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOusso1117%2FGRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum%5D(%2FOusso1117%2FGRPO-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum
tags:
- generated_from_trainer
- trl
- grpo
licence: license
---

# Model Card for GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/mnlp_osy/huggingface/runs/grg4x44r) 


This model was trained with GRPO, a method introduced in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

### Framework versions

- TRL: 0.14.0
- Transformers: 4.48.2
- Pytorch: 2.5.1
- Datasets: 3.0.1
- Tokenizers: 0.21.0

## Citations

Cite GRPO as:

```bibtex
@article{zhihong2024deepseekmath,
    title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
    author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
    year         = 2024,
    eprint       = {arXiv:2402.03300},
}

```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", ""author"": ""Ousso1117"", ""sha"": ""dd13c66b041486b4d84f62101a98866013f35680"", ""last_modified"": ""2025-02-19 16:06:13+00:00"", ""created_at"": ""2025-02-10 18:43:02+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""grpo"", ""arxiv:2402.03300"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum\ntags:\n- generated_from_trainer\n- trl\n- grpo\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% if messages[1]['role'] == 'user' %}{{ bos_token + '[INST] <<SYS>>\\n' + messages[0]['content'] + '\\n<</SYS>>\\n\\n' + messages[1]['content'] + ' [/INST]' }}{% set loop_messages = messages[2:] %}{% else %}{{ bos_token + '[INST] ' + messages[0]['content'] + ' [/INST]' }}{% set loop_messages = messages[1:] %}{% endif %}{% else %}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'].strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-19 16:06:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum\ntags:\n- generated_from_trainer\n- trl\n- grpo\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67aa48b65c7fc40d28c389dc"", ""modelId"": ""Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum"", ""usedStorage"": 320449675}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Ousso1117/GRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOusso1117%2FGRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum%5D(%2FOusso1117%2FGRPO-SFT-meta-Llama-2-7B-meta-Llama-2-7B-mrd3-sum)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CreitinGameplays/Llama-2-7b-chat-reasoning-test,"---
license: mit
datasets:
- CreitinGameplays/reasoning-0.01-content-llama2
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
library_name: transformers
---

# Llama 2 7b reasoning test (1)

Prompt format:
```
[INST] <<SYS>>
{system_prompt} <</SYS>>
[/INST] [THINK]
```

Recommended using this new model instead: [CreitinGameplays/Llama-3.1-8b-reasoning-test](https://huggingface.co/CreitinGameplays/Llama-3.1-8b-reasoning-test)","{""id"": ""CreitinGameplays/Llama-2-7b-chat-reasoning-test"", ""author"": ""CreitinGameplays"", ""sha"": ""5cbfd5a0ca31085e813e53fac1eb62caf93ec2d4"", ""last_modified"": ""2025-02-13 22:48:17+00:00"", ""created_at"": ""2025-02-10 23:23:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 1, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""conversational"", ""en"", ""dataset:CreitinGameplays/reasoning-0.01-content-llama2"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- CreitinGameplays/reasoning-0.01-content-llama2\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-02-13 22:48:17+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- CreitinGameplays/reasoning-0.01-content-llama2\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67aa8a623974a694cbdf0a08"", ""modelId"": ""CreitinGameplays/Llama-2-7b-chat-reasoning-test"", ""usedStorage"": 13477364475}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CreitinGameplays/Llama-2-7b-chat-reasoning-test&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCreitinGameplays%2FLlama-2-7b-chat-reasoning-test%5D(%2FCreitinGameplays%2FLlama-2-7b-chat-reasoning-test)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Can1sters/Bruh,"---
datasets:
- vicgalle/alpaca-gpt4
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""Can1sters/Bruh"", ""author"": ""Can1sters"", ""sha"": ""bf28b7a77571d58a867e3e33f9e1d92f24e49b5e"", ""last_modified"": ""2025-02-13 17:21:29+00:00"", ""created_at"": ""2025-02-13 17:20:42+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""dataset:vicgalle/alpaca-gpt4"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- vicgalle/alpaca-gpt4"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-02-13 17:21:29+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- vicgalle/alpaca-gpt4"", ""transformersInfo"": null, ""_id"": ""67ae29eaf17786fa7227a464"", ""modelId"": ""Can1sters/Bruh"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Can1sters/Bruh&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCan1sters%2FBruh%5D(%2FCan1sters%2FBruh)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
IJyad/llama-2-7b-NDMO-agent,"---
library_name: transformers
tags:
- legal
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
---
# NDMO Chroma DB - Fine-Tuned LLM

## Model Description
This model has been fine-tuned on the **NDMO Chroma DB Dataset**, a collection of key documents related to data governance, privacy, and artificial intelligence (AI) regulations. The fine-tuning process enhances the model's ability to understand and generate responses related to these domains.

### **Developed by:**
- Jyad Aljohani
- Abdulrahman Aljohani 
- Ryan Alshehri
- Saud Altuwaijri
- Ziyad Alharthi


### **Model Type:**
Causal Language Model (CAUSAL_LM)

### **Language(s):**
English

### **License:**
[Specify License]

### **Finetuned from model:**
[meta-llama/Llama-2-7b-chat-hf]

---

## Model Sources

### **Repository:**
[More Information Needed]

### **Paper [optional]:**
[More Information Needed]

### **Demo [optional]:**
[More Information Needed]

---

## Uses

### **Direct Use**
This model is designed for:
- Answering questions on data governance, AI regulations, and privacy policies.
- Assisting compliance professionals with regulatory inquiries.
- Supporting AI policy research and development.

### **Downstream Use [optional]**
- Chatbots and virtual assistants focused on AI and data privacy compliance.
- Automated document summarization for legal and regulatory documents.
- Integration into AI governance frameworks.

### **Out-of-Scope Use**
- The model is not designed for providing legally binding advice.
- Not suitable for tasks requiring real-time regulatory updates.

---

## **Bias, Risks, and Limitations**
- The model may reflect biases present in the training data.
- It may not generalize well to regulations not covered in the dataset.
- Users should verify outputs against official regulatory sources.

### **Recommendations**
- Users should cross-check information with official legal sources.
- Outputs should be reviewed by regulatory professionals for critical applications.

---

## **How to Get Started with the Model**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

model_name = ""IJyad/llama-2-7b-NDMO-agent""
base_model = ""meta-llama/Llama-2-7b-chat-hf""

# Load base model in 4-bit quantized mode
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map=""auto"",
)

# Load LoRA adapters
model = PeftModel.from_pretrained(model, model_name)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
```

---

## **Training Details**

### **Training Data**
The model was fine-tuned using the **NDMO Chroma DB Dataset**, which consists of key regulatory documents, including:

- AI Principles
- Data Classification Policy
- Data Sharing Policy
- Implementing Regulations
- Personal Data Protection Guidelines
- Generative AI Public Guidelines

[NDMO Chroma DB Dataset](https://huggingface.co/datasets/IJyad/NDMO_chroma_db)

### **Training Procedure**

#### **Preprocessing**
- Data was extracted, cleaned, and formatted into question-answer pairs.
- Documents were structured to maximize context retention.

#### **Training Hyperparameters**
- **Epochs:** 1
- **Batch Size:** 10
- **Gradient Accumulation Steps:** 1
- **Learning Rate:** 2e-4
- **Optimizer:** paged_adamw_8bit
- **Scheduler:** Linear decay with warmup steps
- **Evaluation Strategy:** Steps-based

---

## **Evaluation**

### **Testing Data, Factors & Metrics**

#### **Testing Data**
- Held-out subset of the **NDMO Chroma DB Dataset**

#### **Factors Considered**
- Accuracy in responding to regulatory and AI policy-related queries.
- Coherence and relevance of generated text.

#### **Metrics Used**
- **Perplexity:** Measures fluency of the model.
- **BLEU Score:** Evaluates text generation quality.
- **Human Evaluation:** Subject matter experts assessed output correctness.

---

## **Results**
- **Perplexity Score:** [More Information Needed]
- **BLEU Score:** [More Information Needed]
- **Human Evaluation Accuracy:** [More Information Needed]

---

## **Environmental Impact**

The model was fine-tuned on cloud-based infrastructure. 
---

## **Technical Specifications**

### **Model Architecture and Objective**
- **Architecture:** Transformer-based causal language model.
- **Fine-Tuned Objective:** Text generation and AI policy understanding.

### **Compute Infrastructure**
- **Software:** Transformers, BitsAndBytes, PEFT, Hugging Face Trainer.



### **Model Card Authors**
- Jyad Aljohani

### **Contact**
- **Email:** Jyadofficial@gmail.com
- **Hugging Face Profile:** [Ijyad](https://huggingface.co/Ijyad)

For further inquiries, feel free to reach out!","{""id"": ""IJyad/llama-2-7b-NDMO-agent"", ""author"": ""IJyad"", ""sha"": ""65961da1897c3a2b118f0895a527412f327b379f"", ""last_modified"": ""2025-02-26 09:56:54+00:00"", ""created_at"": ""2025-02-24 11:01:54+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 4, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""legal"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- legal"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-02-26 09:56:54+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- legal"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67bc51a2113c3e29a2943f0d"", ""modelId"": ""IJyad/llama-2-7b-NDMO-agent"", ""usedStorage"": 13477364475}",1,,0,,0,https://huggingface.co/mradermacher/llama-2-7b-NDMO-agent-GGUF,1,,0,huggingface/InferenceSupport/discussions/new?title=IJyad/llama-2-7b-NDMO-agent&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BIJyad%2Fllama-2-7b-NDMO-agent%5D(%2FIJyad%2Fllama-2-7b-NDMO-agent)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mayanklohani19/mergekit-slerp-ujysgyd,"---
base_model:
- meta-llama/Llama-2-7b-chat-hf
library_name: transformers
tags:
- mergekit
- merge

---
# merge

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the [SLERP](https://en.wikipedia.org/wiki/Slerp) merge method.

### Models Merged

The following models were included in the merge:
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

### Configuration

The following YAML configuration was used to produce this model:

```yaml
slices:
- sources:
  - model: meta-llama/Llama-2-7b-chat-hf
    layer_range:
    - 0
    - 32
  - model: meta-llama/Llama-2-7b-chat-hf
    layer_range:
    - 0
    - 32
merge_method: slerp
base_model: meta-llama/Llama-2-7b-chat-hf
parameters:
  t:
  - filter: self_attn
    value:
    - 0
    - 0.5
    - 0.3
    - 0.7
    - 1
  - filter: mlp
    value:
    - 1
    - 0.5
    - 0.7
    - 0.3
    - 0
  - value: 0.5
dtype: bfloat16
```
","{""id"": ""mayanklohani19/mergekit-slerp-ujysgyd"", ""author"": ""mayanklohani19"", ""sha"": ""ccd5e89a411e01be47396f06da32d37eed433e7e"", ""last_modified"": ""2025-02-24 14:24:08+00:00"", ""created_at"": ""2025-02-24 14:21:50+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 7, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""mergekit"", ""merge"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\ntags:\n- mergekit\n- merge"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='mergekit_config.yml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-02-24 14:24:08+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\ntags:\n- mergekit\n- merge"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67bc807e2cedbdaed9ed7793"", ""modelId"": ""mayanklohani19/mergekit-slerp-ujysgyd"", ""usedStorage"": 13477364779}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mayanklohani19/mergekit-slerp-ujysgyd&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmayanklohani19%2Fmergekit-slerp-ujysgyd%5D(%2Fmayanklohani19%2Fmergekit-slerp-ujysgyd)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Jennny/eto-Llama-2-7b-chat-hf-webshop-sft,"---
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- generated_from_trainer
model-index:
- name: eto-Llama-2-7b-chat-hf-webshop-sft
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# eto-Llama-2-7b-chat-hf-webshop-sft

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 4
- total_train_batch_size: 64
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3.0

### Training results



### Framework versions

- Transformers 4.37.2
- Pytorch 2.1.0+cu118
- Datasets 3.3.2
- Tokenizers 0.15.2
","{""id"": ""Jennny/eto-Llama-2-7b-chat-hf-webshop-sft"", ""author"": ""Jennny"", ""sha"": ""1289e3f47db2c775fc9f75c745d87d43039d8d64"", ""last_modified"": ""2025-03-09 08:55:41+00:00"", ""created_at"": ""2025-03-09 07:56:55+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 2, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""generated_from_trainer"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: eto-Llama-2-7b-chat-hf-webshop-sft\n  results: []"", ""widget_data"": null, ""model_index"": [{""name"": ""eto-Llama-2-7b-chat-hf-webshop-sft"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F32"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-09 08:55:41+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlicense: llama2\ntags:\n- generated_from_trainer\nmodel-index:\n- name: eto-Llama-2-7b-chat-hf-webshop-sft\n  results: []"", ""transformersInfo"": null, ""_id"": ""67cd49c757633f8151659c07"", ""modelId"": ""Jennny/eto-Llama-2-7b-chat-hf-webshop-sft"", ""usedStorage"": 26954200803}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=Jennny/eto-Llama-2-7b-chat-hf-webshop-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJennny%2Feto-Llama-2-7b-chat-hf-webshop-sft%5D(%2FJennny%2Feto-Llama-2-7b-chat-hf-webshop-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
satyamtripathii/Nagrik_mitra_Fine_tunned_LLaMa_7b,"---
license: mit
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""satyamtripathii/Nagrik_mitra_Fine_tunned_LLaMa_7b"", ""author"": ""satyamtripathii"", ""sha"": ""afb081a94a4d0e892da59ca6e2527e7a7d5a56ec"", ""last_modified"": ""2025-03-16 09:02:16+00:00"", ""created_at"": ""2025-03-16 09:00:37+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:mit"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: mit"", ""widget_data"": null, ""model_index"": null, ""config"": null, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-03-16 09:02:16+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlicense: mit"", ""transformersInfo"": null, ""_id"": ""67d69335473d4edd33c35f39"", ""modelId"": ""satyamtripathii/Nagrik_mitra_Fine_tunned_LLaMa_7b"", ""usedStorage"": 0}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=satyamtripathii/Nagrik_mitra_Fine_tunned_LLaMa_7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsatyamtripathii%2FNagrik_mitra_Fine_tunned_LLaMa_7b%5D(%2Fsatyamtripathii%2FNagrik_mitra_Fine_tunned_LLaMa_7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
arham-15/llama2_7B_qphysics,"---
base_model:
- meta-llama/Llama-2-7b-chat-hf
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
license: apache-2.0
language:
- en
---

### Llama 2 7B Physics

A large language model specialized for quantum physics related queries. It has been fine tuned from llama 2 7B which is a chat model. The model was fine-tuned using the unsloth library in python.

### Usage

You can import and use the model using unsloth:

```python

from unsloth import FastLanguageModel

max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = ""arham-15/llama2_7B_qphysics"",
    max_seq_length = max_seq_length,
    dtype = None,
    load_in_4bit = True,
)
```

Or you can use the hugging face transformers library if you wish to, totally up to you.

```python

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = ""arham-15/llama2_7B_qphysics""

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

### Results

The model has been evaluated with its base model by perplexity score. The model has shown significant improvement on quantum physics related queries. Out of 200 test questions, the model outperformed the base model on 126 with a lower perplexity score.

","{""id"": ""arham-15/llama2_7B_qphysics"", ""author"": ""arham-15"", ""sha"": ""e16daa854ad3f267ce307c1d5b64837b91fb7730"", ""last_modified"": ""2025-03-17 22:55:29+00:00"", ""created_at"": ""2025-03-17 05:06:14+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""text-generation-inference"", ""unsloth"", ""llama"", ""trl"", ""en"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-03-17 22:55:29+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67d7adc66d5914cc9d6b90e8"", ""modelId"": ""arham-15/llama2_7B_qphysics"", ""usedStorage"": 160467603}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=arham-15/llama2_7B_qphysics&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Barham-15%2Fllama2_7B_qphysics%5D(%2Farham-15%2Fllama2_7B_qphysics)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
hazemOmrann14/llama2-7b-screen2words,"---
license: llama2
datasets:
- rootsautomation/RICO-Screen2Words
base_model:
- meta-llama/Llama-2-7b-chat-hf
---","{""id"": ""hazemOmrann14/llama2-7b-screen2words"", ""author"": ""hazemOmrann14"", ""sha"": ""c219ca2726cf4eb3f61422987a28cb0c11427dbf"", ""last_modified"": ""2025-03-18 15:03:40+00:00"", ""created_at"": ""2025-03-18 13:18:46+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""dataset:rootsautomation/RICO-Screen2Words"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- rootsautomation/RICO-Screen2Words\nlicense: llama2"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{%- for message in messages %}\n    {%- if message['role'] == 'user' %}\n        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n    {%- elif message['role'] == 'system' %}\n        {{- '<<SYS>>\\n' + message['content'].strip() + '\\n<</SYS>>\\n\\n' }}\n    {%- elif message['role'] == 'assistant' %}\n        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n    {%- endif %}\n{%- endfor %}\n"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-03-18 15:03:40+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- rootsautomation/RICO-Screen2Words\nlicense: llama2"", ""transformersInfo"": null, ""_id"": ""67d972b6b798ac32186e1632"", ""modelId"": ""hazemOmrann14/llama2-7b-screen2words"", ""usedStorage"": 55321371}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=hazemOmrann14/llama2-7b-screen2words&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BhazemOmrann14%2Fllama2-7b-screen2words%5D(%2FhazemOmrann14%2Fllama2-7b-screen2words)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
mayanklohani19/milan,"---
base_model:
- meta-llama/Llama-2-7b-chat-hf
library_name: transformers
tags:
- mergekit
- merge

---
# merge

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the [SLERP](https://en.wikipedia.org/wiki/Slerp) merge method.

### Models Merged

The following models were included in the merge:
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)

### Configuration

The following YAML configuration was used to produce this model:

```yaml
slices:
- sources:
  - model: meta-llama/Llama-2-7b-chat-hf
    layer_range:
    - 0
    - 32
  - model: meta-llama/Llama-2-7b-chat-hf
    layer_range:
    - 0
    - 32
merge_method: slerp
base_model: meta-llama/Llama-2-7b-chat-hf
parameters:
  t:
  - filter: self_attn
    value:
    - 0
    - 0.5
    - 0.3
    - 0.7
    - 1
  - filter: mlp
    value:
    - 1
    - 0.5
    - 0.7
    - 0.3
    - 0
  - value: 0.5
dtype: bfloat16
```
","{""id"": ""mayanklohani19/milan"", ""author"": ""mayanklohani19"", ""sha"": ""7baa4d984799cf60e9ac404bfff9d1590a30942f"", ""last_modified"": ""2025-03-21 07:21:30+00:00"", ""created_at"": ""2025-03-21 07:19:12+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 278, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""llama"", ""text-generation"", ""mergekit"", ""merge"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\ntags:\n- mergekit\n- merge"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": null, ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='mergekit_config.yml', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-21 07:21:30+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\ntags:\n- mergekit\n- merge"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67dd12f0babeda89ca6db79b"", ""modelId"": ""mayanklohani19/milan"", ""usedStorage"": 13477364779}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=mayanklohani19/milan&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmayanklohani19%2Fmilan%5D(%2Fmayanklohani19%2Fmilan)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama2_openo1_safe_o1_4o_default_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama2_openo1_safe_o1_4o_default_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_openo1_safe_o1_4o_default_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5557

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7761        | 0.7812 | 100  | 0.5642          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama2_openo1_safe_o1_4o_default_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""6880a2a53c98737acfd653a8f46f7866f97c0566"", ""last_modified"": ""2025-03-27 23:34:34+00:00"", ""created_at"": ""2025-03-27 22:49:57+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_default_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_openo1_safe_o1_4o_default_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar27_23-43-46_dgx-a100-11/events.out.tfevents.1743115802.dgx-a100-11.3970637.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar27_23-43-46_dgx-a100-11/events.out.tfevents.1743118176.dgx-a100-11.3970637.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-27 23:34:34+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_default_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67e5d61513b9609e842c9313"", ""modelId"": ""CharlesLi/llama2_openo1_safe_o1_4o_default_4000_100_full"", ""usedStorage"": 13477378933}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama2_openo1_safe_o1_4o_default_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama2_openo1_safe_o1_4o_default_4000_100_full%5D(%2FCharlesLi%2Fllama2_openo1_safe_o1_4o_default_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama2_openo1_safe_o1_4o_default_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama2_openo1_safe_o1_4o_default_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_openo1_safe_o1_4o_default_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5570

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.8083        | 0.6390 | 100  | 0.5895          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama2_openo1_safe_o1_4o_default_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""9b20a0093bd786abfdc895e4cb0a2335a454dcfb"", ""last_modified"": ""2025-03-28 01:08:45+00:00"", ""created_at"": ""2025-03-28 00:19:31+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_default_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_openo1_safe_o1_4o_default_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_01-18-50_dgx-a100-11/events.out.tfevents.1743121176.dgx-a100-11.4050864.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_01-18-50_dgx-a100-11/events.out.tfevents.1743124009.dgx-a100-11.4050864.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 01:08:45+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_default_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67e5eb13da8ffd4da94fddb0"", ""modelId"": ""CharlesLi/llama2_openo1_safe_o1_4o_default_4000_1000_full"", ""usedStorage"": 13477378937}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama2_openo1_safe_o1_4o_default_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama2_openo1_safe_o1_4o_default_4000_1000_full%5D(%2FCharlesLi%2Fllama2_openo1_safe_o1_4o_default_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_100_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama2_openo1_safe_o1_4o_reflect_4000_100_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_openo1_safe_o1_4o_reflect_4000_100_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5555

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7799        | 0.7812 | 100  | 0.5638          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_100_full"", ""author"": ""CharlesLi"", ""sha"": ""38bfb23b7b1e682b406cea129b0f3c629469f455"", ""last_modified"": ""2025-03-28 02:40:59+00:00"", ""created_at"": ""2025-03-28 01:56:13+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_reflect_4000_100_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_openo1_safe_o1_4o_reflect_4000_100_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_02-55-32_dgx-a100-11/events.out.tfevents.1743126978.dgx-a100-11.4131975.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_02-55-32_dgx-a100-11/events.out.tfevents.1743129459.dgx-a100-11.4131975.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 02:40:59+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_reflect_4000_100_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67e601bd7dac83cb62d0bb33"", ""modelId"": ""CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_100_full"", ""usedStorage"": 13477378933}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_100_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama2_openo1_safe_o1_4o_reflect_4000_100_full%5D(%2FCharlesLi%2Fllama2_openo1_safe_o1_4o_reflect_4000_100_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_1000_full,"---
library_name: transformers
license: llama2
base_model: meta-llama/Llama-2-7b-chat-hf
tags:
- alignment-handbook
- trl
- sft
- generated_from_trainer
- trl
- sft
- generated_from_trainer
model-index:
- name: llama2_openo1_safe_o1_4o_reflect_4000_1000_full
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# llama2_openo1_safe_o1_4o_reflect_4000_1000_full

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5488

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 4
- eval_batch_size: 4
- seed: 42
- distributed_type: multi-GPU
- num_devices: 4
- gradient_accumulation_steps: 2
- total_train_batch_size: 32
- total_eval_batch_size: 16
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 1

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.7962        | 0.6390 | 100  | 0.5803          |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1
","{""id"": ""CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_1000_full"", ""author"": ""CharlesLi"", ""sha"": ""301e83563874061f084988e8cba37043d70e2858"", ""last_modified"": ""2025-03-28 04:16:13+00:00"", ""created_at"": ""2025-03-28 03:30:09+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 3, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""llama"", ""text-generation"", ""alignment-handbook"", ""trl"", ""sft"", ""generated_from_trainer"", ""conversational"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:llama2"", ""autotrain_compatible"", ""text-generation-inference"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_reflect_4000_1000_full\n  results: []"", ""widget_data"": [{""text"": ""Hi, what can you help me with?""}, {""text"": ""What is 84 * 3 / 2?""}, {""text"": ""Tell me an interesting fact about the universe!""}, {""text"": ""Explain quantum computing in simple terms.""}], ""model_index"": [{""name"": ""llama2_openo1_safe_o1_4o_reflect_4000_1000_full"", ""results"": []}], ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% set system_message = '<<SYS>>' + messages[0]['content'] | trim + '<</SYS>>' if messages[0]['role'] == 'system' else '' %}{% set messages = messages[1:] if messages[0]['role'] == 'system' else messages %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ bos_token + '[INST] ' + (system_message + message['content'] if loop.index0 == 0 else message['content']) | trim + ' [/INST]' if message['role'] == 'user' else ' ' + message['content'] | trim + ' ' + eos_token }}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_04-29-25_dgx-a100-11/events.out.tfevents.1743132613.dgx-a100-11.16359.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Mar28_04-29-25_dgx-a100-11/events.out.tfevents.1743135158.dgx-a100-11.16359.1', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-28 04:16:13+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nlicense: llama2\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: llama2_openo1_safe_o1_4o_reflect_4000_1000_full\n  results: []"", ""transformersInfo"": {""auto_model"": ""AutoModelForCausalLM"", ""custom_class"": null, ""pipeline_tag"": ""text-generation"", ""processor"": ""AutoTokenizer""}, ""_id"": ""67e617c1a7d8e6572e8bc35f"", ""modelId"": ""CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_1000_full"", ""usedStorage"": 13477378937}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=CharlesLi/llama2_openo1_safe_o1_4o_reflect_4000_1000_full&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BCharlesLi%2Fllama2_openo1_safe_o1_4o_reflect_4000_1000_full%5D(%2FCharlesLi%2Fllama2_openo1_safe_o1_4o_reflect_4000_1000_full)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
wuqiong1/PA-RAG_Llama-2-7b-chat-hf,"---
datasets:
- wuqiong1/PA-RAG_training_data
base_model:
- meta-llama/Llama-2-7b-chat-hf
---


# PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization

🎉🎉🎉 PA-RAG is accepted by NAACL 2025!

Paper Link: https://arxiv.org/pdf/2412.14510  
Github Link: https://github.com/wujwyi/PA-RAG  

This is a model fine-tuned on [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) using PA-RAG.
The training data for PA-RAG, available at [Huggingface link](https://huggingface.co/datasets/wuqiong1/PA-RAG_training_data) or [Google Drive link](https://drive.google.com/file/d/1agP7fi1iX-3qFK7XFBvRu6rC5X_-M8Iy/view?usp=drive_link)



","{""id"": ""wuqiong1/PA-RAG_Llama-2-7b-chat-hf"", ""author"": ""wuqiong1"", ""sha"": ""21eb8bfa086376d043775e2837632be3e155018a"", ""last_modified"": ""2025-03-29 10:34:44+00:00"", ""created_at"": ""2025-03-29 09:55:41+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 11, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""dataset:wuqiong1/PA-RAG_training_data"", ""arxiv:2412.14510"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wuqiong1/PA-RAG_training_data"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>' + content }}{% elif message['role'] == 'assistant' %}{{ content }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='chat.jinja', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""BF16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-03-29 10:34:44+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- wuqiong1/PA-RAG_training_data"", ""transformersInfo"": null, ""_id"": ""67e7c39d6d6505d79a1e5512"", ""modelId"": ""wuqiong1/PA-RAG_Llama-2-7b-chat-hf"", ""usedStorage"": 13477364771}",1,,0,,0,"https://huggingface.co/mradermacher/PA-RAG_Llama-2-7b-chat-hf-GGUF, https://huggingface.co/mradermacher/PA-RAG_Llama-2-7b-chat-hf-i1-GGUF",2,,0,huggingface/InferenceSupport/discussions/new?title=wuqiong1/PA-RAG_Llama-2-7b-chat-hf&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bwuqiong1%2FPA-RAG_Llama-2-7b-chat-hf%5D(%2Fwuqiong1%2FPA-RAG_Llama-2-7b-chat-hf)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
dp0403/results,"---
base_model: meta-llama/Llama-2-7b-chat-hf
library_name: transformers
model_name: results
tags:
- generated_from_trainer
- trl
- sft
licence: license
---

# Model Card for results

This model is a fine-tuned version of [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""dp0403/results"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/machinelearning502-fr-c-rodrigues/huggingface/runs/hft7mscy)

This model was trained with SFT.

### Framework versions

- TRL: 0.12.0
- Transformers: 4.52.0.dev0
- Pytorch: 2.6.0+cu124
- Datasets: 3.5.0
- Tokenizers: 0.21.1

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```","{""id"": ""dp0403/results"", ""author"": ""dp0403"", ""sha"": ""ac18e635888d58fde5cce6e77f0fccf1e3d600a5"", ""last_modified"": ""2025-04-07 14:34:31+00:00"", ""created_at"": ""2025-04-07 14:34:15+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""tensorboard"", ""safetensors"", ""generated_from_trainer"", ""trl"", ""sft"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: results\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""widget_data"": null, ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='runs/Apr07_14-27-39_4c0a2cdcb50e/events.out.tfevents.1744036079.4c0a2cdcb50e.452.0', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-04-07 14:34:31+00:00"", ""cardData"": ""base_model: meta-llama/Llama-2-7b-chat-hf\nlibrary_name: transformers\nmodel_name: results\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67f3e2676427bb658ea2d043"", ""modelId"": ""dp0403/results"", ""usedStorage"": 129537696}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=dp0403/results&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdp0403%2Fresults%5D(%2Fdp0403%2Fresults)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
agoor97/Llama-2-7b-chat-hf-llama-2-7b-chat-guanaco,"---
library_name: transformers
license: apache-2.0
datasets:
- mlabonne/guanaco-llama2-1k
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
pipeline_tag: text-generation
---

# Model Card for Model ID

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.

- **Developed by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** [More Information Needed]

### Model Sources [optional]

<!-- Provide the basic links for the model. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

### Direct Use

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->

[More Information Needed]

### Downstream Use [optional]

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

## How to Get Started with the Model

Use the code below to get started with the model.

[More Information Needed]

## Training Details

### Training Data

<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

[More Information Needed]

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Preprocessing [optional]

[More Information Needed]


#### Training Hyperparameters

- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->

#### Speeds, Sizes, Times [optional]

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

[More Information Needed]

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data, Factors & Metrics

#### Testing Data

<!-- This should link to a Dataset Card if possible. -->

[More Information Needed]

#### Factors

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

[More Information Needed]

#### Metrics

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

[More Information Needed]

### Results

[More Information Needed]

#### Summary



## Model Examination [optional]

<!-- Relevant interpretability work for the model goes here -->

[More Information Needed]

## Environmental Impact

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** [More Information Needed]
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

## Technical Specifications [optional]

### Model Architecture and Objective

[More Information Needed]

### Compute Infrastructure

[More Information Needed]

#### Hardware

[More Information Needed]

#### Software

[More Information Needed]

## Citation [optional]

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Model Card Authors [optional]

[More Information Needed]

## Model Card Contact

[More Information Needed]","{""id"": ""agoor97/Llama-2-7b-chat-hf-llama-2-7b-chat-guanaco"", ""author"": ""agoor97"", ""sha"": ""4f94d3b698c191c568230bd5bdec5d7d9c11d17b"", ""last_modified"": ""2025-04-12 23:24:35+00:00"", ""created_at"": ""2025-04-12 23:23:25+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 0, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": ""transformers"", ""gguf"": null, ""inference"": null, ""tags"": [""transformers"", ""safetensors"", ""text-generation"", ""en"", ""dataset:mlabonne/guanaco-llama2-1k"", ""arxiv:1910.09700"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""endpoints_compatible"", ""region:us""], ""pipeline_tag"": ""text-generation"", ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mlabonne/guanaco-llama2-1k\nlanguage:\n- en\nlibrary_name: transformers\nlicense: apache-2.0\npipeline_tag: text-generation"", ""widget_data"": [{""text"": ""My name is Julien and I like to""}, {""text"": ""I like traveling by train because""}, {""text"": ""Paris is an amazing place to visit,""}, {""text"": ""Once upon a time,""}], ""model_index"": null, ""config"": {""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": null, ""security_repo_status"": null, ""lastModified"": ""2025-04-12 23:24:35+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- mlabonne/guanaco-llama2-1k\nlanguage:\n- en\nlibrary_name: transformers\nlicense: apache-2.0\npipeline_tag: text-generation"", ""transformersInfo"": {""auto_model"": ""AutoModel"", ""custom_class"": null, ""pipeline_tag"": null, ""processor"": null}, ""_id"": ""67faf5edb7df01f81e98d999"", ""modelId"": ""agoor97/Llama-2-7b-chat-hf-llama-2-7b-chat-guanaco"", ""usedStorage"": 34071347}",1,,0,,0,,0,,0,huggingface/InferenceSupport/discussions/new?title=agoor97/Llama-2-7b-chat-hf-llama-2-7b-chat-guanaco&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bagoor97%2FLlama-2-7b-chat-hf-llama-2-7b-chat-guanaco%5D(%2Fagoor97%2FLlama-2-7b-chat-hf-llama-2-7b-chat-guanaco)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
Tim419/Humpback_Myx,"---
license: apache-2.0
datasets:
- timdettmers/openassistant-guanaco
language:
- en
base_model:
- meta-llama/Llama-2-7b-chat-hf
---

## 🐋 Humpback-reproduce

This is a backward model _Myx_ for [Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf) reproduction.

This model (llama2 7B) is trained on the seed data ([openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) __ENGLISH DATA ONLY__) in a reversed order ((output, instruction) pairs {(yi, xi)}). 

In other words, the model is trained by using the output to predict the instruction. 

## 📜 Reference

```bibtex
@misc{li2023selfalignment,
    title={Self-Alignment with Instruction Backtranslation},
    author={Xian Li and Ping Yu and Chunting Zhou and Timo Schick and Luke Zettlemoyer and Omer Levy and Jason Weston and Mike Lewis},
    year={2023},
    eprint={2308.06259},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```","{""id"": ""Tim419/Humpback_Myx"", ""author"": ""Tim419"", ""sha"": ""f3e6f4afa8acff831d5b5cfc8d17045badcea275"", ""last_modified"": ""2025-04-15 01:13:59+00:00"", ""created_at"": ""2025-04-14 12:48:11+00:00"", ""private"": false, ""gated"": false, ""disabled"": false, ""downloads"": 27, ""downloads_all_time"": null, ""likes"": 0, ""library_name"": null, ""gguf"": null, ""inference"": null, ""tags"": [""safetensors"", ""llama"", ""en"", ""dataset:timdettmers/openassistant-guanaco"", ""arxiv:2308.06259"", ""base_model:meta-llama/Llama-2-7b-chat-hf"", ""base_model:finetune:meta-llama/Llama-2-7b-chat-hf"", ""license:apache-2.0"", ""region:us""], ""pipeline_tag"": null, ""mask_token"": null, ""trending_score"": null, ""card_data"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- timdettmers/openassistant-guanaco\nlanguage:\n- en\nlicense: apache-2.0"", ""widget_data"": null, ""model_index"": null, ""config"": {""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}, ""transformers_info"": null, ""siblings"": [""RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)"", ""RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)""], ""spaces"": [], ""safetensors"": {""parameters"": {""F16"": 6738415616}, ""total"": 6738415616}, ""security_repo_status"": null, ""lastModified"": ""2025-04-15 01:13:59+00:00"", ""cardData"": ""base_model:\n- meta-llama/Llama-2-7b-chat-hf\ndatasets:\n- timdettmers/openassistant-guanaco\nlanguage:\n- en\nlicense: apache-2.0"", ""transformersInfo"": null, ""_id"": ""67fd040bc2b0fae9b00f064c"", ""modelId"": ""Tim419/Humpback_Myx"", ""usedStorage"": 13477364475}",1,,0,,0,"https://huggingface.co/mradermacher/Humpback_Myx-GGUF, https://huggingface.co/mradermacher/Humpback_Myx-i1-GGUF",2,,0,huggingface/InferenceSupport/discussions/new?title=Tim419/Humpback_Myx&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTim419%2FHumpback_Myx%5D(%2FTim419%2FHumpback_Myx)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A,1
