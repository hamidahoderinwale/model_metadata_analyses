{
    "models": [
        {
            "model_id": "stabilityai/stable-diffusion-3-medium",
            "card": "---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\ntags:\n- text-to-image\n- stable-diffusion\n- diffusion-single-file\ninference: false\nextra_gated_prompt: >-\n  By clicking \"Agree\", you agree to the [License\n  Agreement](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE.md)\n  and acknowledge Stability AI's [Privacy\n  Policy](https://stability.ai/privacy-policy).\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: country\n  Organization or Affiliation: text\n  Receive email updates and promotions on Stability AI products, services, and research?:\n    type: select\n    options:\n    - 'Yes'\n    - 'No'\n  I agree to the License Agreement and acknowledge Stability AI's Privacy Policy: checkbox\nlanguage:\n- en\npipeline_tag: text-to-image\n---\n# Stable Diffusion 3 Medium\n![sd3 demo images](sd3demo.jpg)\n\n## Model\n\n![mmdit](mmdit.png)\n\n[Stable Diffusion 3 Medium](https://stability.ai/news/stable-diffusion-3-medium) is a Multimodal Diffusion Transformer (MMDiT) text-to-image model that features greatly improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\n\nFor more technical details, please refer to the [Research paper](https://stability.ai/news/stable-diffusion-3-research-paper).\n\nPlease note: this model is released under the Stability Community License. For Enterprise License visit Stability.ai or [contact us](https://stability.ai/enterprise) for commercial licensing details.\n\n\n\n### Model Description\n\n- **Developed by:** Stability AI\n- **Model type:** MMDiT text-to-image generative model\n- **Model Description:** This is a model that can be used to generate images based on text prompts. It is a Multimodal Diffusion Transformer\n(https://arxiv.org/abs/2403.03206) that uses three fixed, pretrained text encoders \n([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip), [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) and [T5-xxl](https://huggingface.co/google/t5-v1_1-xxl))\n\n### License\n\n- **Community License:** Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services. Read more: https://stability.ai/license\n- **For companies above this revenue threshold**: please contact us: https://stability.ai/enterprise\n\n\n### Model Sources\n\nFor local or self-hosted use, we recommend [ComfyUI](https://github.com/comfyanonymous/ComfyUI) for inference.\n\nStable Diffusion 3 Medium is available on our [Stability API Platform](https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1sd3/post). \n\nStable Diffusion 3 models and workflows are available on [Stable Assistant](https://stability.ai/stable-assistant) and on Discord via [Stable Artisan](https://stability.ai/stable-artisan). \n\n- **ComfyUI:** https://github.com/comfyanonymous/ComfyUI\n- **StableSwarmUI:** https://github.com/Stability-AI/StableSwarmUI\n- **Tech report:** https://stability.ai/news/stable-diffusion-3-research-paper\n- **Demo:** https://huggingface.co/spaces/stabilityai/stable-diffusion-3-medium\n- **Diffusers support:** https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers\n\n\n## Training Dataset\n\nWe used synthetic data and filtered publicly available data to train our models. The model was pre-trained on 1 billion images. The fine-tuning data includes 30M high-quality aesthetic images focused on specific visual content and style, as well as 3M preference data images.\n\n## File Structure\n```\n\u251c\u2500\u2500 comfy_example_workflows/\n\u2502   \u251c\u2500\u2500 sd3_medium_example_workflow_basic.json\n\u2502   \u251c\u2500\u2500 sd3_medium_example_workflow_multi_prompt.json\n\u2502   \u2514\u2500\u2500 sd3_medium_example_workflow_upscaling.json\n\u2502\n\u251c\u2500\u2500 text_encoders/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 clip_g.safetensors\n\u2502   \u251c\u2500\u2500 clip_l.safetensors\n\u2502   \u251c\u2500\u2500 t5xxl_fp16.safetensors\n\u2502   \u2514\u2500\u2500 t5xxl_fp8_e4m3fn.safetensors\n\u2502\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 sd3_medium.safetensors\n\u251c\u2500\u2500 sd3_medium_incl_clips.safetensors\n\u251c\u2500\u2500 sd3_medium_incl_clips_t5xxlfp8.safetensors\n\u2514\u2500\u2500 sd3_medium_incl_clips_t5xxlfp16.safetensors\n\n```\n\nWe have prepared three packaging variants of the SD3 Medium model, each equipped with the same set of MMDiT & VAE weights, for user convenience.\n\n* `sd3_medium.safetensors`  includes the MMDiT and VAE weights but does not include any text encoders.\n* `sd3_medium_incl_clips_t5xxlfp16.safetensors` contains all necessary weights, including fp16 version of the T5XXL text encoder.\n* `sd3_medium_incl_clips_t5xxlfp8.safetensors` contains all necessary weights, including fp8 version of the T5XXL text encoder, offering a balance between quality and resource requirements.\n* `sd3_medium_incl_clips.safetensors` includes all necessary weights except for the T5XXL text encoder. It requires minimal resources, but the model's performance will differ without the T5XXL text encoder.\n* The `text_encoders` folder contains three text encoders and their original model card links for user convenience. All components within the text_encoders folder (and their equivalents embedded in other packings)  are subject to their respective original licenses.\n* The `example_workfows` folder contains example comfy workflows.\n\n## Using with Diffusers\n\nThis repository corresponds to the original release weights. You can find the _diffusers_ compatible weights [here](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers). Make sure you upgrade to the latest version of diffusers: `pip install -U diffusers`. And then you can run:\n\n```python\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\n\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nimage = pipe(\n    \"A cat holding a sign that says hello world\",\n    negative_prompt=\"\",\n    num_inference_steps=28,\n    guidance_scale=7.0,\n).images[0]\nimage\n```\n\nRefer to [the documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_3) for more details on optimization and image-to-image support.\n\n## Uses\n\n### Intended Uses\n\nIntended uses include the following: \n* Generation of artworks and use in design and other artistic processes.\n* Applications in educational or creative tools.\n* Research on generative models, including understanding the limitations of generative models.\n\nAll uses of the model should be in accordance with our [Acceptable Use Policy](https://stability.ai/use-policy).\n\n### Out-of-Scope Uses\n\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\n\n## Safety\n\nAs part of our safety-by-design and responsible AI deployment approach, we implement safety measures throughout the development of our models, from the time we begin pre-training a model to the ongoing development, fine-tuning, and deployment of each model. We have implemented a number of safety mitigations that are intended to reduce the risk of severe harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.  \nFor more about our approach to Safety, please visit our [Safety page](https://stability.ai/safety).\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal and external red-teaming testing for specific, severe harms such as child sexual abuse and exploitation, extreme violence, and gore, sexually explicit content, and non-consensual nudity.  Testing was conducted primarily in English and may not cover all possible harms.  As with any model, the model may, at times, produce inaccurate, biased or objectionable responses to user prompts. \n\n### Risks identified and mitigations:\n\n* Harmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. The model may, at times, generate toxic or biased content.  All developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\n* Misuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our Acceptable Use Policy, including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\n* Privacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\n\n### Contact\n\nPlease report any issues with the model or contact us:\n\n* Safety issues:  safety@stability.ai\n* Security issues:  security@stability.ai\n* Privacy issues:  privacy@stability.ai \n* License and general: https://stability.ai/license\n* Enterprise license: https://stability.ai/enterprise",
            "metadata": "{\"id\": \"stabilityai/stable-diffusion-3-medium\", \"author\": \"stabilityai\", \"sha\": \"19b7f516efea082d257947e057e6f419e26fd497\", \"last_modified\": \"2024-08-12 12:37:42+00:00\", \"created_at\": \"2024-05-30 07:38:13+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 16022, \"downloads_all_time\": null, \"likes\": 4758, \"library_name\": \"diffusion-single-file\", \"gguf\": null, \"inference\": \"warm\", \"tags\": [\"diffusion-single-file\", \"text-to-image\", \"stable-diffusion\", \"en\", \"arxiv:2403.03206\", \"license:other\", \"region:us\"], \"pipeline_tag\": \"text-to-image\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"language:\\n- en\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-image\\ntags:\\n- text-to-image\\n- stable-diffusion\\n- diffusion-single-file\\ninference: false\\nextra_gated_prompt: By clicking \\\"Agree\\\", you agree to the [License Agreement](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE.md)\\n  and acknowledge Stability AI's [Privacy Policy](https://stability.ai/privacy-policy).\\nextra_gated_fields:\\n  Name: text\\n  Email: text\\n  Country: country\\n  Organization or Affiliation: text\\n  Receive email updates and promotions on Stability AI products, services, and research?:\\n    type: select\\n    options:\\n    - 'Yes'\\n    - 'No'\\n  I agree to the License Agreement and acknowledge Stability AI's Privacy Policy: checkbox\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='comfy_example_workflows/sd3_medium_example_workflow_basic.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='comfy_example_workflows/sd3_medium_example_workflow_multi_prompt.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='comfy_example_workflows/sd3_medium_example_workflow_upscaling.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (1).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (10).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (2).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (3).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (4).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (5).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (6).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (7).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (8).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='demo_images/demo (9).png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mmdit.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3_medium.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3_medium_incl_clips.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3_medium_incl_clips_t5xxlfp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3_medium_incl_clips_t5xxlfp8.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3demo.jpg', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='sd3demo_prompts.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoders/README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoders/clip_g.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoders/clip_l.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoders/t5xxl_fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoders/t5xxl_fp8_e4m3fn.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"stabilityai/stable-diffusion-3-medium\", \"markmagic/Stable-Diffusion-3\", \"jasperai/flash-sd3\", \"markmagic/Stable-Diffusion-3-FREE\", \"kadirnar/Stable-Diffusion-3\", \"ameerazam08/SD-3-Medium-GPU\", \"alfredplpl/sd3-with-LLM\", \"madebyollin/sd3-with-taesd3-previews\", \"Nymbo/Stable-Diffusion-3\", \"aipicasso/emi-3\", \"rphrp1985/stable-diffusion-3-medium\", \"FallnAI/DiffusersUI\", \"kevinwang676/Diffutoon\", \"awacke1/ImageMultiagentSystem\", \"9kopb/stabilityai-stable-diffusion-3-medium\", \"philipp-zettl/stable-diffusion-3-medium\", \"loki0807/stabilityai-stable-diffusion-3-medium\", \"mocreate/stabilityai-stable-diffusion-3-medium\", \"Javedalam/StabilityAI-SD-V3-Medium_GPU\", \"Wenes/stabilityai-stable-diffusion-3-medium\", \"SanthoshkumarK/stabilityai-stable-diffusion-3-medium\", \"bokkbokk/stabilityai-stable-diffusion-3-medium\", \"FallnAI/stabilityai-stable-diffusion-3-medium\", \"sammyview80/stability-logo\", \"Walid-Ahmed/image_gen\", \"kasper-boy/Transform_Ordinary_Photos_into_Extraordinary_Art\", \"yangtb24/sone\", \"GPUModelSpotlight/Analysis-Of-Image-Song-Video-Prompts\", \"yangtb24/sone-latest\", \"AguaL/Illustrious-xl-early-release-v0\", \"hf-demo-linux/sili\", \"K00B404/FLUX-Wallpaper-HD-Maker_p\", \"hkxiaoyao/sili\", \"jry241/sili\", \"sammyview80/flask_rm_bg-cp\", \"DanHX/KHome\", \"saneowl/stabilityai-stable-diffusion-3-medium\", \"Johey/stabilityai-stable-diffusion-3-medium\", \"saneowl/stabilityai-stable-diffusion-3-medium-2\", \"froggiddin/stabilityai-stable-diffusion-3-medium\", \"KennyOry/stabilityai-stable-diffusion-3-medium\", \"qsdreams/stabilityai-stable-diffusion-3-medium\", \"ValeriOkrd/stabilityai-stable-diffusion-3-medium\", \"amazing666/stabilityai-stable-diffusion-3-medium\", \"Bot314dor/stabilityai-stable-diffusion-3-medium\", \"goodmai/stabilityai-stable-diffusion-3-medium\", \"Durev/stabilityai-stable-diffusion-3-medium\", \"Musimo/stabilityai-stable-diffusion-3-medium\", \"IwanY/stabilityai-stable-diffusion-3-medium\", \"immelstorun/stabilityai-stable-diffusion-3-medium\", \"guloff/stabilityai-stable-diffusion-3-medium\", \"dsdfstrger/stabilityai-stable-diffusion-3-medium\", \"KarmaAsatos/stabilityai-stable-diffusion-3-medium\", \"kjkjte/stabilityai-stable-diffusion-3-medium\", \"Shurley/stabilityai-stable-diffusion-3-medium\", \"davideleos/stabilityai-stable-diffusion-3-medium\", \"leobora/stabilityai-stable-diffusion-3-medium\", \"George12123009/stabilityai-stable-diffusion-3-medium\", \"dezole/stabilityai-stable-diffusion-3-medium\", \"Toniska/stabilityai-stable-diffusion-3-medium\", \"ewnewq/stabilityai-stable-diffusion-3-medium\", \"zacoje/stabilityai-stable-diffusion-3-medium\", \"Evgen58/stabilityai-stable-diffusion-3-medium\", \"Beeniebeen/stabilityai-stable-diffusion-3-medium\", \"lzzzzl/PS2-Filter-AI\", \"zhtr/stabilityai-stable-diffusion-3-medium\", \"Hothail/stabilityai-stable-diffusion-3-medium\", \"3ll107/stabilityai-stable-diffusion-3-medium\", \"FullyRendered/stabilityai-stable-diffusion-3-medium\", \"RonBiswas/stabilityai-stable-diffusion-3-medium\", \"broccy23/stabilityai-stable-diffusion-3-medium\", \"meraposter/stabilityai-stable-diffusion-3-medium\", \"Vagnus/stabilityai-stable-diffusion-3-medium\", \"sbdtadmin/stabilityai-stable-diffusion-3-medium\", \"Abhisksks/stabilityai-stable-diffusion-3-medium\", \"yu65701064/stabilityai-stable-diffusion-3-medium\", \"chama-x/stabilityai-stable-diffusion-3-medium\", \"Alex-Yan/stabilityai-stable-diffusion-3-medium\", \"tomxue/stabilityai-stable-diffusion-3-medium\", \"gergre7654/stabilityai-stable-diffusion-3-medium\", \"andiia/stabilityai-stable-diffusion-3-medium\", \"Creamyy/stabilityai-stable-diffusion-3-medium\", \"Nehal07/stabilityai-stable-diffusion-3-medium\", \"Romel123456/stabilityai-stable-diffusion-3-medium\", \"antonioevans/stabilityai-stable-diffusion-3-medium\", \"Abhishekabysm/stabilityai-stable-diffusion-3-medium\", \"ColinLee111/stabilityai-stable-diffusion-3-medium\", \"Henry1218/stabilityai-stable-diffusion-3-medium\", \"somvedaai/stabilityai-stable-diffusion-3-medium\", \"bojanstef/stabilityai-stable-diffusion-3-medium\", \"mindfox/stabilityai-stable-diffusion-3-medium\", \"paddy82/stabilityai-stable-diffusion-3-medium\", \"AayKC/stabilityai-stable-diffusion-3-medium\", \"valmont999/stabilityai-stable-diffusion-3-medium\", \"jitkost/stabilityai-stable-diffusion-3-medium\", \"aichampions/stable-diffusion-3-medium\", \"ozamyatin/stabilityai-stable-diffusion-3-medium\", \"Borboris/stabilityai-stable-diffusion-3-medium\", \"anakin52/stabilityai-stable-diffusion-3-medium\", \"Yura2112/ssd3medium\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-12 12:37:42+00:00\", \"cardData\": \"language:\\n- en\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-image\\ntags:\\n- text-to-image\\n- stable-diffusion\\n- diffusion-single-file\\ninference: false\\nextra_gated_prompt: By clicking \\\"Agree\\\", you agree to the [License Agreement](https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE.md)\\n  and acknowledge Stability AI's [Privacy Policy](https://stability.ai/privacy-policy).\\nextra_gated_fields:\\n  Name: text\\n  Email: text\\n  Country: country\\n  Organization or Affiliation: text\\n  Receive email updates and promotions on Stability AI products, services, and research?:\\n    type: select\\n    options:\\n    - 'Yes'\\n    - 'No'\\n  I agree to the License Agreement and acknowledge Stability AI's Privacy Policy: checkbox\", \"transformersInfo\": null, \"_id\": \"66582ce552e752b1a72b985d\", \"modelId\": \"stabilityai/stable-diffusion-3-medium\", \"usedStorage\": 67448764458}",
            "depth": 0,
            "children": [
                "https://huggingface.co/rain1011/pyramid-flow-sd3",
                "https://huggingface.co/OPPOer/MultilingualSD3-adapter",
                "https://huggingface.co/Dreyyt/Analog_Madness_Realistic_model",
                "https://huggingface.co/vdo/pyramid-flow-sd3",
                "https://huggingface.co/Ziyaad30/Pyramid-Flow-sd3",
                "https://huggingface.co/boryanagm/beatrix_LoRA"
            ],
            "children_count": 6,
            "adapters": [
                "https://huggingface.co/jasperai/flash-sd3",
                "https://huggingface.co/kttgms/test1",
                "https://huggingface.co/Nestorthera/Titanic_Survival_Predictor",
                "https://huggingface.co/gx123/test-model-1",
                "https://huggingface.co/gx123/my-gx-sd-3",
                "https://huggingface.co/Tsukasa0706/Char_Hutao",
                "https://huggingface.co/Monkey23434242/Garfieldcomics",
                "https://huggingface.co/Felix346/Rahul",
                "https://huggingface.co/adedolllapo/MOBOLAJI",
                "https://huggingface.co/Rgbeast5678/Nikoro",
                "https://huggingface.co/Madyarfeyzi84/Z2",
                "https://huggingface.co/Adam3/Michael-Kranz",
                "https://huggingface.co/Hyype/Brandao085",
                "https://huggingface.co/cali72mero/ai",
                "https://huggingface.co/abhirajputnikku7/A",
                "https://huggingface.co/ismalee/ismu1",
                "https://huggingface.co/gonzalu/YFG-SushiStyle",
                "https://huggingface.co/Shinhati2023/Jegan_Style",
                "https://huggingface.co/ReverseTorque/Jejegev",
                "https://huggingface.co/Goku355/Austin2024",
                "https://huggingface.co/dexadeca99/codax99",
                "https://huggingface.co/Artedaut/Parser",
                "https://huggingface.co/Mahdi01/Dena_sd3",
                "https://huggingface.co/dexadeca99/3dlorarcane",
                "https://huggingface.co/SonicMations/Mason-Parkinson-FIXED",
                "https://huggingface.co/Dolphinman2345/Eris_1",
                "https://huggingface.co/hiperfire/outfit",
                "https://huggingface.co/RAJKOT/1097145198",
                "https://huggingface.co/Mari23/PistolPeteGoofTroop",
                "https://huggingface.co/ekato/AiOtsuka",
                "https://huggingface.co/ekato/ErikaIkuta",
                "https://huggingface.co/ekato/adieu",
                "https://huggingface.co/mol09/ari",
                "https://huggingface.co/ekato/Acane",
                "https://huggingface.co/SonicandTails/Perfect_Feet",
                "https://huggingface.co/SonicandTails/Perfect_Feet_V2",
                "https://huggingface.co/ekato/noa",
                "https://huggingface.co/ekato/MoneKamishiraishi",
                "https://huggingface.co/ElVallle24/RPGGAMES",
                "https://huggingface.co/Cptthunder235/Over-the-mouth-gag",
                "https://huggingface.co/ekato/TOMOO",
                "https://huggingface.co/Mujeeb603/SD3-medium-Geometry-Diagrams-Lora-1",
                "https://huggingface.co/ElTucuGardella/gastonlora",
                "https://huggingface.co/ekato/ZUMA",
                "https://huggingface.co/ekato/Aimyon",
                "https://huggingface.co/ekato/n_buna",
                "https://huggingface.co/Thihasoe/BabyDragon",
                "https://huggingface.co/ekato/tuki",
                "https://huggingface.co/Yuukinarak/ELEGANT",
                "https://huggingface.co/00173R/0.12",
                "https://huggingface.co/ekato/hikaruyamamoto",
                "https://huggingface.co/AnsuFati10/ti",
                "https://huggingface.co/ekato/ainatheend",
                "https://huggingface.co/jimmyturlack/qr_code",
                "https://huggingface.co/ekato/harukafukuhara",
                "https://huggingface.co/pieman321/jill",
                "https://huggingface.co/leonel4rd/nijiv6v4",
                "https://huggingface.co/C0reami/XLsusjelomixV10_",
                "https://huggingface.co/ekato/moeka",
                "https://huggingface.co/cyberduck465/stable-diffusion",
                "https://huggingface.co/Raylowx/Loraylow77",
                "https://huggingface.co/sumanthkumar/lorali",
                "https://huggingface.co/zerocool88/Skel",
                "https://huggingface.co/bugsyjazz/Korilora",
                "https://huggingface.co/Anonx7/anonx-annabell",
                "https://huggingface.co/DerenXd/2HI0-XL",
                "https://huggingface.co/Davidyulianto/Realistic_ChikMix",
                "https://huggingface.co/Luo-Yihong/TDM_sd3_lora"
            ],
            "adapters_count": 68,
            "quantized": [
                "https://huggingface.co/second-state/stable-diffusion-3-medium-GGUF",
                "https://huggingface.co/city96/stable-diffusion-3-medium-gguf",
                "https://huggingface.co/ND911/stable-diffusion-3.5-medium-GGUF",
                "https://huggingface.co/stabilityai/stable-diffusion-3-medium_amdgpu"
            ],
            "quantized_count": 4,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "AguaL/Illustrious-xl-early-release-v0",
                "GPUModelSpotlight/Analysis-Of-Image-Song-Video-Prompts",
                "Walid-Ahmed/image_gen",
                "aipicasso/emi-3",
                "alfredplpl/sd3-with-LLM",
                "hf-demo-linux/sili",
                "kasper-boy/Transform_Ordinary_Photos_into_Extraordinary_Art",
                "madebyollin/sd3-with-taesd3-previews",
                "rphrp1985/stable-diffusion-3-medium",
                "stabilityai/stable-diffusion-3-medium",
                "yangtb24/sone",
                "yangtb24/sone-latest"
            ],
            "spaces_count": 12
        },
        {
            "model_id": "rain1011/pyramid-flow-sd3",
            "card": "---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-video\ntags:\n- image-to-video\n- sd3\n\n---\n\n# \u26a1\ufe0fPyramid Flow SD3\u26a1\ufe0f\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page \u2728]](https://pyramid-flow.github.io) [[Code \ud83d\ude80]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model \u26a1\ufe0f]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [[demo \ud83e\udd17](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow)]\n\nThis is the model repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## News\n\n* `2024.10.29` \u26a1\ufe0f\u26a1\ufe0f\u26a1\ufe0f We release [training code](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training) and [new model checkpoints](https://huggingface.co/rain1011/pyramid-flow-miniflux) with FLUX structure trained from scratch.\n\n  > We have switched the model structure from SD3 to a mini FLUX to fix human structure issues, please try our 1024p image checkpoint and 384p video checkpoint. We will release 768p video checkpoint in a few days.\n\n* `2024.10.11`  \ud83e\udd17\ud83e\udd17\ud83e\udd17 [Hugging Face demo](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow) is available. Thanks [@multimodalart](https://huggingface.co/multimodalart) for the commit! \n\n* `2024.10.10`  \ud83d\ude80\ud83d\ude80\ud83d\ude80 We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Installation\n\nWe recommend setting up the environment with conda. The codebase currently uses Python 3.8.10 and PyTorch 2.1.2, and we are actively working to support a wider range of versions.\n\n```bash\ngit clone https://github.com/jy0205/Pyramid-Flow\ncd Pyramid-Flow\n\n# create env using conda\nconda create -n pyramid python==3.8.10\nconda activate pyramid\npip install -r requirements.txt\n```\n\nThen, download the model from [Huggingface](https://huggingface.co/rain1011) (there are two variants: [miniFLUX](https://huggingface.co/rain1011/pyramid-flow-miniflux) or [SD3](https://huggingface.co/rain1011/pyramid-flow-sd3)). The miniFLUX models support 1024p image and 384p video generation, and the SD3-based models support 768p and 384p video generation. The 384p checkpoint generates 5-second video at 24FPS, while the 768p checkpoint generates up to 10-second video at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = 'PATH'   # The local directory to save downloaded checkpoint\nsnapshot_download(\"rain1011/pyramid-flow-sd3\", local_dir=model_path, local_dir_use_symlinks=False, repo_type='model')\n```\n\n## Usage\n\nFor inference, we provide Gradio demo, single-GPU, multi-GPU, and Apple Silicon inference code, as well as VRAM-efficient features such as CPU offloading. Please check our [code repository](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference) for usage.\n\nBelow is a simplified two-step usage procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16 (not support fp16 yet)\n\nmodel = PyramidDiTForVideoGeneration(\n    'PATH',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant='diffusion_transformer_768p',     # 'diffusion_transformer_384p'\n)\n\nmodel.vae.enable_tiling()\n# model.vae.to(\"cuda\")\n# model.dit.to(\"cuda\")\n# model.text_encoder.to(\"cuda\")\n\n# if you're not using sequential offloading bellow uncomment the lines above ^\nmodel.enable_sequential_cpu_offload()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = \"A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame, set it to 7 for 384p variant\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type=\"pil\",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, \"./text_to_video_sample.mp4\", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open('assets/the_great_wall.jpg').convert(\"RGB\").resize((1280, 768))\nprompt = \"FPV flying over the Great Wall\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type=\"pil\",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, \"./image_to_video_sample.mp4\", fps=24)\n```\n\n## Usage tips\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/waves.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/rail.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```",
            "metadata": "{\"id\": \"rain1011/pyramid-flow-sd3\", \"author\": \"rain1011\", \"sha\": \"a47c8ee2cbeda5813be9bc2bf67bc52ee6698ab3\", \"last_modified\": \"2024-10-30 03:55:43+00:00\", \"created_at\": \"2024-10-09 12:44:41+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 825, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"image-to-video\", \"sd3\", \"text-to-video\", \"arxiv:2410.05954\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"license:other\", \"region:us\"], \"pipeline_tag\": \"text-to-video\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-video\\ntags:\\n- image-to-video\\n- sd3\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/spiece.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"Pyramid-Flow/pyramid-flow\", \"sofianhw/pyramid-flow\", \"AI-Platform/pyramid-flow\", \"yasserrmd/pyramid-flow\", \"Nymbo/pyramid-flow\", \"melromyeah/pyramid-flow\", \"snyderline/pyramid-flow\", \"Raider606/pyramid-flow\", \"Slayerfryed44/pyramid-flow\", \"victim2910/pyramid-flow\", \"diky13/pyramid-flow\", \"coollsd/pyramid-flow\", \"waloneai/pyramid-flow\", \"qrqode/pyramid-flow\", \"hakem32/pyramid-flow\", \"Igogogor/pyramid-flow\", \"henry1962/pyramid-flow\", \"waloneai/wlmov\", \"carpit680/pyramid-flow\", \"lilmeaty/pyramid-flow\", \"adl5423/VideoGeneratorCodaKid\", \"thesab/pyramid-flow-hf\", \"aripsam44/pyramid-flow\", \"sironagasuyagi/Pyramid-Flow\", \"shaiws/pyramid-flow\", \"K00B404/vid_maybe\", \"Gathubaze/pyramid-flow\", \"jcudit/pyramid-flow\", \"CShah2218/Text-Image-To-Video\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-30 03:55:43+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-video\\ntags:\\n- image-to-video\\n- sd3\", \"transformersInfo\": null, \"_id\": \"67067ab9dc08442956a2d518\", \"modelId\": \"rain1011/pyramid-flow-sd3\", \"usedStorage\": 52046558470}",
            "depth": 1,
            "children": [
                "https://huggingface.co/SeanScripts/pyramid-flow-sd3-bf16",
                "https://huggingface.co/Flit/g"
            ],
            "children_count": 2,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [
                "https://huggingface.co/bkbj/Atest"
            ],
            "merges_count": 1,
            "spaces": [
                "AI-Platform/pyramid-flow",
                "Nymbo/pyramid-flow",
                "Pyramid-Flow/pyramid-flow",
                "Raider606/pyramid-flow",
                "Slayerfryed44/pyramid-flow",
                "coollsd/pyramid-flow",
                "diky13/pyramid-flow",
                "huggingface/InferenceSupport/discussions/new?title=rain1011/pyramid-flow-sd3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Brain1011%2Fpyramid-flow-sd3%5D(%2Frain1011%2Fpyramid-flow-sd3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",
                "melromyeah/pyramid-flow",
                "snyderline/pyramid-flow",
                "sofianhw/pyramid-flow",
                "victim2910/pyramid-flow",
                "yasserrmd/pyramid-flow"
            ],
            "spaces_count": 13
        },
        {
            "model_id": "SeanScripts/pyramid-flow-sd3-bf16",
            "card": "---\nbase_model:\n- rain1011/pyramid-flow-sd3\npipeline_tag: text-to-video\nlibrary_name: diffusers\n---\n\nConverted to bfloat16 from [rain1011/pyramid-flow-sd3](https://huggingface.co/rain1011/pyramid-flow-sd3). Use the text encoders and tokenizers from that repo (or from SD3), no point reuploading them over and over unchanged.\n\nInference code is available here: [github.com/jy0205/Pyramid-Flow](https://github.com/jy0205/Pyramid-Flow/tree/main).\n\nBoth 384p and 768p work on 24 GB VRAM. For 16 steps (5 second video), 384p takes a little over a minute on a 3090, and 768p takes about 7 minutes. For 31 steps (10 second video), 384p took about 10 minutes.\n\nI highly recommend using `cpu_offloading=True` when generating, unless you have more than 24 GB VRAM.",
            "metadata": "{\"id\": \"SeanScripts/pyramid-flow-sd3-bf16\", \"author\": \"SeanScripts\", \"sha\": \"18324600d73741033657f28004c63c9bac51832f\", \"last_modified\": \"2024-10-12 05:32:42+00:00\", \"created_at\": \"2024-10-10 20:32:17+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 11, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"text-to-video\", \"base_model:rain1011/pyramid-flow-sd3\", \"base_model:finetune:rain1011/pyramid-flow-sd3\", \"region:us\"], \"pipeline_tag\": \"text-to-video\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- rain1011/pyramid-flow-sd3\\nlibrary_name: diffusers\\npipeline_tag: text-to-video\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-12 05:32:42+00:00\", \"cardData\": \"base_model:\\n- rain1011/pyramid-flow-sd3\\nlibrary_name: diffusers\\npipeline_tag: text-to-video\", \"transformersInfo\": null, \"_id\": \"670839d19ae661eabe563260\", \"modelId\": \"SeanScripts/pyramid-flow-sd3-bf16\", \"usedStorage\": 9010845134}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=SeanScripts/pyramid-flow-sd3-bf16&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSeanScripts%2Fpyramid-flow-sd3-bf16%5D(%2FSeanScripts%2Fpyramid-flow-sd3-bf16)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Flit/g",
            "card": "---\nlicense: mit\ndatasets:\n- neuralwork/arxiver\nlanguage:\n- ab\nmetrics:\n- bleu\nbase_model:\n- rain1011/pyramid-flow-sd3\nnew_version: stabilityai/stable-diffusion-3.5-large\npipeline_tag: translation\nlibrary_name: allennlp\ntags:\n- finance\n---",
            "metadata": "{\"id\": \"Flit/g\", \"author\": \"Flit\", \"sha\": \"9f799d17fb1d58b2d39f3f973e8467ed7ff20c43\", \"last_modified\": \"2024-10-27 17:38:08+00:00\", \"created_at\": \"2024-10-27 17:37:40+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"allennlp\", \"gguf\": null, \"inference\": null, \"tags\": [\"allennlp\", \"finance\", \"translation\", \"ab\", \"dataset:neuralwork/arxiver\", \"base_model:rain1011/pyramid-flow-sd3\", \"base_model:finetune:rain1011/pyramid-flow-sd3\", \"license:mit\", \"region:us\"], \"pipeline_tag\": \"translation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- rain1011/pyramid-flow-sd3\\ndatasets:\\n- neuralwork/arxiver\\nlanguage:\\n- ab\\nlibrary_name: allennlp\\nlicense: mit\\nmetrics:\\n- bleu\\npipeline_tag: translation\\ntags:\\n- finance\\nnew_version: stabilityai/stable-diffusion-3.5-large\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-27 17:38:08+00:00\", \"cardData\": \"base_model:\\n- rain1011/pyramid-flow-sd3\\ndatasets:\\n- neuralwork/arxiver\\nlanguage:\\n- ab\\nlibrary_name: allennlp\\nlicense: mit\\nmetrics:\\n- bleu\\npipeline_tag: translation\\ntags:\\n- finance\\nnew_version: stabilityai/stable-diffusion-3.5-large\", \"transformersInfo\": null, \"_id\": \"671e7a64a2d95e235ac86816\", \"modelId\": \"Flit/g\", \"usedStorage\": 0}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Flit/g&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BFlit%2Fg%5D(%2FFlit%2Fg)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "OPPOer/MultilingualSD3-adapter",
            "card": "---\nlicense: apache-2.0\nlanguage:\n- zh\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-image\n---\n\n\n![FLUX.1 [schnell] Grid](./PEA-Diffusion.png)\n\n`MultilingualSD3-adapter` is a multilingual adapter tailored for the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium). Originating from an ECCV 2024 paper titled [PEA-Diffusion](https://arxiv.org/abs/2311.17086). The open-source code is available at https://github.com/OPPO-Mente-Lab/PEA-Diffusion.\n\n\n# Usage\nWe used the multilingual encoder [umt5-xxl](https://huggingface.co/google/umt5-xxl),[Mul-OpenCLIP](https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k) and [HunyuanDiT_CLIP](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/t2i). We implemented a reverse denoising process for distillation training. \n\n\n## `MultilingualSD3`\n\n\n```python\nimport os\nimport torch\nimport torch.nn as nn\n\nfrom typing import Any, Callable, Dict, List, Optional, Union\nimport inspect\nfrom diffusers.models.transformers import SD3Transformer2DModel\nfrom diffusers.image_processor import VaeImageProcessor\nfrom diffusers.schedulers import FlowMatchEulerDiscreteScheduler\nfrom diffusers import AutoencoderKL\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom transformers import T5Tokenizer,T5EncoderModel,BertModel, BertTokenizer\nimport open_clip\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim=1024, out_dim=2048, hidden_dim=2048, out_dim1=4096, use_residual=True):\n        super().__init__()\n        if use_residual:\n            assert in_dim == out_dim\n        self.layernorm = nn.LayerNorm(in_dim)\n        self.projector = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.GELU(),\n            nn.Linear(hidden_dim, hidden_dim, bias=False),\n            nn.GELU(),\n            nn.Linear(hidden_dim, hidden_dim, bias=False),\n            nn.GELU(),\n            nn.Linear(hidden_dim, out_dim, bias=False),\n        )\n        self.fc = nn.Linear(out_dim, out_dim1)\n        self.use_residual = use_residual\n    def forward(self, x):\n        residual = x\n        x = self.layernorm(x)\n        x = self.projector(x)\n        x2 = nn.GELU()(x)\n        x2 = self.fc(x2)\n        return x2\n\nclass Transformer(nn.Module):\n    def __init__(self, d_model,  n_heads, out_dim1, out_dim2,num_layers=1) -> None:\n        super().__init__()\n\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=2048, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n        self.linear1 = nn.Linear(d_model, out_dim1)\n        self.linear2 = nn.Linear(d_model, out_dim2)\n    \n    def forward(self, x):\n        x = self.transformer_encoder(x)\n        x1 = self.linear1(x)\n        x1 = torch.mean(x1,1)\n        x2 = self.linear2(x)\n        return x1,x2\n\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\ndef retrieve_timesteps(\n    scheduler,\n    num_inference_steps: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    timesteps: Optional[List[int]] = None,\n    sigmas: Optional[List[float]] = None,\n    **kwargs,\n):\n    if timesteps is not None and sigmas is not None:\n        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n    if timesteps is not None:\n        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n        if not accepts_timesteps:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n        num_inference_steps = len(timesteps)\n    elif sigmas is not None:\n        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n        if not accept_sigmas:\n            raise ValueError(\n                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n            )\n        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n        num_inference_steps = len(timesteps)\n    else:\n        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n        timesteps = scheduler.timesteps\n    return timesteps, num_inference_steps\n\nclass StableDiffusionTest():\n    def __init__(self,model_path,text_encoder_path,text_encoder_path1,text_encoder_path2,proj_path,proj_t5_path):\n        super().__init__()\n        self.transformer = SD3Transformer2DModel.from_pretrained(model_path, subfolder=\"transformer\",torch_dtype=dtype).to(device)\n        self.vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\").to(device,dtype=dtype)\n        self.scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n\n        self.vae_scale_factor = (\n            2 ** (len(self.vae.config.block_out_channels) - 1) if hasattr(self, \"vae\") and self.vae is not None else 8\n        )\n        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n        self.default_sample_size = (\n            self.transformer.config.sample_size\n            if hasattr(self, \"transformer\") and self.transformer is not None\n            else 128\n        )\n\n        self.text_encoder_t5 = T5EncoderModel.from_pretrained(text_encoder_path).to(device,dtype=dtype)\n        self.tokenizer_t5 = T5Tokenizer.from_pretrained(text_encoder_path)\n        self.text_encoder = BertModel.from_pretrained(f\"{text_encoder_path1}/clip_text_encoder\", False, revision=None).to(device,dtype=dtype)\n        self.tokenizer = BertTokenizer.from_pretrained(f\"{text_encoder_path1}/tokenizer\")\n\n        self.text_encoder2, _, _ = open_clip.create_model_and_transforms('xlm-roberta-large-ViT-H-14', pretrained=text_encoder_path2)\n        self.tokenizer2 = open_clip.get_tokenizer('xlm-roberta-large-ViT-H-14')\n        self.text_encoder2.text.output_tokens = True\n        self.text_encoder2 = self.text_encoder2.to(device,dtype=dtype)\n\n        self.proj = MLP(2048, 2048, 2048, 4096, use_residual=False).to(device,dtype=dtype)\n        self.proj.load_state_dict(torch.load(proj_path, map_location=\"cpu\"))\n        self.proj_t5 = Transformer(d_model=4096, n_heads=8, out_dim1=2048, out_dim2=4096).to(device,dtype=dtype)\n        self.proj_t5.load_state_dict(torch.load(proj_t5_path, map_location=\"cpu\"))\n\n    def encode_prompt(self, prompt, device, do_classifier_free_guidance=True, negative_prompt=None):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n        text_input_ids_t5 = self.tokenizer_t5(\n            prompt,\n            padding=\"max_length\",\n            max_length=77,\n            truncation=True,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n        ).input_ids.to(device)\n\n        text_embeddings = self.text_encoder_t5(text_input_ids_t5)\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=77,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        input_ids = text_inputs.input_ids.to(device)\n        attention_mask = text_inputs.attention_mask.to(device)\n        encoder_hidden_states  = self.text_encoder(input_ids,attention_mask=attention_mask)[0]\n        text_input_ids = self.tokenizer2(prompt).to(device)\n        _,encoder_hidden_states2  = self.text_encoder2.encode_text(text_input_ids)\n        encoder_hidden_states = torch.cat([encoder_hidden_states, encoder_hidden_states2], dim=-1)\n\n        encoder_hidden_states_t5 = text_embeddings[0]\n        encoder_hidden_states = self.proj(encoder_hidden_states)\n\n        add_text_embeds,encoder_hidden_states_t5 = self.proj_t5(encoder_hidden_states_t5.half())\n        prompt_embeds = torch.cat([encoder_hidden_states, encoder_hidden_states_t5], dim=-2) \n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            else:\n                uncond_tokens = negative_prompt\n            text_input_ids_t5 = self.tokenizer_t5(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=77,\n                truncation=True,\n                add_special_tokens=False,\n                return_tensors=\"pt\",\n            ).input_ids.to(device)\n\n            text_embeddings = self.text_encoder_t5(text_input_ids_t5)\n            text_inputs = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=77,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            input_ids = text_inputs.input_ids.to(device)\n            attention_mask = text_inputs.attention_mask.to(device)\n            encoder_hidden_states  = self.text_encoder(input_ids,attention_mask=attention_mask)[0]\n\n            text_input_ids = self.tokenizer2(uncond_tokens).to(device)\n            _,encoder_hidden_states2  = self.text_encoder2.encode_text(text_input_ids)\n            encoder_hidden_states = torch.cat([encoder_hidden_states, encoder_hidden_states2], dim=-1)\n\n            encoder_hidden_states_t5 = text_embeddings[0]\n            encoder_hidden_states_uncond = self.proj(encoder_hidden_states)\n \n            add_text_embeds_uncond,encoder_hidden_states_t5_uncond = self.proj_t5(encoder_hidden_states_t5.half())\n            prompt_embeds_uncond = torch.cat([encoder_hidden_states_uncond, encoder_hidden_states_t5_uncond], dim=-2)\n\n            prompt_embeds = torch.cat([prompt_embeds_uncond, prompt_embeds], dim=0)\n            pooled_prompt_embeds = torch.cat([add_text_embeds_uncond, add_text_embeds], dim=0)\n\n        return prompt_embeds,pooled_prompt_embeds\n\n\n    def prepare_latents(\n        self,\n        batch_size,\n        num_channels_latents,\n        height,\n        width,\n        dtype,\n        device,\n        generator,\n        latents=None,\n    ):\n        if latents is not None:\n            return latents.to(device=device, dtype=dtype)\n\n        shape = (\n            batch_size,\n            num_channels_latents,\n            int(height) // self.vae_scale_factor,\n            int(width) // self.vae_scale_factor,\n        )\n\n        if isinstance(generator, list) and len(generator) != batch_size:\n            raise ValueError(\n                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n            )\n\n        latents = torch.randn(shape, generator=generator, dtype=dtype).to(device)\n\n        return latents\n\n    @property\n    def guidance_scale(self):\n        return self._guidance_scale\n\n    @property\n    def clip_skip(self):\n        return self._clip_skip\n\n    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n    # corresponds to doing no classifier free guidance.\n    @property\n    def do_classifier_free_guidance(self):\n        return self._guidance_scale > 1\n\n    @property\n    def joint_attention_kwargs(self):\n        return self._joint_attention_kwargs\n\n    @property\n    def num_timesteps(self):\n        return self._num_timesteps\n\n    @property\n    def interrupt(self):\n        return self._interrupt\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        prompt_3: Optional[Union[str, List[str]]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 28,\n        timesteps: List[int] = None,\n        guidance_scale: float = 7.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        negative_prompt_3: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n        clip_skip: Optional[int] = None,\n        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n    ):\n        height = height or self.default_sample_size * self.vae_scale_factor\n        width = width or self.default_sample_size * self.vae_scale_factor\n\n        self._guidance_scale = guidance_scale\n        self._clip_skip = clip_skip\n        self._joint_attention_kwargs = joint_attention_kwargs\n        self._interrupt = False\n\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n\n        prompt_embeds,pooled_prompt_embeds = self.encode_prompt(prompt, device)\n\n        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n        self._num_timesteps = len(timesteps)\n\n        num_channels_latents = self.transformer.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        for i, t in tqdm(enumerate(timesteps)):\n            if self.interrupt:\n                continue\n            latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n            timestep = t.expand(latent_model_input.shape[0]).to(dtype=dtype)\n\n            noise_pred = self.transformer(\n                hidden_states=latent_model_input,\n                timestep=timestep,\n                encoder_hidden_states=prompt_embeds.to(dtype=self.transformer.dtype),\n                pooled_projections=pooled_prompt_embeds.to(dtype=self.transformer.dtype),\n                joint_attention_kwargs=self.joint_attention_kwargs,\n                return_dict=False,\n            )[0]\n\n            if self.do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            latents_dtype = latents.dtype\n            latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n\n            if latents.dtype != latents_dtype:\n                if torch.backends.mps.is_available():\n                    latents = latents.to(latents_dtype)\n\n            if callback_on_step_end is not None:\n                callback_kwargs = {}\n                for k in callback_on_step_end_tensor_inputs:\n                    callback_kwargs[k] = locals()[k]\n                callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n\n                latents = callback_outputs.pop(\"latents\", latents)\n                prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n                negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n                negative_pooled_prompt_embeds = callback_outputs.pop(\n                    \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n                )\n\n        if output_type == \"latent\":\n            image = latents\n        else:\n            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor\n            image = self.vae.decode(latents, return_dict=False)[0]\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        return image\n\n\nif __name__ == '__main__':\n    device = \"cuda\" \n    dtype = torch.float16\n\n    text_encoder_path = 'google/umt5-xxl'\n    text_encoder_path1 = \"Tencent-Hunyuan/HunyuanDiT/t2i\"\n    text_encoder_path2 = 'laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/open_clip_pytorch_model.bin'\n\n    model_path = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n    proj_path =  \"OPPOer/MultilingualSD3-adapter/pytorch_model.bin\"\n    proj_t5_path =  \"OPPOer/MultilingualSD3-adapter/pytorch_model_t5.bin\"\n\n    sdt = StableDiffusionTest(model_path,text_encoder_path,text_encoder_path1,text_encoder_path2,proj_path,proj_t5_path)\n\n    batch=2\n    height = 1024\n    width = 1024      \n    while True:\n        raw_text = input(\"\\nPlease Input Query (stop to exit) >>> \")\n        if not raw_text:\n            print('Query should not be empty!')\n            continue\n        if raw_text == \"stop\":\n            break\n        images = sdt([raw_text]*batch,height=height,width=width)\n        grid = image_grid(images, rows=1, cols=batch)\n        grid.save(\"MultilingualSD3.png\")\n\n\n```\nTo learn more check out the [diffusers](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux) documentation\n\n\n# License\nThe adapter itself is Apache License 2.0, but it must follow the license of the main model.",
            "metadata": "{\"id\": \"OPPOer/MultilingualSD3-adapter\", \"author\": \"OPPOer\", \"sha\": \"f42cb06100e831847cfa71092dd983cdbedb5e78\", \"last_modified\": \"2024-10-26 08:44:06+00:00\", \"created_at\": \"2024-10-26 08:20:12+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"text-to-image\", \"zh\", \"arxiv:2311.17086\", \"arxiv:2205.11487\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-to-image\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlanguage:\\n- zh\\nlicense: apache-2.0\\npipeline_tag: text-to-image\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='PEA-Diffusion.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model_t5.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-26 08:44:06+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlanguage:\\n- zh\\nlicense: apache-2.0\\npipeline_tag: text-to-image\", \"transformersInfo\": null, \"_id\": \"671ca63c255aa50ebb650635\", \"modelId\": \"OPPOer/MultilingualSD3-adapter\", \"usedStorage\": 436399540}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=OPPOer/MultilingualSD3-adapter&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BOPPOer%2FMultilingualSD3-adapter%5D(%2FOPPOer%2FMultilingualSD3-adapter)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Dreyyt/Analog_Madness_Realistic_model",
            "card": "---\nlicense: other\nlicense_name: private\nlicense_link: LICENSE\npipeline_tag: text-to-image\nbase_model:\n- stabilityai/stable-diffusion-3-medium\nlibrary_name: diffusers\n---",
            "metadata": "{\"id\": \"Dreyyt/Analog_Madness_Realistic_model\", \"author\": \"Dreyyt\", \"sha\": \"3a0c6a663c7aba834ebde4fc6b49d9cfb5ec3a91\", \"last_modified\": \"2024-09-14 13:39:49+00:00\", \"created_at\": \"2024-09-14 11:09:32+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"text-to-image\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"doi:10.57967/hf/3069\", \"license:other\", \"region:us\"], \"pipeline_tag\": \"text-to-image\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: private\\nlicense_link: LICENSE\\npipeline_tag: text-to-image\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='analogMadness_v50.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-09-14 13:39:49+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlibrary_name: diffusers\\nlicense: other\\nlicense_name: private\\nlicense_link: LICENSE\\npipeline_tag: text-to-image\", \"transformersInfo\": null, \"_id\": \"66e56eec6e6ce3af72f542ef\", \"modelId\": \"Dreyyt/Analog_Madness_Realistic_model\", \"usedStorage\": 2378778340}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Dreyyt/Analog_Madness_Realistic_model&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BDreyyt%2FAnalog_Madness_Realistic_model%5D(%2FDreyyt%2FAnalog_Madness_Realistic_model)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "vdo/pyramid-flow-sd3",
            "card": "---\nlicense: mit\nbase_model:\n- stabilityai/stable-diffusion-3-medium\n---\n\n# \u26a1\ufe0fPyramid Flow\u26a1\ufe0f\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page \u2728]](https://pyramid-flow.github.io) [[Code \ud83d\ude80]](https://github.com/jy0205/Pyramid-Flow)\n\nThis is the official repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n## News\n\n* `COMING SOON` \u26a1\ufe0f\u26a1\ufe0f\u26a1\ufe0f Training code and new model checkpoints trained from scratch.\n* `2024.10.10`  \ud83d\ude80\ud83d\ude80\ud83d\ude80 We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Usage\n\nYou can directly download the model from [Huggingface](https://huggingface.co/rain1011/pyramid-flow-sd3). We provide both model checkpoints for 768p and 384p video generation. The 384p checkpoint supports 5-second video generation at 24FPS, while the 768p checkpoint supports up to 10-second video generation at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = 'PATH'   # The local directory to save downloaded checkpoint\nsnapshot_download(\"rain1011/pyramid-flow-sd3\", local_dir=model_path, local_dir_use_symlinks=False, repo_type='model')\n```\n\nTo use our model, please follow the inference code in `video_generation_demo.ipynb` at [this link](https://github.com/jy0205/Pyramid-Flow/blob/main/video_generation_demo.ipynb). We further simplify it into the following two-step procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16, fp16 or fp32\t\n\nmodel = PyramidDiTForVideoGeneration(\n    'PATH',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant='diffusion_transformer_768p',     # 'diffusion_transformer_384p'\n)\n\nmodel.vae.to(\"cuda\")\nmodel.dit.to(\"cuda\")\nmodel.text_encoder.to(\"cuda\")\nmodel.vae.enable_tiling()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = \"A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type=\"pil\",\n    )\n\nexport_to_video(frames, \"./text_to_video_sample.mp4\", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open('assets/the_great_wall.jpg').convert(\"RGB\").resize((1280, 768))\nprompt = \"FPV flying over the Great Wall\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type=\"pil\",\n    )\n\nexport_to_video(frames, \"./image_to_video_sample.mp4\", fps=24)\n```\n\nUsage tips:\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/waves.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/rail.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```",
            "metadata": "{\"id\": \"vdo/pyramid-flow-sd3\", \"author\": \"vdo\", \"sha\": \"db75c748d9195dd6e0cc684907a4575e7c6cdfd8\", \"last_modified\": \"2024-10-10 05:58:18+00:00\", \"created_at\": \"2024-10-10 05:50:04+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 5, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"arxiv:2410.05954\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/diffusion_pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/diffusion_pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_768p/diffusion_pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/spiece.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-10 05:58:18+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"67076b0c065c99af40ef7a35\", \"modelId\": \"vdo/pyramid-flow-sd3\", \"usedStorage\": 29184241604}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=vdo/pyramid-flow-sd3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bvdo%2Fpyramid-flow-sd3%5D(%2Fvdo%2Fpyramid-flow-sd3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Ziyaad30/Pyramid-Flow-sd3",
            "card": "---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-video\ntags:\n- image-to-video\n---\n\n# \u26a1\ufe0fPyramid Flow\u26a1\ufe0f\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page \u2728]](https://pyramid-flow.github.io) [[Code \ud83d\ude80]](https://github.com/jy0205/Pyramid-Flow)\n\nThis is the official repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n## News\n\n* `COMING SOON` \u26a1\ufe0f\u26a1\ufe0f\u26a1\ufe0f Training code and new model checkpoints trained from scratch.\n* `2024.10.10`  \ud83d\ude80\ud83d\ude80\ud83d\ude80 We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Usage\n\nYou can directly download the model from [Huggingface](https://huggingface.co/rain1011/pyramid-flow-sd3). We provide both model checkpoints for 768p and 384p video generation. The 384p checkpoint supports 5-second video generation at 24FPS, while the 768p checkpoint supports up to 10-second video generation at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = 'PATH'   # The local directory to save downloaded checkpoint\nsnapshot_download(\"rain1011/pyramid-flow-sd3\", local_dir=model_path, local_dir_use_symlinks=False, repo_type='model')\n```\n\nTo use our model, please follow the inference code in `video_generation_demo.ipynb` at [this link](https://github.com/jy0205/Pyramid-Flow/blob/main/video_generation_demo.ipynb). We further simplify it into the following two-step procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16, fp16 or fp32\t\n\nmodel = PyramidDiTForVideoGeneration(\n    'PATH',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant='diffusion_transformer_768p',     # 'diffusion_transformer_384p'\n)\n\nmodel.vae.to(\"cuda\")\nmodel.dit.to(\"cuda\")\nmodel.text_encoder.to(\"cuda\")\nmodel.vae.enable_tiling()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = \"A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type=\"pil\",\n    )\n\nexport_to_video(frames, \"./text_to_video_sample.mp4\", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open('assets/the_great_wall.jpg').convert(\"RGB\").resize((1280, 768))\nprompt = \"FPV flying over the Great Wall\"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type=\"pil\",\n    )\n\nexport_to_video(frames, \"./image_to_video_sample.mp4\", fps=24)\n```\n\nUsage tips:\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class=\"center\" border=\"0\" style=\"width: 100%; text-align: left;\">\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/waves.mp4\" autoplay muted loop playsinline></video></td>\n  <td><video src=\"https://pyramid-flow.github.io/static/videos/t2v/rail.mp4\" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```",
            "metadata": "{\"id\": \"Ziyaad30/Pyramid-Flow-sd3\", \"author\": \"Ziyaad30\", \"sha\": \"91d07c21bc12bfc1e89515ddac75dc26f9ed5153\", \"last_modified\": \"2024-10-10 20:03:06+00:00\", \"created_at\": \"2024-10-10 20:02:42+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 2, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"image-to-video\", \"text-to-video\", \"arxiv:2410.05954\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"license:other\", \"region:us\"], \"pipeline_tag\": \"text-to-video\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-video\\ntags:\\n- image-to-video\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='causal_video_vae/diffusion_pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_transformer_384p/diffusion_pytorch_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_2/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='text_encoder_3/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/merges.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_2/vocab.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/spiece.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_3/tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-10 20:03:06+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-diffusion-3-medium\\nlicense: other\\nlicense_name: stabilityai-ai-community\\nlicense_link: LICENSE.md\\npipeline_tag: text-to-video\\ntags:\\n- image-to-video\", \"transformersInfo\": null, \"_id\": \"670832e2a98933396d054894\", \"modelId\": \"Ziyaad30/Pyramid-Flow-sd3\", \"usedStorage\": 34025081498}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Ziyaad30/Pyramid-Flow-sd3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BZiyaad30%2FPyramid-Flow-sd3%5D(%2FZiyaad30%2FPyramid-Flow-sd3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "boryanagm/beatrix_LoRA",
            "card": "---\nlibrary_name: diffusers\npipeline_tag: text-to-image\nbase_model: stabilityai/stable-diffusion-3-medium\n---\n# LoRA Model Card for beatrix_LoRA\n### Instance Prompt\nin the style of BEATRIX\n",
            "metadata": "{\"id\": \"boryanagm/beatrix_LoRA\", \"author\": \"boryanagm\", \"sha\": \"7f1a78d292eea36ed6f65ffc28d858a237d6e808\", \"last_modified\": \"2024-11-27 08:52:56+00:00\", \"created_at\": \"2024-11-25 14:14:08+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"text-to-image\", \"base_model:stabilityai/stable-diffusion-3-medium\", \"base_model:finetune:stabilityai/stable-diffusion-3-medium\", \"region:us\"], \"pipeline_tag\": \"text-to-image\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: stabilityai/stable-diffusion-3-medium\\nlibrary_name: diffusers\\npipeline_tag: text-to-image\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-27 08:52:56+00:00\", \"cardData\": \"base_model: stabilityai/stable-diffusion-3-medium\\nlibrary_name: diffusers\\npipeline_tag: text-to-image\", \"transformersInfo\": null, \"_id\": \"674486303c7de9897b96d72f\", \"modelId\": \"boryanagm/beatrix_LoRA\", \"usedStorage\": 0}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=boryanagm/beatrix_LoRA&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bboryanagm%2Fbeatrix_LoRA%5D(%2Fboryanagm%2Fbeatrix_LoRA)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        }
    ]
}