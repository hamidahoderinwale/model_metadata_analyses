{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidahoderinwale/model_metadata_analyses/blob/main/scraping_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxlHKer226vF"
      },
      "source": [
        "# Script 1\n",
        "Takes in a (validated) model url and output its metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmp0ftEomk86",
        "outputId": "44746f1a-a80f-4058-c380-5f5e4db73992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: validators in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Enter model URL: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
            "You entered: https://huggingface.co/deepseek-ai/DeepSeek-R1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_id': 'deepseek-ai/DeepSeek-R1',\n",
              " 'model_info': {'id': 'deepseek-ai/DeepSeek-R1',\n",
              "  'author': 'deepseek-ai',\n",
              "  'sha': '56d4cbbb4d29f4355bab4b9a39ccb717a14ad5ad',\n",
              "  'last_modified': datetime.datetime(2025, 3, 27, 4, 1, 59, tzinfo=datetime.timezone.utc),\n",
              "  'created_at': datetime.datetime(2025, 1, 20, 3, 46, 7, tzinfo=datetime.timezone.utc),\n",
              "  'private': False,\n",
              "  'gated': False,\n",
              "  'disabled': False,\n",
              "  'downloads': 1342593,\n",
              "  'downloads_all_time': None,\n",
              "  'likes': 11857,\n",
              "  'library_name': 'transformers',\n",
              "  'gguf': None,\n",
              "  'inference': 'warm',\n",
              "  'inference_provider_mapping': None,\n",
              "  'tags': ['transformers',\n",
              "   'safetensors',\n",
              "   'deepseek_v3',\n",
              "   'text-generation',\n",
              "   'conversational',\n",
              "   'custom_code',\n",
              "   'arxiv:2501.12948',\n",
              "   'license:mit',\n",
              "   'autotrain_compatible',\n",
              "   'endpoints_compatible',\n",
              "   'fp8',\n",
              "   'region:us'],\n",
              "  'pipeline_tag': 'text-generation',\n",
              "  'mask_token': None,\n",
              "  'trending_score': None,\n",
              "  'card_data': {'base_model': None, 'datasets': None, 'eval_results': None, 'language': None, 'library_name': 'transformers', 'license': 'mit', 'license_name': None, 'license_link': None, 'metrics': None, 'model_name': None, 'pipeline_tag': None, 'tags': None},\n",
              "  'widget_data': [{'text': 'Hi, what can you help me with?'},\n",
              "   {'text': 'What is 84 * 3 / 2?'},\n",
              "   {'text': 'Tell me an interesting fact about the universe!'},\n",
              "   {'text': 'Explain quantum computing in simple terms.'}],\n",
              "  'model_index': None,\n",
              "  'config': {'architectures': ['DeepseekV3ForCausalLM'],\n",
              "   'auto_map': {'AutoConfig': 'configuration_deepseek.DeepseekV3Config',\n",
              "    'AutoModel': 'modeling_deepseek.DeepseekV3Model',\n",
              "    'AutoModelForCausalLM': 'modeling_deepseek.DeepseekV3ForCausalLM'},\n",
              "   'model_type': 'deepseek_v3',\n",
              "   'quantization_config': {'quant_method': 'fp8'},\n",
              "   'tokenizer_config': {'bos_token': {'__type': 'AddedToken',\n",
              "     'content': '<｜begin▁of▁sentence｜>',\n",
              "     'lstrip': False,\n",
              "     'normalized': True,\n",
              "     'rstrip': False,\n",
              "     'single_word': False},\n",
              "    'eos_token': {'__type': 'AddedToken',\n",
              "     'content': '<｜end▁of▁sentence｜>',\n",
              "     'lstrip': False,\n",
              "     'normalized': True,\n",
              "     'rstrip': False,\n",
              "     'single_word': False},\n",
              "    'pad_token': {'__type': 'AddedToken',\n",
              "     'content': '<｜end▁of▁sentence｜>',\n",
              "     'lstrip': False,\n",
              "     'normalized': True,\n",
              "     'rstrip': False,\n",
              "     'single_word': False},\n",
              "    'unk_token': None,\n",
              "    'chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\\\n\\\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- else %}{{'<｜Assistant｜>' + message['content'] + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- endif %}{%- endfor %}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\\\n'}}{% endif %}\"}},\n",
              "  'transformers_info': TransformersInfo(auto_model='AutoModelForCausalLM', custom_class=None, pipeline_tag='text-generation', processor='AutoTokenizer'),\n",
              "  'siblings': [RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='configuration_deepseek.py', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='figures/benchmark.jpg', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00001-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00002-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00003-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00004-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00005-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00006-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00007-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00008-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00009-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00010-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00011-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00012-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00013-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00014-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00015-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00016-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00017-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00018-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00019-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00020-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00021-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00022-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00023-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00024-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00025-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00026-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00027-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00028-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00029-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00030-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00031-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00032-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00033-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00034-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00035-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00036-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00037-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00038-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00039-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00040-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00041-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00042-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00043-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00044-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00045-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00046-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00047-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00048-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00049-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00050-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00051-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00052-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00053-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00054-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00055-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00056-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00057-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00058-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00059-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00060-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00061-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00062-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00063-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00064-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00065-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00066-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00067-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00068-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00069-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00070-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00071-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00072-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00073-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00074-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00075-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00076-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00077-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00078-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00079-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00080-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00081-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00082-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00083-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00084-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00085-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00086-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00087-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00088-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00089-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00090-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00091-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00092-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00093-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00094-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00095-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00096-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00097-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00098-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00099-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00100-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00101-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00102-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00103-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00104-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00105-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00106-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00107-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00108-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00109-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00110-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00111-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00112-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00113-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00114-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00115-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00116-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00117-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00118-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00119-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00120-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00121-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00122-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00123-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00124-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00125-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00126-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00127-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00128-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00129-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00130-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00131-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00132-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00133-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00134-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00135-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00136-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00137-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00138-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00139-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00140-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00141-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00142-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00143-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00144-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00145-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00146-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00147-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00148-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00149-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00150-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00151-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00152-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00153-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00154-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00155-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00156-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00157-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00158-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00159-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00160-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00161-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00162-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model-00163-of-000163.safetensors', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='modeling_deepseek.py', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None),\n",
              "   RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)],\n",
              "  'spaces': ['akhaliq/anycoder',\n",
              "   'llamameta/DeepSeek-R1-Chat-Assistant-Web-Search',\n",
              "   'openfree/Korean-Exam-Leaderboard',\n",
              "   'openfree/deepseek_r1_API',\n",
              "   'seawolf2357/DeepSeek-R1-32b-search',\n",
              "   'Intelligent-Internet/CoT-Lab',\n",
              "   'ruslanmv/DeepSeek-R1-Chatbot',\n",
              "   'KBaba7/Quant',\n",
              "   'hadadrjt/ai',\n",
              "   'barttee/tokenizers',\n",
              "   'HPAI-BSC/TuRTLe-Leaderboard',\n",
              "   'fdaudens/deepseek-download-stats',\n",
              "   'Dima123e/deepseek-ai-DeepSeek-R1',\n",
              "   'awacke1/Deepseek-HPC-GPU-KEDA',\n",
              "   'McLoviniTtt/Reasoner4All',\n",
              "   'bhaskartripathi/LLM_Quantization',\n",
              "   'totolook/Quant',\n",
              "   'openfree/DeepSeek-R1-32b-api',\n",
              "   'FallnAI/Quantize-HF-Models',\n",
              "   'element61/deepseek_togetherai_streamlit',\n",
              "   'victor/deepseek-ai-DeepSeek-R12',\n",
              "   'BarBar288/Chatbot',\n",
              "   'DvorakInnovationAI/Brain-Stroming-Story-Gen',\n",
              "   'ehagey/LLM_Healthcare_Benchmarking',\n",
              "   'drdro1/First_agent_template',\n",
              "   'kolaslab/DeepSeek-R1-Chatbot-70b',\n",
              "   'UltraRonin/LR2Bench_old',\n",
              "   'victor/deepseek-ai-DeepSeek-R1',\n",
              "   'akhaliq/deepseek-ai-DeepSeek-R1',\n",
              "   'UltraRonin/LR2Bench',\n",
              "   'thanhkt/text2manim',\n",
              "   'ruslanmv/convert_to_gguf',\n",
              "   'manojdahal191gom/deepseek-ai-DeepSeek-R1',\n",
              "   'Payknayk/deepseekaiR1',\n",
              "   'rautakshay136/deepseek-ai-DeepSeek-R1',\n",
              "   'Uener/deepseek-ai-DeepSeek-R1',\n",
              "   'Nixic/DeepChat',\n",
              "   'migueldeguzmandev/deepseek-build',\n",
              "   'nikhil-kumar/Financial_Assistant',\n",
              "   'Hkb2001/Medical_Analyzer',\n",
              "   'faizaltkl/First_agent_template',\n",
              "   'openfree/DeepSeek-R1-Chatbot',\n",
              "   'Cenes44/Qwen-2.5-vl-api',\n",
              "   'seawolf2357/DeepSeek-R1-32b-api',\n",
              "   'htilssu/deepseek-ai-DeepSeek-R1',\n",
              "   'Lyte/tokenizer-leaderboard',\n",
              "   'rayajahan/First_agent_template1',\n",
              "   'megatrump/DeepClaudeProxy',\n",
              "   'nathannarrik/TUTOR',\n",
              "   'ikun520/deepseek',\n",
              "   'ItayR31/puchifypro',\n",
              "   'burtenshaw/deepseek-ai-DeepSeek-R1',\n",
              "   'readomni/literate',\n",
              "   'Godking0181/deepseek-ai-DeepSeek-R1',\n",
              "   'Vedantonhf/deepseek-ai-DeepSeek-R1',\n",
              "   'leh146215/deepseek-ai-DeepSeek-R1',\n",
              "   'BarBar288/AI_Tools',\n",
              "   'mianumairsiddiquie/deepseek-ai-DeepSeek-R1',\n",
              "   'rwayz/ModelsChatBot',\n",
              "   'Thsuporte24h/Olkchat',\n",
              "   'zhwang4ai/GenerativeReasoningBenchmark',\n",
              "   'sbudni/sk',\n",
              "   'migueldeguzmandev/migueldeguzmandev-papercliptodd_v2',\n",
              "   'alx-d/philosophy_aristotle',\n",
              "   'Lakshan2003/llama-chat',\n",
              "   'dlflannery/GradioTest',\n",
              "   'hotdeem/mp3',\n",
              "   'ashok2216/SkyTrack',\n",
              "   'holytinz278/Microdot',\n",
              "   'carlosdimare/RSU',\n",
              "   'holytinz278/fishai',\n",
              "   'mazkobot66/candlestick',\n",
              "   'oriolcortes/llama3-text-generator',\n",
              "   'lunarflu/deepseek-ai-DeepSeek-R1',\n",
              "   'spireeewq/deepseek-ai-DeepSeek-R1',\n",
              "   'lukasholovsky/deepseek-ai-DeepSeek-R1',\n",
              "   'yeniu/deepseek-ai-DeepSeek-R1',\n",
              "   'd3m0n/deepseek-ai-DeepSeek-R1',\n",
              "   'theamrelhady/deepseek-ai-DeepSeek-R1',\n",
              "   'Ismael7777/deepseek-ai-DeepSeek-R1',\n",
              "   'kneeyee/deepseek-ai-DeepSeek-R1',\n",
              "   'Jodowo/deepseek-ai-DeepSeek-R1',\n",
              "   'alwaysrunning/deepseek-ai-DeepSeek-R1',\n",
              "   'madmn69/deepseek-ai-DeepSeek-R1',\n",
              "   'assasdf/deepseek-ai-DeepSeek-R1',\n",
              "   'Jevon925/deepseek-ai-DeepSeek-R1',\n",
              "   'Crow34/Deep',\n",
              "   'cdtermux1011/deepseek-ai-DeepSeek-R1',\n",
              "   'Evgenii-Bubolev/deepseek-ai-DeepSeek-R1',\n",
              "   'wheattoast11/deepseek-ai-DeepSeek-R1',\n",
              "   'augustocmarinho/my-first-ia',\n",
              "   'dogsanddogs914/deepseek-ai-DeepSeek-R1',\n",
              "   'zmos1/deepseek-ai-DeepSeek-R1',\n",
              "   'JasGRE/deepseek-ai-DeepSeek-R1',\n",
              "   'Rorolinux/deepseek-ai-DeepSeek-R1',\n",
              "   'kazukikun/deepseek-ai-DeepSeek-R1',\n",
              "   'juanaguirre96/deepseek-ai-DeepSeek-R1',\n",
              "   'unpourcent/deepseek-ai-DeepSeek-R1',\n",
              "   'j0nrages/deepseek-ai-DeepSeek-R1',\n",
              "   'Techguy3389/deepseek-ai-DeepSeek-R1'],\n",
              "  'safetensors': SafeTensorsInfo(parameters={'BF16': 3918786560, 'F8_E4M3': 680571043840, 'F32': 41555600}, total=684531386000),\n",
              "  'security_repo_status': None,\n",
              "  'xet_enabled': None,\n",
              "  'lastModified': datetime.datetime(2025, 3, 27, 4, 1, 59, tzinfo=datetime.timezone.utc),\n",
              "  'cardData': {'base_model': None, 'datasets': None, 'eval_results': None, 'language': None, 'library_name': 'transformers', 'license': 'mit', 'license_name': None, 'license_link': None, 'metrics': None, 'model_name': None, 'pipeline_tag': None, 'tags': None},\n",
              "  'transformersInfo': TransformersInfo(auto_model='AutoModelForCausalLM', custom_class=None, pipeline_tag='text-generation', processor='AutoTokenizer'),\n",
              "  '_id': '678dc6fff905d106be796d8a',\n",
              "  'modelId': 'deepseek-ai/DeepSeek-R1',\n",
              "  'usedStorage': 688624501744}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Script 1: takes input model url, validates url, and gives model metadata\n",
        "!pip install validators\n",
        "from huggingface_hub import HfApi\n",
        "import huggingface_hub as hf\n",
        "import validators\n",
        "import json\n",
        "import csv\n",
        "\n",
        "hf_api = HfApi()\n",
        "\n",
        "def input_url():\n",
        "    while True:\n",
        "        input_model_url = input(\"Enter model URL: \")\n",
        "        print(f\"You entered: {input_model_url}\")\n",
        "\n",
        "        if validators.url(input_model_url) and \"huggingface.co\" in input_model_url:\n",
        "                # Extract the model ID from the URL\n",
        "                model_id = input_model_url.split(\"huggingface.co/\")[-1]\n",
        "                model_info = hf_api.model_info(model_id) # Get model info: https://huggingface.co/docs/huggingface_hub/v0.29.2/en/package_reference/hf_api#huggingface_hub.ModelInfo\n",
        "                model_card = hf.hf_hub_download(models[0].modelId, 'README.md')\n",
        "\n",
        "                metadata = { # Moved metadata assignment inside the if block\n",
        "                    \"model_id\": model_id,\n",
        "                    \"model_info\": model_info.__dict__\n",
        "                }\n",
        "                return metadata # Return is now inside if block\n",
        "                json_output = json.dumps(metadata, indent=4, default=str)\n",
        "                with open('model_info.json', 'w') as json_file:\n",
        "                 json_file.write(json_output)\n",
        "                print(json_output)\n",
        "        else:\n",
        "            print(\"Invalid URL. Please enter a valid Hugging Face model URL.\")\n",
        "            # Optionally: You could return None or an empty dictionary here\n",
        "            # to indicate an invalid URL\n",
        "# To test: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
        "\n",
        "input_url()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOnHUAW23SNE"
      },
      "source": [
        "# Script 2\n",
        "Takes in a (validated) model and outputs the children models/fine-tunes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6petSSd-m2Qi",
        "outputId": "03169ba1-3e8d-4b6e-d66c-4cb133f17416"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-117-ebc51be474d6>\", line 37, in <cell line: 0>\n",
            "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 1177, in raw_input\n",
            "    return self._input_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 1219, in _input_request\n",
            "    raise KeyboardInterrupt(\"Interrupted by user\") from None\n",
            "KeyboardInterrupt: Interrupted by user\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-ebc51be474d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the Hugging Face model URL: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "# Script 2\n",
        "  # 1. Take link as input (format check). This is the \"main model\"\n",
        "  # 2. Give the link to the page with the fine-tunes for the inputted model\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    if match:\n",
        "        return match.groups()  # Returns (org/user, model_name)\n",
        "    return None\n",
        "\n",
        "# Function to find fine-tuned models\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    search_url = f\"https://huggingface.co/models?search={model_name}\"\n",
        "    response = requests.get(search_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_links = [\n",
        "            a[\"href\"] for a in soup.find_all(\"a\", href=True)\n",
        "            if model_name.lower() in a[\"href\"].lower()\n",
        "        ]\n",
        "        return [f\"https://huggingface.co{link}\" for link in model_links if model_org not in link]\n",
        "\n",
        "    return []\n",
        "\n",
        "# Main execution\n",
        "model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "\n",
        "validated = validate_hf_model_url(model_url)\n",
        "if validated:\n",
        "    org, model_name = validated\n",
        "    finetune_links = get_finetuned_models_page(org, model_name)\n",
        "\n",
        "    if finetune_links:\n",
        "        print(\"Fine-tuned models found:\")\n",
        "        for link in finetune_links:\n",
        "            print(link)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found for this model.\")\n",
        "else:\n",
        "    print(\"Invalid Hugging Face model URL format.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruI5-1FA1ipq"
      },
      "source": [
        "# Script 3\n",
        "## Search steps overview\n",
        "- `dfs_finetunes` we take the `model_url` as input. Alternatively, we can add this var as an argument.\n",
        "- We go layer-by-layer and find the children of the current model (i.e. the fine-tunes of a model)\n",
        "- We call the `dfs_funetunes` function recursively and store the models that have been \"visited\" to avoid duplicates.\n",
        "- We have a dictionary of information that we store about the \"current model\" and have the information stored in respective columns (model_id, card, metadata, depth, children (list of model links), children count).\n",
        "- We have a `results` list that has the information about all the models and their fine-tunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Da_92M_Cgv1N",
        "outputId": "2ceb1a6a-2c95-497d-ae46-0959a356a594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers~=4.48.3 in /usr/local/lib/python3.11/dist-packages (from adapters) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2025.1.31)\n",
            "Enter the Hugging Face model URL: https://huggingface.co/perplexity-ai/r1-1776\n",
            "\n",
            "Fetching metadata for: perplexity-ai/r1-1776\n",
            "Found 11 fine-tunes at depth 0.\n",
            "Found 1 adapter models for perplexity-ai/r1-1776.\n",
            "\n",
            "  Fetching metadata for: mlx-community/perplexity-ai-r1-1776-bf16\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for mlx-community/perplexity-ai-r1-1776-bf16.\n",
            "\n",
            "  Fetching metadata for: malypali18/WebWealthWizards\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for malypali18/WebWealthWizards.\n",
            "\n",
            "  Fetching metadata for: dahiya11/Ai-Assistant\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for dahiya11/Ai-Assistant.\n",
            "\n",
            "  Fetching metadata for: Delfileking/Histoirde2005\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for Delfileking/Histoirde2005.\n",
            "\n",
            "  Fetching metadata for: Khewa153/GleemanAI\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for Khewa153/GleemanAI.\n",
            "\n",
            "  Fetching metadata for: unsloth/r1-1776\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for unsloth/r1-1776.\n",
            "\n",
            "  Fetching metadata for: Suziwan/Model1\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for Suziwan/Model1.\n",
            "\n",
            "  Fetching metadata for: Hxh0211/11111\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for Hxh0211/11111.\n",
            "\n",
            "  Fetching metadata for: ALESSIO66/Law_CCII_IT_ProceduresCloud\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for ALESSIO66/Law_CCII_IT_ProceduresCloud.\n",
            "\n",
            "  Fetching metadata for: Renato186/ren\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for Renato186/ren.\n",
            "\n",
            "  Fetching metadata for: rash1dovt/tyncha_ai\n",
            "  Found 0 fine-tunes at depth 1.\n",
            "  Found 0 adapter models for rash1dovt/tyncha_ai.\n",
            "Results saved to r1-1776_finetunes_20250415_002822.json\n",
            "Results saved to r1-1776_finetunes_20250415_002822.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\\nhttps://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Script 3\n",
        "!pip install huggingface_hub\n",
        "!pip install adapters\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    return match.groups() if match else None\n",
        "\n",
        "# page with model finetunes\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    all_model_links = []  # Store all links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:finetune:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_model_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_model_links\n",
        "\n",
        "# model card data\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        # Try to download model card if available\n",
        "        readme_path = hf_hub_download(repo_id=model_id, filename='README.md')\n",
        "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "            card_content = f.read()\n",
        "        return card_content\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not download model card: {str(e)[:100]}...\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Function to get parent model\n",
        "def get_parent_model(model_url):\n",
        "    return model_url.split(f\"/{model_name}/\")[0]\n",
        "\n",
        "# Truncate metadata\n",
        "def filter_metadata(json_metadata):\n",
        "            keys_to_keep = [\"modelId\", \"sha\", \"tags\", \"downloads\", \"pipeline_tag\"]\n",
        "            return {k: json_metadata.get(k) for k in keys_to_keep if k in json_metadata}\n",
        "            filtered_metadata = filter_metadata(api.model_info(model_id).__dict__)\n",
        "\n",
        "# Get adapter models\n",
        "def get_adapter_models_page(model_org, model_name):\n",
        "    all_adapter_links = []  # Store all adapter links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:adapter:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_adapter_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_adapter_links\n",
        "\n",
        "# Recursive DFS (depth-first search) for finding fine-tunes\n",
        "def dfs_finetunes(model_url, visited, depth=0, results=None):\n",
        "       if results is None:\n",
        "           results = []\n",
        "\n",
        "       if model_url in visited:\n",
        "           return results\n",
        "       visited.add(model_url)\n",
        "\n",
        "       validated = validate_hf_model_url(model_url)\n",
        "       if not validated:\n",
        "           print(f\"Invalid URL skipped: {model_url}\")\n",
        "           model_url = \"N/A\"\n",
        "           return results\n",
        "\n",
        "       model_org, model_name = validated\n",
        "       model_id = f\"{model_org}/{model_name}\"\n",
        "\n",
        "\n",
        "       print(f\"\\n{'  ' * depth}Fetching metadata for: {model_id}\")\n",
        "       try:\n",
        "           model_metadata = api.model_info(model_id).__dict__\n",
        "           json_metadata = json.dumps(model_metadata, default=str)\n",
        "           model_card = get_model_card(model_id)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error fetching metadata: {e}\")\n",
        "           return results\n",
        "\n",
        "       finetune_links = get_finetuned_models_page(model_org, model_name)\n",
        "       # Removing Duplicate Children\n",
        "       finetune_links = list(set(finetune_links))\n",
        "       print(f\"{'  ' * depth}Found {len(finetune_links)} fine-tunes at depth {depth}.\")\n",
        "       adapter_links = get_adapter_models_page(model_org, model_name)\n",
        "       print(f\"{'  ' * depth}Found {len(adapter_links)} adapter models for {model_id}.\")\n",
        "\n",
        "       results.append({\n",
        "           \"model_id\": model_id,\n",
        "           \"card\": model_card,\n",
        "           \"metadata\": json_metadata,\n",
        "           \"depth\": depth,\n",
        "           \"children\": finetune_links,\n",
        "           \"children_count\": len(finetune_links),\n",
        "           \"adapters\": adapter_links,\n",
        "           \"adapters_count\": len(adapter_links),\n",
        "           \"parent_model\": get_parent_model(model_url)\n",
        "       })\n",
        "\n",
        "       for link in finetune_links:\n",
        "             dfs_finetunes(link, visited, depth + 1, results)\n",
        "       return results\n",
        "\n",
        "# Timestamp for the run\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Function to save results as JSON\n",
        "def save_json(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.json\"\n",
        "    data = {\n",
        "        \"models\": results\n",
        "    }\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4, default=str)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Function to save results as CSV\n",
        "''' def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_{timestamp}_finetunes.csv\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"model_id\", \"depth\", \"children_count\", \"children\", \"metadata\"])\n",
        "        writer.writeheader()\n",
        "        for entry in results:\n",
        "            # Ensure metadata is a JSON string\n",
        "            if isinstance(entry[\"metadata\"], dict):\n",
        "                entry[\"metadata\"] = json.dumps(entry[\"metadata\"], indent=2, default=str)\n",
        "            # Join children list as a string\n",
        "            entry[\"children\"] = \", \".join(entry[\"children\"])\n",
        "            writer.writerow(entry)\n",
        "    print(f\"Results saved to {filename}\") '''\n",
        "\n",
        "# Function to save results as CSV (pandas)\n",
        "def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=True)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "    visited = set()\n",
        "    results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "    if results:\n",
        "        model_name = results[0][\"model_id\"].split(\"/\")[-1]  # Extract model name for file naming\n",
        "        save_json(results, model_name)\n",
        "        save_csv(results, model_name)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found.\")\n",
        "\n",
        "'''Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\n",
        "https://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWhCzNPsiXgX"
      },
      "source": [
        "# Scripts 4 (Tree Viz)\n",
        "\n",
        "Iteratively produces model tree datasets when given a list of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kn8T_f0IivaD",
        "outputId": "9b0dbfbe-2310-4b8f-c069-ce69e1d5eb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from Bio) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from Bio) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from Bio) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython>=1.80->Bio) (2.0.2)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->Bio) (4.3.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch->Bio) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (2025.1.31)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from biothings-client>=0.2.6->mygene->Bio) (0.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.13.1)\n",
            "Downloading bio-1.7.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.7.1 biopython-1.85 biothings-client-0.4.1 gprofiler-official-1.0.0 mygene-3.2.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-80c2eb8ec151>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load and prepare tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'"
          ]
        }
      ],
      "source": [
        "# 4.1 Phylogenetic tree\n",
        "\n",
        "!pip install Bio\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "from Bio import Phylo\n",
        "from Bio.Phylo.BaseTree import Clade, Tree\n",
        "\n",
        "# Load and prepare tree\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "def build_phylo_tree_from_dfs(df):\n",
        "    clade_map = {row['model_id']: Clade(name=row['model_id']) for _, row in df.iterrows()}\n",
        "    parent_links = {}\n",
        "    for _, row in df.iterrows():\n",
        "        parent = row['model_id']\n",
        "        for child_url in row['children']:\n",
        "            child = '/'.join(child_url.split(\"/\")[-2:])\n",
        "            parent_links[child] = parent\n",
        "    all_models = set(df['model_id'])\n",
        "    child_models = set(parent_links.keys())\n",
        "    root_model_id = list(all_models - child_models)[0]\n",
        "    for child, parent in parent_links.items():\n",
        "        if child in clade_map and parent in clade_map:\n",
        "            clade_map[parent].clades.append(clade_map[child])\n",
        "    return Tree(root=clade_map[root_model_id])\n",
        "\n",
        "tree = build_phylo_tree_from_dfs(df)\n",
        "\n",
        "# Plotting with labels on branches\n",
        "fig = plt.figure(figsize=(24, 36))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "# Draw the tree without axes\n",
        "Phylo.draw(tree, axes=ax, do_show=False)\n",
        "\n",
        "# Remove all axes elements\n",
        "ax.set_axis_off()\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "# Ensure labels are positioned away from branch lines\n",
        "for label in ax.texts:\n",
        "    pos = label.get_position()\n",
        "    label.set_position((pos[0], pos[1]))\n",
        "    label.set_bbox(dict(facecolor='white', alpha=0.8, edgecolor='none', pad=3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('clean_phylogenetic_tree.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAQvfhCa0TJ6",
        "outputId": "44c7e645-1be5-45e5-9c37-96eb58b76418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 2,434 kB of archives.\n",
            "After this operation, 7,681 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libxdot4 amd64 2.42.2-6ubuntu0.1 [16.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6ubuntu0.1 [22.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgraphviz-dev amd64 2.42.2-6ubuntu0.1 [58.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Fetched 2,434 kB in 1s (2,832 kB/s)\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 126315 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libxdot4:amd64.\n",
            "Preparing to unpack .../4-libxdot4_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgraphviz-dev:amd64.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../8-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Collecting pygraphviz\n",
            "  Downloading pygraphviz-1.14.tar.gz (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.14-cp311-cp311-linux_x86_64.whl size=169713 sha256=14200728290892ddc10302fb4aa5e24b52366827561e0a25b6ba05b21b137890\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/5f/df/6fffd2a4353f26dbb0e3672a1baf070c124a1d74a5f9318279\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.14\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88285ae38124>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load and prepare the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'"
          ]
        }
      ],
      "source": [
        "# 4.1 Networkx (in progress)\n",
        "!apt install graphviz libgraphviz-dev\n",
        "!pip install pygraphviz\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Load and prepare the dataset\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for _, row in df.iterrows():\n",
        "    parent = row['model_id']\n",
        "    G.add_node(parent)\n",
        "    for child_url in row['children']:\n",
        "        child = '/'.join(child_url.split(\"/\")[-2:])\n",
        "        G.add_node(child)\n",
        "        G.add_edge(parent, child)\n",
        "\n",
        "# Find root node\n",
        "root = [n for n, d in G.in_degree() if d == 0][0]\n",
        "\n",
        "# Create a hierarchical layout\n",
        "pos = nx.nx_agraph.graphviz_layout(G, prog='dot', root=root)\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(16, 24))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=10, node_color='white', edgecolors='black')\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(G, pos, arrows=False)\n",
        "\n",
        "# Draw labels, rotating only non-root nodes\n",
        "label_pos = {k: (v[0], v[1]) for k, v in pos.items()}\n",
        "for node, (x, y) in label_pos.items():\n",
        "    if node != root:  # Rotate labels for non-root nodes\n",
        "        plt.text(x, y, s=node, rotation=90, ha='center', va='center', fontsize=9)\n",
        "    else:  # Keep root label horizontal\n",
        "        plt.text(x, y, s=node, ha='center', va='center', fontsize=9)\n",
        "\n",
        "# Remove axes\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"mistral_finetune_tree_networkx.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Tree visualization saved to mistral_finetune_tree_networkx.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "JM61pKgp77Vl",
        "outputId": "a6537e84-3749-4ab7-96f8-dff9b0153a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ete3 in /usr/local/lib/python3.11/dist-packages (3.1.3)\n",
            "Requirement already satisfied: tree in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tree) (11.1.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from tree) (1.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tree) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tree) (8.1.8)\n",
            "Requirement already satisfied: PyQt5 in /usr/local/lib/python3.11/dist-packages (5.15.11)\n",
            "Requirement already satisfied: PyQt5-sip<13,>=12.15 in /usr/local/lib/python3.11/dist-packages (from PyQt5) (12.17.0)\n",
            "Requirement already satisfied: PyQt5-Qt5<5.16.0,>=5.15.2 in /usr/local/lib/python3.11/dist-packages (from PyQt5) (5.15.16)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'faces' from 'ete3' (/usr/local/lib/python3.11/dist-packages/ete3/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-842ec570bda0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mete3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttrFace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTreeStyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'faces' from 'ete3' (/usr/local/lib/python3.11/dist-packages/ete3/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 4.3 library (in progress)\n",
        "!pip install ete3\n",
        "!pip install tree\n",
        "!pip install PyQt5\n",
        "import pandas as pd\n",
        "import ast\n",
        "from ete3 import Tree, faces, AttrFace, TreeStyle\n",
        "\n",
        "\n",
        "# Load and prepare the dataset\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "# Function to build the ete3 tree recursively\n",
        "def build_ete3_tree(df, parent_node=None, parent_name=None):\n",
        "    if parent_node is None:\n",
        "        # Create the root node if it's the first call\n",
        "        root_name = df.loc[df['depth'] == 0, 'model_id'].iloc[0]\n",
        "        tree = Tree(name=root_name)\n",
        "        parent_node = tree\n",
        "    else:\n",
        "        tree = parent_node  # Continue building the existing tree\n",
        "\n",
        "    # Find children of the current parent\n",
        "    children_rows = df.loc[df['model_id'] == parent_name]\n",
        "    if not children_rows.empty:\n",
        "        children_urls = children_rows['children'].iloc[0]\n",
        "        for child_url in children_urls:\n",
        "            child_name = '/'.join(child_url.split(\"/\")[-2:])\n",
        "            child_node = tree.add_child(name=child_name)\n",
        "            build_ete3_tree(df, child_node, child_name)  # Recursive call\n",
        "    return tree\n",
        "\n",
        "# Build the tree\n",
        "tree = build_ete3_tree(df)\n",
        "\n",
        "# Style the tree\n",
        "ts.show_leaf_name = False  # Hide leaf names\n",
        "ts.mode = \"c\"  # Circular tree layout\n",
        "ts.root_opening_factor = 1  # Adjust root opening for circular layout\n",
        "\n",
        "# Style the nodes\n",
        "nst = NodeStyle()\n",
        "nst[\"size\"] = 0  # Hide node circles\n",
        "nst[\"hz_line_width\"] = 2\n",
        "nst[\"vt_line_width\"] = 2\n",
        "\n",
        "for n in tree.traverse():\n",
        "    n.set_style(nst)\n",
        "    # Add model name as a branch label\n",
        "    name_face = TextFace(n.name, fsize=10, fgcolor=\"black\")\n",
        "    n.add_face(name_face, column=0, position=\"branch-right\")\n",
        "\n",
        "# Render and save the tree\n",
        "tree.render(\"mistral_finetune_tree_ete3.png\", tree_style=ts, dpi=300)\n",
        "print(\"Tree visualization saved to mistral_finetune_tree_ete3.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script 6\n",
        "\n",
        "Get top 1000 models from the model-hub ordered by the number of likes"
      ],
      "metadata": {
        "id": "4rt2ohl3wnxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "# documentation: https://huggingface.co/docs/hub/en/api, gets top 1000 models sorted by likes in descending order\n",
        "models = api.list_models(sort=\"likes\", direction=\"desc\", page=1, per_page=1000)\n",
        "\n",
        "df = pd.DataFrame(models)\n",
        "df.to_csv(f\"{dt.datetime}_model_list\".csv, index=\"false\")\n",
        "print(\"Models saved to CSV\")\n"
      ],
      "metadata": {
        "id": "cH5A_cZUwnJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXrshhgesGd"
      },
      "source": [
        "# Script 7\n",
        "\n",
        "This script allows you to iteratively run the dfs function for multiple models given as a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS_K1AScbBdE"
      },
      "outputs": [],
      "source": [
        "# Multi-model runner for HuggingFace model analysis\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "def process_models_from_csv(csv_path, output_dir=\"results\"):\n",
        "    \"\"\"\n",
        "    Process multiple models from a CSV file.\n",
        "    CSV should have a column named 'model_url' with HuggingFace model URLs.\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Read CSV file\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Check if 'model_url' column exists\n",
        "    if 'model_url' not in df.columns:\n",
        "        print(\"Error: CSV file must contain a 'model_url' column\")\n",
        "        return\n",
        "\n",
        "    # Process each model URL\n",
        "    total_models = len(df)\n",
        "    for idx, row in df.iterrows():\n",
        "        model_url = row['model_url'].strip()\n",
        "        print(f\"\\n[{idx+1}/{total_models}] Processing: {model_url}\")\n",
        "\n",
        "        try:\n",
        "            # Create a new visited set for each model to track its unique hierarchy\n",
        "            visited = set()\n",
        "            results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "            if results:\n",
        "                # Extract model name for file naming\n",
        "                model_name = results[0][\"model_id\"].split(\"/\")[-1]\n",
        "\n",
        "                # Save results with model name prefix in the output directory\n",
        "                output_json = os.path.join(output_dir, f\"{model_name}_results.json\")\n",
        "                output_csv = os.path.join(output_dir, f\"{model_name}_results.csv\")\n",
        "\n",
        "                save_json(results, model_name)\n",
        "                save_csv(results, model_name)\n",
        "\n",
        "                print(f\"Completed processing {model_name}\")\n",
        "            else:\n",
        "                print(f\"No results found for {model_url}\")\n",
        "\n",
        "            # Add a delay to avoid rate limiting\n",
        "            time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {model_url}: {e}\")\n",
        "\n",
        "    print(f\"\\nAll {total_models} models processed. Results saved to '{output_dir}' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_path = input(\"Enter path to CSV file with model URLs: \")\n",
        "    process_models_from_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OH9jmoQ64X4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME+yS8idb7Esol2LxSeYbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}