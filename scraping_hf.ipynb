{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidahoderinwale/model_metadata_analyses/blob/main/scraping_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "6c30e5bf-3ac1-45c2-a58b-8c2725f2b1a1",
        "id": "zvydG0Vljv4O"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers~=4.48.3 in /usr/local/lib/python3.11/dist-packages (from adapters) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2025.1.31)\n",
            "Enter the Hugging Face model URL: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
            "\n",
            "Fetching metadata for: deepseek-ai/DeepSeek-R1\n",
            "Found 297 fine-tunes at depth 0.\n",
            "Found 121 adapter models for deepseek-ai/DeepSeek-R1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the Hugging Face model URL: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs_finetunes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mdfs_finetunes\u001b[0;34m(model_url, visited, depth, results)\u001b[0m\n\u001b[1;32m    145\u001b[0m            \u001b[0;34m\"adapters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madapter_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m            \u001b[0;34m\"adapters_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m            \u001b[0;34m\"parent_model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m        })\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mget_parent_model\u001b[0;34m(model_url)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Function to get parent model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/{model_name}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Truncate metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "# Script 3\n",
        "!pip install huggingface_hub\n",
        "!pip install adapters\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    return match.groups() if match else None\n",
        "\n",
        "# page with model finetunes\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    all_model_links = []  # Store all links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:finetune:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_model_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_model_links\n",
        "\n",
        "# model card data\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        # Try to download model card if available\n",
        "        readme_path = hf_hub_download(repo_id=model_id, filename='README.md')\n",
        "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "            card_content = f.read()\n",
        "        return card_content\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not download model card: {str(e)[:100]}...\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Function to get parent model\n",
        "def get_parent_model(model_url):\n",
        "    return model_url.split(f\"/{model_name}/\")[0]\n",
        "\n",
        "# Truncate metadata\n",
        "def filter_metadata(json_metadata):\n",
        "            keys_to_keep = [\"modelId\", \"sha\", \"tags\", \"downloads\", \"pipeline_tag\"]\n",
        "            return {k: json_metadata.get(k) for k in keys_to_keep if k in json_metadata}\n",
        "            filtered_metadata = filter_metadata(api.model_info(model_id).__dict__)\n",
        "\n",
        "# Get adapter models\n",
        "def get_adapter_models_page(model_org, model_name):\n",
        "    all_adapter_links = []  # Store all adapter links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:adapter:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_adapter_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_adapter_links\n",
        "\n",
        "# Recursive DFS (depth-first search) for finding fine-tunes\n",
        "def dfs_finetunes(model_url, visited, depth=0, results=None):\n",
        "       if results is None:\n",
        "           results = []\n",
        "\n",
        "       if model_url in visited:\n",
        "           return results\n",
        "       visited.add(model_url)\n",
        "\n",
        "       validated = validate_hf_model_url(model_url)\n",
        "       if not validated:\n",
        "           print(f\"Invalid URL skipped: {model_url}\")\n",
        "           model_url = \"N/A\"\n",
        "           return results\n",
        "\n",
        "       model_org, model_name = validated\n",
        "       model_id = f\"{model_org}/{model_name}\"\n",
        "\n",
        "\n",
        "       print(f\"\\n{'  ' * depth}Fetching metadata for: {model_id}\")\n",
        "       try:\n",
        "           model_metadata = api.model_info(model_id).__dict__\n",
        "           json_metadata = json.dumps(model_metadata, default=str)\n",
        "           model_card = get_model_card(model_id)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error fetching metadata: {e}\")\n",
        "           return results\n",
        "\n",
        "       finetune_links = get_finetuned_models_page(model_org, model_name)\n",
        "       # Removing Duplicate Children\n",
        "       finetune_links = list(set(finetune_links))\n",
        "       print(f\"{'  ' * depth}Found {len(finetune_links)} fine-tunes at depth {depth}.\")\n",
        "       adapter_links = get_adapter_models_page(model_org, model_name)\n",
        "       print(f\"{'  ' * depth}Found {len(adapter_links)} adapter models for {model_id}.\")\n",
        "\n",
        "       results.append({\n",
        "           \"model_id\": model_id,\n",
        "           \"card\": model_card,\n",
        "           \"metadata\": json_metadata,\n",
        "           \"depth\": depth,\n",
        "           \"children\": finetune_links,\n",
        "           \"children_count\": len(finetune_links),\n",
        "           \"adapters\": adapter_links,\n",
        "           \"adapters_count\": len(adapter_links),\n",
        "           \"parent_model\": get_parent_model(model_url)\n",
        "       })\n",
        "\n",
        "       for link in finetune_links:\n",
        "             dfs_finetunes(link, visited, depth + 1, results)\n",
        "       return results\n",
        "\n",
        "# Timestamp for the run\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Function to save results as JSON\n",
        "def save_json(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.json\"\n",
        "    data = {\n",
        "        \"models\": results\n",
        "    }\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4, default=str)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Function to save results as CSV\n",
        "''' def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_{timestamp}_finetunes.csv\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"model_id\", \"depth\", \"children_count\", \"children\", \"metadata\"])\n",
        "        writer.writeheader()\n",
        "        for entry in results:\n",
        "            # Ensure metadata is a JSON string\n",
        "            if isinstance(entry[\"metadata\"], dict):\n",
        "                entry[\"metadata\"] = json.dumps(entry[\"metadata\"], indent=2, default=str)\n",
        "            # Join children list as a string\n",
        "            entry[\"children\"] = \", \".join(entry[\"children\"])\n",
        "            writer.writerow(entry)\n",
        "    print(f\"Results saved to {filename}\") '''\n",
        "\n",
        "# Function to save results as CSV (pandas)\n",
        "def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=True)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "    visited = set()\n",
        "    results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "    if results:\n",
        "        model_name = results[0][\"model_id\"].split(\"/\")[-1]  # Extract model name for file naming\n",
        "        save_json(results, model_name)\n",
        "        save_csv(results, model_name)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found.\")\n",
        "\n",
        "'''Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\n",
        "https://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "6c30e5bf-3ac1-45c2-a58b-8c2725f2b1a1",
        "id": "ExG6gRZZjxAQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers~=4.48.3 in /usr/local/lib/python3.11/dist-packages (from adapters) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2025.1.31)\n",
            "Enter the Hugging Face model URL: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
            "\n",
            "Fetching metadata for: deepseek-ai/DeepSeek-R1\n",
            "Found 297 fine-tunes at depth 0.\n",
            "Found 121 adapter models for deepseek-ai/DeepSeek-R1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the Hugging Face model URL: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs_finetunes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mdfs_finetunes\u001b[0;34m(model_url, visited, depth, results)\u001b[0m\n\u001b[1;32m    145\u001b[0m            \u001b[0;34m\"adapters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madapter_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m            \u001b[0;34m\"adapters_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m            \u001b[0;34m\"parent_model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m        })\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mget_parent_model\u001b[0;34m(model_url)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Function to get parent model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/{model_name}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Truncate metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "# Script 3\n",
        "!pip install huggingface_hub\n",
        "!pip install adapters\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    return match.groups() if match else None\n",
        "\n",
        "# page with model finetunes\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    all_model_links = []  # Store all links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:finetune:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_model_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_model_links\n",
        "\n",
        "# model card data\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        # Try to download model card if available\n",
        "        readme_path = hf_hub_download(repo_id=model_id, filename='README.md')\n",
        "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "            card_content = f.read()\n",
        "        return card_content\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not download model card: {str(e)[:100]}...\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Function to get parent model\n",
        "def get_parent_model(model_url):\n",
        "    return model_url.split(f\"/{model_name}/\")[0]\n",
        "\n",
        "# Truncate metadata\n",
        "def filter_metadata(json_metadata):\n",
        "            keys_to_keep = [\"modelId\", \"sha\", \"tags\", \"downloads\", \"pipeline_tag\"]\n",
        "            return {k: json_metadata.get(k) for k in keys_to_keep if k in json_metadata}\n",
        "            filtered_metadata = filter_metadata(api.model_info(model_id).__dict__)\n",
        "\n",
        "# Get adapter models\n",
        "def get_adapter_models_page(model_org, model_name):\n",
        "    all_adapter_links = []  # Store all adapter links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:adapter:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_adapter_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_adapter_links\n",
        "\n",
        "# Recursive DFS (depth-first search) for finding fine-tunes\n",
        "def dfs_finetunes(model_url, visited, depth=0, results=None):\n",
        "       if results is None:\n",
        "           results = []\n",
        "\n",
        "       if model_url in visited:\n",
        "           return results\n",
        "       visited.add(model_url)\n",
        "\n",
        "       validated = validate_hf_model_url(model_url)\n",
        "       if not validated:\n",
        "           print(f\"Invalid URL skipped: {model_url}\")\n",
        "           model_url = \"N/A\"\n",
        "           return results\n",
        "\n",
        "       model_org, model_name = validated\n",
        "       model_id = f\"{model_org}/{model_name}\"\n",
        "\n",
        "\n",
        "       print(f\"\\n{'  ' * depth}Fetching metadata for: {model_id}\")\n",
        "       try:\n",
        "           model_metadata = api.model_info(model_id).__dict__\n",
        "           json_metadata = json.dumps(model_metadata, default=str)\n",
        "           model_card = get_model_card(model_id)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error fetching metadata: {e}\")\n",
        "           return results\n",
        "\n",
        "       finetune_links = get_finetuned_models_page(model_org, model_name)\n",
        "       # Removing Duplicate Children\n",
        "       finetune_links = list(set(finetune_links))\n",
        "       print(f\"{'  ' * depth}Found {len(finetune_links)} fine-tunes at depth {depth}.\")\n",
        "       adapter_links = get_adapter_models_page(model_org, model_name)\n",
        "       print(f\"{'  ' * depth}Found {len(adapter_links)} adapter models for {model_id}.\")\n",
        "\n",
        "       results.append({\n",
        "           \"model_id\": model_id,\n",
        "           \"card\": model_card,\n",
        "           \"metadata\": json_metadata,\n",
        "           \"depth\": depth,\n",
        "           \"children\": finetune_links,\n",
        "           \"children_count\": len(finetune_links),\n",
        "           \"adapters\": adapter_links,\n",
        "           \"adapters_count\": len(adapter_links),\n",
        "           \"parent_model\": get_parent_model(model_url)\n",
        "       })\n",
        "\n",
        "       for link in finetune_links:\n",
        "             dfs_finetunes(link, visited, depth + 1, results)\n",
        "       return results\n",
        "\n",
        "# Timestamp for the run\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Function to save results as JSON\n",
        "def save_json(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.json\"\n",
        "    data = {\n",
        "        \"models\": results\n",
        "    }\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4, default=str)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Function to save results as CSV\n",
        "''' def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_{timestamp}_finetunes.csv\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"model_id\", \"depth\", \"children_count\", \"children\", \"metadata\"])\n",
        "        writer.writeheader()\n",
        "        for entry in results:\n",
        "            # Ensure metadata is a JSON string\n",
        "            if isinstance(entry[\"metadata\"], dict):\n",
        "                entry[\"metadata\"] = json.dumps(entry[\"metadata\"], indent=2, default=str)\n",
        "            # Join children list as a string\n",
        "            entry[\"children\"] = \", \".join(entry[\"children\"])\n",
        "            writer.writerow(entry)\n",
        "    print(f\"Results saved to {filename}\") '''\n",
        "\n",
        "# Function to save results as CSV (pandas)\n",
        "def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=True)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "    visited = set()\n",
        "    results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "    if results:\n",
        "        model_name = results[0][\"model_id\"].split(\"/\")[-1]  # Extract model name for file naming\n",
        "        save_json(results, model_name)\n",
        "        save_csv(results, model_name)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found.\")\n",
        "\n",
        "'''Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\n",
        "https://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "6c30e5bf-3ac1-45c2-a58b-8c2725f2b1a1",
        "id": "kUiZZoDqjyOO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers~=4.48.3 in /usr/local/lib/python3.11/dist-packages (from adapters) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2025.1.31)\n",
            "Enter the Hugging Face model URL: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
            "\n",
            "Fetching metadata for: deepseek-ai/DeepSeek-R1\n",
            "Found 297 fine-tunes at depth 0.\n",
            "Found 121 adapter models for deepseek-ai/DeepSeek-R1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mmodel_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the Hugging Face model URL: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs_finetunes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mdfs_finetunes\u001b[0;34m(model_url, visited, depth, results)\u001b[0m\n\u001b[1;32m    145\u001b[0m            \u001b[0;34m\"adapters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madapter_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m            \u001b[0;34m\"adapters_count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m            \u001b[0;34m\"parent_model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m        })\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-c00c90202382>\u001b[0m in \u001b[0;36mget_parent_model\u001b[0;34m(model_url)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Function to get parent model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_parent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/{model_name}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Truncate metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "# Script 3\n",
        "!pip install huggingface_hub\n",
        "!pip install adapters\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    return match.groups() if match else None\n",
        "\n",
        "# page with model finetunes\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    all_model_links = []  # Store all links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:finetune:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_model_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_model_links\n",
        "\n",
        "# model card data\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        # Try to download model card if available\n",
        "        readme_path = hf_hub_download(repo_id=model_id, filename='README.md')\n",
        "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "            card_content = f.read()\n",
        "        return card_content\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not download model card: {str(e)[:100]}...\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Function to get parent model\n",
        "def get_parent_model(model_url):\n",
        "    return model_url.split(f\"/{model_name}/\")[0]\n",
        "\n",
        "# Truncate metadata\n",
        "def filter_metadata(json_metadata):\n",
        "            keys_to_keep = [\"modelId\", \"sha\", \"tags\", \"downloads\", \"pipeline_tag\"]\n",
        "            return {k: json_metadata.get(k) for k in keys_to_keep if k in json_metadata}\n",
        "            filtered_metadata = filter_metadata(api.model_info(model_id).__dict__)\n",
        "\n",
        "# Get adapter models\n",
        "def get_adapter_models_page(model_org, model_name):\n",
        "    all_adapter_links = []  # Store all adapter links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:adapter:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_adapter_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_adapter_links\n",
        "\n",
        "# Recursive DFS (depth-first search) for finding fine-tunes\n",
        "def dfs_finetunes(model_url, visited, depth=0, results=None):\n",
        "       if results is None:\n",
        "           results = []\n",
        "\n",
        "       if model_url in visited:\n",
        "           return results\n",
        "       visited.add(model_url)\n",
        "\n",
        "       validated = validate_hf_model_url(model_url)\n",
        "       if not validated:\n",
        "           print(f\"Invalid URL skipped: {model_url}\")\n",
        "           model_url = \"N/A\"\n",
        "           return results\n",
        "\n",
        "       model_org, model_name = validated\n",
        "       model_id = f\"{model_org}/{model_name}\"\n",
        "\n",
        "\n",
        "       print(f\"\\n{'  ' * depth}Fetching metadata for: {model_id}\")\n",
        "       try:\n",
        "           model_metadata = api.model_info(model_id).__dict__\n",
        "           json_metadata = json.dumps(model_metadata, default=str)\n",
        "           model_card = get_model_card(model_id)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error fetching metadata: {e}\")\n",
        "           return results\n",
        "\n",
        "       finetune_links = get_finetuned_models_page(model_org, model_name)\n",
        "       # Removing Duplicate Children\n",
        "       finetune_links = list(set(finetune_links))\n",
        "       print(f\"{'  ' * depth}Found {len(finetune_links)} fine-tunes at depth {depth}.\")\n",
        "       adapter_links = get_adapter_models_page(model_org, model_name)\n",
        "       print(f\"{'  ' * depth}Found {len(adapter_links)} adapter models for {model_id}.\")\n",
        "\n",
        "       results.append({\n",
        "           \"model_id\": model_id,\n",
        "           \"card\": model_card,\n",
        "           \"metadata\": json_metadata,\n",
        "           \"depth\": depth,\n",
        "           \"children\": finetune_links,\n",
        "           \"children_count\": len(finetune_links),\n",
        "           \"adapters\": adapter_links,\n",
        "           \"adapters_count\": len(adapter_links),\n",
        "           \"parent_model\": get_parent_model(model_url)\n",
        "       })\n",
        "\n",
        "       for link in finetune_links:\n",
        "             dfs_finetunes(link, visited, depth + 1, results)\n",
        "       return results\n",
        "\n",
        "# Timestamp for the run\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Function to save results as JSON\n",
        "def save_json(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.json\"\n",
        "    data = {\n",
        "        \"models\": results\n",
        "    }\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4, default=str)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Function to save results as CSV\n",
        "''' def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_{timestamp}_finetunes.csv\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"model_id\", \"depth\", \"children_count\", \"children\", \"metadata\"])\n",
        "        writer.writeheader()\n",
        "        for entry in results:\n",
        "            # Ensure metadata is a JSON string\n",
        "            if isinstance(entry[\"metadata\"], dict):\n",
        "                entry[\"metadata\"] = json.dumps(entry[\"metadata\"], indent=2, default=str)\n",
        "            # Join children list as a string\n",
        "            entry[\"children\"] = \", \".join(entry[\"children\"])\n",
        "            writer.writerow(entry)\n",
        "    print(f\"Results saved to {filename}\") '''\n",
        "\n",
        "# Function to save results as CSV (pandas)\n",
        "def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=True)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "    visited = set()\n",
        "    results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "    if results:\n",
        "        model_name = results[0][\"model_id\"].split(\"/\")[-1]  # Extract model name for file naming\n",
        "        save_json(results, model_name)\n",
        "        save_csv(results, model_name)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found.\")\n",
        "\n",
        "'''Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\n",
        "https://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxlHKer226vF"
      },
      "source": [
        "# Script 1\n",
        "Takes in a (validated) model url and output its metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "Fmp0ftEomk86",
        "outputId": "3b3d0b58-f682-4d40-d0cb-9d2f0f493bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting validators\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m781.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators\n",
            "Successfully installed validators-0.34.0\n",
            "Enter model URL: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
            "You entered: https://huggingface.co/deepseek-ai/DeepSeek-R1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'generator' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b08047d983af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# To test: https://huggingface.co/deepseek-ai/DeepSeek-R1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0minput_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-b08047d983af>\u001b[0m in \u001b[0;36minput_url\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_model_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"huggingface.co/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mmodel_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get model info: https://huggingface.co/docs/huggingface_hub/v0.29.2/en/package_reference/hf_api#huggingface_hub.ModelInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mmodel_card\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_hub_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'README.md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 metadata = { # Moved metadata assignment inside the if block\n",
            "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Script 1: takes input model url, validates url, and gives model metadata\n",
        "!pip install validators\n",
        "from huggingface_hub import HfApi\n",
        "import huggingface_hub as hf\n",
        "import validators\n",
        "import json\n",
        "import csv\n",
        "\n",
        "hf_api = HfApi()\n",
        "\n",
        "def input_url():\n",
        "    while True:\n",
        "        input_model_url = input(\"Enter model URL: \")\n",
        "        print(f\"You entered: {input_model_url}\")\n",
        "\n",
        "        if validators.url(input_model_url) and \"huggingface.co\" in input_model_url:\n",
        "                # Extract the model ID from the URL\n",
        "                model_id = input_model_url.split(\"huggingface.co/\")[-1]\n",
        "                model_info = hf_api.model_info(model_id) # Get model info: https://huggingface.co/docs/huggingface_hub/v0.29.2/en/package_reference/hf_api#huggingface_hub.ModelInfo\n",
        "                model_card = hf.hf_hub_download(models[0].modelId, 'README.md')\n",
        "\n",
        "                metadata = { # Moved metadata assignment inside the if block\n",
        "                    \"model_id\": model_id,\n",
        "                    \"model_info\": model_info.__dict__\n",
        "                }\n",
        "                return metadata # Return is now inside if block\n",
        "                json_output = json.dumps(metadata, indent=4, default=str)\n",
        "                with open('model_info.json', 'w') as json_file:\n",
        "                 json_file.write(json_output)\n",
        "                print(json_output)\n",
        "        else:\n",
        "            print(\"Invalid URL. Please enter a valid Hugging Face model URL.\")\n",
        "            # Optionally: You could return None or an empty dictionary here\n",
        "            # to indicate an invalid URL\n",
        "# To test: https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
        "\n",
        "input_url()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOnHUAW23SNE"
      },
      "source": [
        "# Script 2\n",
        "Takes in a (validated) model and outputs the children models/fine-tunes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6petSSd-m2Qi"
      },
      "outputs": [],
      "source": [
        "# Script 2\n",
        "  # 1. Take link as input (format check). This is the \"main model\"\n",
        "  # 2. Give the link to the page with the fine-tunes for the inputted model\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    if match:\n",
        "        return match.groups()  # Returns (org/user, model_name)\n",
        "    return None\n",
        "\n",
        "# Function to find fine-tuned models\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    search_url = f\"https://huggingface.co/models?search={model_name}\"\n",
        "    response = requests.get(search_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_links = [\n",
        "            a[\"href\"] for a in soup.find_all(\"a\", href=True)\n",
        "            if model_name.lower() in a[\"href\"].lower()\n",
        "        ]\n",
        "        return [f\"https://huggingface.co{link}\" for link in model_links if model_org not in link]\n",
        "\n",
        "    return []\n",
        "\n",
        "# Main execution\n",
        "model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "\n",
        "validated = validate_hf_model_url(model_url)\n",
        "if validated:\n",
        "    org, model_name = validated\n",
        "    finetune_links = get_finetuned_models_page(org, model_name)\n",
        "\n",
        "    if finetune_links:\n",
        "        print(\"Fine-tuned models found:\")\n",
        "        for link in finetune_links:\n",
        "            print(link)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found for this model.\")\n",
        "else:\n",
        "    print(\"Invalid Hugging Face model URL format.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruI5-1FA1ipq"
      },
      "source": [
        "# Script 3\n",
        "## Search steps overview\n",
        "- `dfs_finetunes` we take the `model_url` as input. Alternatively, we can add this var as an argument.\n",
        "- We go layer-by-layer and find the children of the current model (i.e. the fine-tunes of a model)\n",
        "- We call the `dfs_funetunes` function recursively and store the models that have been \"visited\" to avoid duplicates.\n",
        "- We have a dictionary of information that we store about the \"current model\" and have the information stored in respective columns (model_id, card, metadata, depth, children (list of model links), children count).\n",
        "- We have a `results` list that has the information about all the models and their fine-tunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da_92M_Cgv1N"
      },
      "outputs": [],
      "source": [
        "# Script 3\n",
        "!pip install huggingface_hub\n",
        "!pip install adapters\n",
        "!pip install backoff # handle rate-limiting\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "from backoff import\n",
        "\n",
        "\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Function to validate Hugging Face model URL\n",
        "def validate_hf_model_url(url):\n",
        "    pattern = r\"^https://huggingface.co/([\\w\\-]+)/([\\w\\-]+)$\"\n",
        "    match = re.match(pattern, url)\n",
        "    return match.groups() if match else None\n",
        "\n",
        "# page with model finetunes\n",
        "def get_finetuned_models_page(model_org, model_name):\n",
        "    all_model_links = []  # Store all links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:finetune:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_model_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_model_links\n",
        "\n",
        "# model card data\n",
        "@backoff.on_exception(backoff.expo,\n",
        "                      requests.exceptions.RequestException,\n",
        "                      max_tries=5)\n",
        "def get_model_card(model_id):\n",
        "    try:\n",
        "        # Try to download model card if available\n",
        "        readme_path = hf_hub_download(repo_id=model_id, filename='README.md')\n",
        "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "            card_content = f.read()\n",
        "        return card_content\n",
        "    except Exception as e:\n",
        "        print(f\"  Could not download model card: {str(e)[:100]}...\")\n",
        "        return \"\"\n",
        "\n",
        "# Truncate metadata\n",
        "def filter_metadata(json_metadata):\n",
        "            keys_to_keep = [\"modelId\", \"sha\", \"tags\", \"downloads\", \"pipeline_tag\"]\n",
        "            return {k: json_metadata.get(k) for k in keys_to_keep if k in json_metadata}\n",
        "            filtered_metadata = filter_metadata(api.model_info(model_id).__dict__)\n",
        "\n",
        "# Get adapter models\n",
        "@backoff.on_exception(backoff.expo,\n",
        "                      requests.exceptions.RequestException,\n",
        "                      max_tries=5)\n",
        "def get_adapter_models_page(model_org, model_name):\n",
        "    all_adapter_links = []  # Store all adapter links across pages\n",
        "    page_num = 0\n",
        "    while True:\n",
        "        search_url = f\"https://huggingface.co/models?other=base_model:adapter:{model_org}/{model_name}&p={page_num}\"\n",
        "        response = requests.get(search_url)\n",
        "        if response.status_code != 200:\n",
        "            break  # Exit if page not found\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        model_divs = soup.find_all(\"div\", class_=\"w-full truncate\")\n",
        "        if not model_divs:\n",
        "            break  # Exit if no more models on the page\n",
        "\n",
        "        for div in model_divs:\n",
        "            header = div.find(\"header\")\n",
        "            if header:\n",
        "                model_link = header.get(\"title\")\n",
        "                if model_link:\n",
        "                    all_adapter_links.append(f\"https://huggingface.co/{model_link}\")\n",
        "\n",
        "        page_num += 1  # Move to the next page\n",
        "\n",
        "    return all_adapter_links\n",
        "\n",
        "# Recursive DFS (depth-first search) for finding fine-tunes\n",
        "@backoff.on_exception(backoff.expo,\n",
        "                      (requests.exceptions.RequestException,\n",
        "                       huggingface_hub.utils.HfHubHTTPError),\n",
        "                      max_tries=5)\n",
        "def dfs_finetunes(model_url, visited, depth=0, results=None):\n",
        "       if results is None:\n",
        "           results = []\n",
        "\n",
        "       if model_url in visited:\n",
        "           return results\n",
        "       visited.add(model_url)\n",
        "\n",
        "       validated = validate_hf_model_url(model_url)\n",
        "       if not validated:\n",
        "           print(f\"Invalid URL skipped: {model_url}\")\n",
        "           model_url = \"N/A\"\n",
        "           return results\n",
        "\n",
        "       model_org, model_name = validated\n",
        "       model_id = f\"{model_org}/{model_name}\"\n",
        "\n",
        "\n",
        "       print(f\"\\n{'  ' * depth}Fetching metadata for: {model_id}\")\n",
        "       try:\n",
        "           model_metadata = api.model_info(model_id).__dict__\n",
        "           json_metadata = json.dumps(model_metadata, default=str)\n",
        "           model_card = get_model_card(model_id)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error fetching metadata: {e}\")\n",
        "           return results\n",
        "\n",
        "       finetune_links = get_finetuned_models_page(model_org, model_name)\n",
        "       # Removing Duplicate Children\n",
        "       finetune_links = list(set(finetune_links))\n",
        "       print(f\"{'  ' * depth}Found {len(finetune_links)} fine-tunes at depth {depth}.\")\n",
        "       adapter_links = get_adapter_models_page(model_org, model_name)\n",
        "       print(f\"{'  ' * depth}Found {len(adapter_links)} adapter models for {model_id}.\")\n",
        "\n",
        "       results.append({\n",
        "           \"model_id\": model_id,\n",
        "           \"card\": model_card,\n",
        "           \"metadata\": json_metadata,\n",
        "           \"depth\": depth,\n",
        "           \"children\": finetune_links,\n",
        "           \"children_count\": len(finetune_links),\n",
        "           \"adapters\": adapter_links,\n",
        "           \"adapters_count\": len(adapter_links)\n",
        "       })\n",
        "\n",
        "       for link in finetune_links:\n",
        "             dfs_finetunes(link, visited, depth + 1, results)\n",
        "       return results\n",
        "\n",
        "# Timestamp for the run\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Function to save results as JSON\n",
        "def save_json(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.json\"\n",
        "    data = {\n",
        "        \"models\": results\n",
        "    }\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4, default=str)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Function to save results as CSV\n",
        "''' def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_{timestamp}_finetunes.csv\"\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"model_id\", \"depth\", \"children_count\", \"children\", \"metadata\"])\n",
        "        writer.writeheader()\n",
        "        for entry in results:\n",
        "            # Ensure metadata is a JSON string\n",
        "            if isinstance(entry[\"metadata\"], dict):\n",
        "                entry[\"metadata\"] = json.dumps(entry[\"metadata\"], indent=2, default=str)\n",
        "            # Join children list as a string\n",
        "            entry[\"children\"] = \", \".join(entry[\"children\"])\n",
        "            writer.writerow(entry)\n",
        "    print(f\"Results saved to {filename}\") '''\n",
        "\n",
        "# Function to save results as CSV (pandas)\n",
        "def save_csv(results, model_name):\n",
        "    filename = f\"{model_name}_finetunes_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=True)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    model_url = input(\"Enter the Hugging Face model URL: \").strip()\n",
        "    visited = set()\n",
        "    results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "    if results:\n",
        "        model_name = results[0][\"model_id\"].split(\"/\")[-1]  # Extract model name for file naming\n",
        "        save_json(results, model_name)\n",
        "        save_csv(results, model_name)\n",
        "    else:\n",
        "        print(\"No fine-tuned models found.\")\n",
        "\n",
        "'''Links for testing: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview (3 fine-tunes at depth 0, 1 fine-tune at depth 1 for 'AlSamCur123/DeepHermes-3-Mistral-24BContinuedFine')\n",
        "https://huggingface.co/perplexity-ai/r1-1776 (11 fine-tunes at depth 0)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWhCzNPsiXgX"
      },
      "source": [
        "# Scripts 4 (Tree Viz)\n",
        "\n",
        "Iteratively produces model tree datasets when given a list of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn8T_f0IivaD"
      },
      "outputs": [],
      "source": [
        "# 4.1 Phylogenetic tree\n",
        "\n",
        "!pip install Bio\n",
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "from Bio import Phylo\n",
        "from Bio.Phylo.BaseTree import Clade, Tree\n",
        "\n",
        "# Load and prepare tree\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "def build_phylo_tree_from_dfs(df):\n",
        "    clade_map = {row['model_id']: Clade(name=row['model_id']) for _, row in df.iterrows()}\n",
        "    parent_links = {}\n",
        "    for _, row in df.iterrows():\n",
        "        parent = row['model_id']\n",
        "        for child_url in row['children']:\n",
        "            child = '/'.join(child_url.split(\"/\")[-2:])\n",
        "            parent_links[child] = parent\n",
        "    all_models = set(df['model_id'])\n",
        "    child_models = set(parent_links.keys())\n",
        "    root_model_id = list(all_models - child_models)[0]\n",
        "    for child, parent in parent_links.items():\n",
        "        if child in clade_map and parent in clade_map:\n",
        "            clade_map[parent].clades.append(clade_map[child])\n",
        "    return Tree(root=clade_map[root_model_id])\n",
        "\n",
        "tree = build_phylo_tree_from_dfs(df)\n",
        "\n",
        "# Plotting with labels on branches\n",
        "fig = plt.figure(figsize=(24, 36))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "# Draw the tree without axes\n",
        "Phylo.draw(tree, axes=ax, do_show=False)\n",
        "\n",
        "# Remove all axes elements\n",
        "ax.set_axis_off()\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "# Ensure labels are positioned away from branch lines\n",
        "for label in ax.texts:\n",
        "    pos = label.get_position()\n",
        "    label.set_position((pos[0], pos[1]))\n",
        "    label.set_bbox(dict(facecolor='white', alpha=0.8, edgecolor='none', pad=3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('clean_phylogenetic_tree.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uAQvfhCa0TJ6",
        "outputId": "44c7e645-1be5-45e5-9c37-96eb58b76418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk librsvg2-common libxdot4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 2,434 kB of archives.\n",
            "After this operation, 7,681 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libxdot4 amd64 2.42.2-6ubuntu0.1 [16.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgvc6-plugins-gtk amd64 2.42.2-6ubuntu0.1 [22.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgraphviz-dev amd64 2.42.2-6ubuntu0.1 [58.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Fetched 2,434 kB in 1s (2,832 kB/s)\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 126315 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libxdot4:amd64.\n",
            "Preparing to unpack .../4-libxdot4_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgraphviz-dev:amd64.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.42.2-6ubuntu0.1_amd64.deb ...\n",
            "Unpacking libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../8-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libxdot4:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgvc6-plugins-gtk (2.42.2-6ubuntu0.1) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgraphviz-dev:amd64 (2.42.2-6ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Collecting pygraphviz\n",
            "  Downloading pygraphviz-1.14.tar.gz (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.0/106.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.14-cp311-cp311-linux_x86_64.whl size=169713 sha256=14200728290892ddc10302fb4aa5e24b52366827561e0a25b6ba05b21b137890\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/5f/df/6fffd2a4353f26dbb0e3672a1baf070c124a1d74a5f9318279\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.14\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88285ae38124>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load and prepare the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'"
          ]
        }
      ],
      "source": [
        "# 4.1 Networkx (in progress)\n",
        "!apt install graphviz libgraphviz-dev\n",
        "!pip install pygraphviz\n",
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Load and prepare the dataset\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for _, row in df.iterrows():\n",
        "    parent = row['model_id']\n",
        "    G.add_node(parent)\n",
        "    for child_url in row['children']:\n",
        "        child = '/'.join(child_url.split(\"/\")[-2:])\n",
        "        G.add_node(child)\n",
        "        G.add_edge(parent, child)\n",
        "\n",
        "# Find root node\n",
        "root = [n for n, d in G.in_degree() if d == 0][0]\n",
        "\n",
        "# Create a hierarchical layout\n",
        "pos = nx.nx_agraph.graphviz_layout(G, prog='dot', root=root)\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(16, 24))\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=10, node_color='white', edgecolors='black')\n",
        "\n",
        "# Draw edges\n",
        "nx.draw_networkx_edges(G, pos, arrows=False)\n",
        "\n",
        "# Draw labels, rotating only non-root nodes\n",
        "label_pos = {k: (v[0], v[1]) for k, v in pos.items()}\n",
        "for node, (x, y) in label_pos.items():\n",
        "    if node != root:  # Rotate labels for non-root nodes\n",
        "        plt.text(x, y, s=node, rotation=90, ha='center', va='center', fontsize=9)\n",
        "    else:  # Keep root label horizontal\n",
        "        plt.text(x, y, s=node, ha='center', va='center', fontsize=9)\n",
        "\n",
        "# Remove axes\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"mistral_finetune_tree_networkx.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Tree visualization saved to mistral_finetune_tree_networkx.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JM61pKgp77Vl",
        "outputId": "78c8c140-000b-45a2-bdcb-f724ec9fee7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ete3\n",
            "  Downloading ete3-3.1.3.tar.gz (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ete3\n",
            "  Building wheel for ete3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ete3: filename=ete3-3.1.3-py3-none-any.whl size=2273786 sha256=11e90a6cb410702e112dd5bb1b8cef8bb0a9090cbb05bb8d9a45eb80da1f6665\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/a8/60/0a29caa9f8ceb7316704be63c1578ab13c36668abb646366ac\n",
            "Successfully built ete3\n",
            "Installing collected packages: ete3\n",
            "Successfully installed ete3-3.1.3\n",
            "Collecting tree\n",
            "  Downloading Tree-0.2.4.tar.gz (6.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tree) (11.1.0)\n",
            "Collecting svgwrite (from tree)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tree) (75.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tree) (8.1.8)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tree\n",
            "  Building wheel for tree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree: filename=Tree-0.2.4-py3-none-any.whl size=7860 sha256=59cd350907ddf5fc946a6284821b20adcb2e8efbe4421508eb98085447f94f41\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/ed/fe/b4c6a9b7a5b8df6d966ea673e26a46a7451b020af754eafa6b\n",
            "Successfully built tree\n",
            "Installing collected packages: svgwrite, tree\n",
            "Successfully installed svgwrite-1.4.3 tree-0.2.4\n",
            "Collecting PyQt5\n",
            "  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting PyQt5-sip<13,>=12.15 (from PyQt5)\n",
            "  Downloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (472 bytes)\n",
            "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from PyQt5)\n",
            "  Downloading PyQt5_Qt5-5.15.16-1-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n",
            "Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_Qt5-5.15.16-1-py3-none-manylinux2014_x86_64.whl (61.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl (276 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\n",
            "Successfully installed PyQt5-5.15.11 PyQt5-Qt5-5.15.16 PyQt5-sip-12.17.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-205e6eecddea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load and prepare the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/DeepSeek-R1_finetunes_20250408_202441.csv'"
          ]
        }
      ],
      "source": [
        "# 4.3 library (in progress)\n",
        "!pip install ete3\n",
        "!pip install tree\n",
        "!pip install PyQt5\n",
        "import pandas as pd\n",
        "import ast\n",
        "from ete3 import Tree, faces, AttrFace, TreeStyle\n",
        "\n",
        "\n",
        "# Load and prepare the dataset\n",
        "df = pd.read_csv(\"/content/DeepSeek-R1_finetunes_20250408_202441.csv\")\n",
        "df['children'] = df['children'].apply(ast.literal_eval)\n",
        "\n",
        "# Function to build the ete3 tree recursively\n",
        "def build_ete3_tree(df, parent_node=None, parent_name=None):\n",
        "    if parent_node is None:\n",
        "        # Create the root node if it's the first call\n",
        "        root_name = df.loc[df['depth'] == 0, 'model_id'].iloc[0]\n",
        "        tree = Tree(name=root_name)\n",
        "        parent_node = tree\n",
        "    else:\n",
        "        tree = parent_node  # Continue building the existing tree\n",
        "\n",
        "    # Find children of the current parent\n",
        "    children_rows = df.loc[df['model_id'] == parent_name]\n",
        "    if not children_rows.empty:\n",
        "        children_urls = children_rows['children'].iloc[0]\n",
        "        for child_url in children_urls:\n",
        "            child_name = '/'.join(child_url.split(\"/\")[-2:])\n",
        "            child_node = tree.add_child(name=child_name)\n",
        "            build_ete3_tree(df, child_node, child_name)  # Recursive call\n",
        "    return tree\n",
        "\n",
        "# Build the tree\n",
        "tree = build_ete3_tree(df)\n",
        "\n",
        "# Style the tree\n",
        "ts.show_leaf_name = False  # Hide leaf names\n",
        "ts.mode = \"c\"  # Circular tree layout\n",
        "ts.root_opening_factor = 1  # Adjust root opening for circular layout\n",
        "\n",
        "# Style the nodes\n",
        "nst = NodeStyle()\n",
        "nst[\"size\"] = 0  # Hide node circles\n",
        "nst[\"hz_line_width\"] = 2\n",
        "nst[\"vt_line_width\"] = 2\n",
        "\n",
        "for n in tree.traverse():\n",
        "    n.set_style(nst)\n",
        "    # Add model name as a branch label\n",
        "    name_face = TextFace(n.name, fsize=10, fgcolor=\"black\")\n",
        "    n.add_face(name_face, column=0, position=\"branch-right\")\n",
        "\n",
        "# Render and save the tree\n",
        "tree.render(\"mistral_finetune_tree_ete3.png\", tree_style=ts, dpi=300)\n",
        "print(\"Tree visualization saved to mistral_finetune_tree_ete3.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script 6\n",
        "\n",
        "Get top 1000 models from the model-hub ordered by the number of likes"
      ],
      "metadata": {
        "id": "4rt2ohl3wnxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "# documentation: https://huggingface.co/docs/hub/en/api, gets top 1000 models sorted by likes in descending order\n",
        "models = api.list_models(sort=\"likes\", direction=\"-1\", limit=1000)\n",
        "\n",
        "df = pd.DataFrame(models)\n",
        "models_file = df.to_csv(f\"{time}_model_list.csv\", index=\"false\")\n",
        "print(\"Models saved to CSV\")\n"
      ],
      "metadata": {
        "id": "cH5A_cZUwnJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgXrshhgesGd"
      },
      "source": [
        "# Script 7\n",
        "\n",
        "This script allows you to iteratively run the dfs function for multiple models given as a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS_K1AScbBdE"
      },
      "outputs": [],
      "source": [
        "# Multi-model runner for HuggingFace model analysis (sampled from top 10 models)\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "from adapters import list_adapters\n",
        "from huggingface_hub import hf_hub_download\n",
        "from adapters import AutoAdapterModel\n",
        "import re\n",
        "\n",
        "# Multi-model runner for HuggingFace model analysis (sampled from top 10 models)\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from huggingface_hub import HfApi\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "# Make sure these are imported or defined in the same file or via `import script3`\n",
        "#from /content/script3.py import dfs_finetunes, save_json, save_csv  # Import from script3.py\n",
        "# Assuming script3.py is in the same directory, and dfs_finetunes, save_json, and save_csv are defined there\n",
        "from script3 import dfs_finetunes, save_json, save_csv\n",
        "\n",
        "\n",
        "# Timestamp to share across all model saves\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def process_models_from_csv(csv_path, output_dir=\"results\", limit=10):\n",
        "    \"\"\"\n",
        "    Processes top N models from a CSV by calling `dfs_finetunes` for each.\n",
        "    Constructs model URLs from the 'id' column and saves results in JSON/CSV.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'id' not in df.columns:\n",
        "        raise ValueError(\"CSV must contain a column named 'id'.\")\n",
        "\n",
        "    df = df.head(limit)\n",
        "    total_models = len(df)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        model_id = row['id'].strip()\n",
        "        model_url = f\"https://huggingface.co/{model_id}\"\n",
        "        print(f\"\\n[{idx + 1}/{total_models}] Running analysis for: {model_url}\")\n",
        "\n",
        "        try:\n",
        "            visited = set()\n",
        "            results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "            if results:\n",
        "                model_name = model_id.split(\"/\")[-1]\n",
        "                json_path = os.path.join(output_dir, f\"{model_name}_finetunes_{timestamp}.json\")\n",
        "                csv_path = os.path.join(output_dir, f\"{model_name}_finetunes_{timestamp}.csv\")\n",
        "\n",
        "                save_json(results, model_name)\n",
        "                save_csv(results, model_name)\n",
        "\n",
        "                print(f\"Completed: {model_name}\")\n",
        "            else:\n",
        "                print(f\"No results for {model_id}\")\n",
        "\n",
        "            time.sleep(2)  # Rate limit buffer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {model_id}: {e}\")\n",
        "\n",
        "    print(f\"\\nFinished processing {total_models} models. Saved to '{output_dir}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_input = input(\"Path to CSV with model IDs: \").strip()\n",
        "    process_models_from_csv(csv_input)\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def process_models_from_csv(csv_path, output_dir=\"results\", limit=10):\n",
        "    \"\"\"\n",
        "    Processes top N models from a CSV by calling `dfs_finetunes` for each.\n",
        "    Constructs model URLs from the 'id' column and saves results in JSON/CSV.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'id' not in df.columns:\n",
        "        raise ValueError(\"CSV must contain a column named 'id'.\")\n",
        "\n",
        "    df = df.head(limit)\n",
        "    total_models = len(df)\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        model_id = row['id'].strip()\n",
        "        model_url = f\"https://huggingface.co/{model_id}\"\n",
        "        print(f\"\\n[{idx + 1}/{total_models}] Running analysis for: {model_url}\")\n",
        "\n",
        "        try:\n",
        "            visited = set()\n",
        "            results = dfs_finetunes(model_url, visited)\n",
        "\n",
        "            if results:\n",
        "                model_name = model_id.split(\"/\")[-1]\n",
        "                json_path = os.path.join(output_dir, f\"{model_name}_finetunes_{timestamp}.json\")\n",
        "                csv_path = os.path.join(output_dir, f\"{model_name}_finetunes_{timestamp}.csv\")\n",
        "\n",
        "                save_json(results, model_name)\n",
        "                save_csv(results, model_name)\n",
        "\n",
        "                print(f\"Completed: {model_name}\")\n",
        "            else:\n",
        "                print(f\"No results for {model_id}\")\n",
        "\n",
        "            time.sleep(2)  # Rate limit buffer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {model_id}: {e}\")\n",
        "\n",
        "    print(f\"\\nFinished processing {total_models} models. Saved to '{output_dir}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_input = input(\"Path to CSV with model IDs: \").strip()\n",
        "    process_models_from_csv(csv_input)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OH9jmoQ64X4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAfur/SMu7DEQcigkWWXcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}