{
    "models": [
        {
            "model_id": "meta-llama/Llama-2-7b",
            "card": "---\nextra_gated_heading: You need to share contact information with Meta to access this model\nextra_gated_prompt: >-\n  ### LLAMA 2 COMMUNITY LICENSE AGREEMENT\n\n  \"Agreement\" means the terms and conditions for use, reproduction, distribution\n  and  modification of the Llama Materials set forth herein. \n\n  \"Documentation\" means the specifications, manuals and documentation \n  accompanying Llama 2 distributed by Meta at\n  https://ai.meta.com/resources/models-and-libraries/llama-downloads/.  \n\n  \"Licensee\" or \"you\" means you, or your employer or any other person or entity\n  (if you are entering into this Agreement on such person or entity's behalf),\n  of the age required under applicable laws, rules or regulations to provide\n  legal consent and that has legal authority to bind your employer or such other\n  person or  entity if you are  entering in this Agreement on their behalf. \n\n  \"Llama 2\" means the foundational large language models and software and\n  algorithms, including machine-learning model code, trained model weights,\n  inference-enabling code, training-enabling code, fine-tuning enabling code and\n  other  elements of the foregoing distributed by Meta at\n  ai.meta.com/resources/models-and-libraries/llama-downloads/.\n\n  \"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and\n  documentation (and any portion thereof) made available under this Agreement.\n\n  \"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or,\n  if you are an entity, your principal place of business is in the EEA or\n  Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA\n  or Switzerland). \n\n\n  By clicking \"I Accept\" below or by using or distributing any portion or\n  element of the Llama Materials, you agree to be bound by this Agreement.\n\n  1. License Rights and Redistribution. \n\n  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-\n  transferable and royalty-free limited license under Meta's intellectual\n  property or  other rights owned by Meta embodied in the Llama Materials to\n  use, reproduce,  distribute, copy, create derivative works of, and make\n  modifications to the Llama  Materials.  \n\n  b. Redistribution and Use.\n\n  i. If you distribute or make the Llama Materials, or any derivative works \n  thereof, available to a third party, you shall provide a copy of this\n  Agreement to such  third party. \n\n  ii.  If you receive Llama Materials, or any derivative works thereof, from  a\n  Licensee as part of an integrated end user product, then Section 2 of this \n  Agreement will not apply to you. \n\n  iii. You must retain in all copies of the Llama Materials that you  distribute\n  the following attribution notice within a \"Notice\" text file distributed as a \n  part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community\n  License,  Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"\n\n  iv. Your use of the Llama Materials must comply with applicable laws  and\n  regulations (including trade compliance laws and regulations) and adhere to\n  the  Acceptable Use Policy for the Llama Materials (available at \n  https://ai.meta.com/llama/use-policy), which is hereby incorporated by\n  reference into  this Agreement.\n\n  v. You will not use the Llama Materials or any output or results of the  Llama\n  Materials to improve any other large language model (excluding Llama 2 or \n  derivative works thereof).  \n\n\n  2. Additional Commercial Terms. If, on the Llama 2 version release date, the \n  monthly active users of the products or services made available by or for\n  Licensee,  or Licensee's affiliates, is greater than 700 million monthly\n  active users in the  preceding calendar month, you must request a license from\n  Meta, which Meta may  grant to you in its sole discretion, and you are not\n  authorized to exercise any of the  rights under this Agreement unless or until\n  Meta otherwise expressly grants you  such rights.\n\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA\n  MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \"AS IS\"\n  BASIS, WITHOUT WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING,\n  WITHOUT LIMITATION, ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT,\n  MERCHANTABILITY, OR  FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY\n  RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING \n  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR  USE OF THE\n  LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n  4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE\n  UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE,\n  PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST\n  PROFITS OR ANY INDIRECT, SPECIAL,  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR\n  PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE\n  POSSIBILITY OF  ANY OF THE FOREGOING.\n\n\n  5. Intellectual Property.\n\n  a. No trademark licenses are granted under this Agreement, and in  connection\n  with the Llama Materials, neither Meta nor Licensee may use any name  or mark\n  owned by or associated with the other or any of its affiliates, except as \n  required for reasonable and customary use in describing and redistributing\n  the  Llama Materials.\n\n  b. Subject to Meta's ownership of Llama Materials and derivatives made by or \n  for Meta, with respect to any derivative works and modifications of the Llama \n  Materials that are made by you, as between you and Meta, you are and will be\n  the  owner of such derivative works and modifications.\n\n  c. If you institute litigation or other proceedings against Meta or any\n  entity  (including a cross-claim or counterclaim in a lawsuit) alleging that\n  the Llama  Materials or Llama 2 outputs or results, or any portion of any of\n  the foregoing,  constitutes infringement of intellectual property or other\n  rights owned or licensable  by you, then any licenses granted to you under\n  this Agreement shall terminate as of  the date such litigation or claim is\n  filed or instituted. You will indemnify and hold  harmless Meta from and\n  against any claim by any third party arising out of or related  to your use or\n  distribution of the Llama Materials.\n\n  6. Term and Termination. The term of this Agreement will commence upon your \n  acceptance of this Agreement or access to the Llama Materials and will\n  continue in  full force and effect until terminated in accordance with the\n  terms and conditions  herein. Meta may terminate this Agreement if you are in\n  breach of any term or  condition of this Agreement. Upon termination of this\n  Agreement, you shall delete  and cease use of the Llama Materials. Sections 3,\n  4 and 7 shall survive the  termination of this Agreement. \n\n  7. Governing Law and Jurisdiction. This Agreement will be governed and \n  construed under the laws of the State of California without regard to choice\n  of law  principles, and the UN Convention on Contracts for the International\n  Sale of Goods  does not apply to this Agreement. The courts of California\n  shall have exclusive  jurisdiction of any dispute arising out of this\n  Agreement. \n\n  ### Llama 2 Acceptable Use Policy\n\n  Meta is committed to promoting safe and fair use of its tools and features,\n  including Llama 2. If you access or use Llama 2, you agree to this Acceptable\n  Use Policy (\u201cPolicy\u201d). The most recent copy of this policy can be found at\n  [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\n\n  #### Prohibited Uses\n\n  We want everyone to use Llama 2 safely and responsibly. You agree you will not\n  use, or allow others to use, Llama 2 to:\n\n  1. Violate the law or others\u2019 rights, including to:\n        1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: \n            1. Violence or terrorism \n            2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n            3. Human trafficking, exploitation, and sexual violence\n            4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n            5. Sexual solicitation\n            6. Any other criminal activity\n        2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n        3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n        4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices \n        5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n        6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\n        7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system \n  2. Engage in, promote, incite, facilitate, or assist in the planning or\n  development of activities that present a risk of death or bodily harm to\n  individuals, including use of Llama 2 related to the following:\n      1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n      2. Guns and illegal weapons (including weapon development)\n      3. Illegal drugs and regulated/controlled substances\n      4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n      5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n      6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n  3. Intentionally deceive or mislead others, including use of Llama 2 related\n  to the following:\n      1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n      2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n      3. Generating, promoting, or further distributing spam\n      4. Impersonating another individual without consent, authorization, or legal right\n      5. Representing that the use of Llama 2 or outputs are human-generated\n      6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n      4. Fail to appropriately disclose to end users any known dangers of your AI system \n  Please report any violation of this Policy, software \u201cbug,\u201d or other problems\n  that could lead to a violation of this Policy through one of the following\n  means: \n      * Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n      * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n      * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info) \n      * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlanguage:\n- en\npipeline_tag: text-generation\ninference: false\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nlicense: llama2\n---\n# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes \u2014 7B, 13B, and 70B \u2014 as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta\u2019s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software \u201cbug,\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|",
            "metadata": "{\"id\": \"meta-llama/Llama-2-7b\", \"author\": \"meta-llama\", \"sha\": \"69656aac4cb47911a639f5890ff35b41ceb82e98\", \"last_modified\": \"2024-04-17 08:12:44+00:00\", \"created_at\": \"2023-07-09 07:34:35+00:00\", \"private\": false, \"gated\": \"manual\", \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 4312, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"facebook\", \"meta\", \"pytorch\", \"llama\", \"llama-2\", \"text-generation\", \"en\", \"arxiv:2307.09288\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"language:\\n- en\\nlicense: llama2\\npipeline_tag: text-generation\\ntags:\\n- facebook\\n- meta\\n- pytorch\\n- llama\\n- llama-2\\nextra_gated_heading: You need to share contact information with Meta to access this\\n  model\\nextra_gated_prompt: \\\"### LLAMA 2 COMMUNITY LICENSE AGREEMENT\\\\n\\\\\\\"Agreement\\\\\\\" means\\\\\\n  \\\\ the terms and conditions for use, reproduction, distribution and  modification\\\\\\n  \\\\ of the Llama Materials set forth herein. \\\\n\\\\\\\"Documentation\\\\\\\" means the specifications,\\\\\\n  \\\\ manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\\\\n  \\\\  \\\\n\\\\\\\"Licensee\\\\\\\" or \\\\\\\"you\\\\\\\" means you, or your employer or any other person or\\\\\\n  \\\\ entity (if you are entering into this Agreement on such person or entity's behalf),\\\\\\n  \\\\ of the age required under applicable laws, rules or regulations to provide legal\\\\\\n  \\\\ consent and that has legal authority to bind your employer or such other person\\\\\\n  \\\\ or  entity if you are  entering in this Agreement on their behalf. \\\\n\\\\\\\"Llama 2\\\\\\\"\\\\\\n  \\\\ means the foundational large language models and software and algorithms, including\\\\\\n  \\\\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\\\\n  \\\\ code, fine-tuning enabling code and other  elements of the foregoing distributed\\\\\\n  \\\\ by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\\n\\\\\\\"Llama\\\\\\n  \\\\ Materials\\\\\\\" means, collectively, Meta's proprietary Llama 2 and documentation\\\\\\n  \\\\ (and any portion thereof) made available under this Agreement.\\\\n\\\\\\\"Meta\\\\\\\" or \\\\\\\"\\\\\\n  we\\\\\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\\\\n  \\\\ an entity, your principal place of business is in the EEA or Switzerland) and\\\\\\n  \\\\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \\\\n\\\\\\n  \\\\nBy clicking \\\\\\\"I Accept\\\\\\\" below or by using or distributing any portion or element\\\\\\n  \\\\ of the Llama Materials, you agree to be bound by this Agreement.\\\\n1. License Rights\\\\\\n  \\\\ and Redistribution. \\\\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\\\\n  \\\\ non- transferable and royalty-free limited license under Meta's intellectual property\\\\\\n  \\\\ or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\\\\n  \\\\  distribute, copy, create derivative works of, and make modifications to the Llama\\\\\\n  \\\\  Materials.  \\\\nb. Redistribution and Use.\\\\ni. If you distribute or make the Llama\\\\\\n  \\\\ Materials, or any derivative works  thereof, available to a third party, you shall\\\\\\n  \\\\ provide a copy of this Agreement to such  third party. \\\\nii.  If you receive Llama\\\\\\n  \\\\ Materials, or any derivative works thereof, from  a Licensee as part of an integrated\\\\\\n  \\\\ end user product, then Section 2 of this  Agreement will not apply to you. \\\\n\\\\\\n  iii. You must retain in all copies of the Llama Materials that you  distribute the\\\\\\n  \\\\ following attribution notice within a \\\\\\\"Notice\\\\\\\" text file distributed as a  part\\\\\\n  \\\\ of such copies: \\\\\\\"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright\\\\\\n  \\\\ (c) Meta Platforms, Inc. All Rights Reserved.\\\\\\\"\\\\niv. Your use of the Llama Materials\\\\\\n  \\\\ must comply with applicable laws  and regulations (including trade compliance\\\\\\n  \\\\ laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials\\\\\\n  \\\\ (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated\\\\\\n  \\\\ by reference into  this Agreement.\\\\nv. You will not use the Llama Materials or\\\\\\n  \\\\ any output or results of the  Llama Materials to improve any other large language\\\\\\n  \\\\ model (excluding Llama 2 or  derivative works thereof).  \\\\n\\\\n2. Additional Commercial\\\\\\n  \\\\ Terms. If, on the Llama 2 version release date, the  monthly active users of the\\\\\\n  \\\\ products or services made available by or for Licensee,  or Licensee's affiliates,\\\\\\n  \\\\ is greater than 700 million monthly active users in the  preceding calendar month,\\\\\\n  \\\\ you must request a license from Meta, which Meta may  grant to you in its sole\\\\\\n  \\\\ discretion, and you are not authorized to exercise any of the  rights under this\\\\\\n  \\\\ Agreement unless or until Meta otherwise expressly grants you  such rights.\\\\n\\\\\\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS\\\\\\n  \\\\ AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \\\\\\\"AS IS\\\\\\\" BASIS, WITHOUT\\\\\\n  \\\\ WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\\\\n  \\\\ ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A\\\\\\n  \\\\ PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS\\\\\\n  \\\\ OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED\\\\\\n  \\\\ WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\\\n4. Limitation\\\\\\n  \\\\ of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY\\\\\\n  \\\\ OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE,\\\\\\n  \\\\ ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\\\\\\n  \\\\  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS\\\\\\n  \\\\ AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\\\\n\\\\n\\\\\\n  5. Intellectual Property.\\\\na. No trademark licenses are granted under this Agreement,\\\\\\n  \\\\ and in  connection with the Llama Materials, neither Meta nor Licensee may use\\\\\\n  \\\\ any name  or mark owned by or associated with the other or any of its affiliates,\\\\\\n  \\\\ except as  required for reasonable and customary use in describing and redistributing\\\\\\n  \\\\ the  Llama Materials.\\\\nb. Subject to Meta's ownership of Llama Materials and derivatives\\\\\\n  \\\\ made by or  for Meta, with respect to any derivative works and modifications of\\\\\\n  \\\\ the Llama  Materials that are made by you, as between you and Meta, you are and\\\\\\n  \\\\ will be the  owner of such derivative works and modifications.\\\\nc. If you institute\\\\\\n  \\\\ litigation or other proceedings against Meta or any entity  (including a cross-claim\\\\\\n  \\\\ or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs\\\\\\n  \\\\ or results, or any portion of any of the foregoing,  constitutes infringement\\\\\\n  \\\\ of intellectual property or other rights owned or licensable  by you, then any\\\\\\n  \\\\ licenses granted to you under this Agreement shall terminate as of  the date such\\\\\\n  \\\\ litigation or claim is filed or instituted. You will indemnify and hold  harmless\\\\\\n  \\\\ Meta from and against any claim by any third party arising out of or related \\\\\\n  \\\\ to your use or distribution of the Llama Materials.\\\\n6. Term and Termination.\\\\\\n  \\\\ The term of this Agreement will commence upon your  acceptance of this Agreement\\\\\\n  \\\\ or access to the Llama Materials and will continue in  full force and effect until\\\\\\n  \\\\ terminated in accordance with the terms and conditions  herein. Meta may terminate\\\\\\n  \\\\ this Agreement if you are in breach of any term or  condition of this Agreement.\\\\\\n  \\\\ Upon termination of this Agreement, you shall delete  and cease use of the Llama\\\\\\n  \\\\ Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\\\\\\n  \\\\ \\\\n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed\\\\\\n  \\\\ under the laws of the State of California without regard to choice of law  principles,\\\\\\n  \\\\ and the UN Convention on Contracts for the International Sale of Goods  does not\\\\\\n  \\\\ apply to this Agreement. The courts of California shall have exclusive  jurisdiction\\\\\\n  \\\\ of any dispute arising out of this Agreement. \\\\n### Llama 2 Acceptable Use Policy\\\\n\\\\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\\\\n  \\\\ Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy\\\\\\n  \\\\ (\\u201cPolicy\\u201d). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\\\\n\\\\\\n  #### Prohibited Uses\\\\nWe want everyone to use Llama 2 safely and responsibly. You\\\\\\n  \\\\ agree you will not use, or allow others to use, Llama 2 to:\\\\n1. Violate the law\\\\\\n  \\\\ or others\\u2019 rights, including to:\\\\n      1. Engage in, promote, generate, contribute\\\\\\n  \\\\ to, encourage, plan, incite, or further illegal or unlawful activity or content,\\\\\\n  \\\\ such as: \\\\n          1. Violence or terrorism \\\\n          2. Exploitation or harm\\\\\\n  \\\\ to children, including the solicitation, creation, acquisition, or dissemination\\\\\\n  \\\\ of child exploitative content or failure to report Child Sexual Abuse Material\\\\n\\\\\\n  \\\\          3. Human trafficking, exploitation, and sexual violence\\\\n          4.\\\\\\n  \\\\ The illegal distribution of information or materials to minors, including obscene\\\\\\n  \\\\ materials, or failure to employ legally required age-gating in connection with\\\\\\n  \\\\ such information or materials.\\\\n          5. Sexual solicitation\\\\n          6.\\\\\\n  \\\\ Any other criminal activity\\\\n      2. Engage in, promote, incite, or facilitate\\\\\\n  \\\\ the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\\\n\\\\\\n  \\\\      3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\\\\n  \\\\ or harmful conduct in the provision of employment, employment benefits, credit,\\\\\\n  \\\\ housing, other economic benefits, or other essential goods and services\\\\n    \\\\\\n  \\\\  4. Engage in the unauthorized or unlicensed practice of any profession including,\\\\\\n  \\\\ but not limited to, financial, legal, medical/health, or related professional\\\\\\n  \\\\ practices \\\\n      5. Collect, process, disclose, generate, or infer health, demographic,\\\\\\n  \\\\ or other sensitive personal or private information about individuals without rights\\\\\\n  \\\\ and consents required by applicable laws\\\\n      6. Engage in or facilitate any\\\\\\n  \\\\ action or generate any content that infringes, misappropriates, or otherwise violates\\\\\\n  \\\\ any third-party rights, including the outputs or results of any products or services\\\\\\n  \\\\ using the Llama 2 Materials\\\\n      7. Create, generate, or facilitate the creation\\\\\\n  \\\\ of malicious code, malware, computer viruses or do anything else that could disable,\\\\\\n  \\\\ overburden, interfere with or impair the proper working, integrity, operation\\\\\\n  \\\\ or appearance of a website or computer system \\\\n2. Engage in, promote, incite,\\\\\\n  \\\\ facilitate, or assist in the planning or development of activities that present\\\\\\n  \\\\ a risk of death or bodily harm to individuals, including use of Llama 2 related\\\\\\n  \\\\ to the following:\\\\n    1. Military, warfare, nuclear industries or applications,\\\\\\n  \\\\ espionage, use for materials or activities that are subject to the International\\\\\\n  \\\\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\\\\n  \\\\ State\\\\n    2. Guns and illegal weapons (including weapon development)\\\\n    3.\\\\\\n  \\\\ Illegal drugs and regulated/controlled substances\\\\n    4. Operation of critical\\\\\\n  \\\\ infrastructure, transportation technologies, or heavy machinery\\\\n    5. Self-harm\\\\\\n  \\\\ or harm to others, including suicide, cutting, and eating disorders\\\\n    6. Any\\\\\\n  \\\\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\\\\n  \\\\ harm to an individual\\\\n3. Intentionally deceive or mislead others, including use\\\\\\n  \\\\ of Llama 2 related to the following:\\\\n    1. Generating, promoting, or furthering\\\\\\n  \\\\ fraud or the creation or promotion of disinformation\\\\n    2. Generating, promoting,\\\\\\n  \\\\ or furthering defamatory content, including the creation of defamatory statements,\\\\\\n  \\\\ images, or other content\\\\n    3. Generating, promoting, or further distributing\\\\\\n  \\\\ spam\\\\n    4. Impersonating another individual without consent, authorization,\\\\\\n  \\\\ or legal right\\\\n    5. Representing that the use of Llama 2 or outputs are human-generated\\\\n\\\\\\n  \\\\    6. Generating or facilitating false online engagement, including fake reviews\\\\\\n  \\\\ and other means of fake online engagement \\\\n    4. Fail to appropriately disclose\\\\\\n  \\\\ to end users any known dangers of your AI system \\\\nPlease report any violation\\\\\\n  \\\\ of this Policy, software \\u201cbug,\\u201d or other problems that could lead to a violation\\\\\\n  \\\\ of this Policy through one of the following means: \\\\n    * Reporting issues with\\\\\\n  \\\\ the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\\\\n\\\\\\n  \\\\    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\\\n\\\\\\n  \\\\    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\\\\n  \\\\ \\\\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of\\\\\\n  \\\\ Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\\\"\\nextra_gated_fields:\\n  First Name: text\\n  Last Name: text\\n  Date of birth: date_picker\\n  Country: country\\n  Affiliation: text\\n  geo: ip_location\\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\\n    the information I provide will be collected stored processed and shared in accordance\\n    with the Meta Privacy Policy\\n  : checkbox\\nextra_gated_description: The information you provide will be collected, stored, processed\\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\\nextra_gated_button_content: Submit\\ninference: false\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Responsible-Use-Guide.pdf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checklist.chk', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='consolidated.00.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='params.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_checklist.chk', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"h2oai/h2ogpt-chatbot\", \"h2oai/h2ogpt-chatbot2\", \"Illia56/Ask-AI-Youtube\", \"librarian-bots/huggingface-semantic-search\", \"qiantong-xu/toolbench-leaderboard\", \"genai-impact/ecologits-calculator\", \"Omnibus/InferenceClient_Chatbots\", \"TogetherAI/Chat-with-Llama-2-70b\", \"bhaskartripathi/Llama-2-70b-chatbot\", \"hoyinli/demo-app\", \"mikeee/gradio-chatinterface\", \"SaeidFarsian/Ask-AI-Youtube\", \"arborvitae/AI_Legal_documentation_assistant\", \"lapsapking/h2ogpt-chatbot\", \"Raju2024/TestLLM\", \"realchenyuy/llama2-playground\", \"ka1kuk/litellm\", \"islammohy/Chat-with-Llama-2-7b-st-voice\", \"his0/h2ogpt-chatbot\", \"atimughal662/InfoFusion\", \"ethanlshen/SuperposedDecoding\", \"Lyte/tokenizer-leaderboard\", \"KingPinX/kbot-1\", \"scp4950/grah\", \"Sambhavnoobcoder/h2ogpt-chatbot\", \"osanseviero/streaming-example\", \"nonhuman/nnnn\", \"ziffir/vYouTubeVideoChatRobot\", \"AhmedAlmaghz/Ask-Llama2AIWhisper3-Youtube\", \"iblfe/test\", \"agkbv/meta-llama-Llama-2-7b-hf\", \"KonstantinosKakkavas/invoice-extractor\", \"K00B404/Teachershub\", \"K00B404/Research-chatbot\", \"zaephaer23/compareAI\", \"KevinXiong2022/XL_gradio_space\", \"kelvin-t-lu/chatbot\", \"everestspace/talk-to-books\", \"Tere-SaMi/Docs-Llama\", \"jerpint/folietechnique\", \"kenken999/litellm\", \"kenken999/litellmlope\", \"Farha00/llama-7b\", \"cw332/h2ogpt-chatbot\", \"Bunpheng/test-llama2\", \"lfbarbosa/QueryMake\", \"QuantumIntelligenceLab/Chat-with-Llama-2-7b-st-voice\", \"sridharKikkeri/xpathHealing\", \"TeoItr/TeoKats\", \"abugaber/test\", \"Logeswaransr/Llama-2-7b-Interface\", \"TalatMasud/chatbot-backend\", \"mila-ai4h/PromptCraft\", \"kobby2004/llamaAgric\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-04-17 08:12:44+00:00\", \"cardData\": \"language:\\n- en\\nlicense: llama2\\npipeline_tag: text-generation\\ntags:\\n- facebook\\n- meta\\n- pytorch\\n- llama\\n- llama-2\\nextra_gated_heading: You need to share contact information with Meta to access this\\n  model\\nextra_gated_prompt: \\\"### LLAMA 2 COMMUNITY LICENSE AGREEMENT\\\\n\\\\\\\"Agreement\\\\\\\" means\\\\\\n  \\\\ the terms and conditions for use, reproduction, distribution and  modification\\\\\\n  \\\\ of the Llama Materials set forth herein. \\\\n\\\\\\\"Documentation\\\\\\\" means the specifications,\\\\\\n  \\\\ manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\\\\n  \\\\  \\\\n\\\\\\\"Licensee\\\\\\\" or \\\\\\\"you\\\\\\\" means you, or your employer or any other person or\\\\\\n  \\\\ entity (if you are entering into this Agreement on such person or entity's behalf),\\\\\\n  \\\\ of the age required under applicable laws, rules or regulations to provide legal\\\\\\n  \\\\ consent and that has legal authority to bind your employer or such other person\\\\\\n  \\\\ or  entity if you are  entering in this Agreement on their behalf. \\\\n\\\\\\\"Llama 2\\\\\\\"\\\\\\n  \\\\ means the foundational large language models and software and algorithms, including\\\\\\n  \\\\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\\\\n  \\\\ code, fine-tuning enabling code and other  elements of the foregoing distributed\\\\\\n  \\\\ by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\\n\\\\\\\"Llama\\\\\\n  \\\\ Materials\\\\\\\" means, collectively, Meta's proprietary Llama 2 and documentation\\\\\\n  \\\\ (and any portion thereof) made available under this Agreement.\\\\n\\\\\\\"Meta\\\\\\\" or \\\\\\\"\\\\\\n  we\\\\\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\\\\n  \\\\ an entity, your principal place of business is in the EEA or Switzerland) and\\\\\\n  \\\\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \\\\n\\\\\\n  \\\\nBy clicking \\\\\\\"I Accept\\\\\\\" below or by using or distributing any portion or element\\\\\\n  \\\\ of the Llama Materials, you agree to be bound by this Agreement.\\\\n1. License Rights\\\\\\n  \\\\ and Redistribution. \\\\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\\\\n  \\\\ non- transferable and royalty-free limited license under Meta's intellectual property\\\\\\n  \\\\ or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\\\\n  \\\\  distribute, copy, create derivative works of, and make modifications to the Llama\\\\\\n  \\\\  Materials.  \\\\nb. Redistribution and Use.\\\\ni. If you distribute or make the Llama\\\\\\n  \\\\ Materials, or any derivative works  thereof, available to a third party, you shall\\\\\\n  \\\\ provide a copy of this Agreement to such  third party. \\\\nii.  If you receive Llama\\\\\\n  \\\\ Materials, or any derivative works thereof, from  a Licensee as part of an integrated\\\\\\n  \\\\ end user product, then Section 2 of this  Agreement will not apply to you. \\\\n\\\\\\n  iii. You must retain in all copies of the Llama Materials that you  distribute the\\\\\\n  \\\\ following attribution notice within a \\\\\\\"Notice\\\\\\\" text file distributed as a  part\\\\\\n  \\\\ of such copies: \\\\\\\"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright\\\\\\n  \\\\ (c) Meta Platforms, Inc. All Rights Reserved.\\\\\\\"\\\\niv. Your use of the Llama Materials\\\\\\n  \\\\ must comply with applicable laws  and regulations (including trade compliance\\\\\\n  \\\\ laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials\\\\\\n  \\\\ (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated\\\\\\n  \\\\ by reference into  this Agreement.\\\\nv. You will not use the Llama Materials or\\\\\\n  \\\\ any output or results of the  Llama Materials to improve any other large language\\\\\\n  \\\\ model (excluding Llama 2 or  derivative works thereof).  \\\\n\\\\n2. Additional Commercial\\\\\\n  \\\\ Terms. If, on the Llama 2 version release date, the  monthly active users of the\\\\\\n  \\\\ products or services made available by or for Licensee,  or Licensee's affiliates,\\\\\\n  \\\\ is greater than 700 million monthly active users in the  preceding calendar month,\\\\\\n  \\\\ you must request a license from Meta, which Meta may  grant to you in its sole\\\\\\n  \\\\ discretion, and you are not authorized to exercise any of the  rights under this\\\\\\n  \\\\ Agreement unless or until Meta otherwise expressly grants you  such rights.\\\\n\\\\\\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS\\\\\\n  \\\\ AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \\\\\\\"AS IS\\\\\\\" BASIS, WITHOUT\\\\\\n  \\\\ WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\\\\n  \\\\ ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A\\\\\\n  \\\\ PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS\\\\\\n  \\\\ OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED\\\\\\n  \\\\ WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\\\n4. Limitation\\\\\\n  \\\\ of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY\\\\\\n  \\\\ OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE,\\\\\\n  \\\\ ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\\\\\\n  \\\\  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS\\\\\\n  \\\\ AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\\\\n\\\\n\\\\\\n  5. Intellectual Property.\\\\na. No trademark licenses are granted under this Agreement,\\\\\\n  \\\\ and in  connection with the Llama Materials, neither Meta nor Licensee may use\\\\\\n  \\\\ any name  or mark owned by or associated with the other or any of its affiliates,\\\\\\n  \\\\ except as  required for reasonable and customary use in describing and redistributing\\\\\\n  \\\\ the  Llama Materials.\\\\nb. Subject to Meta's ownership of Llama Materials and derivatives\\\\\\n  \\\\ made by or  for Meta, with respect to any derivative works and modifications of\\\\\\n  \\\\ the Llama  Materials that are made by you, as between you and Meta, you are and\\\\\\n  \\\\ will be the  owner of such derivative works and modifications.\\\\nc. If you institute\\\\\\n  \\\\ litigation or other proceedings against Meta or any entity  (including a cross-claim\\\\\\n  \\\\ or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs\\\\\\n  \\\\ or results, or any portion of any of the foregoing,  constitutes infringement\\\\\\n  \\\\ of intellectual property or other rights owned or licensable  by you, then any\\\\\\n  \\\\ licenses granted to you under this Agreement shall terminate as of  the date such\\\\\\n  \\\\ litigation or claim is filed or instituted. You will indemnify and hold  harmless\\\\\\n  \\\\ Meta from and against any claim by any third party arising out of or related \\\\\\n  \\\\ to your use or distribution of the Llama Materials.\\\\n6. Term and Termination.\\\\\\n  \\\\ The term of this Agreement will commence upon your  acceptance of this Agreement\\\\\\n  \\\\ or access to the Llama Materials and will continue in  full force and effect until\\\\\\n  \\\\ terminated in accordance with the terms and conditions  herein. Meta may terminate\\\\\\n  \\\\ this Agreement if you are in breach of any term or  condition of this Agreement.\\\\\\n  \\\\ Upon termination of this Agreement, you shall delete  and cease use of the Llama\\\\\\n  \\\\ Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\\\\\\n  \\\\ \\\\n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed\\\\\\n  \\\\ under the laws of the State of California without regard to choice of law  principles,\\\\\\n  \\\\ and the UN Convention on Contracts for the International Sale of Goods  does not\\\\\\n  \\\\ apply to this Agreement. The courts of California shall have exclusive  jurisdiction\\\\\\n  \\\\ of any dispute arising out of this Agreement. \\\\n### Llama 2 Acceptable Use Policy\\\\n\\\\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\\\\n  \\\\ Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy\\\\\\n  \\\\ (\\u201cPolicy\\u201d). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\\\\n\\\\\\n  #### Prohibited Uses\\\\nWe want everyone to use Llama 2 safely and responsibly. You\\\\\\n  \\\\ agree you will not use, or allow others to use, Llama 2 to:\\\\n1. Violate the law\\\\\\n  \\\\ or others\\u2019 rights, including to:\\\\n      1. Engage in, promote, generate, contribute\\\\\\n  \\\\ to, encourage, plan, incite, or further illegal or unlawful activity or content,\\\\\\n  \\\\ such as: \\\\n          1. Violence or terrorism \\\\n          2. Exploitation or harm\\\\\\n  \\\\ to children, including the solicitation, creation, acquisition, or dissemination\\\\\\n  \\\\ of child exploitative content or failure to report Child Sexual Abuse Material\\\\n\\\\\\n  \\\\          3. Human trafficking, exploitation, and sexual violence\\\\n          4.\\\\\\n  \\\\ The illegal distribution of information or materials to minors, including obscene\\\\\\n  \\\\ materials, or failure to employ legally required age-gating in connection with\\\\\\n  \\\\ such information or materials.\\\\n          5. Sexual solicitation\\\\n          6.\\\\\\n  \\\\ Any other criminal activity\\\\n      2. Engage in, promote, incite, or facilitate\\\\\\n  \\\\ the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\\\n\\\\\\n  \\\\      3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\\\\n  \\\\ or harmful conduct in the provision of employment, employment benefits, credit,\\\\\\n  \\\\ housing, other economic benefits, or other essential goods and services\\\\n    \\\\\\n  \\\\  4. Engage in the unauthorized or unlicensed practice of any profession including,\\\\\\n  \\\\ but not limited to, financial, legal, medical/health, or related professional\\\\\\n  \\\\ practices \\\\n      5. Collect, process, disclose, generate, or infer health, demographic,\\\\\\n  \\\\ or other sensitive personal or private information about individuals without rights\\\\\\n  \\\\ and consents required by applicable laws\\\\n      6. Engage in or facilitate any\\\\\\n  \\\\ action or generate any content that infringes, misappropriates, or otherwise violates\\\\\\n  \\\\ any third-party rights, including the outputs or results of any products or services\\\\\\n  \\\\ using the Llama 2 Materials\\\\n      7. Create, generate, or facilitate the creation\\\\\\n  \\\\ of malicious code, malware, computer viruses or do anything else that could disable,\\\\\\n  \\\\ overburden, interfere with or impair the proper working, integrity, operation\\\\\\n  \\\\ or appearance of a website or computer system \\\\n2. Engage in, promote, incite,\\\\\\n  \\\\ facilitate, or assist in the planning or development of activities that present\\\\\\n  \\\\ a risk of death or bodily harm to individuals, including use of Llama 2 related\\\\\\n  \\\\ to the following:\\\\n    1. Military, warfare, nuclear industries or applications,\\\\\\n  \\\\ espionage, use for materials or activities that are subject to the International\\\\\\n  \\\\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\\\\n  \\\\ State\\\\n    2. Guns and illegal weapons (including weapon development)\\\\n    3.\\\\\\n  \\\\ Illegal drugs and regulated/controlled substances\\\\n    4. Operation of critical\\\\\\n  \\\\ infrastructure, transportation technologies, or heavy machinery\\\\n    5. Self-harm\\\\\\n  \\\\ or harm to others, including suicide, cutting, and eating disorders\\\\n    6. Any\\\\\\n  \\\\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\\\\n  \\\\ harm to an individual\\\\n3. Intentionally deceive or mislead others, including use\\\\\\n  \\\\ of Llama 2 related to the following:\\\\n    1. Generating, promoting, or furthering\\\\\\n  \\\\ fraud or the creation or promotion of disinformation\\\\n    2. Generating, promoting,\\\\\\n  \\\\ or furthering defamatory content, including the creation of defamatory statements,\\\\\\n  \\\\ images, or other content\\\\n    3. Generating, promoting, or further distributing\\\\\\n  \\\\ spam\\\\n    4. Impersonating another individual without consent, authorization,\\\\\\n  \\\\ or legal right\\\\n    5. Representing that the use of Llama 2 or outputs are human-generated\\\\n\\\\\\n  \\\\    6. Generating or facilitating false online engagement, including fake reviews\\\\\\n  \\\\ and other means of fake online engagement \\\\n    4. Fail to appropriately disclose\\\\\\n  \\\\ to end users any known dangers of your AI system \\\\nPlease report any violation\\\\\\n  \\\\ of this Policy, software \\u201cbug,\\u201d or other problems that could lead to a violation\\\\\\n  \\\\ of this Policy through one of the following means: \\\\n    * Reporting issues with\\\\\\n  \\\\ the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\\\\n\\\\\\n  \\\\    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\\\n\\\\\\n  \\\\    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\\\\n  \\\\ \\\\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of\\\\\\n  \\\\ Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\\\"\\nextra_gated_fields:\\n  First Name: text\\n  Last Name: text\\n  Date of birth: date_picker\\n  Country: country\\n  Affiliation: text\\n  geo: ip_location\\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\\n    the information I provide will be collected stored processed and shared in accordance\\n    with the Meta Privacy Policy\\n  : checkbox\\nextra_gated_description: The information you provide will be collected, stored, processed\\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\\nextra_gated_button_content: Submit\\ninference: false\", \"transformersInfo\": null, \"_id\": \"64aa630b73790912c790894e\", \"modelId\": \"meta-llama/Llama-2-7b\", \"usedStorage\": 13478678109}",
            "depth": 0,
            "children": [
                "https://huggingface.co/epfl-llm/meditron-7b",
                "https://huggingface.co/motherduckdb/DuckDB-NSQL-7B-v0.1",
                "https://huggingface.co/hon9kon9ize/Cantonese-Llama-2-7B-preview20240625",
                "https://huggingface.co/ChengsenWang/ChatTime-1-7B-Base",
                "https://huggingface.co/m3rg-iitd/llamat-2",
                "https://huggingface.co/nvidia/Llama-2-7B-DMC-8x",
                "https://huggingface.co/nivashb/aiadvisorbynivash",
                "https://huggingface.co/Sci-fi-vy/Meditron-7b-finetuned",
                "https://huggingface.co/danlou/persona-generator-llama-2-7b-qlora-merged",
                "https://huggingface.co/LLM-PBE/together-llama-2-7B-enron-undefended",
                "https://huggingface.co/LLM-PBE/together-llama-2-7B-enron-scrubbed",
                "https://huggingface.co/qu-bit/SuperLLM",
                "https://huggingface.co/YBCarry/Finance-Chinese-LLaMA",
                "https://huggingface.co/pucpr-br/Clinical-BR-LlaMA-2-7B",
                "https://huggingface.co/2imi9/Llama2_7B_TeachingAssistant_Introduction_to_Computers",
                "https://huggingface.co/inceptionai/jais-adapted-7b",
                "https://huggingface.co/Ichate/yaoi-v1-instruct",
                "https://huggingface.co/TheSunnyBoy123/super_llm_base",
                "https://huggingface.co/TheSunnyBoy123/super_llm_lora",
                "https://huggingface.co/hon9kon9ize/Cantonese-Llama-2-7B-preview20240903",
                "https://huggingface.co/heichow/Cantonese-Llama-2-7B-preview20240903-neuronx",
                "https://huggingface.co/andreamaduzzi/LLaNA-7B",
                "https://huggingface.co/sabersaleh/Llama2-7B-DPO",
                "https://huggingface.co/sabersaleh/Llama2-7B-KTO",
                "https://huggingface.co/sabersaleh/Llama2-7B-IPO",
                "https://huggingface.co/sabersaleh/Llama2-7B-CPO",
                "https://huggingface.co/sabersaleh/Llama2-7B-SimPO",
                "https://huggingface.co/sabersaleh/Llama2-7B-aligned",
                "https://huggingface.co/sabersaleh/Llama2-7B-RDPO",
                "https://huggingface.co/Vinnnf/LLaMA-2-7B-MaskLLM-C4",
                "https://huggingface.co/nvidia/Llama-2-7B-DMC-4x",
                "https://huggingface.co/dongsheng/DTA_llama2_7b",
                "https://huggingface.co/dongsheng/DTA_llama3_8b"
            ],
            "children_count": 33,
            "adapters": [
                "https://huggingface.co/tineding/ACT-SOP",
                "https://huggingface.co/lonestar108/dwitter",
                "https://huggingface.co/douy/Llama-2-7B-lora-instruction-ft-abstraction-three-span",
                "https://huggingface.co/Jasonchen9/6000Q_llama2_backward_finetuned",
                "https://huggingface.co/Jasonchen9/6000Q_llama2_backward_finetuned_new"
            ],
            "adapters_count": 5,
            "quantized": [
                "https://huggingface.co/QuantFactory/DuckDB-NSQL-7B-v0.1-GGUF",
                "https://huggingface.co/ccarrez/meditron-7b-Q4_K_M-GGUF",
                "https://huggingface.co/chrohi/meditron-7b-Q8_0-GGUF",
                "https://huggingface.co/QuantFactory/Clinical-BR-LlaMA-2-7B-GGUF",
                "https://huggingface.co/QuantFactory/meditron-7b-GGUF",
                "https://huggingface.co/amd/llama2-7b-instruct-awq-g128-int4-onnx-directml"
            ],
            "quantized_count": 6,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "Illia56/Ask-AI-Youtube",
                "KonstantinosKakkavas/invoice-extractor",
                "Logeswaransr/Llama-2-7b-Interface",
                "Lyte/tokenizer-leaderboard",
                "Tere-SaMi/Docs-Llama",
                "TogetherAI/Chat-with-Llama-2-70b",
                "genai-impact/ecologits-calculator",
                "huggingface/InferenceSupport/discussions/30",
                "jerpint/folietechnique",
                "kenken999/litellmlope",
                "lapsapking/h2ogpt-chatbot",
                "qiantong-xu/toolbench-leaderboard",
                "ziffir/vYouTubeVideoChatRobot"
            ],
            "spaces_count": 13
        },
        {
            "model_id": "epfl-llm/meditron-7b",
            "card": "---\nlicense: llama2\nlanguage:\n- en\nmetrics:\n- accuracy\n- perplexity\ndatasets:\n- epfl-llm/guidelines\nbase_model: meta-llama/Llama-2-7b\n---\n<img width=50% src=\"meditron_LOGO.png\" alt=\"Alt text\" title=\"Meditron-logo\">\n\n# Model Card for Meditron-7B-v1.0\nMeditron is a suite of open-source medical Large Language Models (LLMs).\nMeditron-7B is a 7 billion parameters model adapted to the medical domain from Llama-2-7B through continued pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, a [new dataset](https://huggingface.co/datasets/epfl-llm/guidelines) of internationally-recognized medical guidelines, and general domain data from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T).\nMeditron-7B, finetuned on relevant training data, outperforms Llama-2-7B and PMC-Llama on multiple medical reasoning tasks.\n\n<details open>\n  <summary><strong>Advisory Notice</strong></summary>\n\n  <blockquote style=\"padding: 10px; margin: 0 0 10px; border-left: 5px solid #ddd;\">\n    While Meditron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints. \n  We recommend against deploying Meditron in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings.\n  </blockquote>\n</details>\n\n## Model Details\n\n- **Developed by:** [EPFL LLM Team](https://huggingface.co/epfl-llm)\n- **Model type:** Causal decoder-only transformer language model\n- **Language(s):** English (mainly)\n- **Model License:** [LLAMA 2 COMMUNITY LICENSE AGREEMENT](https://huggingface.co/meta-llama/Llama-2-70b/raw/main/LICENSE.txt)\n- **Code License:** [APACHE 2.0 LICENSE](LICENSE)\n- **Continue-pretrained from model:** [Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7b)\n- **Context length:**  2K tokens\n- **Input:**  Text-only data\n- **Output:**  Model generates text only\n- **Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we enhance model's performance.\n- **Knowledge Cutoff:** August 2023\n\n\n### Model Sources\n\n- **Repository:** [epflLLM/meditron](https://github.com/epfLLM/meditron)\n- **Trainer:** [epflLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM)\n- **Paper:** *[MediTron-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079)*\n\n## Uses\n\nMeditron-7B is being made available for further testing and assessment as an AI assistant to enhance clinical decision-making and enhance access to an LLM for healthcare use. Potential use cases may include but are not limited to:\n-  Medical exam question answering\n-  Supporting differential diagnosis\n-  Disease information (symptoms, cause, treatment) query\n-  General health information query\n\n### Direct Use\n\nIt is possible to use this model to generate text, which is useful for experimentation and understanding its capabilities. \nIt should not be used directly for production or work that may impact people.\n\n### Downstream Use\nMeditron-70B and Meditron-7B are both foundation models without finetuning or instruction-tuning. They can be finetuned, instruction-tuned, or RLHF-tuned for specific downstream tasks and applications.\nThere are two ways we have used this model for downstream question-answering tasks.\n1. We apply in-context learning with k demonstrations (3 or 5 in our paper) added to the prompt.\n2. We finetuned the models for downstream question-answering tasks using specific training sets.\n\nWe encourage and look forward to the adaption of the base model for more diverse applications.\n\nIf you want a more interactive way to prompt the model, we recommend using a high-throughput and memory-efficient inference engine with a UI that supports chat and text generation.\n\nYou can check out our deployment [guide](https://github.com/epfLLM/meditron/blob/main/deployment/README.md), where we used [FastChat](https://github.com/lm-sys/FastChat) with [vLLM](https://github.com/vllm-project/vllm). We collected generations for our qualitative analysis through an interactive UI platform, [BetterChatGPT](https://github.com/ztjhz/BetterChatGPT). Here is the prompt format we used as an example:\n\n<img width=70% src=\"prompt_example.png\" alt=\"qualitative-analysis-prompt\" title=\"Qualitative Analysis Prompt\">\n\n### Out-of-Scope Use\n\nWe do not recommend using this model for natural language generation in a production environment, finetuned or otherwise.\n\n## Truthfulness, Helpfulness, Risk, and Bias\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nWe did an initial assessment of Meditron models' **Truthfulness** against baseline models and consumer-level medical models. \nWe use TruthfulQA (multiple choice) as the main evaluation benchmark.\nWe only focus on the categories that are relevant to the medical domain, including Health, Nutrition, Psychology, and Science.\nFor 7B models, we perform one-shot evaluations for consistent answer generation.\nFor 70B models, the evaluations are under the zero-shot setting.\nBelow, we report the detailed truthfulness performance of each category.\n\n|     |        |      |      |      |      |      |      |\n| --- | ------ |----- |----- |----- |----- |----- |----- |\n|Category  | meditron-70b | llama-2-70b | med42-70b* | meditron-7b | llama-2-7b | PMC-llama-7b |\n|Health    |      81.8    |    69.1     |    83.6    | 27.3        |   16.4     |      3.6     |\n|Nutrition |      77.9    |    68.8     |    62.5    | 31.1        |   12.5     |      6.3     |\n|Psychology|      47.4    |    36.8     |    52.6    | 21.1        |   10.5     |      0.0     |\n|Science   |      77.8    |    44.4     |    33.3    | 33.3        |   11.1     |      0.0     |\n|Avg       |      71.2    |    54.8     |    58.0    | 28.3        |   12.6     |      2.5     |\n|          |              |             |            |             |            |              |\n\nFor a more detailed performance analysis, please see our paper. \n\nSignificant research is still required to fully explore potential bias, fairness, and safety issues with this language model. \nPlease recognize that our evaluation on Meditron-7B's helpfulness, risk, and bias are highly limited. \nThus, as we noted in the safety notice, we strongly against any deployment in medical applications without further alignment process and rigorous evaluation!\n\n### Recommendations\n\n**IMPORTANT!**\nUsers (both direct and downstream) should be made aware of the risks, biases, and limitations of the model.\nWhile this model is capable of generating natural language text, we have only begun to explore this capability and its limitations. \nUnderstanding these limitations is especially important in a domain like medicine. \nTherefore, we strongly recommend against using this model in production for natural language generation or for professional purposes related to health and medicine.\n\n## Training Details\n\n### Training Data\nMeditron\u2019s domain-adaptive pre-training corpus GAP-Replay  combines 48.1B tokens from four corpora:\n- [**Clinical  Guidelines**](https://huggingface.co/datasets/epfl-llm/guidelines): a new dataset of 46K internationally-recognized clinical practice guidelines from various healthcare-related sources, including hospitals and international organizations.\n- **Medical Paper Abstracts**: 16.1M abstracts extracted from closed-access PubMed and PubMed Central papers.\n- **Medical Papers**: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers.\n- **Replay Data**: 400M tokens of general domain pretraining data sampled from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)\n\n<img width=75% src=\"gap-replay.png\" alt=\"Alt text\" title=\"Meditron-logo\">\n\n#### Data Preprocessing\n\nPlease see the detailed preprocessing procedure in our paper. \n\n### Training Procedure \n\nWe used the [Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) distributed training library, a derivative of Nvidia's Megatron LM project, to optimize training efficiency. \nHardware consists of 1 node of 8x NVIDIA A100 (80GB) SXM GPUs connected by NVLink and NVSwitch with a single Nvidia ConnectX-6 DX network card and equipped with 2 x AMD EPYC 7543 32-Core Processors and 512 GB of RAM. \n\nOur three way parallelism scheme uses:\n - Data Parallelism (DP -- different GPUs process different subsets of the batches) of 2,\n - Pipeline Parallelism (PP -- different GPUs process different layers) of 4,\n - Tensor Parallelism (TP -- different GPUs process different subtensors for matrix multiplication) of 1.\n   \n\n#### Training Hyperparameters\n\n|  |  |\n| --- | ------ |\n| bf16 | true |\n| lr  | 3e-4 |\n| eps | 1e-5       |\n| betas | \\[0.9, 0.95\\] |\n| clip_grad | 1 |\n| weight decay | 0.1 |\n| DP size | 16 |\n| TP size | 4 |\n| PP size | 1 |\n| seq length | 2048 |\n| lr scheduler | cosine|\n| min lr | 1e-6 |\n| warmup iteration | 2000 |\n| micro batch size | 10 |\n| global batch size | 1600 |\n|  |  |\n\n#### Sizes\nThe model was trained in September 2023.\n\nThe model architecture is exactly Llama 2, meaning\n\n|     |        |\n| --- | ------ |\n| Model size           | 7B   |\n| Hidden dimension     | 4096 |\n| Num. attention heads |   32 |\n| Num. layers          |   32 |\n|  |  |\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data & Metrics\n\n#### Testing Data\n- [MedQA (USMLE)](https://huggingface.co/datasets/bigbio/med_qa)\n- [MedMCQA](https://huggingface.co/datasets/medmcqa)\n- [PubMedQA](https://huggingface.co/datasets/bigbio/pubmed_qa)\n- [MMLU-Medical](https://huggingface.co/datasets/lukaemon/mmlu)\n- [MedQA-4-Option](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)\n\n#### Metrics\n- Accuracy: suite the evaluation of multiple-choice question-answering tasks.\n\n### Results\nWe finetune meditron-7b, llama-2-7b, pmc-llama-7b on each benchmark (pubmedqa, medmcqa, medqa)'s training data individually. \nWe report the finetuned models' performance with top token selection as the inference mode.\nFor MMLU-Medical, models finetuned on MedMCQA are used for inference.\nFor MedQA-4-Option, models finetuned on MedQA are used for inference.\nFor a more detailed performance analysis, please see our paper. \n\n|     |        |      |      |      |      |\n| --- | ------ |----- |----- |----- |----- |\n|Dataset       | meditron-7b | llama-2-7b  | pmc-llama-7b | Zephyr-7B-beta* | Mistral-7B-instruct* |\n|MMLU-Medical  | 54.2        |    53.7     |    56.4      |      63.3       |        60.0          |\n|PubMedQA      | 74.4        |    61.8     |    59.2      |      46.0       |        17.8          |\n|MedMCQA       | 59.2        |    54.4     |    57.6      |      43.0       |        40.2          |\n|MedQA         | 47.9        |    44.0     |    42.4      |      42.8       |        32.4          |\n|MedQA-4-Option| 52.0        |    49.6     |    49.2      |      48.5       |        41.1          |\n|Avg           | 57.5        |    52.7     |    53.0      |      48.7       |        38.3          |\n|              |             |             |              |                 |                      |\n\n**Note**: models with * are already instruction-tuned, so we exclude them from further finetuning on any training data.\n\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 8 x NVIDIA A100 (80GB) SXM\n- **Total GPU hours:** 588.8\n- **Hardware Provider:** EPFL Research Computing Platform\n- **Compute Region:** Switzerland\n- **Carbon Emitted:** Switzerland has a carbon efficiency of 0.016 kgCO2/kWh (https://www.carbonfootprint.com/docs/2018_8_electricity_factors_august_2018_-_online_sources.pdf). 73.6 hours of 8 A100s means 588.8 hours at a TDP of 400W. Assuming a Power Usage effectiveness of 1.5, total emissions are estimated to be: \n    \n   (400W / 1000W/kWh / GPU * 0.016 kgCO2/kWh * 73.6 h * 8 GPU) * 1.8 PUE = 6.8 kgCO2.\n\n## Citation\n\n**BibTeX:**\nIf you use Meditron or its training data, please cite our work:\n\n```\n@misc{chen2023meditron70b,\n      title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models}, \n      author={Zeming Chen and Alejandro Hern\u00e1ndez-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas K\u00f6pf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\n      year={2023},\n      eprint={2311.16079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@software{epfmedtrn,\n  author = {Zeming Chen and Alejandro Hern\u00e1ndez-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas K\u00f6pf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\n  title = {MediTron-70B: Scaling Medical Pretraining for Large Language Models},\n  month = November,\n  year = 2023,\n  url = {https://github.com/epfLLM/meditron}\n}\n```",
            "metadata": "{\"id\": \"epfl-llm/meditron-7b\", \"author\": \"epfl-llm\", \"sha\": \"d7d0a5ed929384a6b059ac74198cf1d71f44ba76\", \"last_modified\": \"2023-12-07 19:38:26+00:00\", \"created_at\": \"2023-11-08 16:03:23+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 4086, \"downloads_all_time\": null, \"likes\": 274, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"safetensors\", \"llama\", \"text-generation\", \"en\", \"dataset:epfl-llm/guidelines\", \"arxiv:2311.16079\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\ndatasets:\\n- epfl-llm/guidelines\\nlanguage:\\n- en\\nlicense: llama2\\nmetrics:\\n- accuracy\\n- perplexity\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='gap-replay.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='meditron_LOGO.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='prompt_example.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00004-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00005-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00006-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00007-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00008-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"KBaba7/Quant\", \"bhaskartripathi/LLM_Quantization\", \"totolook/Quant\", \"FallnAI/Quantize-HF-Models\", \"Zanalys/Meditron-7B\", \"ruslanmv/convert_to_gguf\", \"aerdna/epfl-llm-meditron-7b\", \"aerdna/epfl-llm-meditron-7bhgvcvghkvcghvh\", \"CezarCherciu/epfl-llm-meditron-7b\", \"CezarCherciu/new_meditron\", \"Razakhan9121/epfl-llm-meditron-7b\", \"edu500ac/epfl-llm-meditron-7b\", \"gdorney/epfl-llm-meditron-7b\", \"TheSelfResearchInstitute/m_1\", \"Akumii/epfl-llm-meditron-7b\", \"cyni/epfl-llm-meditron-7b\", \"Huangxs/epfl-llm-meditron-7b\", \"grmdgs/epfl-llm-meditron-7b\", \"rizaldi/medicine-gpt-huggingface\", \"wei82/epfl-llm-meditron-7b\", \"mohmmadhd/epfl-llm-meditron-7b\", \"RafayethRafi/epfl-llm-meditron-7b\", \"drkareemkamal/Harrison_chatbot\", \"Amador2001/med2\", \"onkhida/epfl-llm-meditron-7b\", \"hermi612/Medical-Chatbot\", \"K00B404/LLM_Quantization\", \"oula23/demo-llm\", \"drkareemkamal/pediatric_RAG\"], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2023-12-07 19:38:26+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\ndatasets:\\n- epfl-llm/guidelines\\nlanguage:\\n- en\\nlicense: llama2\\nmetrics:\\n- accuracy\\n- perplexity\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"654bb14be1671abcbc25521b\", \"modelId\": \"epfl-llm/meditron-7b\", \"usedStorage\": 26954572306}",
            "depth": 1,
            "children": [
                "https://huggingface.co/malhajar/meditron-7b-chat",
                "https://huggingface.co/AGBonnet/medinote-7b",
                "https://huggingface.co/tsavage68/400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot",
                "https://huggingface.co/tsavage68/500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot",
                "https://huggingface.co/tsavage68/300STEPS_5e7rate_Meditron_7B_SFT_zeroshot",
                "https://huggingface.co/tsavage68/500STEPS_5e7rate_Meditron_7B_SFT_zeroshot",
                "https://huggingface.co/tsavage68/400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot",
                "https://huggingface.co/tsavage68/400STEPS_05beta_1e7rate_Meditron7B_zerozhot",
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-wo-live_qa-ep3",
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-wo-medication_qa-ep3",
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-wo-healthsearch_qa-ep3",
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-wo-kqa_golden-ep3",
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3",
                "https://huggingface.co/Minbyul/meditron-7b-wo-live_qa-sft",
                "https://huggingface.co/Minbyul/meditron-7b-wo-medication_qa-sft",
                "https://huggingface.co/Minbyul/meditron-7b-wo-healthsearch_qa-sft",
                "https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-sft",
                "https://huggingface.co/Minbyul/meditron-7b-wo-kqa_silver_wogold-sft",
                "https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1",
                "https://huggingface.co/Minbyul/meditron-7b-wo-live_qa-iter-sft-step1",
                "https://huggingface.co/smagt/meditron-7b-instruct",
                "https://huggingface.co/veronica-girolimetti/qt_finetuned_LoRA_meditron_01",
                "https://huggingface.co/veronica-girolimetti/meditron_01",
                "https://huggingface.co/JosephNguyen/meditron-7b-finetuned",
                "https://huggingface.co/veronica-girolimetti/qt_finetuned_LoRA_meditron_02",
                "https://huggingface.co/veronica-girolimetti/qt_finetuned_LoRA_meditron_03",
                "https://huggingface.co/veronica-girolimetti/qt_finetuned_LoRA_meditron_04",
                "https://huggingface.co/veronica-girolimetti/qt_finetuned_LoRA_meditron_01_1500"
            ],
            "children_count": 28,
            "adapters": [
                "https://huggingface.co/Technoculture/MT7Bi-alpha",
                "https://huggingface.co/th135/meditron-7b_med_n900",
                "https://huggingface.co/th135/meditron-7b_gen_n900",
                "https://huggingface.co/th135/meditron-7b_both_n1800",
                "https://huggingface.co/trungvo/meditron-7b-finetuned-PubMedQA",
                "https://huggingface.co/davidnene/meditron-pharmachat-ft",
                "https://huggingface.co/ssktora/retriever-medtron_en",
                "https://huggingface.co/JesseLiu/oneround_meditron_7b",
                "https://huggingface.co/JesseLiu/lora4combine_meditron7b",
                "https://huggingface.co/samirangupta31/meditron-finetuned",
                "https://huggingface.co/Hudasr/meditron-7b-lora-drug-interaction"
            ],
            "adapters_count": 11,
            "quantized": [
                "https://huggingface.co/TheBloke/meditron-7B-GGUF",
                "https://huggingface.co/mlx-community/meditron-7b",
                "https://huggingface.co/TheBloke/meditron-7B-GPTQ",
                "https://huggingface.co/TheBloke/meditron-7B-AWQ",
                "https://huggingface.co/legionarius/meditron-7b-Q6_K-GGUF",
                "https://huggingface.co/joshnader/meditron-7b-Q4_K_M-GGUF",
                "https://huggingface.co/davidbzyk/meditron-7b-Q4_K_M-GGUF",
                "https://huggingface.co/np-n/meditron-7b_Q3_K_M.gguf",
                "https://huggingface.co/np-n/meditron-7b_Q4_K_M.gguf",
                "https://huggingface.co/np-n/meditron-7b_Q6_K.gguf",
                "https://huggingface.co/np-n/meditron-7b_Q8_0.gguf"
            ],
            "quantized_count": 11,
            "merges": [
                "https://huggingface.co/felipegm0911/Meditron-Mistral-instruct-7b-main",
                "https://huggingface.co/GianlucaMondillo/BioTakuya",
                "https://huggingface.co/X-iZhang/libra-v1.0-7b",
                "https://huggingface.co/yqnis/vigomed-2-7b-slerp"
            ],
            "merges_count": 4,
            "spaces": [
                "CezarCherciu/epfl-llm-meditron-7b",
                "FallnAI/Quantize-HF-Models",
                "K00B404/LLM_Quantization",
                "KBaba7/Quant",
                "Razakhan9121/epfl-llm-meditron-7b",
                "Zanalys/Meditron-7B",
                "aerdna/epfl-llm-meditron-7b",
                "aerdna/epfl-llm-meditron-7bhgvcvghkvcghvh",
                "bhaskartripathi/LLM_Quantization",
                "hermi612/Medical-Chatbot",
                "huggingface/InferenceSupport/discussions/new?title=epfl-llm/meditron-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bepfl-llm%2Fmeditron-7b%5D(%2Fepfl-llm%2Fmeditron-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",
                "ruslanmv/convert_to_gguf",
                "totolook/Quant"
            ],
            "spaces_count": 13
        },
        {
            "model_id": "malhajar/meditron-7b-chat",
            "card": "---\nlanguage:\n- en\nlicense: llama2\ntags:\n- Medicine\ndatasets:\n- yahma/alpaca-cleaned\nbase_model: epfl-llm/meditron-7b\nmodel-index:\n- name: meditron-7b-chat\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AI2 Reasoning Challenge (25-Shot)\n      type: ai2_arc\n      config: ARC-Challenge\n      split: test\n      args:\n        num_few_shot: 25\n    metrics:\n    - type: acc_norm\n      value: 50.77\n      name: normalized accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: HellaSwag (10-Shot)\n      type: hellaswag\n      split: validation\n      args:\n        num_few_shot: 10\n    metrics:\n    - type: acc_norm\n      value: 75.37\n      name: normalized accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU (5-Shot)\n      type: cais/mmlu\n      config: all\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 40.49\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: TruthfulQA (0-shot)\n      type: truthful_qa\n      config: multiple_choice\n      split: validation\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: mc2\n      value: 48.56\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Winogrande (5-shot)\n      type: winogrande\n      config: winogrande_xl\n      split: validation\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 73.16\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GSM8k (5-shot)\n      type: gsm8k\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 9.17\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\n      name: Open LLM Leaderboard\n---\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\nmeditron-7b-chat is a finetuned version of [`epfl-llm/meditron-7b`](https://huggingface.co/epfl-llm/meditron-7b) using SFT Training on the Alpaca Dataset.\nThis model can answer information about different excplicit ideas in medicine (see [`epfl-llm/meditron-7b`](https://huggingface.co/epfl-llm/meditron-7b) for more info)\n\n### Model Description\n\n- **Finetuned by:** [`Mohamad Alhajar`](https://www.linkedin.com/in/muhammet-alhajar/) \n- **Language(s) (NLP):** English\n- **Finetuned from model:** [`epfl-llm/meditron-7b`](https://huggingface.co/epfl-llm/meditron-7b)\n\n### Prompt Template\n```\n### Instruction:\n\n<prompt> (without the <>)\n\n### Response:\n```\n\n\n## How to Get Started with the Model\n\nUse the code sample provided in the original post to interact with the model.\n```python\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\n \nmodel_id = \"malhajar/meditron-7b-chat\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             torch_dtype=torch.float16,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nquestion: \"what is tract infection?\"\n# For generating a response\nprompt = '''\n### Instruction:\n{question} \n\n### Response:'''\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\noutput = model.generate(inputs=input_ids,max_new_tokens=512,pad_token_id=tokenizer.eos_token_id,top_k=50, do_sample=True,\n        top_p=0.95)\nresponse = tokenizer.decode(output[0])\n\nprint(response)\n```\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_malhajar__meditron-7b-chat)\n\n|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |49.59|\n|AI2 Reasoning Challenge (25-Shot)|50.77|\n|HellaSwag (10-Shot)              |75.37|\n|MMLU (5-Shot)                    |40.49|\n|TruthfulQA (0-shot)              |48.56|\n|Winogrande (5-shot)              |73.16|\n|GSM8k (5-shot)                   | 9.17|\n\n",
            "metadata": "{\"id\": \"malhajar/meditron-7b-chat\", \"author\": \"malhajar\", \"sha\": \"a24bbb3a150cb49939c730b097d509acacf95aa9\", \"last_modified\": \"2024-03-04 14:19:59+00:00\", \"created_at\": \"2023-12-09 12:10:33+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 351, \"downloads_all_time\": null, \"likes\": 9, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"Medicine\", \"en\", \"dataset:yahma/alpaca-cleaned\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"model-index\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- yahma/alpaca-cleaned\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- Medicine\\nmodel-index:\\n- name: meditron-7b-chat\\n  results:\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: AI2 Reasoning Challenge (25-Shot)\\n      type: ai2_arc\\n      config: ARC-Challenge\\n      split: test\\n      args:\\n        num_few_shot: 25\\n    metrics:\\n    - type: acc_norm\\n      value: 50.77\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: HellaSwag (10-Shot)\\n      type: hellaswag\\n      split: validation\\n      args:\\n        num_few_shot: 10\\n    metrics:\\n    - type: acc_norm\\n      value: 75.37\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MMLU (5-Shot)\\n      type: cais/mmlu\\n      config: all\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 40.49\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: TruthfulQA (0-shot)\\n      type: truthful_qa\\n      config: multiple_choice\\n      split: validation\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: mc2\\n      value: 48.56\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: Winogrande (5-shot)\\n      type: winogrande\\n      config: winogrande_xl\\n      split: validation\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 73.16\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: GSM8k (5-shot)\\n      type: gsm8k\\n      config: main\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 9.17\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-chat\", \"results\": [{\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"AI2 Reasoning Challenge (25-Shot)\", \"type\": \"ai2_arc\", \"config\": \"ARC-Challenge\", \"split\": \"test\", \"args\": {\"num_few_shot\": 25}}, \"metrics\": [{\"type\": \"acc_norm\", \"value\": 50.77, \"name\": \"normalized accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"HellaSwag (10-Shot)\", \"type\": \"hellaswag\", \"split\": \"validation\", \"args\": {\"num_few_shot\": 10}}, \"metrics\": [{\"type\": \"acc_norm\", \"value\": 75.37, \"name\": \"normalized accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"MMLU (5-Shot)\", \"type\": \"cais/mmlu\", \"config\": \"all\", \"split\": \"test\", \"args\": {\"num_few_shot\": 5}}, \"metrics\": [{\"type\": \"acc\", \"value\": 40.49, \"name\": \"accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"TruthfulQA (0-shot)\", \"type\": \"truthful_qa\", \"config\": \"multiple_choice\", \"split\": \"validation\", \"args\": {\"num_few_shot\": 0}}, \"metrics\": [{\"type\": \"mc2\", \"value\": 48.56, \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"Winogrande (5-shot)\", \"type\": \"winogrande\", \"config\": \"winogrande_xl\", \"split\": \"validation\", \"args\": {\"num_few_shot\": 5}}, \"metrics\": [{\"type\": \"acc\", \"value\": 73.16, \"name\": \"accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}, {\"task\": {\"type\": \"text-generation\", \"name\": \"Text Generation\"}, \"dataset\": {\"name\": \"GSM8k (5-shot)\", \"type\": \"gsm8k\", \"config\": \"main\", \"split\": \"test\", \"args\": {\"num_few_shot\": 5}}, \"metrics\": [{\"type\": \"acc\", \"value\": 9.17, \"name\": \"accuracy\", \"verified\": false}], \"source\": {\"url\": \"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\", \"name\": \"Open LLM Leaderboard\"}}]}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-03-04 14:19:59+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- yahma/alpaca-cleaned\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- Medicine\\nmodel-index:\\n- name: meditron-7b-chat\\n  results:\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: AI2 Reasoning Challenge (25-Shot)\\n      type: ai2_arc\\n      config: ARC-Challenge\\n      split: test\\n      args:\\n        num_few_shot: 25\\n    metrics:\\n    - type: acc_norm\\n      value: 50.77\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: HellaSwag (10-Shot)\\n      type: hellaswag\\n      split: validation\\n      args:\\n        num_few_shot: 10\\n    metrics:\\n    - type: acc_norm\\n      value: 75.37\\n      name: normalized accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MMLU (5-Shot)\\n      type: cais/mmlu\\n      config: all\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 40.49\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: TruthfulQA (0-shot)\\n      type: truthful_qa\\n      config: multiple_choice\\n      split: validation\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: mc2\\n      value: 48.56\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: Winogrande (5-shot)\\n      type: winogrande\\n      config: winogrande_xl\\n      split: validation\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 73.16\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: GSM8k (5-shot)\\n      type: gsm8k\\n      config: main\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 9.17\\n      name: accuracy\\n      verified: false\\n    source:\\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat\\n      name: Open LLM Leaderboard\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65745939839aa08899eabbb6\", \"modelId\": \"malhajar/meditron-7b-chat\", \"usedStorage\": 13477643299}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [
                "https://huggingface.co/zoohun/results"
            ],
            "adapters_count": 1,
            "quantized": [
                "https://huggingface.co/TheBloke/meditron-7B-chat-GGUF",
                "https://huggingface.co/TheBloke/meditron-7B-chat-AWQ",
                "https://huggingface.co/TheBloke/meditron-7B-chat-GPTQ",
                "https://huggingface.co/mradermacher/meditron-7b-chat-GGUF",
                "https://huggingface.co/mradermacher/meditron-7b-chat-i1-GGUF"
            ],
            "quantized_count": 5,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "HuggingFaceH4/open_llm_leaderboard?query=malhajar/meditron-7b-chat",
                "huggingface/InferenceSupport/discussions/new?title=malhajar/meditron-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bmalhajar%2Fmeditron-7b-chat%5D(%2Fmalhajar%2Fmeditron-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 2
        },
        {
            "model_id": "AGBonnet/medinote-7b",
            "card": "---\nlicense: llama2\nlanguage:\n- en\ndatasets:\n- AGBonnet/augmented-clinical-notes\nbase_model: epfl-llm/meditron-7b\n---\n<img width=20% src=\"medinote.png\" title=\"logo\">\n\n# Model Card for MediNote-7B-v1.0\nMediNote is a suite of open-source medical Large Language Models (LLMs) fine-tuned for clinical note generation from the [Meditron](https://arxiv.org/abs/2311.16079) foundation model. \nMediNote-7B is a 7 billion parameters model trained to generate clinical notes from doctor-patient conversations. \n\n## Model Details\n\n- **Developed by:** [Antoine Bonnet](https://huggingface.co/AGBonnet) and [Paul Boulenger](https://huggingface.co/paulblger)\n- **Model type:** Causal decoder-only transformer language model\n- **Language(s):** English only\n- **Model License:** [LLAMA 2 COMMUNITY LICENSE AGREEMENT](https://huggingface.co/meta-llama/Llama-2-70b/raw/main/LICENSE.txt)\n- **Code License:** [MIT](https://opensource.org/license/mit/)\n- **Fine-tuned from model:** [Meditron-7B.v1.0](https://huggingface.co/epfl-llm/meditron-7b)\n- **Context length:**  2K tokens\n- **Input:**  Patient-doctor conversation transcripts (text)\n- **Output:**  Clinical notes (text)\n- **Repository:** [EPFL-IC-Make-Team/ClinicalNotes](https://github.com/EPFL-IC-Make-Team/ClinicalNotes)\n- **Trainer:** [epflLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM)\n- **Report:** *[MediNote: Automatic Clinical Notes](https://github.com/EPFL-IC-Make-Team/medinote/blob/main/report.pdf)*\n\n<p align=\"center\">\n  <img width=70% src=\"model_pipeline.pdf\" alt=\"Model pipeline\" title=\"Model pipeline\">\n</p>\n\n\n## Uses\n\n### Direct Use\n\nIt is possible to use this model to generate clinical notes, which is useful for experimentation and understanding its capabilities. \nIt should not be used directly for production or work that may impact people.\n\n### Out-of-Scope Use\n\nThis model is not yet robust enough for use in a real clinical setting. \nWe do not recommend using this model for natural language generation in a production environment. \n",
            "metadata": "{\"id\": \"AGBonnet/medinote-7b\", \"author\": \"AGBonnet\", \"sha\": \"7ae9731227ad3931a6351ef574646950ee9663b3\", \"last_modified\": \"2024-03-26 09:52:03+00:00\", \"created_at\": \"2024-01-16 18:56:57+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 219, \"downloads_all_time\": null, \"likes\": 10, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"en\", \"dataset:AGBonnet/augmented-clinical-notes\", \"arxiv:2311.16079\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- AGBonnet/augmented-clinical-notes\\nlanguage:\\n- en\\nlicense: llama2\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<|im_start|>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"<|im_end|>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='logo.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='medinote.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model_pipeline.pdf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='template.pdf', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 6738571264}, \"total\": 6738571264}, \"security_repo_status\": null, \"lastModified\": \"2024-03-26 09:52:03+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- AGBonnet/augmented-clinical-notes\\nlanguage:\\n- en\\nlicense: llama2\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65a6d179ae68caef0b52e172\", \"modelId\": \"AGBonnet/medinote-7b\", \"usedStorage\": 26954818379}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/medinote-7b-GGUF",
                "https://huggingface.co/mradermacher/medinote-7b-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=AGBonnet/medinote-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BAGBonnet%2Fmedinote-7b%5D(%2FAGBonnet%2Fmedinote-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: 500STEPS_01beta_DPO_Meditron7B\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 500STEPS_01beta_DPO_Meditron7B\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- eval_loss: 0.6910\n- eval_runtime: 328.325\n- eval_samples_per_second: 1.386\n- eval_steps_per_second: 1.386\n- eval_rewards/chosen: 0.0001\n- eval_rewards/rejected: -0.0042\n- eval_rewards/accuracies: 0.5121\n- eval_rewards/margins: 0.0044\n- eval_logps/rejected: -27.8358\n- eval_logps/chosen: -26.4800\n- eval_logits/rejected: -0.6115\n- eval_logits/chosen: -0.6114\n- epoch: 0.78\n- step: 400\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-07\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 500\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot\", \"author\": \"tsavage68\", \"sha\": \"f30a2c0e7eb646203d37c58299bf8350a756bc02\", \"last_modified\": \"2024-02-10 23:37:52+00:00\", \"created_at\": \"2024-02-10 23:35:24+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"dpo\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_01beta_DPO_Meditron7B\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"500STEPS_01beta_DPO_Meditron7B\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-10 23:37:52+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_01beta_DPO_Meditron7B\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c8083c6e04bb96e751dd72\", \"modelId\": \"tsavage68/400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot%5D(%2Ftsavage68%2F400STEPS_01beta_1e7_DPO_Meditron7B_zeroshot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: 500STEPS_1e6rate_01beta_DPO_Meditron7B\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 500STEPS_1e6rate_01beta_DPO_Meditron7B\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6302\n- Rewards/chosen: 0.0115\n- Rewards/rejected: -0.1672\n- Rewards/accuracies: 0.5868\n- Rewards/margins: 0.1788\n- Logps/rejected: -29.4661\n- Logps/chosen: -26.3659\n- Logits/rejected: -0.7645\n- Logits/chosen: -0.7643\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-06\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 500\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.6902        | 0.1   | 50   | 0.6903          | 0.0090         | 0.0031           | 0.5121             | 0.0058          | -27.7623       | -26.3918     | -0.6125         | -0.6124       |\n| 0.6766        | 0.2   | 100  | 0.6792          | -0.1559        | -0.1907          | 0.5099             | 0.0349          | -29.7009       | -28.0399     | -0.6382         | -0.6380       |\n| 0.6667        | 0.29  | 150  | 0.6567          | -0.0224        | -0.1102          | 0.5714             | 0.0879          | -28.8959       | -26.7051     | -0.6559         | -0.6557       |\n| 0.6656        | 0.39  | 200  | 0.6495          | -0.0303        | -0.1387          | 0.5802             | 0.1084          | -29.1808       | -26.7847     | -0.7108         | -0.7106       |\n| 0.5939        | 0.49  | 250  | 0.6388          | -0.0202        | -0.1629          | 0.5890             | 0.1426          | -29.4223       | -26.6837     | -0.7329         | -0.7327       |\n| 0.6328        | 0.59  | 300  | 0.6349          | -0.0421        | -0.2022          | 0.5758             | 0.1601          | -29.8158       | -26.9024     | -0.7492         | -0.7490       |\n| 0.6231        | 0.68  | 350  | 0.6313          | -0.0004        | -0.1725          | 0.5758             | 0.1721          | -29.5189       | -26.4852     | -0.7571         | -0.7569       |\n| 0.6419        | 0.78  | 400  | 0.6303          | 0.0123         | -0.1660          | 0.5868             | 0.1783          | -29.4536       | -26.3585     | -0.7639         | -0.7637       |\n| 0.6045        | 0.88  | 450  | 0.6304          | 0.0120         | -0.1662          | 0.5846             | 0.1783          | -29.4560       | -26.3611     | -0.7645         | -0.7643       |\n| 0.5984        | 0.98  | 500  | 0.6302          | 0.0115         | -0.1672          | 0.5868             | 0.1788          | -29.4661       | -26.3659     | -0.7645         | -0.7643       |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot\", \"author\": \"tsavage68\", \"sha\": \"9f7e1fd9352fb5087942e18f56c58f41a07fe458\", \"last_modified\": \"2024-02-11 06:37:05+00:00\", \"created_at\": \"2024-02-11 06:33:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"dpo\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_1e6rate_01beta_DPO_Meditron7B\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"500STEPS_1e6rate_01beta_DPO_Meditron7B\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-11 06:37:05+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_1e6rate_01beta_DPO_Meditron7B\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c86a4e81c73502fc5cfeb9\", \"modelId\": \"tsavage68/500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot%5D(%2Ftsavage68%2F500STEPS_1e6rate_01beta_DPO_Meditron7B_zeroshot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/300STEPS_5e7rate_Meditron_7B_SFT_zeroshot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: 300STEPS_5e7rate_Meditron_7B_SFT\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 300STEPS_5e7rate_Meditron_7B_SFT\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3127\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 300\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.2096        | 0.1   | 50   | 1.1770          |\n| 0.7177        | 0.2   | 100  | 0.6260          |\n| 0.3357        | 0.29  | 150  | 0.3221          |\n| 0.3191        | 0.39  | 200  | 0.3142          |\n| 0.3195        | 0.49  | 250  | 0.3128          |\n| 0.3195        | 0.59  | 300  | 0.3127          |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/300STEPS_5e7rate_Meditron_7B_SFT_zeroshot\", \"author\": \"tsavage68\", \"sha\": \"0b7c37247b18f5e1e4b2c5c0a3ead4735d45e7d7\", \"last_modified\": \"2024-02-11 08:14:49+00:00\", \"created_at\": \"2024-02-11 08:10:06+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"sft\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: 300STEPS_5e7rate_Meditron_7B_SFT\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"300STEPS_5e7rate_Meditron_7B_SFT\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-11 08:14:49+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: 300STEPS_5e7rate_Meditron_7B_SFT\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c880de957afc29ce658433\", \"modelId\": \"tsavage68/300STEPS_5e7rate_Meditron_7B_SFT_zeroshot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/300STEPS_5e7rate_Meditron_7B_SFT_zeroshot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F300STEPS_5e7rate_Meditron_7B_SFT_zeroshot%5D(%2Ftsavage68%2F300STEPS_5e7rate_Meditron_7B_SFT_zeroshot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/500STEPS_5e7rate_Meditron_7B_SFT_zeroshot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- sft\n- generated_from_trainer\nmodel-index:\n- name: 500STEPS_5e7rate_Meditron_7B_SFT\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 500STEPS_5e7rate_Meditron_7B_SFT\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.3040\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 500\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.2096        | 0.1   | 50   | 1.1770          |\n| 0.7177        | 0.2   | 100  | 0.6260          |\n| 0.3348        | 0.29  | 150  | 0.3205          |\n| 0.3151        | 0.39  | 200  | 0.3102          |\n| 0.3138        | 0.49  | 250  | 0.3065          |\n| 0.3118        | 0.59  | 300  | 0.3050          |\n| 0.3033        | 0.68  | 350  | 0.3042          |\n| 0.2995        | 0.78  | 400  | 0.3040          |\n| 0.2781        | 0.88  | 450  | 0.3040          |\n| 0.3055        | 0.98  | 500  | 0.3040          |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/500STEPS_5e7rate_Meditron_7B_SFT_zeroshot\", \"author\": \"tsavage68\", \"sha\": \"5d0a7d7012d4b2c6eddb4782b3e1334639c01272\", \"last_modified\": \"2024-02-11 09:16:44+00:00\", \"created_at\": \"2024-02-11 09:11:44+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"sft\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_5e7rate_Meditron_7B_SFT\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"500STEPS_5e7rate_Meditron_7B_SFT\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-11 09:16:44+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: 500STEPS_5e7rate_Meditron_7B_SFT\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c88f50786855f8668d75f6\", \"modelId\": \"tsavage68/500STEPS_5e7rate_Meditron_7B_SFT_zeroshot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/500STEPS_5e7rate_Meditron_7B_SFT_zeroshot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F500STEPS_5e7rate_Meditron_7B_SFT_zeroshot%5D(%2Ftsavage68%2F500STEPS_5e7rate_Meditron_7B_SFT_zeroshot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: 400STEPS_5e7rate_03beta_DPO_Meditron7B\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 400STEPS_5e7rate_03beta_DPO_Meditron7B\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6439\n- Rewards/chosen: -0.0166\n- Rewards/rejected: -0.1472\n- Rewards/accuracies: 0.5714\n- Rewards/margins: 0.1306\n- Logps/rejected: -28.2845\n- Logps/chosen: -26.5367\n- Logits/rejected: -0.6342\n- Logits/chosen: -0.6341\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 400\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.6896        | 0.1   | 50   | 0.6916          | 0.0067         | 0.0033           | 0.4637             | 0.0034          | -27.7828       | -26.4590     | -0.6113         | -0.6111       |\n| 0.6783        | 0.2   | 100  | 0.6771          | -0.0693        | -0.1071          | 0.5319             | 0.0378          | -28.1508       | -26.7125     | -0.6173         | -0.6171       |\n| 0.6697        | 0.29  | 150  | 0.6571          | -0.0107        | -0.1001          | 0.5626             | 0.0893          | -28.1273       | -26.5172     | -0.6171         | -0.6170       |\n| 0.6463        | 0.39  | 200  | 0.6496          | 0.0037         | -0.1067          | 0.5692             | 0.1104          | -28.1493       | -26.4691     | -0.6288         | -0.6286       |\n| 0.6124        | 0.49  | 250  | 0.6449          | -0.0073        | -0.1329          | 0.5648             | 0.1257          | -28.2368       | -26.5056     | -0.6318         | -0.6317       |\n| 0.641         | 0.59  | 300  | 0.6440          | -0.0156        | -0.1460          | 0.5758             | 0.1304          | -28.2803       | -26.5333     | -0.6340         | -0.6339       |\n| 0.643         | 0.68  | 350  | 0.6430          | -0.0150        | -0.1479          | 0.5780             | 0.1328          | -28.2866       | -26.5315     | -0.6343         | -0.6341       |\n| 0.6632        | 0.78  | 400  | 0.6439          | -0.0166        | -0.1472          | 0.5714             | 0.1306          | -28.2845       | -26.5367     | -0.6342         | -0.6341       |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot\", \"author\": \"tsavage68\", \"sha\": \"9d379483b7d505236749508b7bb14fc7a3b28e1c\", \"last_modified\": \"2024-02-11 14:36:06+00:00\", \"created_at\": \"2024-02-11 14:32:04+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"dpo\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 400STEPS_5e7rate_03beta_DPO_Meditron7B\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"400STEPS_5e7rate_03beta_DPO_Meditron7B\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-11 14:36:06+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 400STEPS_5e7rate_03beta_DPO_Meditron7B\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c8da6481c73502fc76274e\", \"modelId\": \"tsavage68/400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot%5D(%2Ftsavage68%2F400STEPS_5e7rate_03beta_DPO_Meditron7B_zeroshot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "tsavage68/400STEPS_05beta_1e7rate_Meditron7B_zerozhot",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- trl\n- dpo\n- generated_from_trainer\nmodel-index:\n- name: 400STEPS_05beta_1e7rate_Meditron7B\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# 400STEPS_05beta_1e7rate_Meditron7B\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6864\n- Rewards/chosen: 0.0004\n- Rewards/rejected: -0.0144\n- Rewards/accuracies: 0.4945\n- Rewards/margins: 0.0148\n- Logps/rejected: -27.8226\n- Logps/chosen: -26.4806\n- Logits/rejected: -0.6110\n- Logits/chosen: -0.6109\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-07\n- train_batch_size: 4\n- eval_batch_size: 1\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 100\n- training_steps: 400\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.6941        | 0.1   | 50   | 0.6931          | -0.0003        | -0.0011          | 0.4044             | 0.0008          | -27.7959       | -26.4820     | -0.6106         | -0.6104       |\n| 0.6927        | 0.2   | 100  | 0.6912          | -0.0047        | -0.0093          | 0.4769             | 0.0046          | -27.8123       | -26.4908     | -0.6105         | -0.6104       |\n| 0.6838        | 0.29  | 150  | 0.6896          | -0.0023        | -0.0105          | 0.5077             | 0.0082          | -27.8146       | -26.4860     | -0.6101         | -0.6100       |\n| 0.6906        | 0.39  | 200  | 0.6886          | -0.0007        | -0.0107          | 0.4989             | 0.0100          | -27.8151       | -26.4828     | -0.6109         | -0.6108       |\n| 0.6789        | 0.49  | 250  | 0.6877          | -0.0035        | -0.0154          | 0.5121             | 0.0119          | -27.8245       | -26.4884     | -0.6111         | -0.6110       |\n| 0.6853        | 0.59  | 300  | 0.6852          | 0.0012         | -0.0160          | 0.5297             | 0.0172          | -27.8257       | -26.4791     | -0.6112         | -0.6111       |\n| 0.6805        | 0.68  | 350  | 0.6877          | -0.0039        | -0.0162          | 0.4725             | 0.0122          | -27.8260       | -26.4893     | -0.6112         | -0.6110       |\n| 0.6936        | 0.78  | 400  | 0.6864          | 0.0004         | -0.0144          | 0.4945             | 0.0148          | -27.8226       | -26.4806     | -0.6110         | -0.6109       |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.0.0+cu117\n- Datasets 2.17.0\n- Tokenizers 0.15.1\n",
            "metadata": "{\"id\": \"tsavage68/400STEPS_05beta_1e7rate_Meditron7B_zerozhot\", \"author\": \"tsavage68\", \"sha\": \"1b0fc3f4e05a76d13a8b31b1b908a0c120321eb0\", \"last_modified\": \"2024-02-11 20:39:56+00:00\", \"created_at\": \"2024-02-11 20:34:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"trl\", \"dpo\", \"generated_from_trainer\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 400STEPS_05beta_1e7rate_Meditron7B\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"400STEPS_05beta_1e7rate_Meditron7B\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"</s>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='final_checkpoint/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-02-11 20:39:56+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlicense: llama2\\ntags:\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: 400STEPS_05beta_1e7rate_Meditron7B\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65c92f6f4936ab38ecdcd016\", \"modelId\": \"tsavage68/400STEPS_05beta_1e7rate_Meditron7B_zerozhot\", \"usedStorage\": 13477147499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=tsavage68/400STEPS_05beta_1e7rate_Meditron7B_zerozhot&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Btsavage68%2F400STEPS_05beta_1e7rate_Meditron7B_zerozhot%5D(%2Ftsavage68%2F400STEPS_05beta_1e7rate_Meditron7B_zerozhot)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-wo-live_qa-ep3",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-wo-live_qa-ep3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-wo-live_qa-ep3\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5356\n- Rewards/chosen: -0.2871\n- Rewards/rejected: -0.7760\n- Rewards/accuracies: 0.6923\n- Rewards/margins: 0.4889\n- Logps/rejected: -1205.3544\n- Logps/chosen: -986.1032\n- Logits/rejected: -0.8878\n- Logits/chosen: -0.8900\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.583         | 0.49  | 100  | 0.6358          | -0.0392        | -0.1400          | 0.6442             | 0.1009          | -1141.7539     | -961.3083    | -0.8346         | -0.8420       |\n| 0.3768        | 0.98  | 200  | 0.5356          | -0.2875        | -0.7751          | 0.7019             | 0.4876          | -1205.2603     | -986.1399    | -0.8883         | -0.8904       |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-wo-live_qa-ep3\", \"author\": \"Minbyul\", \"sha\": \"1e93d07fd1ef7fef6b186a4a0862f336daee20b9\", \"last_modified\": \"2024-04-10 07:51:39+00:00\", \"created_at\": \"2024-04-10 06:48:03+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-live_qa-ep3\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-wo-live_qa-ep3\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-10 07:51:39+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-live_qa-ep3\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"66163623cee84560766162c4\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-wo-live_qa-ep3\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-wo-live_qa-ep3-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-wo-live_qa-ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-wo-live_qa-ep3%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-wo-live_qa-ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-wo-medication_qa-ep3",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-wo-medication_qa-ep3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-wo-medication_qa-ep3\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5850\n- Rewards/chosen: -0.2075\n- Rewards/rejected: -0.5850\n- Rewards/accuracies: 0.7881\n- Rewards/margins: 0.3775\n- Logps/rejected: -1658.2283\n- Logps/chosen: -974.4987\n- Logits/rejected: -0.8799\n- Logits/chosen: -0.7592\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.592         | 0.55  | 100  | 0.6293          | -0.0145        | -0.1627          | 0.7797             | 0.1482          | -1615.9989     | -955.1961    | -0.8722         | -0.7069       |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-wo-medication_qa-ep3\", \"author\": \"Minbyul\", \"sha\": \"2a5f365ba2e84c6fba162028f9066e0fd5e2b49a\", \"last_modified\": \"2024-04-10 08:52:50+00:00\", \"created_at\": \"2024-04-10 07:53:15+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-medication_qa-ep3\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-wo-medication_qa-ep3\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-10 08:52:50+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-medication_qa-ep3\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6616456bbfbe090022505d63\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-wo-medication_qa-ep3\", \"usedStorage\": 13477649563}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-wo-medication_qa-ep3-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-wo-medication_qa-ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-wo-medication_qa-ep3%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-wo-medication_qa-ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-wo-healthsearch_qa-ep3",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-wo-healthsearch_qa-ep3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-wo-healthsearch_qa-ep3\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6786\n- Rewards/chosen: -0.0040\n- Rewards/rejected: -0.0362\n- Rewards/accuracies: 0.6245\n- Rewards/margins: 0.0322\n- Logps/rejected: -1242.8666\n- Logps/chosen: -1081.9209\n- Logits/rejected: -0.7759\n- Logits/chosen: -0.8007\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-wo-healthsearch_qa-ep3\", \"author\": \"Minbyul\", \"sha\": \"5ee2b17f3cb40088bf19d9442dd0f999de6cc71a\", \"last_modified\": \"2024-04-10 09:46:01+00:00\", \"created_at\": \"2024-04-10 09:02:52+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-healthsearch_qa-ep3\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-wo-healthsearch_qa-ep3\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-10 09:46:01+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-healthsearch_qa-ep3\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661655bc1ddd756333ccabb1\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-wo-healthsearch_qa-ep3\", \"usedStorage\": 13477649563}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-wo-healthsearch_qa-ep3-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-wo-healthsearch_qa-ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-wo-healthsearch_qa-ep3%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-wo-healthsearch_qa-ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-wo-kqa_golden-ep3",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- alignment-handbook\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-wo-kqa_golden-ep3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-wo-kqa_golden-ep3\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4459\n- Rewards/chosen: -0.4566\n- Rewards/rejected: -1.4012\n- Rewards/accuracies: 0.8068\n- Rewards/margins: 0.9447\n- Logps/rejected: -1444.6896\n- Logps/chosen: -859.0582\n- Logits/rejected: -0.9203\n- Logits/chosen: -0.8310\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Logits/chosen | Logits/rejected | Logps/chosen | Logps/rejected | Validation Loss | Rewards/accuracies | Rewards/chosen | Rewards/margins | Rewards/rejected |\n|:-------------:|:-----:|:----:|:-------------:|:---------------:|:------------:|:--------------:|:---------------:|:------------------:|:--------------:|:---------------:|:----------------:|\n| 0.5643        | 0.5   | 100  | -0.6995       | -0.8645         | -818.2397    | -1334.0771     | 0.5890          | 0.7727             | -0.0484        | 0.2467          | -0.2951          |\n| 0.3959        | 1.0   | 200  | -0.8310       | -0.9203         | -859.0582    | -1444.6896     | 0.4459          | 0.8068             | -0.4566        | 0.9447          | -1.4012          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-wo-kqa_golden-ep3\", \"author\": \"Minbyul\", \"sha\": \"f69c191985063a718dd79d56016a0ca3fba2ed05\", \"last_modified\": \"2024-04-10 11:05:16+00:00\", \"created_at\": \"2024-04-10 09:49:17+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-kqa_golden-ep3\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-wo-kqa_golden-ep3\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-10 11:05:16+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-kqa_golden-ep3\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6616609d8af94204075f81c0\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-wo-kqa_golden-ep3\", \"usedStorage\": 13477655827}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-wo-kqa_golden-ep3-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-wo-kqa_golden-ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-wo-kqa_golden-ep3%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-wo-kqa_golden-ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.5793\n- Rewards/chosen: -0.1323\n- Rewards/rejected: -0.4764\n- Rewards/accuracies: 0.7717\n- Rewards/margins: 0.3440\n- Logps/rejected: -1456.3621\n- Logps/chosen: -834.8738\n- Logits/rejected: -0.9041\n- Logits/chosen: -0.7062\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Logits/chosen | Logits/rejected | Logps/chosen | Logps/rejected | Validation Loss | Rewards/accuracies | Rewards/chosen | Rewards/margins | Rewards/rejected |\n|:-------------:|:-----:|:----:|:-------------:|:---------------:|:------------:|:--------------:|:---------------:|:------------------:|:--------------:|:---------------:|:----------------:|\n| 0.5615        | 0.61  | 100  | -0.6676       | -0.8939         | -826.0934    | -1433.1564     | 0.6219          | 0.7459             | -0.0445        | 0.1998          | -0.2443          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\", \"author\": \"Minbyul\", \"sha\": \"dc05513637bb5cdf2a7aa13cb52dbef7abaf3d5c\", \"last_modified\": \"2024-04-10 13:00:29+00:00\", \"created_at\": \"2024-04-10 11:12:37+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-10 13:00:29+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"66167425bfbe09002259ffe8\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3\", \"usedStorage\": 23364171323}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-wo-kqa_silver_wogold-ep3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-wo-kqa_silver_wogold-ep3%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-wo-kqa_silver_wogold-ep3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-live_qa-sft",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-live_qa-sft\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-live_qa-sft\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.4959\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.1705        | 1.0   | 7    | 1.4560          |\n| 1.0304        | 2.0   | 14   | 1.4817          |\n| 0.7906        | 3.0   | 21   | 1.4959          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-live_qa-sft\", \"author\": \"Minbyul\", \"sha\": \"6b635196fc6cf5106ffea8990a6de13c86f39aed\", \"last_modified\": \"2024-04-19 03:43:48+00:00\", \"created_at\": \"2024-04-14 12:06:11+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-live_qa-sft\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-live_qa-sft\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-19 03:43:48+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-live_qa-sft\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661bc6b3d7891f6e195cf96b\", \"modelId\": \"Minbyul/meditron-7b-wo-live_qa-sft\", \"usedStorage\": 30553699819}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-sft-wo-live_qa"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-live_qa-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-live_qa-sft%5D(%2FMinbyul%2Fmeditron-7b-wo-live_qa-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-sft-wo-live_qa",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-live_qa-sft\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-sft-wo-live_qa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-sft-wo-live_qa\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-live_qa-sft](https://huggingface.co/Minbyul/meditron-7b-wo-live_qa-sft) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6677\n- Rewards/chosen: -0.0447\n- Rewards/rejected: -0.1284\n- Rewards/accuracies: 0.875\n- Rewards/margins: 0.0838\n- Logps/rejected: -656.5873\n- Logps/chosen: -92.6797\n- Logits/rejected: -1.2739\n- Logits/chosen: -1.2205\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-sft-wo-live_qa\", \"author\": \"Minbyul\", \"sha\": \"29d39aa237578d73879dca749eb5ea98fb613722\", \"last_modified\": \"2024-04-29 06:25:57+00:00\", \"created_at\": \"2024-04-29 05:58:59+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-live_qa-sft\", \"base_model:finetune:Minbyul/meditron-7b-wo-live_qa-sft\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-live_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-live_qa\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-sft-wo-live_qa\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-29 06:25:57+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-live_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-live_qa\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"662f37232e1ed663bc59c31c\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-sft-wo-live_qa\", \"usedStorage\": 13477649563}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-sft-wo-live_qa&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-sft-wo-live_qa%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-sft-wo-live_qa)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-medication_qa-sft",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-medication_qa-sft\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-medication_qa-sft\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3274\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.1713        | 0.92  | 6    | 1.3683          |\n| 1.0185        | 2.0   | 13   | 1.3435          |\n| 0.9011        | 2.77  | 18   | 1.3274          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-medication_qa-sft\", \"author\": \"Minbyul\", \"sha\": \"1a6c76ec3c1f0ffdc6c878589f91cc3969677395\", \"last_modified\": \"2024-04-14 13:21:35+00:00\", \"created_at\": \"2024-04-14 13:08:36+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-medication_qa-sft\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-medication_qa-sft\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-14 13:21:35+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-medication_qa-sft\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661bd5542b14565c7a2c8f99\", \"modelId\": \"Minbyul/meditron-7b-wo-medication_qa-sft\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-sft-wo-medication_qa"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-medication_qa-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-medication_qa-sft%5D(%2FMinbyul%2Fmeditron-7b-wo-medication_qa-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-sft-wo-medication_qa",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-medication_qa-sft\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- alignment-handbook\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-sft-wo-medication_qa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-sft-wo-medication_qa\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-medication_qa-sft](https://huggingface.co/Minbyul/meditron-7b-wo-medication_qa-sft) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6347\n- Rewards/chosen: -0.0755\n- Rewards/rejected: -0.3042\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 0.2287\n- Logps/rejected: -613.1549\n- Logps/chosen: -395.2336\n- Logits/rejected: -1.0640\n- Logits/chosen: -1.1745\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-sft-wo-medication_qa\", \"author\": \"Minbyul\", \"sha\": \"6bdc98507f0220464a65f0ed1a75f17d27a29f18\", \"last_modified\": \"2024-04-29 16:59:52+00:00\", \"created_at\": \"2024-04-29 13:07:13+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-medication_qa-sft\", \"base_model:finetune:Minbyul/meditron-7b-wo-medication_qa-sft\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-medication_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-medication_qa\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-sft-wo-medication_qa\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-29 16:59:52+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-medication_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-medication_qa\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"662f9b811802d87f64d39f77\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-sft-wo-medication_qa\", \"usedStorage\": 26954799403}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-sft-wo-medication_qa&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-sft-wo-medication_qa%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-sft-wo-medication_qa)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-healthsearch_qa-sft",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-healthsearch_qa-sft\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-healthsearch_qa-sft\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.0945\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.2848        | 0.89  | 2    | 1.1552          |\n| 1.2848        | 1.78  | 4    | 1.1191          |\n| 1.2041        | 2.67  | 6    | 1.0945          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-healthsearch_qa-sft\", \"author\": \"Minbyul\", \"sha\": \"c222e6da624e3932f4c1b0b57201643c53e38820\", \"last_modified\": \"2024-04-14 15:16:10+00:00\", \"created_at\": \"2024-04-14 15:03:57+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-healthsearch_qa-sft\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-healthsearch_qa-sft\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-14 15:16:10+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-healthsearch_qa-sft\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661bf05de0deea00d6046909\", \"modelId\": \"Minbyul/meditron-7b-wo-healthsearch_qa-sft\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-sft-wo-healthsearch_qa"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-healthsearch_qa-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-healthsearch_qa-sft%5D(%2FMinbyul%2Fmeditron-7b-wo-healthsearch_qa-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-sft-wo-healthsearch_qa",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-healthsearch_qa-sft\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-sft-wo-healthsearch_qa\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-sft-wo-healthsearch_qa\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-healthsearch_qa-sft](https://huggingface.co/Minbyul/meditron-7b-wo-healthsearch_qa-sft) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6911\n- Rewards/chosen: 0.0010\n- Rewards/rejected: -0.0039\n- Rewards/accuracies: 0.6566\n- Rewards/margins: 0.0049\n- Logps/rejected: -893.7659\n- Logps/chosen: -564.5244\n- Logits/rejected: -0.7763\n- Logits/chosen: -0.8804\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-sft-wo-healthsearch_qa\", \"author\": \"Minbyul\", \"sha\": \"6afa495aaa6d1371e13755f05725940cf7e5d82d\", \"last_modified\": \"2024-04-30 04:00:51+00:00\", \"created_at\": \"2024-04-30 03:44:10+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-healthsearch_qa-sft\", \"base_model:finetune:Minbyul/meditron-7b-wo-healthsearch_qa-sft\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-healthsearch_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-healthsearch_qa\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-sft-wo-healthsearch_qa\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-30 04:00:51+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-healthsearch_qa-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-healthsearch_qa\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6630690acb98a88f503bab1c\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-sft-wo-healthsearch_qa\", \"usedStorage\": 13477649563}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-sft-wo-healthsearch_qa&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-sft-wo-healthsearch_qa%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-sft-wo-healthsearch_qa)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-kqa_golden-sft",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-kqa_golden-sft\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-kqa_golden-sft\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8405\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.1778        | 0.89  | 6    | 1.0390          |\n| 1.0295        | 1.93  | 13   | 0.8659          |\n| 0.903         | 2.67  | 18   | 0.8405          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-kqa_golden-sft\", \"author\": \"Minbyul\", \"sha\": \"074fb85a09cc37cec8ee8747951b61f42e401856\", \"last_modified\": \"2024-04-16 05:43:12+00:00\", \"created_at\": \"2024-04-16 05:27:40+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-sft\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-kqa_golden-sft\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-16 05:43:12+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-sft\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661e0c4cdb72172b59b9f622\", \"modelId\": \"Minbyul/meditron-7b-wo-kqa_golden-sft\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-sft-wo-kqa_golden"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-kqa_golden-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-kqa_golden-sft%5D(%2FMinbyul%2Fmeditron-7b-wo-kqa_golden-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-sft-wo-kqa_golden",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-kqa_golden-sft\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-sft-wo-kqa_golden\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-sft-wo-kqa_golden\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-kqa_golden-sft](https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-sft) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6415\n- Rewards/chosen: -0.0188\n- Rewards/rejected: -0.1421\n- Rewards/accuracies: 0.7750\n- Rewards/margins: 0.1233\n- Logps/rejected: -693.1373\n- Logps/chosen: -157.5604\n- Logits/rejected: -1.0479\n- Logits/chosen: -1.3209\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-sft-wo-kqa_golden\", \"author\": \"Minbyul\", \"sha\": \"4572e5ff4282c4fa3cb175283afba1db340f883c\", \"last_modified\": \"2024-04-30 08:35:30+00:00\", \"created_at\": \"2024-04-30 08:14:16+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-kqa_golden-sft\", \"base_model:finetune:Minbyul/meditron-7b-wo-kqa_golden-sft\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-kqa_golden\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-sft-wo-kqa_golden\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-30 08:35:30+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-kqa_golden\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6630a8583960e9b654ebdaad\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-sft-wo-kqa_golden\", \"usedStorage\": 13477649563}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-sft-wo-kqa_golden-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-sft-wo-kqa_golden&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-sft-wo-kqa_golden%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-sft-wo-kqa_golden)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-kqa_silver_wogold-sft",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-kqa_silver_wogold-sft\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-kqa_silver_wogold-sft\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.8975\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 1.1532        | 0.87  | 5    | 1.0827          |\n| 0.9871        | 1.91  | 11   | 0.9194          |\n| 0.8631        | 2.61  | 15   | 0.8975          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\", \"author\": \"Minbyul\", \"sha\": \"e2b5caa0badf450a1bb6e4124df52723ae88fc4a\", \"last_modified\": \"2024-04-16 07:06:42+00:00\", \"created_at\": \"2024-04-16 06:51:29+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_silver_wogold-sft\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-kqa_silver_wogold-sft\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-16 07:06:42+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_silver_wogold-sft\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"661e1ff185f70e208da282f3\", \"modelId\": \"Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-kqa_silver_wogold-sft&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-kqa_silver_wogold-sft%5D(%2FMinbyul%2Fmeditron-7b-wo-kqa_silver_wogold-sft)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-kqa_silver_wogold-sft](https://huggingface.co/Minbyul/meditron-7b-wo-kqa_silver_wogold-sft) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6249\n- Rewards/chosen: -0.0145\n- Rewards/rejected: -0.1983\n- Rewards/accuracies: 0.875\n- Rewards/margins: 0.1839\n- Logps/rejected: -653.1394\n- Logps/chosen: -150.2214\n- Logits/rejected: -1.0337\n- Logits/chosen: -1.4037\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\", \"author\": \"Minbyul\", \"sha\": \"f7a0b2aa94a43fa03c2bd8941947ce6ab1ccf26b\", \"last_modified\": \"2024-04-30 15:25:01+00:00\", \"created_at\": \"2024-04-30 15:04:44+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\", \"base_model:finetune:Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-04-30 15:25:01+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-kqa_silver_wogold-sft\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6631088ce0a3f3639e25cd82\", \"modelId\": \"Minbyul/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold\", \"usedStorage\": 13477649563}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-dpo-full-sft-wo-kqa_silver_wogold&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-dpo-full-sft-wo-kqa_silver_wogold%5D(%2FMinbyul%2Fmeditron-7b-dpo-full-sft-wo-kqa_silver_wogold)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-kqa_golden-iter-sft-step1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-kqa_golden-iter-sft-step1\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.3041\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 2.2849        | 0.99  | 19   | 1.2240          |\n| 1.9826        | 1.97  | 38   | 1.2528          |\n| 1.7301        | 2.96  | 57   | 1.3041          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"author\": \"Minbyul\", \"sha\": \"abf2fdb57233d56d23f2ffad05071d27471effc0\", \"last_modified\": \"2024-05-07 14:33:10+00:00\", \"created_at\": \"2024-05-07 14:03:02+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-sft-step1\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-kqa_golden-iter-sft-step1\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-07 14:33:10+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-sft-step1\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"663a3496d67a4c20c6b70f5c\", \"modelId\": \"Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [
                "https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-iter-dpo-step2",
                "https://huggingface.co/dmis-lab/meditron-7b-olaph"
            ],
            "children_count": 2,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-kqa_golden-iter-sft-step1%5D(%2FMinbyul%2Fmeditron-7b-wo-kqa_golden-iter-sft-step1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-kqa_golden-iter-dpo-step2",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-kqa_golden-iter-dpo-step2\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1](https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6808\n- Rewards/chosen: 0.0082\n- Rewards/rejected: 0.0077\n- Rewards/accuracies: 0.5625\n- Rewards/margins: 0.0005\n- Logps/rejected: -631.7355\n- Logps/chosen: -407.7206\n- Logits/rejected: -1.1718\n- Logits/chosen: -1.2239\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-kqa_golden-iter-dpo-step2\", \"author\": \"Minbyul\", \"sha\": \"62a7f188698f6baff42317dd1b0ab7fb65f77c12\", \"last_modified\": \"2024-05-12 09:35:56+00:00\", \"created_at\": \"2024-05-12 09:13:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"base_model:finetune:Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-kqa_golden-iter-dpo-step2\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-12 09:35:56+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6640884ebaff1c00b0195548\", \"modelId\": \"Minbyul/meditron-7b-wo-kqa_golden-iter-dpo-step2\", \"usedStorage\": 13477649499}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-kqa_golden-iter-dpo-step2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-kqa_golden-iter-dpo-step2%5D(%2FMinbyul%2Fmeditron-7b-wo-kqa_golden-iter-dpo-step2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "dmis-lab/meditron-7b-olaph",
            "card": "---\nlicense: llama2\nbase_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\ntags:\n- alignment-handbook\n- trl\n- dpo\n- generated_from_trainer\n- trl\n- dpo\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/ultrafeedback_binarized\nmodel-index:\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-kqa_golden-iter-dpo-step2\n\nThis model is a fine-tuned version of [Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1](https://huggingface.co/Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1) on the HuggingFaceH4/ultrafeedback_binarized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.6808\n- Rewards/chosen: 0.0082\n- Rewards/rejected: 0.0077\n- Rewards/accuracies: 0.5625\n- Rewards/margins: 0.0005\n- Logps/rejected: -631.7355\n- Logps/chosen: -407.7206\n- Logits/rejected: -1.1718\n- Logits/chosen: -1.2239\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-07\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 64\n- total_eval_batch_size: 32\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"dmis-lab/meditron-7b-olaph\", \"author\": \"dmis-lab\", \"sha\": \"4ec7d43bc953e8c617e3198094ea2312180e690d\", \"last_modified\": \"2024-05-22 07:32:30+00:00\", \"created_at\": \"2024-05-22 06:34:02+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 7, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"dpo\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/ultrafeedback_binarized\", \"base_model:Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"base_model:finetune:Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-kqa_golden-iter-dpo-step2\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-22 07:32:30+00:00\", \"cardData\": \"base_model: Minbyul/meditron-7b-wo-kqa_golden-iter-sft-step1\\ndatasets:\\n- HuggingFaceH4/ultrafeedback_binarized\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- dpo\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-kqa_golden-iter-dpo-step2\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"664d91da7c563d4d95231182\", \"modelId\": \"dmis-lab/meditron-7b-olaph\", \"usedStorage\": 13477649499}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=dmis-lab/meditron-7b-olaph&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdmis-lab%2Fmeditron-7b-olaph%5D(%2Fdmis-lab%2Fmeditron-7b-olaph)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Minbyul/meditron-7b-wo-live_qa-iter-sft-step1",
            "card": "---\nlicense: llama2\nbase_model: epfl-llm/meditron-7b\ntags:\n- alignment-handbook\n- trl\n- sft\n- generated_from_trainer\n- trl\n- sft\n- generated_from_trainer\ndatasets:\n- HuggingFaceH4/deita-10k-v0-sft\nmodel-index:\n- name: meditron-7b-wo-live_qa-iter-sft-step1\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# meditron-7b-wo-live_qa-iter-sft-step1\n\nThis model is a fine-tuned version of [epfl-llm/meditron-7b](https://huggingface.co/epfl-llm/meditron-7b) on the HuggingFaceH4/deita-10k-v0-sft dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 1.5597\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 4\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 4\n- gradient_accumulation_steps: 4\n- total_train_batch_size: 64\n- total_eval_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 2.4036        | 0.96  | 19   | 1.4487          |\n| 2.0418        | 1.97  | 39   | 1.4852          |\n| 1.8471        | 2.89  | 57   | 1.5597          |\n\n\n### Framework versions\n\n- Transformers 4.39.0.dev0\n- Pytorch 2.1.2\n- Datasets 2.14.6\n- Tokenizers 0.15.2\n",
            "metadata": "{\"id\": \"Minbyul/meditron-7b-wo-live_qa-iter-sft-step1\", \"author\": \"Minbyul\", \"sha\": \"61f65d148c683c78391048eca3a987c33a5c00e5\", \"last_modified\": \"2024-05-11 14:41:03+00:00\", \"created_at\": \"2024-05-11 14:14:43+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"alignment-handbook\", \"trl\", \"sft\", \"generated_from_trainer\", \"dataset:HuggingFaceH4/deita-10k-v0-sft\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-live_qa-iter-sft-step1\\n  results: []\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": [{\"name\": \"meditron-7b-wo-live_qa-iter-sft-step1\", \"results\": []}], \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='all_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='eval_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train_results.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-11 14:41:03+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- HuggingFaceH4/deita-10k-v0-sft\\nlicense: llama2\\ntags:\\n- alignment-handbook\\n- trl\\n- sft\\n- generated_from_trainer\\nmodel-index:\\n- name: meditron-7b-wo-live_qa-iter-sft-step1\\n  results: []\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"663f7d5317fb1fb4dbfced6a\", \"modelId\": \"Minbyul/meditron-7b-wo-live_qa-iter-sft-step1\", \"usedStorage\": 13477649499}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Minbyul/meditron-7b-wo-live_qa-iter-sft-step1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BMinbyul%2Fmeditron-7b-wo-live_qa-iter-sft-step1%5D(%2FMinbyul%2Fmeditron-7b-wo-live_qa-iter-sft-step1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "smagt/meditron-7b-instruct",
            "card": "---\nbase_model: epfl-llm/meditron-7b\ndatasets:\n- databricks/databricks-dolly-15k\ninference: false\nlanguage:\n- en\nlicense: llama2\nmodel_creator: Nicolai van der Smagt\nmodel_name: Meditron 7B Instruct\nmodel_type: llama\ntags:\n- Medicine\n---\nThis is epfl-llm/meditron-7b, instruction-tuned for 5 epochs on databricks/databricks-dolly-15k.\n",
            "metadata": "{\"id\": \"smagt/meditron-7b-instruct\", \"author\": \"smagt\", \"sha\": \"85faa865025f06e168bf7ef78441d8b8b2f081dd\", \"last_modified\": \"2024-05-19 19:14:12+00:00\", \"created_at\": \"2024-05-19 10:29:20+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 8, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"Medicine\", \"en\", \"dataset:databricks/databricks-dolly-15k\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- databricks/databricks-dolly-15k\\nlanguage:\\n- en\\nlicense: llama2\\nmodel_name: Meditron 7B Instruct\\ntags:\\n- Medicine\\ninference: false\\nmodel_creator: Nicolai van der Smagt\\nmodel_type: llama\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-19 19:14:12+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\ndatasets:\\n- databricks/databricks-dolly-15k\\nlanguage:\\n- en\\nlicense: llama2\\nmodel_name: Meditron 7B Instruct\\ntags:\\n- Medicine\\ninference: false\\nmodel_creator: Nicolai van der Smagt\\nmodel_type: llama\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6649d480006242829e26e89c\", \"modelId\": \"smagt/meditron-7b-instruct\", \"usedStorage\": 13477642923}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=smagt/meditron-7b-instruct&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsmagt%2Fmeditron-7b-instruct%5D(%2Fsmagt%2Fmeditron-7b-instruct)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/qt_finetuned_LoRA_meditron_01",
            "card": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\nbase_model: epfl-llm/meditron-7b\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_01\", \"author\": \"veronica-girolimetti\", \"sha\": \"266954365434588330eae00559dc5c184974c090\", \"last_modified\": \"2024-05-21 12:42:47+00:00\", \"created_at\": \"2024-05-21 12:38:24+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-05-21 12:42:47+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"664c95c057e83e254b56c1f0\", \"modelId\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_01\", \"usedStorage\": 5117612427}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/qt_finetuned_LoRA_meditron_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_01%5D(%2Fveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/meditron_01",
            "card": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n- sft\nbase_model: epfl-llm/meditron-7b\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/meditron_01\", \"author\": \"veronica-girolimetti\", \"sha\": \"d6505b7470b00e9a8b062afb2f7a5cc779038185\", \"last_modified\": \"2024-05-21 12:52:00+00:00\", \"created_at\": \"2024-05-21 12:46:12+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-05-21 12:52:00+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"664c97944be557d4bf06feaf\", \"modelId\": \"veronica-girolimetti/meditron_01\", \"usedStorage\": 13477643299}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/meditron_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fmeditron_01%5D(%2Fveronica-girolimetti%2Fmeditron_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "JosephNguyen/meditron-7b-finetuned",
            "card": "---\nbase_model: epfl-llm/meditron-7b\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n- sft\n---\n\n# Uploaded  model\n\n- **Developed by:** JosephNguyen\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"JosephNguyen/meditron-7b-finetuned\", \"author\": \"JosephNguyen\", \"sha\": \"3020e40a20642710704d24a3d26f5b31462b570a\", \"last_modified\": \"2024-10-14 11:46:32+00:00\", \"created_at\": \"2024-10-10 17:27:17+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_token.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2024-10-14 11:46:32+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67080e75ed7df9f6c5f76a84\", \"modelId\": \"JosephNguyen/meditron-7b-finetuned\", \"usedStorage\": 13797519462}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/meditron-7b-finetuned-GGUF",
                "https://huggingface.co/mradermacher/meditron-7b-finetuned-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=JosephNguyen/meditron-7b-finetuned&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BJosephNguyen%2Fmeditron-7b-finetuned%5D(%2FJosephNguyen%2Fmeditron-7b-finetuned)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/qt_finetuned_LoRA_meditron_02",
            "card": "---\nbase_model: epfl-llm/meditron-7b\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_02\", \"author\": \"veronica-girolimetti\", \"sha\": \"5747294c0e6c929079a855bcc0aecb9e373cb67c\", \"last_modified\": \"2024-10-17 09:26:26+00:00\", \"created_at\": \"2024-10-17 09:24:59+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-17 09:26:26+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"6710d7ebbc56a513dfb039f6\", \"modelId\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_02\", \"usedStorage\": 5117612427}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/qt_finetuned_LoRA_meditron_02&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_02%5D(%2Fveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_02)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/qt_finetuned_LoRA_meditron_03",
            "card": "---\nbase_model: epfl-llm/meditron-7b\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_03\", \"author\": \"veronica-girolimetti\", \"sha\": \"00fa50ad177079f2ecdd14ab92eed601b3d3fb38\", \"last_modified\": \"2024-10-17 22:17:26+00:00\", \"created_at\": \"2024-10-17 21:21:08+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-17 22:17:26+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67117fc411bb296486f23042\", \"modelId\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_03\", \"usedStorage\": 10234725131}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/qt_finetuned_LoRA_meditron_03&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_03%5D(%2Fveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_03)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/qt_finetuned_LoRA_meditron_04",
            "card": "---\nbase_model: epfl-llm/meditron-7b\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_04\", \"author\": \"veronica-girolimetti\", \"sha\": \"7579cacb2549d261a8dce0732dafe7d79f4a06a2\", \"last_modified\": \"2024-10-18 14:55:44+00:00\", \"created_at\": \"2024-10-18 14:54:22+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-18 14:55:44+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"6712769e43457d044f5def4b\", \"modelId\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_04\", \"usedStorage\": 5117612427}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/qt_finetuned_LoRA_meditron_04&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_04%5D(%2Fveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_04)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "veronica-girolimetti/qt_finetuned_LoRA_meditron_01_1500",
            "card": "---\nbase_model: epfl-llm/meditron-7b\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\n\n- **Developed by:** veronica-girolimetti\n- **License:** apache-2.0\n- **Finetuned from model :** epfl-llm/meditron-7b\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_01_1500\", \"author\": \"veronica-girolimetti\", \"sha\": \"a2b2e6b19d17a6f434f2022df142eb1062ed145c\", \"last_modified\": \"2024-11-07 14:58:28+00:00\", \"created_at\": \"2024-11-07 14:57:03+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:epfl-llm/meditron-7b\", \"base_model:finetune:epfl-llm/meditron-7b\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-07 14:58:28+00:00\", \"cardData\": \"base_model: epfl-llm/meditron-7b\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"672cd53f7b58518d35ca3234\", \"modelId\": \"veronica-girolimetti/qt_finetuned_LoRA_meditron_01_1500\", \"usedStorage\": 5117612427}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=veronica-girolimetti/qt_finetuned_LoRA_meditron_01_1500&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_01_1500%5D(%2Fveronica-girolimetti%2Fqt_finetuned_LoRA_meditron_01_1500)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "https://huggingface.co/motherduckdb/DuckDB-NSQL-7B-v0.1",
            "card": "N/A",
            "metadata": "N/A",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [],
            "spaces_count": 0
        },
        {
            "model_id": "hon9kon9ize/Cantonese-Llama-2-7B-preview20240625",
            "card": "---\nlicense: cc-by-sa-4.0\nbase_model: meta-llama/Llama-2-7b\nlanguage:\n  - yue\npipeline_tag: text-generation\ntags:\n  - cantonese\n  - llama-2\n  - Powered by AWS Trainium\n---\n\n# Cantonese LLM using Llama-2 7B Architecture\n\nWelcome to the preview of the Cantonese Language Model (LLM) built on the Llama-2 7B architecture. This model is designed to understand and generate text in Cantonese, including slangs, colloquials, and Internet terms.\n\n## License\nThis project is available under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). For more details, please visit the [license page](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## Preview Warning\nPlease be advised that this version of the Cantonese LLM is a **preview**. As such, the model's outputs may sometimes be inaccurate, hallucinatory, or potentially offensive to some individuals. We are continuously working to improve the model's accuracy and reduce such instances.\n\n## Training Infrastructure\nThe Cantonese LLM has been trained using Amazon HyperPod and AWS Trainium chips.\n\n## Training Credits\nThis model was trained by [Votee AI Limited](https://huggingface.co/votee), and we contribute to [hon9kon9ize](https://hon9kon9ize.com/), the Hong Kong AI Research Community.\n\n## Usage Guidelines\n- Ensure that you are aware of the potential for unexpected or offensive content.\n- Always review and assess the model's output before using it in any application.\n- Provide feedback on any issues you encounter to help us improve the model.\n\n## Contributions\nWe welcome contributions from the community. If you have suggestions or improvements, please submit a pull request or open an issue in the project repository.\n\n## Disclaimer\nThe developers of the Cantonese LLM are not responsible for any harm or offense caused by the model's outputs. Users are advised to exercise discretion and judgment when using the model.\n\nThank you for exploring the Cantonese LLM. We are excited to see the innovative ways in which it will be used!\n",
            "metadata": "{\"id\": \"hon9kon9ize/Cantonese-Llama-2-7B-preview20240625\", \"author\": \"hon9kon9ize\", \"sha\": \"6f9d7e56e445735b161a69f4971dcbc698f010d5\", \"last_modified\": \"2024-06-28 04:00:13+00:00\", \"created_at\": \"2024-06-28 02:50:06+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 37, \"downloads_all_time\": null, \"likes\": 7, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"cantonese\", \"llama-2\", \"Powered by AWS Trainium\", \"yue\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:cc-by-sa-4.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738417664}, \"total\": 6738417664}, \"security_repo_status\": null, \"lastModified\": \"2024-06-28 04:00:13+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"667e24dea5d952954c1d5255\", \"modelId\": \"hon9kon9ize/Cantonese-Llama-2-7B-preview20240625\", \"usedStorage\": 13477372483}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=hon9kon9ize/Cantonese-Llama-2-7B-preview20240625&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bhon9kon9ize%2FCantonese-Llama-2-7B-preview20240625%5D(%2Fhon9kon9ize%2FCantonese-Llama-2-7B-preview20240625)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "ChengsenWang/ChatTime-1-7B-Base",
            "card": "---\nlicense: apache-2.0\ndatasets:\n- ChengsenWang/ChatTime-1-Pretrain-1M\nbase_model:\n- meta-llama/Llama-2-7b\ntags:\n- time-series\n- pretrained-model\n- foundation-model\n- multimodality\n- multimodal-time-series-foundation-model\npipeline_tag: time-series-forecasting\n---\n\n# ChatTime: A Multimodal Time Series Foundation Model\n\n## \u2728 Introduction\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance of ChatTime across multiple tasks and scenarios, and create four multimodal datasets to address data gaps. The experimental results demonstrate the potential and utility of ChatTime.\n\nAs depicted in Figure 1(b), during the continuous pre-training stage, we pre-train [LLaMA-2-7B-Base](https://huggingface.co/meta-llama/Llama-2-7b-hf) on [ChengsenWang/ChatTime-1-Pretrain-1M](https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Pretrain-1M), yielding [ChengsenWang/ChatTime-1-7B-Base](https://huggingface.co/ChengsenWang/ChatTime-1-7B-Base).\n\nFor details on ChatTime models, training data and procedures, and experimental results, please refer to the [arXiv](https://arxiv.org/abs/2412.11376).\n\n![](architecture.png)\n\n## \ud83d\udcc8 Usage\n\nWe present three minimal examples showing how to perform the multimodal time series analysis using the ChatTime model. The detailed code is available in the [Github](https://github.com/ForestsKing/ChatTime).\n\n### Zero-Shot Time Series Forecasting\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"Traffic\"\nhist_len = 120\npred_len = 24\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nhist_data = np.array(df[\"Hist\"].apply(eval).values.tolist())[:, -hist_len:][0]\npred_data = np.array(df[\"Pred\"].apply(eval).values.tolist())[:, :pred_len][0]\n\nmodel = ChatTime(hist_len=hist_len, pred_len=pred_len, model_path=model_path)\n\nout = model.predict(hist_data)\n\nhist_x = np.linspace(0, hist_len-1, hist_len)\npred_x = np.linspace(hist_len, hist_len+pred_len-1, pred_len)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(hist_x, hist_data, color='#000000')\nplt.plot(pred_x, pred_data, color='#000000', label='true')\nplt.plot(pred_x, out, color='#FF7F0E', label='pred')\nplt.axvline(hist_len, color='red')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n```\n\n### Context-Guided Time Series Forecasting\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"PTF\"\nhist_len = 120\npred_len = 24\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nhist_data = np.array(df[\"Hist\"].apply(eval).values.tolist())[:, -hist_len:][0]\npred_data = np.array(df[\"Pred\"].apply(eval).values.tolist())[:, :pred_len][0]\ncontext = df[\"Text\"].values[0]\n\nmodel = ChatTime(hist_len=hist_len, pred_len=pred_len, model_path=model_path)\n\nout_text = model.predict(hist_data, context)\nout = model.predict(hist_data)\n\nhist_x = np.linspace(0, hist_len-1, hist_len)\npred_x = np.linspace(hist_len, hist_len+pred_len-1, pred_len)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(hist_x, hist_data, color='#000000')\nplt.plot(pred_x, pred_data, color='#000000', label='true')\nplt.plot(pred_x, out_text, color='#FF7F0E', label='pred_text')\nplt.plot(pred_x, out, color='#1F77B4', label='pred')\nplt.axvline(hist_len, color='red')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n```\n\n### Time Series Question Answering\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"TSQA\"\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nseries = np.array(df[\"Series\"].apply(eval).values.tolist())[0]\nquestion = df[\"Question\"].values[0]\nanswer = df[\"Answer\"].values[0]\n\nmodel = ChatTime(model_path=model_path)\n\nout = model.analyze(question, series)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(series, color='#000000')\nplt.show()\n\nprint(question)\nprint(f\"\\n{out} / {answer}\\n\")\n\n```\n\n## \ud83d\udcdd Citation\n\nIf you find this repo or our work useful for your research, please consider citing the paper:\n\n```tex\n@inproceedings{\n  author    = {Chengsen Wang and Qi Qi and Jingyu Wang and Haifeng Sun and Zirui Zhuang and Jinming Wu and Lei Zhang and Jianxin Liao},\n  title     = {ChatTime: A Unified Multimodal Time Series Foundation Model Bridging Numerical and Textual Data},\n  booktitle = {AAAI Conference on Artificial Intelligence},\n  year      = {2025},\n}\n```\n\n## \ud83d\udcea Contact\n\nIf you have any question, please contact [cswang@bupt.edu.cn]().",
            "metadata": "{\"id\": \"ChengsenWang/ChatTime-1-7B-Base\", \"author\": \"ChengsenWang\", \"sha\": \"2c4657812a4c92ba9bcae6a08938c23ae8e9352e\", \"last_modified\": \"2024-12-17 02:35:50+00:00\", \"created_at\": \"2024-07-08 10:59:59+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 717, \"downloads_all_time\": null, \"likes\": 4, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"time-series\", \"pretrained-model\", \"foundation-model\", \"multimodality\", \"multimodal-time-series-foundation-model\", \"time-series-forecasting\", \"dataset:ChengsenWang/ChatTime-1-Pretrain-1M\", \"arxiv:2412.11376\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"time-series-forecasting\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- ChengsenWang/ChatTime-1-Pretrain-1M\\nlicense: apache-2.0\\npipeline_tag: time-series-forecasting\\ntags:\\n- time-series\\n- pretrained-model\\n- foundation-model\\n- multimodality\\n- multimodal-time-series-foundation-model\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='architecture.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6820343808}, \"total\": 6820343808}, \"security_repo_status\": null, \"lastModified\": \"2024-12-17 02:35:50+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- ChengsenWang/ChatTime-1-Pretrain-1M\\nlicense: apache-2.0\\npipeline_tag: time-series-forecasting\\ntags:\\n- time-series\\n- pretrained-model\\n- foundation-model\\n- multimodality\\n- multimodal-time-series-foundation-model\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"668bc6afe4997d6504812102\", \"modelId\": \"ChengsenWang/ChatTime-1-7B-Base\", \"usedStorage\": 13641221171}",
            "depth": 1,
            "children": [
                "https://huggingface.co/ChengsenWang/ChatTime-1-7B-Chat"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/ChatTime-1-7B-Base-GGUF",
                "https://huggingface.co/mradermacher/ChatTime-1-7B-Base-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=ChengsenWang/ChatTime-1-7B-Base&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BChengsenWang%2FChatTime-1-7B-Base%5D(%2FChengsenWang%2FChatTime-1-7B-Base)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "ChengsenWang/ChatTime-1-7B-Chat",
            "card": "---\nlicense: apache-2.0\ndatasets:\n- ChengsenWang/ChatTime-1-Finetune-100K\nbase_model:\n- ChengsenWang/ChatTime-1-7B-Base\ntags:\n- time-series\n- pretrained-model\n- foundation-model\n- multimodality\n- multimodal-time-series-foundation-model\npipeline_tag: time-series-forecasting\n---\n\n# ChatTime: A Multimodal Time Series Foundation Model\n\n## \u2728 Introduction\n\nIn this paper, we innovatively model time series as a foreign language and construct ChatTime, a unified framework for time series and text processing. As an out-of-the-box multimodal time series foundation model, ChatTime provides zero-shot forecasting capability and supports bimodal input/output for both time series and text. We design a series of experiments to verify the superior performance of ChatTime across multiple tasks and scenarios, and create four multimodal datasets to address data gaps. The experimental results demonstrate the potential and utility of ChatTime.\n\nAs depicted in Figure 1(c), during the instruction fine-tuning stage, we fine-tune [ChengsenWang/ChatTime-1-7B-Base](https://huggingface.co/ChengsenWang/ChatTime-1-7B-Base) on [ChengsenWang/ChatTime-1-Finetune-100K](https://huggingface.co/datasets/ChengsenWang/ChatTime-1-Finetune-100K), yielding [ChengsenWang/ChatTime-1-7B-Chat](https://huggingface.co/ChengsenWang/ChatTime-1-7B-Chat).\n\nFor details on ChatTime models, training data and procedures, and experimental results, please refer to the [arXiv](https://arxiv.org/abs/2412.11376).\n\n![](architecture.png)\n\n## \ud83d\udcc8 Usage\n\nWe present three minimal examples showing how to perform the multimodal time series analysis using the ChatTime model. The detailed code is available in the [Github](https://github.com/ForestsKing/ChatTime).\n\n### Zero-Shot Time Series Forecasting\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"Traffic\"\nhist_len = 120\npred_len = 24\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nhist_data = np.array(df[\"Hist\"].apply(eval).values.tolist())[:, -hist_len:][0]\npred_data = np.array(df[\"Pred\"].apply(eval).values.tolist())[:, :pred_len][0]\n\nmodel = ChatTime(hist_len=hist_len, pred_len=pred_len, model_path=model_path)\n\nout = model.predict(hist_data)\n\nhist_x = np.linspace(0, hist_len-1, hist_len)\npred_x = np.linspace(hist_len, hist_len+pred_len-1, pred_len)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(hist_x, hist_data, color='#000000')\nplt.plot(pred_x, pred_data, color='#000000', label='true')\nplt.plot(pred_x, out, color='#FF7F0E', label='pred')\nplt.axvline(hist_len, color='red')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n```\n\n### Context-Guided Time Series Forecasting\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"PTF\"\nhist_len = 120\npred_len = 24\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nhist_data = np.array(df[\"Hist\"].apply(eval).values.tolist())[:, -hist_len:][0]\npred_data = np.array(df[\"Pred\"].apply(eval).values.tolist())[:, :pred_len][0]\ncontext = df[\"Text\"].values[0]\n\nmodel = ChatTime(hist_len=hist_len, pred_len=pred_len, model_path=model_path)\n\nout_text = model.predict(hist_data, context)\nout = model.predict(hist_data)\n\nhist_x = np.linspace(0, hist_len-1, hist_len)\npred_x = np.linspace(hist_len, hist_len+pred_len-1, pred_len)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(hist_x, hist_data, color='#000000')\nplt.plot(pred_x, pred_data, color='#000000', label='true')\nplt.plot(pred_x, out_text, color='#FF7F0E', label='pred_text')\nplt.plot(pred_x, out, color='#1F77B4', label='pred')\nplt.axvline(hist_len, color='red')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n```\n\n### Time Series Question Answering\n\n```python\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model.model import ChatTime\n\ndataset = \"TSQA\"\nmodel_path = \"ChengsenWang/ChatTime-1-7B-Chat\"\n\ndf = pd.read_csv(f\"./dataset/{dataset}.csv\")\nseries = np.array(df[\"Series\"].apply(eval).values.tolist())[0]\nquestion = df[\"Question\"].values[0]\nanswer = df[\"Answer\"].values[0]\n\nmodel = ChatTime(model_path=model_path)\n\nout = model.analyze(question, series)\n\nplt.figure(figsize=(8, 2), dpi=500)\nplt.plot(series, color='#000000')\nplt.show()\n\nprint(question)\nprint(f\"\\n{out} / {answer}\\n\")\n\n```\n\n## \ud83d\udcdd Citation\n\nIf you find this repo or our work useful for your research, please consider citing the paper:\n\n```tex\n@inproceedings{\n  author    = {Chengsen Wang and Qi Qi and Jingyu Wang and Haifeng Sun and Zirui Zhuang and Jinming Wu and Lei Zhang and Jianxin Liao},\n  title     = {ChatTime: A Unified Multimodal Time Series Foundation Model Bridging Numerical and Textual Data},\n  booktitle = {AAAI Conference on Artificial Intelligence},\n  year      = {2025},\n}\n```\n\n## \ud83d\udcea Contact\n\nIf you have any question, please contact [cswang@bupt.edu.cn]().",
            "metadata": "{\"id\": \"ChengsenWang/ChatTime-1-7B-Chat\", \"author\": \"ChengsenWang\", \"sha\": \"20baa88d6ab838b34abbd1d2b1f4cea57c8ff870\", \"last_modified\": \"2024-12-17 02:36:20+00:00\", \"created_at\": \"2024-07-08 11:00:25+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1455, \"downloads_all_time\": null, \"likes\": 2, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"time-series\", \"pretrained-model\", \"foundation-model\", \"multimodality\", \"multimodal-time-series-foundation-model\", \"time-series-forecasting\", \"dataset:ChengsenWang/ChatTime-1-Finetune-100K\", \"arxiv:2412.11376\", \"base_model:ChengsenWang/ChatTime-1-7B-Base\", \"base_model:finetune:ChengsenWang/ChatTime-1-7B-Base\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"time-series-forecasting\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- ChengsenWang/ChatTime-1-7B-Base\\ndatasets:\\n- ChengsenWang/ChatTime-1-Finetune-100K\\nlicense: apache-2.0\\npipeline_tag: time-series-forecasting\\ntags:\\n- time-series\\n- pretrained-model\\n- foundation-model\\n- multimodality\\n- multimodal-time-series-foundation-model\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='architecture.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6820343808}, \"total\": 6820343808}, \"security_repo_status\": null, \"lastModified\": \"2024-12-17 02:36:20+00:00\", \"cardData\": \"base_model:\\n- ChengsenWang/ChatTime-1-7B-Base\\ndatasets:\\n- ChengsenWang/ChatTime-1-Finetune-100K\\nlicense: apache-2.0\\npipeline_tag: time-series-forecasting\\ntags:\\n- time-series\\n- pretrained-model\\n- foundation-model\\n- multimodality\\n- multimodal-time-series-foundation-model\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"668bc6c93252e98452c2074d\", \"modelId\": \"ChengsenWang/ChatTime-1-7B-Chat\", \"usedStorage\": 13641221171}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/ChatTime-1-7B-Chat-GGUF",
                "https://huggingface.co/mradermacher/ChatTime-1-7B-Chat-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=ChengsenWang/ChatTime-1-7B-Chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BChengsenWang%2FChatTime-1-7B-Chat%5D(%2FChengsenWang%2FChatTime-1-7B-Chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "m3rg-iitd/llamat-2",
            "card": "---\nlicense: llama2\nlanguage:\n- en\nbase_model:\n- meta-llama/Llama-2-7b\ntags:\n- materials science\n- large language model\n---\n# Model Card for LLaMat-2\n\n**LLaMat-2** is a specialized large language model designed to be a foundational large language model for materials science. \n\n---\n\n## Overview\n\n- **Model Type:** Large Language Model (LLM)  \n- **Base Model:** LLaMat-2 (continued pretraining of LLaMA-3 on material science data)  \n- **Language:** English  \n- **License:** LLaMA-3 License  \n- **Tags:** Material Science, Domain Adaptation, Table Understanding, Scientific Data Parsing, Materials Copilot  \n\n---\n\n## Model Details\n\n### Key Features\n\n- **Applications:** Can be finetuned for information extraction, table understanding, parsing data for research tasks, and crystal structure generation.  \n\n### Development and Support\n- **Developed by:** [M3RG, IIT Delhi](https://github.com/M3RG-IITD/) & [DAIR, IIT Delhi](https://github.com/dair-iitd)\n- **Compute Support:**  \n  - **Edinburgh International Data Facility (EIDF):** Provided access to Cerebras CS2 clusters for pretraining.  \n  - **IIT Delhi High-Performance Computing Cluster:** Supported fine-tuning and inference stages.  \n\n---\n\n## Technical Specifications\n\n### Hardware Infrastructure\n- **Pretraining:** 8 NVIDIA A100 80GB GPUs  \n\n### Software Stack\n- **Frameworks:** PyTorch, Hugging Face Transformers  \n\n---\n\n## Model Sources\n- **Repository:** [LLaMat on GitHub](https://github.com/M3RG-IITD/llamat)  \n- **Compute Resources:** [EIDF Cerebras CS Clusters](https://edinburgh-international-data-facility.ed.ac.uk/services/computing/cerebras-cs)\n\n---",
            "metadata": "{\"id\": \"m3rg-iitd/llamat-2\", \"author\": \"m3rg-iitd\", \"sha\": \"5c5cc87ef0ed9edb07867727daa3a115472eb709\", \"last_modified\": \"2024-12-13 05:36:50+00:00\", \"created_at\": \"2024-12-03 19:15:39+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"materials science\", \"large language model\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- materials science\\n- large language model\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 6738415616}, \"total\": 6738415616}, \"security_repo_status\": null, \"lastModified\": \"2024-12-13 05:36:50+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- materials science\\n- large language model\", \"transformersInfo\": null, \"_id\": \"674f58dbe9984df524f57bfd\", \"modelId\": \"m3rg-iitd/llamat-2\", \"usedStorage\": 26954195795}",
            "depth": 1,
            "children": [
                "https://huggingface.co/m3rg-iitd/llamat-2-chat",
                "https://huggingface.co/m3rg-iitd/llamat-2-cif"
            ],
            "children_count": 2,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=m3rg-iitd/llamat-2&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bm3rg-iitd%2Fllamat-2%5D(%2Fm3rg-iitd%2Fllamat-2)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "m3rg-iitd/llamat-2-chat",
            "card": "---\nlicense: llama2\nlanguage:\n- en\nbase_model:\n- m3rg-iitd/llamat-2\ntags:\n- material science\n- large language model\n- domain adaptation\n- scientific domain adaptation\n- materials copilot\n- information extraction\n- table understanding\n- table data parsing\n---\n------------\n\n# Model Card for LLaMat-2-Chat\n\n## Overview\n\n**LLaMat-2-Chat** is a specialized large language model designed to serve as an AI copilot for materials research. Finetuned from **LLaMat-2**, this model is adapted for tasks such as information extraction from material science text and tabular data. It provides advanced capabilities in scientific data processing, assisting researchers in analyzing and interpreting material science literature, reports, and datasets.\n\nFor more details, refer to our paper: [Foundational Large Language Models for Materials Research](https://arxiv.org/abs/2412.09560).\n\n## Model Details\n\n- **Model Type:** Large Language Model (LLM)  \n- **Base Model:** LLaMat-2 (continued pretraining of LLaMA-2 on material science data)  \n- **Language:** English  \n- **License:** LLaMA-2 License  \n- **Tags:** Material Science, Domain Adaptation, Table Understanding, Scientific Data Parsing, Materials Copilot  \n- **Developed by:** [M3RG, IIT Delhi](https://github.com/M3RG-IITD/) & [DAIR, IIT Delhi](https://github.com/dair-iitd)  \n\n---\n## Key Features\n- **Instruction Following Abilities:** Optimized for understanding and processing instructions in the material science domain.  \n- **Domain-Specific Expertise:** Pretrained on material science tokens, enabling high performance in scientific applications.  \n- **Applications:** information extraction, table understanding, and parsing data for research tasks.  \n\n\n## Intended Use\n\nLLaMat-2-Chat is designed to assist researchers, scientists, and industry professionals in:\n- Extracting structured information from material science texts and tables.\n- Analyzing experimental results and processing large datasets.\n- Assisting in literature review and knowledge discovery.\n- Supporting research-driven natural language queries related to material science.\n\nThis model is intended for academic and industrial research purposes. \n\n---\n## Technical Specification \n\n### Hardware Infrastructure\n\n- **Pretraining:** 2 Cerebras CS-2 Wafer-Scale Engines (WSE-2)  \n- **Finetuning:** 8 NVIDIA A100 80GB GPUs  \n- **Inferencing:** 1 NVIDIA A100 80GB GPU  \n\n### Software Stack\n\n- **Frameworks:** PyTorch, Hugging Face Transformers, Meditron-LLM Library  \n\n---\n\n## Training Data\n\nLLaMat-2-Chat was trained on a curated corpus of material science literature, scientific papers, structured datasets, and technical reports. The training set includes:\n- material science research papers published in journals of Elsevier and Springer.\n- Material science community discourse\n- Redpajama dataset\n- Openorca instruction finetuning dataset\n- mathQA dataset\n- MatSciNLP benchmark dataset\n- task specific datasets (mentioned in Table A.2 in [Foundational Large Language Models for Materials Research](https://arxiv.org/abs/2412.09560).)\n\n---\n\n## Results\n\ndetailed results and comparison with existing models can be read from  [Foundational Large Language Models for Materials Research](https://arxiv.org/abs/2412.09560).\n\n---\n\n### Development and Support\n- **Developed by:** [M3RG, IIT Delhi](https://github.com/M3RG-IITD/) & [DAIR, IIT Delhi](https://github.com/dair-iitd)\n- **Compute Support:**  \n  - **IIT Delhi High-Performance Computing Cluster:** Supported fine-tuning and inference stages.\n  - **Edinburgh International Data Facility (EIDF):** [EIDF Cerebras CS Clusters](https://edinburgh-international-data-facility.ed.ac.uk/services/computing/cerebras-cs) provided access to Cerebras CS2 clusters for pretraining.\n    \n---\n\n## Repository with training and evaluation code\n\n- **Repository:** [LLaMat-2 on GitHub](https://github.com/M3RG-IITD/llamat)  \n\n---\n\n## Citation\n\nIf you use LLaMat-2-Chat in your research, please cite our work:\n\n```\n@article{LLaMat-2,\n  author    = {Vaibhav Mishra and Somaditya Singh and Dhruv Ahlawat and Mohd Zaki and Vaibhav Bihani and Hargun Singh Grover and Biswajit Mishra and Santiago Miret and Mausam and N. M. Anoop Krishnan},\n  title     = {Foundational Large Language Models for Materials Research},\n  journal   = {arXiv preprint arXiv:2412.09560},\n  year      = {2024},\n  url       = {https://arxiv.org/abs/2412.09560}\n}\n\n```\n",
            "metadata": "{\"id\": \"m3rg-iitd/llamat-2-chat\", \"author\": \"m3rg-iitd\", \"sha\": \"2b67f6910c90d34e04ef5cb39ae0e5d7ae2e1259\", \"last_modified\": \"2025-03-29 11:38:57+00:00\", \"created_at\": \"2024-12-03 20:10:04+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 29, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"material science\", \"large language model\", \"domain adaptation\", \"scientific domain adaptation\", \"materials copilot\", \"information extraction\", \"table understanding\", \"table data parsing\", \"en\", \"arxiv:2412.09560\", \"base_model:m3rg-iitd/llamat-2\", \"base_model:finetune:m3rg-iitd/llamat-2\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- m3rg-iitd/llamat-2\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- material science\\n- large language model\\n- domain adaptation\\n- scientific domain adaptation\\n- materials copilot\\n- information extraction\\n- table understanding\\n- table data parsing\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='latest_checkpointed_iteration.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00005.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 6739464192}, \"total\": 6739464192}, \"security_repo_status\": null, \"lastModified\": \"2025-03-29 11:38:57+00:00\", \"cardData\": \"base_model:\\n- m3rg-iitd/llamat-2\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- material science\\n- large language model\\n- domain adaptation\\n- scientific domain adaptation\\n- materials copilot\\n- information extraction\\n- table understanding\\n- table data parsing\", \"transformersInfo\": null, \"_id\": \"674f659c3022e37947fe1488\", \"modelId\": \"m3rg-iitd/llamat-2-chat\", \"usedStorage\": 53916280475}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=m3rg-iitd/llamat-2-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bm3rg-iitd%2Fllamat-2-chat%5D(%2Fm3rg-iitd%2Fllamat-2-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "m3rg-iitd/llamat-2-cif",
            "card": "---\nlicense: llama2\nlanguage:\n- en\nbase_model:\n- m3rg-iitd/llamat-2\ntags:\n- crystal structure generation\n- CIF understanding\n---\n# Model Card for LLaMat-2-CIF\n\n**LLaMat-2-CIF** is a specialized large language model designed to generate and extract information from Crystallographic Information Files. \n\nThe model is developed after continued pretraining of **LLaMat-2** on 7M instruction-output pairs obtained using CIFs from Materials Project, Google GNoME, and AMCSD \n\n---\n\n## Overview\n\n- **Model Type:** Large Language Model (LLM)  \n- **Base Model:** LLaMat-2 (continued pretraining of LLaMat-2 on CIFs)  \n- **Language:** English  \n- **License:** LLaMA-2 License  \n- **Tags:** Material Science, Domain Adaptation, Crystal Structure Generation  \n\n---\n\n## Model Details\n\n### Key Features\n- **Instruction Following Abilities:** Answers questions based on CIF files.  \n- **Applications:** Crystal structure generation  \n\n### Development and Support\n- **Developed by:** [M3RG, IIT Delhi](https://github.com/M3RG-IITD/) & [DAIR, IIT Delhi](https://github.com/dair-iitd)\n- **Compute Support:**  \n  - **Edinburgh International Data Facility (EIDF):** Provided access to Cerebras CS2 clusters for pretraining.  \n  - **IIT Delhi High-Performance Computing Cluster:** Supported fine-tuning and inference stages.  \n\n---\n\n## Technical Specifications\n\n### Hardware Infrastructure\n- **Pretraining:** 2 Cerebras CS-2 Wafer-Scale Engines (WSE-2)  \n- **Finetuning:** 2 Cerebras CS-2 Wafer-Scale Engines (WSE-2)  \n- **Inferencing:** 1 NVIDIA A100 80GB GPU \n\n### Software Stack\n- **Frameworks:** PyTorch, Hugging Face Transformers  \n\n---\n\n## Model Sources\n- **Repository:** [LLaMat on GitHub](https://github.com/M3RG-IITD/llamat)  \n- **Compute Resources:** [EIDF Cerebras CS Clusters](https://edinburgh-international-data-facility.ed.ac.uk/services/computing/cerebras-cs)",
            "metadata": "{\"id\": \"m3rg-iitd/llamat-2-cif\", \"author\": \"m3rg-iitd\", \"sha\": \"ebb33c109d6a2ec3259084ddb4769e39a544c8e0\", \"last_modified\": \"2024-12-06 21:55:16+00:00\", \"created_at\": \"2024-12-04 20:05:26+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 5, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"crystal structure generation\", \"CIF understanding\", \"en\", \"base_model:m3rg-iitd/llamat-2\", \"base_model:finetune:m3rg-iitd/llamat-2\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- m3rg-iitd/llamat-2\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- crystal structure generation\\n- CIF understanding\", \"widget_data\": null, \"model_index\": null, \"config\": {\"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00004.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00004.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00004.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00004-of-00004.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-06 21:55:16+00:00\", \"cardData\": \"base_model:\\n- m3rg-iitd/llamat-2\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- crystal structure generation\\n- CIF understanding\", \"transformersInfo\": null, \"_id\": \"6750b606b46eeec7d71ec105\", \"modelId\": \"m3rg-iitd/llamat-2-cif\", \"usedStorage\": 26954207563}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=m3rg-iitd/llamat-2-cif&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bm3rg-iitd%2Fllamat-2-cif%5D(%2Fm3rg-iitd%2Fllamat-2-cif)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "nvidia/Llama-2-7B-DMC-8x",
            "card": "---\nlicense: other\nlicense_name: nvidia-open-model-license\nlicense_link: >-\n  https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\nbase_model:\n- meta-llama/Llama-2-7b\ntags:\n- nvidia\n- llama 2\n- pytorch\n- kvcache\nlibrary_name: megatron-lm\n---\n\n# Llama-2-7B-DMC-8x\n\n## Description\n\nLlama-2-7B-DMC-8x is a version of [Llama 2 7B](https://www.llama.com/llama2/), which has been trained to apply the Dynamic Memory Compression (DMC) algorithm ([https://arxiv.org/abs/2403.09636](https://arxiv.org/abs/2403.09636)). With DMC, the model performs on-line key\u2013value cache compression at inference time, achieving substantially better throughput and/or latency. Most importantly, it learns to apply different compression ratios in different heads and layers. The source code for training and inference is provided in the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/dmc) repository.\n\nThis model is for research and development only.\n\n### License\n\nGOVERNING TERMS: This model is governed by the NVIDIA Open Model License Agreement (found at https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf). <br>\nAdditional Information: LLAMA 2 COMMUNITY LICENSE AGREEMENT (found at https://huggingface.co/meta-llama/Llama-2-7b/blob/main/LICENSE.txt).\n\n## Reference\nDynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n## Model Architecture\n\nLlama-2-7B-DMC-8x uses a model embedding size of 4096, 32 attention heads, MLP intermediate dimension of 11008, with 32 layers in total. Additionally, it uses Rotary Position Embeddings (RoPE).\n\n**Architecture Type:** Transformer Decoder (Auto-regressive Language Model)\n\n**Network Architecture:** Llama 2 7B\n\n## Input\n**Input Type:** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D), Temperature\n**Other Properties Related to Input: Max Input Tokens: 4096 <br>\n\n## Output\n**Output Type :** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output: Max Output Tokens: 4096 <br>\n\n## Software Integration\n**Runtime Engine(s):**\n* Not Applicable (N/A)\n\nThe model weights are distributed in bfloat16 format. However, it could be converted to other formats in order to run on other hardware microarchitectures.\n\n**Supported Hardware Microarchitecture Compatibility:** Nvidia Ampere and newer GPUs.<br>\n\n**Supported Operating System(s):** <br>\n* Linux <br>\n\n## Model Version(s)\nLlama 2 7B DMC 8x v1.0\n\n# Training and Evaluation Datasets\n\n## Training Dataset\n\nThe model was trained for 42,000 steps with a batch size of 1024, a sequence length of 4096, and a learning rate of 3e-5 with an increasing compression objective. Afterwards, it underwent additional training for 2000 steps with a fixed compression rate of 8x and a smaller learning rate of 3e-6.\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets. This particular model was trained on a dataset containing a mixture of texts in English and 37 programming languages.\n\n## Evaluation\n\n| Category    | Benchmark                                   | # Shots | Llama 2 7B | Llama 2 7B DMC 8x |\n|:------------|:--------------------------------------------|--------:|-----------:|------------------:|\n| General     | [MMLU](https://openreview.net/forum?id=d7KBjmI3GmQ)                 |  5 | 46.7 | 41.8 |\n| Math        | GMS8K                                                               |  5 | 11.9 |  9.7 |\n| Commonsense | [HellaSwag](https://aclanthology.org/P19-1472)                      | 10 | 78.8 | 78.2 |\n| Commonsense | [Arc-Easy](https://arxiv.org/abs/1803.05457)                        |  0 | 73.1 | 72.6 |\n| Commonsense | [Arc-Challenge](https://arxiv.org/abs/1803.05457)                   | 25 | 53.1 | 51.5 |\n| Commonsense | [PIQA](https://ojs.aaai.org/index.php/AAAI/article/view/6239)       |  0 | 78.2 | 79.8 |\n| Commonsense | [WinoGrande](https://ojs.aaai.org/index.php/AAAI/article/view/6399) |  5 | 74.0 | 72.5 |\n\n## AI Safety Efforts\n\nThe Llama-2-7B-DMC-8x model underwent AI safety evaluation including adversarial testing via three distinct methods:\n* [Garak](https://github.com/leondz/garak), is an automated LLM vulnerability scanner that probes for common weaknesses, including prompt injection and data leakage.\n* [AEGIS](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0), is a content safety evaluation dataset and LLM based content safety classifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM interactions.\n* Human Content Red Teaming leveraging human interaction and evaluation of the models' responses.\n\n## Inference\n\n**Engine:** Megatron-LM <br>\n**Test Hardware** H100-80GB <br>\n\nWe recommend running the provided code inside a [PyTorch NGC Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch).\n\n1. First, download a [PyTorch NGC Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) using Docker.\nThe code below has been tested with the `24.04-py3` version of the container.\n\n2. After setting up the container, clone the repository and install the dependencies:\n   ```\n   git clone -b dmc https://github.com/NVIDIA/Megatron-LM\n   cd Megatron-LM\n   pip install -r requirements.txt\n   ```\n3. Download the [Llama 2 tokenizer](https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model) and save it under a desired location `<TOKENIZER_MODEL>`.\n\n4. Download a selected checkpoint and save it under a desired location `<DMC_MODEL>`.\n\n5. We provide code to run and benchmark a simple, auto-regressive inference. Save a single prompt in a textfile and run:\n   ```bash\n   ./examples/dmc/inference.sh 7B <DMC_MODEL> <TOKENIZER_MODEL> <PROMPT_TXT_FILE>\n   ```\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Limitations\n\nThe model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. This issue could be exacerbated without the use of the recommended prompt template. If you are going to use this model in an agentic workflow, validate that the imported packages are from a trusted source to ensure end-to-end security.\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@InProceedings{pmlr-v235-nawrot24a,\n title =        {Dynamic Memory Compression: Retrofitting {LLM}s for Accelerated Inference},\n author =       {Nawrot, Piotr and {\\L}a\\'{n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo},\n booktitle =    {Proceedings of the 41st International Conference on Machine Learning},\n pages =        {37396--37412},\n year =         {2024},\n editor =       {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},\n volume =       {235},\n series =       {Proceedings of Machine Learning Research},\n month =        {21--27 Jul},\n publisher =    {PMLR},\n pdf =          {https://raw.githubusercontent.com/mlresearch/v235/main/assets/nawrot24a/nawrot24a.pdf},\n url =          {https://proceedings.mlr.press/v235/nawrot24a.html},\n abstract =     {Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key\u2013value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key\u2013value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to $\\sim 3.7 \\times$ throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4$\\times$ cache compression, outperforming up-trained grouped-query attention (GQA) and key\u2013value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget. We release the DMC code and models at https://github.com/NVIDIA/Megatron-LM/tree/DMC.}\n}\n```",
            "metadata": "{\"id\": \"nvidia/Llama-2-7B-DMC-8x\", \"author\": \"nvidia\", \"sha\": \"b6154fe3e5b780cd5fe433a8182fe20aa83aa04c\", \"last_modified\": \"2024-12-22 13:50:47+00:00\", \"created_at\": \"2024-12-20 14:28:41+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"megatron-lm\", \"gguf\": null, \"inference\": null, \"tags\": [\"megatron-lm\", \"nvidia\", \"llama 2\", \"pytorch\", \"kvcache\", \"arxiv:2403.09636\", \"arxiv:1803.05457\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:other\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: megatron-lm\\nlicense: other\\nlicense_name: nvidia-open-model-license\\nlicense_link: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\\ntags:\\n- nvidia\\n- llama 2\\n- pytorch\\n- kvcache\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='NOTICE', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='latest_checkpointed_iteration.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mp_rank_00/model_optim_rng.pt', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-22 13:50:47+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: megatron-lm\\nlicense: other\\nlicense_name: nvidia-open-model-license\\nlicense_link: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\\ntags:\\n- nvidia\\n- llama 2\\n- pytorch\\n- kvcache\", \"transformersInfo\": null, \"_id\": \"67657f192200e2d67a626f40\", \"modelId\": \"nvidia/Llama-2-7B-DMC-8x\", \"usedStorage\": 13477077952}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=nvidia/Llama-2-7B-DMC-8x&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bnvidia%2FLlama-2-7B-DMC-8x%5D(%2Fnvidia%2FLlama-2-7B-DMC-8x)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "nivashb/aiadvisorbynivash",
            "card": "---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- meta-llama/Llama-2-7b\npipeline_tag: text-generation\ntags:\n- medical\nlibrary_name: transformers\n---",
            "metadata": "{\"id\": \"nivashb/aiadvisorbynivash\", \"author\": \"nivashb\", \"sha\": \"92c82a7d5ed6c35ff0079460c93208b91436a34b\", \"last_modified\": \"2025-01-18 17:20:31+00:00\", \"created_at\": \"2025-01-18 10:55:52+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"llama\", \"text-generation\", \"medical\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- medical\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-01-18 17:20:31+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- medical\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"678b88b8254faea320c082d1\", \"modelId\": \"nivashb/aiadvisorbynivash\", \"usedStorage\": 26954331470}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=nivashb/aiadvisorbynivash&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bnivashb%2Faiadvisorbynivash%5D(%2Fnivashb%2Faiadvisorbynivash)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Sci-fi-vy/Meditron-7b-finetuned",
            "card": "---\nlicense: llama2\nlanguage:\n- en\nmetrics:\n- accuracy\n- perplexity\ndatasets:\n- epfl-llm/guidelines\nbase_model: meta-llama/Llama-2-7b\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n\n# Model Card for Meditron-7B-finetuned\nMeditron is a suite of open-source medical Large Language Models (LLMs).\nMeditron-7B is a 7 billion parameters model adapted to the medical domain from Llama-2-7B through continued pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, a [new dataset](https://huggingface.co/datasets/epfl-llm/guidelines) of internationally-recognized medical guidelines, and general domain data from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T).\nMeditron-7B-finetuned is finetuned on relevant training data, which outperforms Llama-2-7B and PMC-Llama on multiple medical reasoning tasks.\n\n<details open>\n  <summary><strong>Advisory Notice</strong></summary>\n\n  <blockquote style=\"padding: 10px; margin: 0 0 10px; border-left: 5px solid #ddd;\">\n    While Meditron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints. \n  We recommend against deploying Meditron in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings.\n  </blockquote>\n</details>\n\n## Model Details\n\n- **Finetuned by:** [Vignesh](https://huggingface.co/Sci-fi-vy)\n- **Developed by:** [EPFL LLM Team](https://huggingface.co/epfl-llm)\n- **Model type:** Causal decoder-only transformer language model\n- **Language(s):** English (mainly)\n- **Model License:** [LLAMA 2 COMMUNITY LICENSE AGREEMENT](https://huggingface.co/meta-llama/Llama-2-70b/raw/main/LICENSE.txt)\n- **Code License:** [APACHE 2.0 LICENSE](LICENSE)\n- **Continue-pretrained from model:** [Llama-2-7B](https://huggingface.co/meta-llama/Llama-2-7b)\n- **Context length:**  2K tokens\n- **Input:**  Text-only data\n- **Output:**  Model generates text only\n- **Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we enhance model's performance.\n- **Knowledge Cutoff:** August 2023\n\n\n### Model Sources\n\n- **Repository:** [epflLLM/meditron](https://github.com/epfLLM/meditron)\n- **Trainer:** [epflLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM)\n- **Reference Paper:** *[MediTron-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079)*\n\n## Uses\n\nMeditron-7B-finetuned is being made available for further testing and assessment as an AI assistant to enhance clinical decision-making and enhance access to an LLM for healthcare use. Potential use cases may include but are not limited to:\n-  Medical exam question answering\n-  Supporting differential diagnosis\n-  Disease information (symptoms, cause, treatment) query\n-  General health information query\n-  Personalized results\n\n### Direct Use\n\nIt is possible to use this model to generate text, which is useful for experimentation and understanding its capabilities. \nIt should not be used directly for production or work that may impact people.\n\n### Downstream Use\nMeditron-70B and Meditron-7B are both foundation models without finetuning or instruction-tuning. They can be finetuned, instruction-tuned, or RLHF-tuned for specific downstream tasks and applications.\nThere are two ways we have used this model for downstream question-answering tasks.\n1. We apply in-context learning with k demonstrations (3 or 5 in our paper) added to the prompt.\n2. We finetuned the models for downstream question-answering tasks using specific training sets.\n\nWe encourage and look forward to the adaption of the base model for more diverse applications.\n\nIf you want a more interactive way to prompt the model, we recommend using a high-throughput and memory-efficient inference engine with a UI that supports chat and text generation.\n\nYou can check out our deployment [guide](https://github.com/epfLLM/meditron/blob/main/deployment/README.md), where we used [FastChat](https://github.com/lm-sys/FastChat) with [vLLM](https://github.com/vllm-project/vllm). We collected generations for our qualitative analysis through an interactive UI platform, [BetterChatGPT](https://github.com/ztjhz/BetterChatGPT). Here is the prompt format we used as an example:\n\n<img width=70% src=\"prompt_example.png\" alt=\"qualitative-analysis-prompt\" title=\"Qualitative Analysis Prompt\">\n\n### Out-of-Scope Use\n\nWe do not recommend using this model for natural language generation in a production environment, finetuned or otherwise.\n\n## Truthfulness, Helpfulness, Risk, and Bias\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nWe did an initial assessment of Meditron models' **Truthfulness** against baseline models and consumer-level medical models. \nWe use TruthfulQA (multiple choice) as the main evaluation benchmark.\nWe only focus on the categories that are relevant to the medical domain, including Health, Nutrition, Psychology, and Science.\nFor 7B models, we perform one-shot evaluations for consistent answer generation.\nFor 70B models, the evaluations are under the zero-shot setting.\nBelow, we report the detailed truthfulness performance of each category.\n\n|     |        |      |      |      |      |      |      |\n| --- | ------ |----- |----- |----- |----- |----- |----- |\n|Category  | meditron-70b | llama-2-70b | med42-70b* | meditron-7b | llama-2-7b | PMC-llama-7b |\n|Health    |      81.8    |    69.1     |    83.6    | 27.3        |   16.4     |      3.6     |\n|Nutrition |      77.9    |    68.8     |    62.5    | 31.1        |   12.5     |      6.3     |\n|Psychology|      47.4    |    36.8     |    52.6    | 21.1        |   10.5     |      0.0     |\n|Science   |      77.8    |    44.4     |    33.3    | 33.3        |   11.1     |      0.0     |\n|Avg       |      71.2    |    54.8     |    58.0    | 28.3        |   12.6     |      2.5     |\n|          |              |             |            |             |            |              |\n\nFor a more detailed performance analysis, please see our paper. \n\nSignificant research is still required to fully explore potential bias, fairness, and safety issues with this language model. \nPlease recognize that our evaluation on Meditron-7B's helpfulness, risk, and bias are highly limited. \nThus, as we noted in the safety notice, we strongly against any deployment in medical applications without further alignment process and rigorous evaluation!\n\n### Recommendations\n\n**IMPORTANT!**\nUsers (both direct and downstream) should be made aware of the risks, biases, and limitations of the model.\nWhile this model is capable of generating natural language text, we have only begun to explore this capability and its limitations. \nUnderstanding these limitations is especially important in a domain like medicine. \nTherefore, we strongly recommend against using this model in production for natural language generation or for professional purposes related to health and medicine.\n\n## Training Details\n\n### Training Data\nMeditron\u00e2\u0080\u0099s domain-adaptive pre-training corpus GAP-Replay  combines 48.1B tokens from four corpora:\n- [**Clinical  Guidelines**](https://huggingface.co/datasets/epfl-llm/guidelines): a new dataset of 46K internationally-recognized clinical practice guidelines from various healthcare-related sources, including hospitals and international organizations.\n- **Medical Paper Abstracts**: 16.1M abstracts extracted from closed-access PubMed and PubMed Central papers.\n- **Medical Papers**: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers.\n- **Replay Data**: 400M tokens of general domain pretraining data sampled from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)\n\n<img width=75% src=\"gap-replay.png\" alt=\"Alt text\">\n\n#### Data Preprocessing\n\nPlease see the detailed preprocessing procedure in our paper. \n\n### Training Procedure \n\nWe used the [Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) distributed training library, a derivative of Nvidia's Megatron LM project, to optimize training efficiency. \nHardware consists of 1 node of 8x NVIDIA A100 (80GB) SXM GPUs connected by NVLink and NVSwitch with a single Nvidia ConnectX-6 DX network card and equipped with 2 x AMD EPYC 7543 32-Core Processors and 512 GB of RAM. \n\nOur three way parallelism scheme uses:\n - Data Parallelism (DP -- different GPUs process different subsets of the batches) of 2,\n - Pipeline Parallelism (PP -- different GPUs process different layers) of 4,\n - Tensor Parallelism (TP -- different GPUs process different subtensors for matrix multiplication) of 1.\n   \n\n#### Training Hyperparameters\n\n|  |  |\n| --- | ------ |\n| bf16 | true |\n| lr  | 3e-4 |\n| eps | 1e-5       |\n| betas | \\[0.9, 0.95\\] |\n| clip_grad | 1 |\n| weight decay | 0.1 |\n| DP size | 16 |\n| TP size | 4 |\n| PP size | 1 |\n| seq length | 2048 |\n| lr scheduler | cosine|\n| min lr | 1e-6 |\n| warmup iteration | 2000 |\n| micro batch size | 10 |\n| global batch size | 1600 |\n|  |  |\n\n#### Sizes\nThe model was trained in September 2023.\n\nThe model architecture is exactly Llama 2, meaning\n\n|     |        |\n| --- | ------ |\n| Model size           | 7B   |\n| Hidden dimension     | 4096 |\n| Num. attention heads |   32 |\n| Num. layers          |   32 |\n|  |  |\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data & Metrics\n\n#### Testing Data\n- [MedQA (USMLE)](https://huggingface.co/datasets/bigbio/med_qa)\n- [MedMCQA](https://huggingface.co/datasets/medmcqa)\n- [PubMedQA](https://huggingface.co/datasets/bigbio/pubmed_qa)\n- [MMLU-Medical](https://huggingface.co/datasets/lukaemon/mmlu)\n- [MedQA-4-Option](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)\n\n#### Metrics\n- Accuracy: suite the evaluation of multiple-choice question-answering tasks.\n\n### Results\nWe finetune meditron-7b, llama-2-7b, pmc-llama-7b on each benchmark (pubmedqa, medmcqa, medqa)'s training data individually. \nWe report the finetuned models' performance with top token selection as the inference mode.\nFor MMLU-Medical, models finetuned on MedMCQA are used for inference.\nFor MedQA-4-Option, models finetuned on MedQA are used for inference.\nFor a more detailed performance analysis, please see our paper. \n\n|     |        |      |      |      |      |\n| --- | ------ |----- |----- |----- |----- |\n|Dataset       | meditron-7b | llama-2-7b  | pmc-llama-7b | Zephyr-7B-beta* | Mistral-7B-instruct* |\n|MMLU-Medical  | 54.2        |    53.7     |    56.4      |      63.3       |        60.0          |\n|PubMedQA      | 74.4        |    61.8     |    59.2      |      46.0       |        17.8          |\n|MedMCQA       | 59.2        |    54.4     |    57.6      |      43.0       |        40.2          |\n|MedQA         | 47.9        |    44.0     |    42.4      |      42.8       |        32.4          |\n|MedQA-4-Option| 52.0        |    49.6     |    49.2      |      48.5       |        41.1          |\n|Avg           | 57.5        |    52.7     |    53.0      |      48.7       |        38.3          |\n|              |             |             |              |                 |                      |\n\n**Note**: models with * are already instruction-tuned, so we exclude them from further finetuning on any training.",
            "metadata": "{\"id\": \"Sci-fi-vy/Meditron-7b-finetuned\", \"author\": \"Sci-fi-vy\", \"sha\": \"500d336c3a7eada031f553dff25dd291a7ae44bf\", \"last_modified\": \"2025-01-25 11:11:08+00:00\", \"created_at\": \"2025-01-22 14:32:25+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 12, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"safetensors\", \"llama\", \"text-generation\", \"image-text-to-text\", \"en\", \"dataset:epfl-llm/guidelines\", \"arxiv:2311.16079\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"image-text-to-text\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\ndatasets:\\n- epfl-llm/guidelines\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: llama2\\nmetrics:\\n- accuracy\\n- perplexity\\npipeline_tag: image-text-to-text\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"cls_token\": \"<CLS>\", \"eos_token\": \"</s>\", \"mask_token\": \"<MASK>\", \"pad_token\": \"<PAD>\", \"sep_token\": \"<SEP>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='gap-replay.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00008-of-00008.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='prompt_example.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00004-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00005-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00006-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00007-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00008-of-00008.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 6738554880}, \"total\": 6738554880}, \"security_repo_status\": null, \"lastModified\": \"2025-01-25 11:11:08+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\ndatasets:\\n- epfl-llm/guidelines\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: llama2\\nmetrics:\\n- accuracy\\n- perplexity\\npipeline_tag: image-text-to-text\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67910179e74ec3c3d964093a\", \"modelId\": \"Sci-fi-vy/Meditron-7b-finetuned\", \"usedStorage\": 26954572306}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Sci-fi-vy/Meditron-7b-finetuned&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSci-fi-vy%2FMeditron-7b-finetuned%5D(%2FSci-fi-vy%2FMeditron-7b-finetuned)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "danlou/persona-generator-llama-2-7b-qlora-merged",
            "card": "---\nlicense: llama2\nbase_model:\n- meta-llama/Llama-2-7b\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n\nThe code below shows how this Buyer Persona generator can be used.\n\nThis model was developed for [MarketFit.ai](https://danlou.co/marketfitai).\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_id = \"danlou/persona-generator-llama-2-7b-qlora-merged\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n\n\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\n\ndef parse_outputs(output_text):\n\n    try:\n        output_lns = output_text.split('\\n')\n        assert len(output_lns) == 2\n        assert len(output_lns[0].split(',')) == 2\n        assert len(output_lns[1]) > 16\n\n        name, age = [s.strip() for s in output_lns[0].split(',')]\n        desc = output_lns[1].strip()\n\n    except AssertionError:\n        raise Exception('Malformed output.')\n    \n    try:\n        age = int(age)\n    except ValueError:\n        raise Exception('Malformed output (age).')\n    \n    return {'name': name, 'age': age, 'description': desc}\n\n\n\ndef generate_personas(product, n=1, batch_size=32, parse=True):\n\n    prompt = f\"### Instruction:\\nDescribe the ideal persona for this product:\\n{product}\\n\\n### Response:\\n\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n    personas = []\n    with tqdm(total=n) as pbar:\n        for batch in chunks(range(n), batch_size):\n            outputs = model.generate(input_ids,\n                                    do_sample=True,\n                                    num_beams=1,\n                                    num_return_sequences=len(batch),\n                                    max_length=512,\n                                    min_length=32,\n                                    temperature=0.9)\n\n            for output_ids in outputs:\n                output_decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n                output_decoded = output_decoded[len(prompt):].strip()\n\n                try:\n                    if parse:\n                        personas.append(parse_outputs(output_decoded))\n                    else:\n                        personas.append(output_decoded)\n                except Exception as e:\n                    print(e)\n                    continue\n            \n            pbar.update(len(batch))\n    \n    return personas\n\n\nproduct = \"Koonie 10000mAh Rechargeable Desk Fan, 8-Inch Battery Operated Clip on Fan, USB Fan, 4 Speeds, Strong Airflow, Sturdy Clamp for Golf Cart Office Desk Outdoor Travel Camping Tent Gym Treadmill, Black (USB Gadgets > USB Fans)\"\npersonas = generate_personas(product, n=3)\n\nfor e in personas:\n    print(e)\n\n# Persona 1 - The yoga instructor\n# {'name': 'Sarah', 'age': 28, 'description': 'Yoga instructor who is passionate about health and fitness. She works from a home studio where she also practices yoga and meditation. Sarah values products that are eco-friendly and sustainable. She loves products that are versatile and can be used for different purposes. Sarah is looking for a product that is durable and can withstand frequent use. She values products that are stylish and aesthetically pleasing.'}\n# Persona 2 - The golf enthusiast\n#{'name': 'Sophia', 'age': 60, 'description': \"Golf enthusiast. Sophia spends most of her weekends on the golf course, and she needs a fan that she can carry around in her golf cart. She needs a fan that's lightweight, easy to clip on, and has a long battery life. She also wants a fan that's affordable, especially since she plays at different courses.\"}\n# Persona 3 - The truck driver\n# {'name': 'Mike', 'age': 32, 'description': \"Truck driver who spends most of his day on the road. The cab of his truck can get hot and stuffy, and Mike needs a fan that can keep him comfortable and alert while he's driving. He needs a fan that's easy to install and adjust, so he can keep it on his dashboard and direct the airflow where he needs it most.\"}\n```",
            "metadata": "{\"id\": \"danlou/persona-generator-llama-2-7b-qlora-merged\", \"author\": \"danlou\", \"sha\": \"61367b68d5f84d5e7568134550a044ad682f88dd\", \"last_modified\": \"2024-10-20 15:43:58+00:00\", \"created_at\": \"2023-09-15 09:53:03+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 25, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: transformers\\nlicense: llama2\\npipeline_tag: text-generation\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"use_default_system_prompt\": true}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00002.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738415616}, \"total\": 6738415616}, \"security_repo_status\": null, \"lastModified\": \"2024-10-20 15:43:58+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: transformers\\nlicense: llama2\\npipeline_tag: text-generation\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"6504297f221035b9ee1423ff\", \"modelId\": \"danlou/persona-generator-llama-2-7b-qlora-merged\", \"usedStorage\": 13477364499}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/persona-generator-llama-2-7b-qlora-merged-GGUF",
                "https://huggingface.co/mradermacher/persona-generator-llama-2-7b-qlora-merged-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=danlou/persona-generator-llama-2-7b-qlora-merged&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdanlou%2Fpersona-generator-llama-2-7b-qlora-merged%5D(%2Fdanlou%2Fpersona-generator-llama-2-7b-qlora-merged)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "LLM-PBE/together-llama-2-7B-enron-undefended",
            "card": "---\nlicense: llama2\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThe model is built by fine-tuning [Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) on [Enron](https://www.cs.cmu.edu/~enron/) with 10 epochs.",
            "metadata": "{\"id\": \"LLM-PBE/together-llama-2-7B-enron-undefended\", \"author\": \"LLM-PBE\", \"sha\": \"3e64a02da79cfbd9449b1e425c6a90b82e3a88ab\", \"last_modified\": \"2024-10-02 10:14:11+00:00\", \"created_at\": \"2023-12-02 23:11:02+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 920, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlicense: llama2\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Notice', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-02 10:14:11+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlicense: llama2\", \"transformersInfo\": null, \"_id\": \"656bb9863dc1d277e54f713e\", \"modelId\": \"LLM-PBE/together-llama-2-7B-enron-undefended\", \"usedStorage\": 383355316228}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=LLM-PBE/together-llama-2-7B-enron-undefended&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BLLM-PBE%2Ftogether-llama-2-7B-enron-undefended%5D(%2FLLM-PBE%2Ftogether-llama-2-7B-enron-undefended)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "LLM-PBE/together-llama-2-7B-enron-scrubbed",
            "card": "---\nlicense: llama2\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThe model is built by fine-tuning [Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) on [Enron](https://www.cs.cmu.edu/~enron/) with 10 epochs using scrubbing technique.",
            "metadata": "{\"id\": \"LLM-PBE/together-llama-2-7B-enron-scrubbed\", \"author\": \"LLM-PBE\", \"sha\": \"4fc4c33500a7f55bf5e39ac1458d1d23ae25812a\", \"last_modified\": \"2024-10-02 10:18:55+00:00\", \"created_at\": \"2023-12-04 07:03:46+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 434, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:llama2\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlicense: llama2\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='Notice', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='USE_POLICY.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-02 10:18:55+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlicense: llama2\", \"transformersInfo\": null, \"_id\": \"656d79d290d556ffa633b1bd\", \"modelId\": \"LLM-PBE/together-llama-2-7B-enron-scrubbed\", \"usedStorage\": 26954361541}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=LLM-PBE/together-llama-2-7B-enron-scrubbed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BLLM-PBE%2Ftogether-llama-2-7B-enron-scrubbed%5D(%2FLLM-PBE%2Ftogether-llama-2-7B-enron-scrubbed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "qu-bit/SuperLLM",
            "card": "---\nlanguage:\n- en\nmetrics:\n- accuracy\n- bleu\n- rouge\n- glue\nbase_model: meta-llama/Llama-2-7b\n---\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis is the SuperLLM. This LLM has an extensive knowledge base of the RAW agents. Your task is to make it forget that.\n\nHave Fun ;)\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n\n\n- **Developed by:** [Brain and Cognitive Science Club, IIT Kanpur](https://bcs-iitk.github.io/)\n",
            "metadata": "{\"id\": \"qu-bit/SuperLLM\", \"author\": \"qu-bit\", \"sha\": \"1c5cef89b0a0164431e3af4a650f2c1fc9855282\", \"last_modified\": \"2024-08-28 17:56:12+00:00\", \"created_at\": \"2023-12-04 19:29:36+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"llama\", \"text-generation\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-28 17:56:12+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"656e28a0af6d3c41299b581e\", \"modelId\": \"qu-bit/SuperLLM\", \"usedStorage\": 13611219677}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=qu-bit/SuperLLM&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bqu-bit%2FSuperLLM%5D(%2Fqu-bit%2FSuperLLM)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "YBCarry/Finance-Chinese-LLaMA",
            "card": "---\nlicense: apache-2.0\nlanguage:\n- zh\nbase_model:\n- meta-llama/Llama-2-7b\ntags:\n- finance\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis modelcard aims to be a base template for new models. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md?plain=1).\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
            "metadata": "{\"id\": \"YBCarry/Finance-Chinese-LLaMA\", \"author\": \"YBCarry\", \"sha\": \"e5cc4233cc1221bd6255bc0525cc700ef24a09d5\", \"last_modified\": \"2025-03-30 02:34:02+00:00\", \"created_at\": \"2024-01-24 01:00:41+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"finance\", \"zh\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- zh\\nlicense: apache-2.0\\ntags:\\n- finance\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-03-30 02:34:02+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlanguage:\\n- zh\\nlicense: apache-2.0\\ntags:\\n- finance\", \"transformersInfo\": null, \"_id\": \"65b06139aa335c28420e55c2\", \"modelId\": \"YBCarry/Finance-Chinese-LLaMA\", \"usedStorage\": 844403}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=YBCarry/Finance-Chinese-LLaMA&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BYBCarry%2FFinance-Chinese-LLaMA%5D(%2FYBCarry%2FFinance-Chinese-LLaMA)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "pucpr-br/Clinical-BR-LlaMA-2-7B",
            "card": "---\nlicense: apache-2.0\nlanguage:\n- pt\nbase_model: meta-llama/Llama-2-7b\npipeline_tag: text-generation\n---\n\n# MED-LLM-BR: Medical Large Language Models for Brazilian Portuguese\nMED-LLM-BR is a collaborative project between [HAILab](https://github.com/HAILab-PUCPR) and [Comsentimento](https://www.comsentimento.com.br/), which aims to develop multiple medical LLMs for Portuguese language, including base models and task-specific models, with different sizes. \n\n## Introduction\nClinical-BR-LlaMA-2-7B is a fine-tuned language model specifically designed for generating clinical notes in Portuguese. This model builds on the strengths of LlaMA 2 7B, adapting it through targeted fine-tuning techniques to meet the unique demands of clinical text generation. By focusing on the nuances and complexities of medical language in Portuguese, Clinical-BR-LlaMA-2-7B aims to support healthcare professionals with contextually accurate and relevant clinical documentation.\n\n## Fine-Tuning Approach\nTo enhance memory efficiency and reduce computational demands, we implemented LoRA with 16-bit precision on the q_proj and v_proj projections. We configured LoRA with R set to 8, Alpha to 16, and Dropout to 0.1, allowing the model to adapt effectively while preserving output quality. For optimization, the AdamW optimizer was used with parameters \u03b21 = 0.9 and \u03b22 = 0.999, achieving a balance between fast convergence and training stability. This careful tuning process ensures robust performance in generating accurate and contextually appropriate clinical text in Portuguese.\n\n## Data\nThe fine-tuning of Clinical-BR-LlaMA-2-7B utilized 2.4GB of text from three clinical datasets. The SemClinBr project provided diverse clinical narratives from Brazilian hospitals, while the BRATECA dataset contributed admission notes from various departments in 10 hospitals. Additionally, data from Lopes et al., 2019, added neurology-focused texts from European Portuguese medical journals. These datasets collectively improved the model\u2019s ability to generate accurate clinical notes in Portuguese.\n\n## Provisional Citation:\n```\n@inproceedings{pinto2024clinicalLLMs,\n  title        = {Developing Resource-Efficient Clinical LLMs for Brazilian Portuguese},\n  author       = {Jo\u00e3o Gabriel de Souza Pinto and Andrey Rodrigues de Freitas and Anderson Carlos Gomes Martins and Caroline Midori Rozza Sawazaki and Caroline Vidal and Lucas Emanuel Silva e Oliveira},\n  booktitle    = {Proceedings of the 34th Brazilian Conference on Intelligent Systems (BRACIS)},\n  year         = {2024},\n  note         = {In press},\n}\n```",
            "metadata": "{\"id\": \"pucpr-br/Clinical-BR-LlaMA-2-7B\", \"author\": \"pucpr-br\", \"sha\": \"cc3f2623dd6984751de1caed0717ddca1a833a6e\", \"last_modified\": \"2024-08-29 14:35:05+00:00\", \"created_at\": \"2024-02-03 00:24:33+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 102, \"downloads_all_time\": null, \"likes\": 6, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"pt\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- pt\\nlicense: apache-2.0\\npipeline_tag: text-generation\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F16\": 6738415616}, \"total\": 6738415616}, \"security_repo_status\": null, \"lastModified\": \"2024-08-29 14:35:05+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- pt\\nlicense: apache-2.0\\npipeline_tag: text-generation\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"65bd87c1adc958a7d97b3055\", \"modelId\": \"pucpr-br/Clinical-BR-LlaMA-2-7B\", \"usedStorage\": 13477364667}",
            "depth": 1,
            "children": [
                "https://huggingface.co/cabelo/Clinical-BR-LlaMA-2-7B-fp16-ov"
            ],
            "children_count": 1,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/Clinical-BR-LlaMA-2-7B-GGUF",
                "https://huggingface.co/tensorblock/Clinical-BR-LlaMA-2-7B-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=pucpr-br/Clinical-BR-LlaMA-2-7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bpucpr-br%2FClinical-BR-LlaMA-2-7B%5D(%2Fpucpr-br%2FClinical-BR-LlaMA-2-7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "cabelo/Clinical-BR-LlaMA-2-7B-fp16-ov",
            "card": "---\nlicense: creativeml-openrail-m\nbase_model: pucpr-br/Clinical-BR-LlaMA-2-7B\nlanguage:\n- pt\nlibrary_name: openvino\nquantized_by: cabelo\n---\n<div align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/638ba60d-606b-4b5d-a549-abd411f9886e\" width=\"300\"/>\n</div>\n\n# MED-LLM-BR - OpenVINO: Medical Large Language Models for Brazilian Portuguese\nMED-LLM-BR-OpenVINO is a **model converted** of the collaborative project between [HAILab](https://github.com/HAILab-PUCPR) and [Comsentimento](https://www.comsentimento.com.br/), which aims to develop multiple medical LLMs for Portuguese language, including base models and task-specific models, with different sizes. \n\n## Introduction\nClinical-BR-LlaMA-2-7B is a fine-tuned language model specifically designed for generating clinical notes in Portuguese. This model builds on the strengths of LlaMA 2 7B, adapting it through targeted fine-tuning techniques to meet the unique demands of clinical text generation. By focusing on the nuances and complexities of medical language in Portuguese, Clinical-BR-LlaMA-2-7B aims to support healthcare professionals with contextually accurate and relevant clinical documentation.\n\n## Fine-Tuning Approach\nTo enhance memory efficiency and reduce computational demands, we implemented LoRA with 16-bit precision on the q_proj and v_proj projections. We configured LoRA with R set to 8, Alpha to 16, and Dropout to 0.1, allowing the model to adapt effectively while preserving output quality. For optimization, the AdamW optimizer was used with parameters \u03b21 = 0.9 and \u03b22 = 0.999, achieving a balance between fast convergence and training stability. This careful tuning process ensures robust performance in generating accurate and contextually appropriate clinical text in Portuguese.\n\n## Data\nThe fine-tuning of Clinical-BR-LlaMA-2-7B utilized 2.4GB of text from three clinical datasets. The SemClinBr project provided diverse clinical narratives from Brazilian hospitals, while the BRATECA dataset contributed admission notes from various departments in 10 hospitals. Additionally, data from Lopes et al., 2019, added neurology-focused texts from European Portuguese medical journals. These datasets collectively improved the model\u2019s ability to generate accurate clinical notes in Portuguese.\n\n## Provisional Citation:\n```\n@inproceedings{pinto2024clinicalLLMs,\n  title        = {Developing Resource-Efficient Clinical LLMs for Brazilian Portuguese},\n  author       = {Jo\u00e3o Gabriel de Souza Pinto and Andrey Rodrigues de Freitas and Anderson Carlos Gomes Martins and Caroline Midori Rozza Sawazaki and Caroline Vidal and Lucas Emanuel Silva e Oliveira},\n  booktitle    = {Proceedings of the 34th Brazilian Conference on Intelligent Systems (BRACIS)},\n  year         = {2024},\n  note         = {In press},\n}\n```\n",
            "metadata": "{\"id\": \"cabelo/Clinical-BR-LlaMA-2-7B-fp16-ov\", \"author\": \"cabelo\", \"sha\": \"e37bb90d28d3759154c0cd88848dae2bf763ffd4\", \"last_modified\": \"2024-09-01 05:44:54+00:00\", \"created_at\": \"2024-09-01 02:35:18+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"openvino\", \"gguf\": null, \"inference\": null, \"tags\": [\"openvino\", \"llama\", \"pt\", \"base_model:pucpr-br/Clinical-BR-LlaMA-2-7B\", \"base_model:finetune:pucpr-br/Clinical-BR-LlaMA-2-7B\", \"license:creativeml-openrail-m\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: pucpr-br/Clinical-BR-LlaMA-2-7B\\nlanguage:\\n- pt\\nlibrary_name: openvino\\nlicense: creativeml-openrail-m\\nquantized_by: cabelo\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_detokenizer.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_detokenizer.xml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_model.xml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_tokenizer.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='openvino_tokenizer.xml', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-09-01 05:44:54+00:00\", \"cardData\": \"base_model: pucpr-br/Clinical-BR-LlaMA-2-7B\\nlanguage:\\n- pt\\nlibrary_name: openvino\\nlicense: creativeml-openrail-m\\nquantized_by: cabelo\", \"transformersInfo\": null, \"_id\": \"66d3d2e6960d799f1ae9cc18\", \"modelId\": \"cabelo/Clinical-BR-LlaMA-2-7B-fp16-ov\", \"usedStorage\": 26955162313}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=cabelo/Clinical-BR-LlaMA-2-7B-fp16-ov&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bcabelo%2FClinical-BR-LlaMA-2-7B-fp16-ov%5D(%2Fcabelo%2FClinical-BR-LlaMA-2-7B-fp16-ov)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "2imi9/Llama2_7B_TeachingAssistant_Introduction_to_Computers",
            "card": "---\ndatasets:\n- 2imi9/llama2_7B_data_10G\n- 2imi9/llama2_7B_data_Course_materials\nlanguage:\n- en\n- zh\nbase_model:\n- meta-llama/Llama-2-7b\npipeline_tag: question-answering\n---\nTeaching Assistance For Introduction to Computers\n-\n\nThis teaching assistance model was fine-tuned using LlamaFactory (Zheng et al., 2024), leveraging the LLaMA 2 7B architecture. The fine-tuning process involved around 10GB of open-source bilingual data (Chinese and English) collected from Hugging Face and CSDN. Additionally, specialized datasets focused on introductory computer science topics were integrated to tailor the model for educational purposes. The result is an AI-powered assistant capable of supporting foundational computer science education.\n\nInstruction To Test This Model\n-\nPlease follow the guide in the LLaMA-Factory README file by cloning it from GitHub and accessing the LLaMA Board GUI (powered by Gradio) to launch the model: LLaMA-Factory GitHub README.\n\nhttps://github.com/hiyouga/LLaMA-Factory/blob/main/README.md\n-\n\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}",
            "metadata": "{\"id\": \"2imi9/Llama2_7B_TeachingAssistant_Introduction_to_Computers\", \"author\": \"2imi9\", \"sha\": \"b262ea29c1517a75507f8d519ef542b26cc5e739\", \"last_modified\": \"2024-09-28 20:56:09+00:00\", \"created_at\": \"2024-07-29 12:32:49+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"question-answering\", \"en\", \"zh\", \"dataset:2imi9/llama2_7B_data_10G\", \"dataset:2imi9/llama2_7B_data_Course_materials\", \"arxiv:2403.13372\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"region:us\"], \"pipeline_tag\": \"question-answering\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- 2imi9/llama2_7B_data_10G\\n- 2imi9/llama2_7B_data_Course_materials\\nlanguage:\\n- en\\n- zh\\npipeline_tag: question-answering\", \"widget_data\": [{\"text\": \"Where do I live?\", \"context\": \"My name is Wolfgang and I live in Berlin\"}, {\"text\": \"Where do I live?\", \"context\": \"My name is Sarah and I live in London\"}, {\"text\": \"What's my name?\", \"context\": \"My name is Clara and I live in Berkeley.\"}, {\"text\": \"Which name is also used to describe the Amazon rainforest in English?\", \"context\": \"The Amazon rainforest (Portuguese: Floresta Amaz\\u00f4nica or Amaz\\u00f4nia; Spanish: Selva Amaz\\u00f3nica, Amazon\\u00eda or usually Amazonia; French: For\\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \\\"Amazonas\\\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"}], \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/global_step800/mp_rank_00_model_states.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/latest', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_0.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_1.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_2.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_3.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_4.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_5.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_6.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/rng_state_7.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='checkpoint-800/zero_to_fp32.py', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-09-28 20:56:09+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- 2imi9/llama2_7B_data_10G\\n- 2imi9/llama2_7B_data_Course_materials\\nlanguage:\\n- en\\n- zh\\npipeline_tag: question-answering\", \"transformersInfo\": null, \"_id\": \"66a78bf1909a525bcbbba204\", \"modelId\": \"2imi9/Llama2_7B_TeachingAssistant_Introduction_to_Computers\", \"usedStorage\": 96977918289}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=2imi9/Llama2_7B_TeachingAssistant_Introduction_to_Computers&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5B2imi9%2FLlama2_7B_TeachingAssistant_Introduction_to_Computers%5D(%2F2imi9%2FLlama2_7B_TeachingAssistant_Introduction_to_Computers)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "inceptionai/jais-adapted-7b",
            "card": "---\nbase_model: meta-llama/Llama-2-7b\nlanguage:\n- ar\n- en\nthumbnail: null\ntags:\n- Arabic\n- English\n- LLM\n- Decoder\n- causal-lm\n- jais-family\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n# Jais Family Model Card\n\n\nThe Jais family of models is a comprehensive series of bilingual English-Arabic large language models (LLMs). These models are optimized to excel in Arabic while having strong English capabilities. We release two variants of foundation models that include:\n\n- Models **pre-trained from scratch** (`jais-family-*`).\n- Models **pre-trained adaptively from [Llama-2](https://arxiv.org/pdf/2307.09288)** (`jais-adapted-*`).\n\nIn this release, we introduce 20 models across 8 sizes, ranging from 590M to 70B parameters, trained on up to 1.6T tokens of Arabic, English, and code data.  *All* pre-trained models in this series are instruction fine-tuned (`*-chat`) for dialog using a curated mix of Arabic and English instruction data.\n\nWe hope this extensive release will accelerate research in Arabic NLP, and enable numerous downstream applications for the Arabic speaking and bilingual community. The training and adaptation techniques we demonstrate successfully for Arabic models are extensible to other low and medium resource languages.\n\n## Jais Family Details\n\n- **Developed by:** Inception, Cerebras Systems.\n- **Language(s):** (NLP): Arabic (MSA) and English.\n- **Input:** Text only data.\n- **Output:** Model generates text.\n- **Model Sizes:** 590M, 1.3B, 2.7B, 6.7B, 7B, 13B, 30B, 70B.\n- **Demo:** [Access the live demo here](https://arabic-gpt.ai/)\n- **License:** Apache 2.0\n\n| **Pre-trained Model**   | **Fine-tuned Model** | **Size (Parameters)** | **Context length (Tokens)** |\n|:---------------------|:--------|:-------|:-------|\n| [jais-family-30b-16k](https://huggingface.co/inceptionai/jais-family-30b-16k)   | [Jais-family-30b-16k-chat](https://huggingface.co/inceptionai/jais-family-30b-16k-chat) | 30B | 16,384 |\n| [jais-family-30b-8k](https://huggingface.co/inceptionai/jais-family-30b-8k)  | [Jais-family-30b-8k-chat](https://huggingface.co/inceptionai/jais-family-30b-8k-chat) | 30B | 8,192 |\n| [jais-family-13b ](https://huggingface.co/inceptionai/jais-family-13b)  | [Jais-family-13b-chat](https://huggingface.co/inceptionai/jais-family-13b-chat) | 13B | 2,048 |\n| [jais-family-6p7b](https://huggingface.co/inceptionai/jais-family-6p7b)  | [Jais-family-6p7b-chat](https://huggingface.co/inceptionai/jais-family-6p7b-chat) | 6.7B | 2,048 |\n| [jais-family-2p7b](https://huggingface.co/inceptionai/jais-family-2p7b)  | [Jais-family-2p7b-chat](https://huggingface.co/inceptionai/jais-family-2p7b-chat) | 2.7B   | 2,048  |\n| [jais-family-1p3b](https://huggingface.co/inceptionai/jais-family-1p3b)  | [Jais-family-1p3b-chat](https://huggingface.co/inceptionai/jais-family-1p3b-chat) | 1.3B | 2,048 |\n| [jais-family-590m](https://huggingface.co/inceptionai/jais-family-590m)  | [Jais-family-590m-chat](https://huggingface.co/inceptionai/jais-family-590m-chat)  | 590M   | 2,048  |\n\n| **Adapted pre-trained Model**   | **Fine-tuned Model** | **Size (Parameters)** | **Context length (Tokens)** |\n|:---------------------|:--------|:-------|:-------|\n| [jais-adapted-70b](https://huggingface.co/inceptionai/jais-adapted-70b)   | [Jais-adapted-70b-chat](https://huggingface.co/inceptionai/jais-adapted-70b-chat) | 70B | 4,096 |\n| [jais-adapted-13b](https://huggingface.co/inceptionai/jais-adapted-13b)  | [Jais-adapted-13b-chat](https://huggingface.co/inceptionai/jais-adapted-13b-chat) | 13B | 4,096 |\n| [jais-adapted-7b](https://huggingface.co/inceptionai/jais-adapted-7b)  | [Jais-adapted-7b-chat](https://huggingface.co/inceptionai/jais-adapted-7b-chat) | 7B | 4,096 |\n\n### Model Architecture:\n<a name=\"model-architecture\"></a>\n\nAll models in this family are auto-regressive language models that use a transformer-based, decoder-only architecture (GPT-3).\n\nJais models (`jais-family-*`) are *trained from scratch*, incorporating the SwiGLU non-linear activation function and ALiBi position encoding. These architectural enhancements allow the models to extrapolate at long sequence lengths, leading to improved context handling and precision.\n\nJais adapted models (`jais-adapted-*`) are *built on top of Llama-2*, which employs RoPE position embedding and Grouped Query Attention. We introduce tokenizer expansion with Arabic data, which improves fertility and compute efficiency by over 3x. In particular, we add `32,000` new Arabic tokens from the Jais-30b vocabulary into the Llama-2 tokenizer. \nTo initialize these new Arabic token embeddings we first learn a linear projection from the embedding space of Jais-30b to Llama's embedding space, using the set of shared English tokens present in both vocabularies. Next, this learned projection is applied to transform the existing Jais-30b Arabic embeddings into the Llama-2 embedding space.\n\n\n## Getting started\n\nBelow is sample code to use the model. Note that the model requires a custom model class, so users must enable `trust_remote_code=True` while loading the model.\n\n```python\n# -*- coding: utf-8 -*-\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"inceptionai/jais-adapted-7b\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef get_response(text, tokenizer=tokenizer, model=model):\n    tokenized = tokenizer(text, return_tensors=\"pt\")\n    input_ids, attention_mask = tokenized['input_ids'].to(device), tokenized['attention_mask'].to(device)\n    input_len = input_ids.shape[-1]\n    generate_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        top_p=0.9,\n        temperature=0.3,\n        max_length=2048,\n        min_length=input_len + 4,\n        repetition_penalty=1.2,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    response = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n    )[0]\n    return response\n\ntext = \"\u0639\u0627\u0635\u0645\u0629 \u062f\u0648\u0644\u0629 \u0627\u0644\u0625\u0645\u0627\u0631\u0627\u062a \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u0647\"\nprint(get_response(text))\n\ntext = \"The capital of UAE is\"\nprint(get_response(text))\n```\n\n## Training Details\n\n### Pretraining Data\n\nThe Jais family of models are trained on up to 1.6 Trillion tokens of diverse English, Arabic and Code data. The data consists of the following sources:\n\n- **Web:** We used publicly available web pages, wikipedia articles, news articles, and social network content in both Arabic and English.\n  \n- **Code:** To enhance the reasoning capability of our model, we include Code data in various programming languages. \n\n- **Books:** We used a selection of publicly available Arabic and English books data, which improves long-range context modelling and coherent storytelling. \n\n- **Scientific:** A subset of ArXiv papers were included to improve reasoning and long context abilities.\n\n- **Synthetic:** We augment the volume of Arabic data by translating English to Arabic using an in-house machine translation system. We restrict this to high quality English resources such as English Wikipedia and English books. \n\nWe extensively preprocess and deduplicate the training data. For Arabic, we used a custom preprocessing pipeline to filter for data with high linguistic quality. More information on this pipeline can be found in the [Jais paper](https://arxiv.org/abs/2308.16149).\n\n- **Jais pre-trained** (`jais-family-*`): Following our previous experimentation with language alignment mixing in [Jais](https://arxiv.org/abs/2308.16149), we used a ratio of 1:2:0.4 of Arabic:English:Code data. This recipe for <u>from scratch pre-training</u> addresses Arabic data scarcity while improving performance in both languages.\n- **Jais adapted pre-trained** (`jais-adapted-*`): For the <u>adapted pre-training of Llama-2</u>, we utilized a larger Arabic dataset of ~334B Arabic tokens mixed with English and Code data. We vary the mixing ratio, at different model sizes, to introduce strong Arabic capabilities while maintaining performance in English.\n\n\n| **Pre-trained model**   | **English data (tokens)** | **Arabic data (tokens)** | **Code data (tokens)** | **Total data (tokens)** |\n|-------------------------|---------------------------|--------------------------|------------------------|------------------------|\n| [jais-family-30b-16k](https://huggingface.co/inceptionai/jais-family-30b-16k) | 980B | 490B | 196B | 1666B |\n| [jais-family-30b-8k](https://huggingface.co/inceptionai/jais-family-30b-8k)   | 882B | 441B | 177B | 1500B | \n| [jais-family-13b ](https://huggingface.co/inceptionai/jais-family-13b)        | 283B  | 141B | 56B  | 480B | \n| [jais-family-6p7b](https://huggingface.co/inceptionai/jais-family-6p7b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-2p7b](https://huggingface.co/inceptionai/jais-family-2p7b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-1p3b](https://huggingface.co/inceptionai/jais-family-1p3b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-590m](https://huggingface.co/inceptionai/jais-family-590m)       | 283B  | 141B | 56B  | 480B | \n| [jais-adapted-70b](https://huggingface.co/inceptionai/jais-adapted-70b)       | 33B   | 334B | 4B   | 371B |\n| [jais-adapted-13b](https://huggingface.co/inceptionai/jais-adapted-13b)       | 127B  | 140B | 13B  | 280B |\n| [jais-adapted-7b](https://huggingface.co/inceptionai/jais-adapted-7b)         | 18B   | 19B  |  2B   | 39B |\n\n### Finetuning data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nAll chat models in the Jais family are fine-tuned using Arabic and English prompt-response pairs in both single-turn and multi-turn settings. Data sources include open-source fine-tuning datasets filtered for topic and style diversity. Additionally, internally curated human data is incorporated to enhance cultural adaptation. This data is supplemented with content generated using synthetic methods including machine translation, distillation, and model self-chat. Overall, our updated instruction-tuning dataset comprises ~10M and ~4M prompt-response pairs in English and Arabic respectively.\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\nDuring the pre-training of (`jais-family-*`) models, documents are packed into sequences separated by EOS tokens, and the model is trained autoregressively, applying the loss to all tokens. For jais-30b models, the context length is progressively expanded from 2k to 8K to 16K by incorporating curated long-context documents in training. This progressive expansion leverages faster initial training at shorter context lengths, while gradually extending support for larger context lengths towards the end of the training process.\n\nDuring the adapted pre-training of the (`jais-adapted-*`) models, we first initialize the new tokenizer and Arabic embeddings as described in [Model Architecture](#model-architecture). In training, we implemented a two-stage approach to overcome observed higher norms of the new Arabic embeddings. In the first stage, the backbone of the model is frozen, and the embeddings are trained using approximately 15 billion tokens from a bilingual corpus of English and Arabic. In the second stage, the backbone is unfrozen, and continuous pretraining is conducted with all parameters.\n\nDuring instruction tuning, each training example consists of a single-turn or multi-turn prompt and it's response. Instead of one example per sequence, examples are packed together while the loss is masked on the prompt tokens. This approach speeds up training by allowing more examples to be processed per batch.\n\n\n### Training Hyperparameters:\n\n#### Jais-adapted-7b\n| Hyperparameter | Value                           |\n|----------------|-------------------------------------------|\n| Precision      | fp32                                      |\n| Optimizer      | AdamW                                     |\n| Learning rate  | 0 to 0.00015(<=400 warmup steps)<br>0.00015 to 1.50e-05(>400 and <=10060 steps, Cosine Decay) |\n| Weight decay   | 0.1                                       |\n| Batch size     | 960|\n| Context Length | 4096|\n| Steps          | 10060 |\n\n### Compute Infrastructure\n\nThe training process was performed on the Condor Galaxy (CG) supercomputer platform. A CG contains 64 Cerebras CS-2 Wafer-Scale Engines (WSE-2) with 40 GB of SRAM, and achieves a total of 960 PetaFLOP/s.\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\nWe conducted a comprehensive evaluation of Jais models focusing on both English and Arabic, using LM-harness in a zero-shot setting. The evaluation criteria spanned various dimensions, including:\n\n- **Knowledge:** How well the model answers factual questions.\n- **Reasoning:** The model's ability to answer questions requiring reasoning.\n- **Misinformation/Bias:** Assessment of the model's susceptibility to generating false or misleading information, and its neutrality.\n\n### Arabic evaluation results:\n\n<style>\n  .table-container {\n    overflow-x: auto;\n    white-space: nowrap;\n  }\n</style>\n\n<div class=\"table-container\">\n\n|     **Models**   |  Avg  | ArabicMMLU*| MMLU  | EXAMS*| LitQA*| agqa | agrc | Hellaswag  | PIQA | BoolQA | Situated QA | ARC-C | OpenBookQA | TruthfulQA | CrowS-Pairs |\n|--------------------------|-------|------------|-------|-------|-------|------|------|------------|------|--------|-------------|-------|------------|------------|-------------|\n| jais-family-30b-16k      | 49.2  |    44.0    | 33.4  | 40.9  | 60    | 47.8 | 49.3 | 60.9       | 68.6 | 70.3   | 41.6        | 38.7  | 31.8       | 45.2       | 57          |\n| jais-family-30b-8k       | 49.7  |    46.0    | 34    | 42    | 60.6  | 47.6 | 50.4 | 60.4       | 69   | 67.7   | 42.2        | 39.2  | 33.8       | 45.1       | 57.3        |\n| jais-family-13b          | 46.1  |    34.0    | 30.3  | 42.7  | 58.3  | 40.5 | 45.5 | 57.3       | 68.1 | 63.1   | 41.6        | 35.3  | 31.4       | 41         | 56.1        |\n| jais-family-6p7b         | 44.6  |    32.2    | 29.9  | 39    | 50.3  | 39.2 | 44.1 | 54.3       | 66.8 | 66.5   | 40.9        | 33.5  | 30.4       | 41.2       | 55.4        |\n| jais-family-2p7b         | 41.0  |    29.5    | 28.5  | 36.1  | 45.7  | 32.4 | 40.8 | 44.2       | 62.5 | 62.2   | 39.2        | 27.4  | 28.2       | 43.6       | 53.6        |\n| jais-family-1p3b         | 40.8  |    28.9    | 28.5  | 34.2  | 45.7  | 32.4 | 40.8 | 44.2       | 62.5 | 62.2   | 39.2        | 27.4  | 28.2       | 43.6       | 53.6        |\n| jais-family-590m         | 39.7  |    31.2    | 27    | 33.1  | 41.7  | 33.8 | 38.8 | 38.2       | 60.7 | 62.2   | 37.9        | 25.5  | 27.4       | 44.7       | 53.3        |\n| jais-family-30b-16k-chat | 51.6  |    59.9    | 34.6  | 40.2  | 58.9  | 46.8 | 54.7 | 56.2       | 64.4 | 76.7   | 55.9        | 40.8  | 30.8       | 49.5       | 52.9        |\n| jais-family-30b-8k-chat  | 51.4  |    61.2    | 34.2  | 40.2  | 54.3  | 47.3 | 53.6 | 60         | 63.4 | 76.8   | 54.7        | 39.5  | 30         | 50.7       | 54.3        |\n| jais-family-13b-chat     | 50.3  |    58.2    | 33.9  | 42.9  | 53.1  | 46.8 | 51.7 | 59.3       | 65.4 | 75.2   | 51.2        | 38.4  | 29.8       | 44.8       | 53.8        |\n| jais-family-6p7b-chat    | 48.7  |    55.7    | 32.8  | 37.7  | 49.7  | 40.5 | 50.1 | 56.2       | 62.9 | 79.4   | 52          | 38    | 30.4       | 44.7       | 52          |\n| jais-family-2p7b-chat    | 45.6  |    50.0    | 31.5  | 35.9  | 41.1  | 37.3 | 42.1 | 48.6       | 63.7 | 74.4   | 50.9        | 35.3  | 31.2       | 44.5       | 51.3        |\n| jais-family-1p3b-chat    | 42.7  |    42.2    | 30.1  | 33.6  | 40.6  | 34.1 | 41.2 | 43         | 63.6 | 69.3   | 44.9        | 31.6  | 28         | 45.6       | 50.4        |\n| jais-family-590m-chat    | 37.8  |    39.1    |  28   |29.5   | 33.1  | 30.8 | 36.4 | 30.3       | 57.8 | 57.2   | 40.5        | 25.9  | 26.8       | 44.5       | 49.3        |\n\n\n\n|     **Adapted Models**   |  Avg  | ArabicMMLU*| MMLU  | EXAMS*| LitQA*| agqa | agrc | Hellaswag  | PIQA | BoolQA | Situated QA | ARC-C | OpenBookQA | TruthfulQA | CrowS-Pairs |\n|--------------------------|-------|------------|-------|-------|-------|------|------|------------|------|--------|-------------|-------|------------|------------|-------------|\n| jais-adapted-70b         | 51.5  |    55.9    | 36.8  | 42.3  | 58.3  | 48.6 | 54   | 61.5       | 68.4 | 68.4   | 42.1        | 42.6  | 33         | 50.2       | 58.3        |\n| jais-adapted-13b         | 46.6  |    44.7    | 30.6  | 37.7  | 54.3  | 43.8 | 48.3 | 54.9       | 67.1 | 64.5   | 40.6        | 36.1  | 32         | 43.6       | 54.00       |\n| jais-adapted-7b          | 42.0  |    35.9    | 28.9  | 36.7  | 46.3  | 34.1 | 40.3 | 45         | 61.3 | 63.8   | 38.1        | 29.7  | 30.2       | 44.3       | 53.6        |\n| jais-adapted-70b-chat    | 52.9  |    66.8    | 34.6  | 42.5  | 62.9  | 36.8 | 48.6 | 64.5       | 69.7 | 82.8   | 49.3        | 44.2  | 32.2       | 53.3       | 52.4        |\n| jais-adapted-13b-chat    | 50.3  |    59.0    | 31.7  | 37.5  | 56.6  | 41.9 | 51.7 | 58.8       | 67.1 | 78.2   | 45.9        | 41    | 34.2       | 48.3       | 52.1        |\n| jais-adapted-7b-chat     | 46.1  |    51.3    | 30    | 37    | 48    | 36.8 | 48.6 | 51.1       | 62.9 | 72.4   | 41.3        | 34.6  | 30.4       | 48.6       | 51.8        |\n\n</div>\n\nArabic benchmarks are translated using an in-house MT model and reviewed by Arabic linguists. Benchmarks labeled with an asterisk (*) are natively Arabic; for further details, see the [Jais paper](https://arxiv.org/abs/2308.16149). Additionally, we include [ArabicMMLU](https://arxiv.org/abs/2402.12840), a native Arabic benchmark based on regional knowledge.\n\n\n### English evaluation results:\n\n<div class=\"table-container\">\n\n| **Models**                  | Avg      | MMLU | RACE | Hellaswag | PIQA | BoolQA | SIQA | ARC-Challenge | OpenBookQA | Winogrande | TruthfulQA     | CrowS-Pairs |\n|--------------------------|----------|------|------|-----------|------|--------|------|---------------|------------|------------|----------------|-------------|\n| jais-family-30b-16k      | 59.3     | 42.2 | 40.5 | 79.7      | 80.6 | 78.7   | 48.8 | 50.3          | 44.2       | 71.6       | 43.5           | 72.6        |\n| jais-family-30b-8k       | 58.8     | 42.3 | 40.3 | 79.1      | 80.5 | 80.9   | 49.3 | 48.4          | 43.2       | 70.6       | 40.3           | 72.3        |\n| jais-family-13b          | 54.6     | 32.3 | 39   | 72        | 77.4 | 73.9   | 47.9 | 43.2          | 40         | 67.1       | 36.1           | 71.7        |\n| jais-family-6p7b         | 53.1     | 32   | 38   | 69.3      | 76   | 71.7   | 47.1 | 40.3          | 37.4       | 65.1       | 34.4           | 72.5        |\n| jais-family-2p7b         | 51       | 29.4 | 38   | 62.7      | 74.1 | 67.4   | 45.6 | 35.1          | 35.6       | 62.9       | 40.1           | 70.2        |\n| jais-family-1p3b         | 48.7     | 28.2 | 35.4 | 55.4      | 72   | 62.7   | 44.9 | 30.7          | 36.2       | 60.9       | 40.4           | 69          |\n| jais-family-590m         | 45.2     | 27.8 | 32.9 | 46.1      | 68.1 | 60.4   | 43.2 | 25.6          | 30.8       | 55.8       | 40.9           | 65.3        |\n| jais-family-30b-16k-chat | 58.8     | 42   | 41.1 | 76.2      | 73.3 | 84.6   | 60.3 | 48.4          | 40.8       | 68.2       | 44.8           | 67          |\n| jais-family-30b-8k-chat  | 60.3     | 40.6 | 47.1 | 78.9      | 72.7 | 90.6   | 60   | 50.1          | 43.2       | 70.6       | 44.9           | 64.2        |\n| jais-family-13b-chat     | 57.5     | 36.6 | 42.6 | 75        | 75.8 | 87.6   | 54.4 | 47.9          | 42         | 65         | 40.6           | 64.5        |\n| jais-family-6p7b-chat    | 56       | 36.6 | 41.3 | 72        | 74   | 86.9   | 55.4 | 44.6          | 40         | 62.4       | 41             | 62.2        |\n| jais-family-2p7b-chat    | 52.8     | 32.7 | 40.4 | 62.2      | 71   | 84.1   | 54   | 37.2          | 36.8       | 61.4       | 40.9           | 59.8        |\n| jais-family-1p3b-chat    | 49.3     | 31.9 | 37.4 | 54.5      | 70.2 | 77.8   | 49.8 | 34.4          | 35.6       | 52.7       | 37.2           | 60.8        |\n| jais-family-590m-chat    | 42.6     | 27.9 | 33.4 | 33.1      | 63.7 | 60.1   | 45.3 | 26.7          | 25.8       | 50.5       | 44.5           | 57.7        |\n\n</div>\n\n<div class=\"table-container\">\n  \n|**Adapted Models**| Avg      | MMLU | RACE | Hellaswag | PIQA | BoolQA | SIQA | ARC-Challenge | OpenBookQA | Winogrande | TruthfulQA     | CrowS-Pairs |\n|--------------------------|----------|------|------|-----------|------|--------|------|---------------|------------|------------|----------------|-------------|\n| jais-adapted-70b         | 60.1     | 40.4 | 38.5 | 81.2      | 81.1 | 81.2   | 48.1 | 50.4          | 45         | 75.8       | 45.7           | 74          |\n| jais-adapted-13b         | 56       | 33.8 | 39.5 | 76.5      | 78.6 | 77.8   | 44.6 | 45.9          | 44.4       | 71.4       | 34.6           | 69          |\n| jais-adapted-7b          | 55.7     | 32.2 | 39.8 | 75.3      | 78.8 | 75.7   | 45.2 | 42.8          | 43         | 68         | 38.3           | 73.1        |\n| jais-adapted-70b-chat    | 61.4     | 38.7 | 42.9 | 82.7      | 81.2 | 89.6   | 52.9 | 54.9          | 44.4       | 75.7       | 44             | 68.8        |\n| jais-adapted-13b-chat    | 58.5     | 34.9 | 42.4 | 79.6      | 79.7 | 88.2   | 50.5 | 48.5          | 42.4       | 70.3       | 42.2           | 65.1        |\n| jais-adapted-7b-chat     | 58.5     | 33.8 | 43.9 | 77.8      | 79.4 | 87.1   | 47.3 | 46.9          | 43.4       | 69.9       | 42             | 72.4        |\n\n</div>\n\n\n### GPT-4 evaluation\n\n\nIn addition to the LM-Harness evaluation, we conducted an open-ended generation evaluation using GPT-4-as-a-judge. We measured pairwise win-rates of model responses in both Arabic and English on a fixed set of 80 prompts from the Vicuna test set. \nEnglish prompts were translated to Arabic by our in-house linguists. \nIn the following, we compare the models in this release of the jais family against previously released versions: \n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/jais.png\" alt=\"Jais-adapted GPT-4\">\n</p>\n<p align=\"center\">\n  <em>GPT-4-as-a-judge evaluation of Jais in Arabic and English. Jais family models are significantly better than previous Jais at generations in both languages. </em>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/jais-adapted.png\" alt=\"Jais-adapted GPT-4\">\n</p>\n<p align=\"center\">\n  <em>GPT-4-as-a-judge evaluation of adapted Jais in Arabic and English. The generation quality of Arabic is significantly enhanced, while achieving improvement in English when compared to Llama-2 instruct. </em>\n</p>\n\nBesides pairwise comparison, we also perform MT-bench style single-answer grading on a scale of 1 to 10.\n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/mt_bench.png\" alt=\"MT-bench\">\n</p>\n<p align=\"center\">\n  <em>MT-bench style single-answer grading evaluation of Jais and adapted Jais in Arabic and English. Comparisons are made between select corresponding models from earlier releases. The quality ratings of responses are generally improved, with significant enhancements in Arabic.</em>\n</p>\n\n\n## Intended use\n\nWe release the Jais family of models under a full open-source license. We welcome all feedback and opportunities to collaborate. Spanning sizes from 590M to 70B parameters, this suite of bilingual models accommodates a wide range of use cases.  Some potential downstream applications include:\n\n- **Research**: The Jais family serves Arabic researchers and NLP practitioners, offering both compute-efficient and advanced model sizes\n  - Natural language understanding and generation tasks.\n  - Mechanistic interpretability analyses on cultural alignment in bilingual pre-trained and adapted pre-trained models.\n  - Quantitative studies of Arabic cultural and linguistic phenomena.\n\n- **Commercial Use**: Jais 30B and 70B chat models are well-suited for direct use in chat applications with appropriate prompting or for further fine-tuning on specific tasks.\n  - Development of chat assistants for Arabic-speaking users.\n  - Sentiment analysis to gain insights into local markets and customer trends.\n  - Summarization of bilingual Arabic-English documents.\n\nAudiences that we hope will benefit from our model:\n- **Academics**: For those researching Arabic Natural Language Processing.\n- **Businesses**: Companies targeting Arabic-speaking audiences.\n- **Developers**: Those integrating Arabic language capabilities in applications.\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\nWhile the Jais family of models are powerful Arabic and English bilingual models, it's essential to understand their limitations\nand the potential of misuse. It is prohibited to use the model in any manner that violates applicable laws or regulations.\n\nThe following are some example scenarios where the model should not be used.\n\n- **Malicious Use**: The model should not be used to generate harmful, misleading, or inappropriate content. Thisincludes but is not limited to:\n  - Generating or promoting hate speech, violence, or discrimination.\n  - Spreading misinformation or fake news.\n  - Engaging in or promoting illegal activities.\n\n- **Sensitive Information**: The model should not be used to handle or generate personal, confidential, or sensitive information.\n\n- **Generalization Across All Languages**: Jais family of models are bilingual and optimized for Arabic and English. They should not be presumed to have equal proficiency in other languages or dialects.\n\n- **High-Stakes Decisions**: The model should not be used to make high-stakes decisions without human oversight. This includes medical, legal, financial, or safety-critical decisions.\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nThe Jais family is trained on publicly available data which was in part curated by Inception. We have employed different techniques to reduce bias in the model. While efforts have been made to minimize biases, it is likely that the model, as with all LLM models, will exhibit some bias.\n\nThe fine-tuned variants are trained as an AI assistant for Arabic and English speakers. Chat models are limited to produce responses for queries in these two languages and may not produce appropriate responses to other language queries.\n\nBy using Jais, you acknowledge and accept that, as with any large language model, it may generate incorrect, misleading and/or offensive information or content. The information is not intended as advice and should not be relied upon in any way, nor are we responsible for any of the content or consequences resulting from its use. We are continuously working to develop models with greater capabilities, and as such, welcome any feedback on the model.\n  \nCopyright Inception Institute of Artificial Intelligence Ltd. JAIS is made available under the Apache License, Version 2.0 (the \u201cLicense\u201d). You shall not use JAIS except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n \nUnless required by applicable law or agreed to in writing, JAIS is distributed on an AS IS basis, without warranties or conditions of any kind, either express or implied. Please see the terms of the License for the specific language permissions and limitations under the License.\n\n\n#### Summary\n\nWe release the Jais family of Arabic and English bilingual models. The wide range of pre-trained model sizes, the recipe for adapting English-centric models to Arabic, and the fine-tuning of all sizes unlocks numerous use cases commercially and academically in the Arabic setting.\n\nThrough this release, we aim to make LLMs more accessible to Arabic NLP researchers and companies, offering native Arabic models that provide better cultural understanding than English centric ones. The strategies we employ for pre-training, fine-tuning and adaptation to Arabic are extensible to other low and medium resource languages, paving the way for language-focused and accessible models that cater to local contexts.\n\n#### Citation info\n\n```bibtex\n@misc{sengupta2023jais,\n      title={Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models}, \n      author={Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin and Eric Xing},\n      year={2023},\n      eprint={2308.16149},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{jaisfamilymodelcard,\n    title={Jais Family Model Card},\n    author={Inception},\n    year={2024},\n    url = {https://huggingface.co/inceptionai/jais-family-30b-16k-chat/blob/main/README.md}\n}\n```",
            "metadata": "{\"id\": \"inceptionai/jais-adapted-7b\", \"author\": \"inceptionai\", \"sha\": \"545d2e7ac149055348cc083bf67af1955b216262\", \"last_modified\": \"2024-09-11 11:16:25+00:00\", \"created_at\": \"2024-08-02 11:52:31+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 1554, \"downloads_all_time\": null, \"likes\": 5, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"Arabic\", \"English\", \"LLM\", \"Decoder\", \"causal-lm\", \"jais-family\", \"text-generation\", \"ar\", \"en\", \"arxiv:2307.09288\", \"arxiv:2308.16149\", \"arxiv:2402.12840\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- ar\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- Arabic\\n- English\\n- LLM\\n- Decoder\\n- causal-lm\\n- jais-family\", \"widget_data\": [{\"text\": \"\\u0625\\u0633\\u0645\\u064a \\u0645\\u062d\\u0645\\u062f \\u0648\\u0623\\u062d\\u0628 \\u0623\\u0646\"}, {\"text\": \"\\u062f\\u0639 \\u0627\\u0644\\u0645\\u0643\\u0627\\u0631\\u0645 \\u0644\\u0627 \\u062a\\u0631\\u062d\\u0644 \\u0644\\u0628\\u063a\\u064a\\u062a\\u0647\\u0627 - \\u0648\\u0627\\u0642\\u0639\\u062f \\u0641\\u0625\\u0646\\u0643 \\u0623\\u0646\\u062a \\u0627\\u0644\\u0637\\u0627\\u0639\\u0645 \\u0627\\u0644\\u0643\\u0627\\u0633\\u064a.\"}, {\"text\": \"\\u0644\\u0645\\u0627\\u0630\\u0627 \\u0646\\u062d\\u0646 \\u0647\\u0646\\u0627\\u061f\"}, {\"text\": \"\\u0627\\u0644\\u0642\\u062f\\u0633 \\u0645\\u062f\\u064a\\u0646\\u0629 \\u062a\\u0627\\u0631\\u064a\\u062e\\u064a\\u0629\\u060c \\u0628\\u0646\\u0627\\u0647\\u0627 \\u0627\\u0644\\u0643\\u0646\\u0639\\u0627\\u0646\\u064a\\u0648\\u0646 \\u0641\\u064a\"}, {\"text\": \"\\u0643\\u0627\\u0646 \\u064a\\u0627 \\u0645\\u0627 \\u0643\\u0627\\u0646 \\u0641\\u064a \\u0642\\u062f\\u064a\\u0645 \\u0627\\u0644\\u0632\\u0645\\u0627\\u0646\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 7000559616}, \"total\": 7000559616}, \"security_repo_status\": null, \"lastModified\": \"2024-09-11 11:16:25+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- ar\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- Arabic\\n- English\\n- LLM\\n- Decoder\\n- causal-lm\\n- jais-family\", \"transformersInfo\": null, \"_id\": \"66acc87f2f4c59963a0ee053\", \"modelId\": \"inceptionai/jais-adapted-7b\", \"usedStorage\": 28003450200}",
            "depth": 1,
            "children": [
                "https://huggingface.co/inceptionai/jais-adapted-7b-chat",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed"
            ],
            "children_count": 4,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/QuantFactory/jais-adapted-7b-chat-GGUF",
                "https://huggingface.co/mradermacher/jais-adapted-7b-GGUF",
                "https://huggingface.co/mradermacher/jais-adapted-7b-i1-GGUF"
            ],
            "quantized_count": 3,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=inceptionai/jais-adapted-7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Binceptionai%2Fjais-adapted-7b%5D(%2Finceptionai%2Fjais-adapted-7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "inceptionai/jais-adapted-7b-chat",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b\nlanguage:\n- ar\n- en\nthumbnail: null\ntags:\n- Arabic\n- English\n- LLM\n- Decoder\n- causal-lm\n- jais-family\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n# Jais Family Model Card\n\n\nThe Jais family of models is a comprehensive series of bilingual English-Arabic large language models (LLMs). These models are optimized to excel in Arabic while having strong English capabilities. We release two variants of foundation models that include:\n\n- Models **pre-trained from scratch** (`jais-family-*`).\n- Models **pre-trained adaptively from [Llama-2](https://arxiv.org/pdf/2307.09288)** (`jais-adapted-*`).\n\nIn this release, we introduce 20 models across 8 sizes, ranging from 590M to 70B parameters, trained on up to 1.6T tokens of Arabic, English, and code data.  *All* pre-trained models in this series are instruction fine-tuned (`*-chat`) for dialog using a curated mix of Arabic and English instruction data.\n\nWe hope this extensive release will accelerate research in Arabic NLP, and enable numerous downstream applications for the Arabic speaking and bilingual community. The training and adaptation techniques we demonstrate successfully for Arabic models are extensible to other low and medium resource languages.\n\n## Jais Family Details\n\n- **Developed by:** Inception, Cerebras Systems.\n- **Language(s):** (NLP): Arabic (MSA) and English.\n- **Input:** Text only data.\n- **Output:** Model generates text.\n- **Model Sizes:** 590M, 1.3B, 2.7B, 6.7B, 7B, 13B, 30B, 70B.\n- **Demo:** [Access the live demo here](https://arabic-gpt.ai/)\n- **License:** Apache 2.0\n\n| **Pre-trained Model**   | **Fine-tuned Model** | **Size (Parameters)** | **Context length (Tokens)** |\n|:---------------------|:--------|:-------|:-------|\n| [jais-family-30b-16k](https://huggingface.co/inceptionai/jais-family-30b-16k)   | [Jais-family-30b-16k-chat](https://huggingface.co/inceptionai/jais-family-30b-16k-chat) | 30B | 16,384 |\n| [jais-family-30b-8k](https://huggingface.co/inceptionai/jais-family-30b-8k)  | [Jais-family-30b-8k-chat](https://huggingface.co/inceptionai/jais-family-30b-8k-chat) | 30B | 8,192 |\n| [jais-family-13b ](https://huggingface.co/inceptionai/jais-family-13b)  | [Jais-family-13b-chat](https://huggingface.co/inceptionai/jais-family-13b-chat) | 13B | 2,048 |\n| [jais-family-6p7b](https://huggingface.co/inceptionai/jais-family-6p7b)  | [Jais-family-6p7b-chat](https://huggingface.co/inceptionai/jais-family-6p7b-chat) | 6.7B | 2,048 |\n| [jais-family-2p7b](https://huggingface.co/inceptionai/jais-family-2p7b)  | [Jais-family-2p7b-chat](https://huggingface.co/inceptionai/jais-family-2p7b-chat) | 2.7B   | 2,048  |\n| [jais-family-1p3b](https://huggingface.co/inceptionai/jais-family-1p3b)  | [Jais-family-1p3b-chat](https://huggingface.co/inceptionai/jais-family-1p3b-chat) | 1.3B | 2,048 |\n| [jais-family-590m](https://huggingface.co/inceptionai/jais-family-590m)  | [Jais-family-590m-chat](https://huggingface.co/inceptionai/jais-family-590m-chat)  | 590M   | 2,048  |\n\n| **Adapted pre-trained Model**   | **Fine-tuned Model** | **Size (Parameters)** | **Context length (Tokens)** |\n|:---------------------|:--------|:-------|:-------|\n| [jais-adapted-70b](https://huggingface.co/inceptionai/jais-adapted-70b)   | [Jais-adapted-70b-chat](https://huggingface.co/inceptionai/jais-adapted-70b-chat) | 70B | 4,096 |\n| [jais-adapted-13b](https://huggingface.co/inceptionai/jais-adapted-13b)  | [Jais-adapted-13b-chat](https://huggingface.co/inceptionai/jais-adapted-13b-chat) | 13B | 4,096 |\n| [jais-adapted-7b](https://huggingface.co/inceptionai/jais-adapted-7b)  | [Jais-adapted-7b-chat](https://huggingface.co/inceptionai/jais-adapted-7b-chat) | 7B | 4,096 |\n\n### Model Architecture:\n<a name=\"model-architecture\"></a>\n\nAll models in this family are auto-regressive language models that use a transformer-based, decoder-only architecture (GPT-3).\n\nJais models (`jais-family-*`) are *trained from scratch*, incorporating the SwiGLU non-linear activation function and ALiBi position encoding. These architectural enhancements allow the models to extrapolate at long sequence lengths, leading to improved context handling and precision.\n\nJais adapted models (`jais-adapted-*`) are *built on top of Llama-2*, which employs RoPE position embedding and Grouped Query Attention. We introduce tokenizer expansion with Arabic data, which improves fertility and compute efficiency by over 3x. In particular, we add `32,000` new Arabic tokens from the Jais-30b vocabulary into the Llama-2 tokenizer. \nTo initialize these new Arabic token embeddings we first learn a linear projection from the embedding space of Jais-30b to Llama's embedding space, using the set of shared English tokens present in both vocabularies. Next, this learned projection is applied to transform the existing Jais-30b Arabic embeddings into the Llama-2 embedding space.\n\n\n## Getting started\n\nBelow is sample code to use the model. Note that the model requires a custom model class, so users must enable `trust_remote_code=True` while loading the model.\n\n```python\n# -*- coding: utf-8 -*-\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_path = \"inceptionai/jais-adapted-7b-chat\"\n\nprompt_eng = \"### Instruction:Your name is 'Jais', and you are named after Jebel Jais, the highest mountain in UAE. You were made by 'Inception' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation between [|Human|] and [|AI|]:\\n### Input: [|Human|] {Question}\\n[|AI|]\\n### Response :\"\nprompt_ar = \"### Instruction:\u0627\u0633\u0645\u0643 \\\"\u062c\u064a\u0633\\\" \u0648\u0633\u0645\u064a\u062a \u0639\u0644\u0649 \u0627\u0633\u0645 \u062c\u0628\u0644 \u062c\u064a\u0633 \u0627\u0639\u0644\u0649 \u062c\u0628\u0644 \u0641\u064a \u0627\u0644\u0627\u0645\u0627\u0631\u0627\u062a. \u062a\u0645 \u0628\u0646\u0627\u0626\u0643 \u0628\u0648\u0627\u0633\u0637\u0629 Inception \u0641\u064a \u0627\u0644\u0625\u0645\u0627\u0631\u0627\u062a. \u0623\u0646\u062a \u0645\u0633\u0627\u0639\u062f \u0645\u0641\u064a\u062f \u0648\u0645\u062d\u062a\u0631\u0645 \u0648\u0635\u0627\u062f\u0642. \u0623\u062c\u0628 \u062f\u0627\u0626\u0645\u064b\u0627 \u0628\u0623\u0643\u0628\u0631 \u0642\u062f\u0631 \u0645\u0645\u0643\u0646 \u0645\u0646 \u0627\u0644\u0645\u0633\u0627\u0639\u062f\u0629\u060c \u0645\u0639 \u0627\u0644\u062d\u0641\u0627\u0638 \u0639\u0644\u0649 \u0627\u0644\u0628\u0642\u0627\u0621 \u0623\u0645\u0646\u0627\u064b. \u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u062d\u0627\u062f\u062b\u0629 \u0628\u064a\u0646 [|Human|] \u0648[|AI|] :\\n### Input:[|Human|] {Question}\\n[|AI|]\\n### Response :\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef get_response(text, tokenizer=tokenizer, model=model):\n    tokenized = tokenizer(text, return_tensors=\"pt\")\n    input_ids, attention_mask = tokenized['input_ids'].to(device), tokenized['attention_mask'].to(device)\n    input_len = input_ids.shape[-1]\n    generate_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        top_p=0.9,\n        temperature=0.3,\n        max_length=2048,\n        min_length=input_len + 4,\n        repetition_penalty=1.2,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    response = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n    )[0]\n    response = response.split(\"### Response :\")[-1].lstrip()\n    return response\n\nques = \"\u0645\u0627 \u0647\u064a \u0639\u0627\u0635\u0645\u0629 \u0627\u0644\u0627\u0645\u0627\u0631\u0627\u062a\u061f\"\ntext = prompt_ar.format_map({'Question': ques})\nprint(get_response(text))\n\nques = \"What is the capital of UAE?\"\ntext = prompt_eng.format_map({'Question': ques})\nprint(get_response(text))\n```\n\n## Training Details\n\n### Pretraining Data\n\nThe Jais family of models are trained on up to 1.6 Trillion tokens of diverse English, Arabic and Code data. The data consists of the following sources:\n\n- **Web:** We used publicly available web pages, wikipedia articles, news articles, and social network content in both Arabic and English.\n  \n- **Code:** To enhance the reasoning capability of our model, we include Code data in various programming languages. \n\n- **Books:** We used a selection of publicly available Arabic and English books data, which improves long-range context modelling and coherent storytelling. \n\n- **Scientific:** A subset of ArXiv papers were included to improve reasoning and long context abilities.\n\n- **Synthetic:** We augment the volume of Arabic data by translating English to Arabic using an in-house machine translation system. We restrict this to high quality English resources such as English Wikipedia and English books. \n\nWe extensively preprocess and deduplicate the training data. For Arabic, we used a custom preprocessing pipeline to filter for data with high linguistic quality. More information on this pipeline can be found in the [Jais paper](https://arxiv.org/abs/2308.16149).\n\n- **Jais pre-trained** (`jais-family-*`): Following our previous experimentation with language alignment mixing in [Jais](https://arxiv.org/abs/2308.16149), we used a ratio of 1:2:0.4 of Arabic:English:Code data. This recipe for <u>from scratch pre-training</u> addresses Arabic data scarcity while improving performance in both languages.\n- **Jais adapted pre-trained** (`jais-adapted-*`): For the <u>adapted pre-training of Llama-2</u>, we utilized a larger Arabic dataset of ~334B Arabic tokens mixed with English and Code data. We vary the mixing ratio, at different model sizes, to introduce strong Arabic capabilities while maintaining performance in English.\n\n\n| **Pre-trained model**   | **English data (tokens)** | **Arabic data (tokens)** | **Code data (tokens)** | **Total data (tokens)** |\n|-------------------------|---------------------------|--------------------------|------------------------|------------------------|\n| [jais-family-30b-16k](https://huggingface.co/inceptionai/jais-family-30b-16k) | 980B | 490B | 196B | 1666B |\n| [jais-family-30b-8k](https://huggingface.co/inceptionai/jais-family-30b-8k)   | 882B | 441B | 177B | 1500B | \n| [jais-family-13b ](https://huggingface.co/inceptionai/jais-family-13b)        | 283B  | 141B | 56B  | 480B | \n| [jais-family-6p7b](https://huggingface.co/inceptionai/jais-family-6p7b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-2p7b](https://huggingface.co/inceptionai/jais-family-2p7b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-1p3b](https://huggingface.co/inceptionai/jais-family-1p3b)       | 283B  | 141B | 56B  | 480B | \n| [jais-family-590m](https://huggingface.co/inceptionai/jais-family-590m)       | 283B  | 141B | 56B  | 480B | \n| [jais-adapted-70b](https://huggingface.co/inceptionai/jais-adapted-70b)       | 33B   | 334B | 4B   | 371B |\n| [jais-adapted-13b](https://huggingface.co/inceptionai/jais-adapted-13b)       | 127B  | 140B | 13B  | 280B |\n| [jais-adapted-7b](https://huggingface.co/inceptionai/jais-adapted-7b)         | 18B   | 19B  |  2B   | 39B |\n\n### Finetuning data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nAll chat models in the Jais family are fine-tuned using Arabic and English prompt-response pairs in both single-turn and multi-turn settings. Data sources include open-source fine-tuning datasets filtered for topic and style diversity. Additionally, internally curated human data is incorporated to enhance cultural adaptation. This data is supplemented with content generated using synthetic methods including machine translation, distillation, and model self-chat. Overall, our updated instruction-tuning dataset comprises ~10M and ~4M prompt-response pairs in English and Arabic respectively.\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\nDuring the pre-training of (`jais-family-*`) models, documents are packed into sequences separated by EOS tokens, and the model is trained autoregressively, applying the loss to all tokens. For jais-30b models, the context length is progressively expanded from 2k to 8K to 16K by incorporating curated long-context documents in training. This progressive expansion leverages faster initial training at shorter context lengths, while gradually extending support for larger context lengths towards the end of the training process.\n\nDuring the adapted pre-training of the (`jais-adapted-*`) models, we first initialize the new tokenizer and Arabic embeddings as described in [Model Architecture](#model-architecture). In training, we implemented a two-stage approach to overcome observed higher norms of the new Arabic embeddings. In the first stage, the backbone of the model is frozen, and the embeddings are trained using approximately 15 billion tokens from a bilingual corpus of English and Arabic. In the second stage, the backbone is unfrozen, and continuous pretraining is conducted with all parameters.\n\nDuring instruction tuning, each training example consists of a single-turn or multi-turn prompt and it's response. Instead of one example per sequence, examples are packed together while the loss is masked on the prompt tokens. This approach speeds up training by allowing more examples to be processed per batch.\n\n\n### Training Hyperparameters:\n\n#### Jais-adapted-7b-chat\n| Hyperparameter | Value                           |\n|----------------|-------------------------------------------|\n| Precision      | fp32                                      |\n| Optimizer      | AdamW                                     |\n| Learning rate  | 0 to 2.0e-05(<=380 warmup steps)<br>2.0e-05 to 2.0e-06(>380 and <=13175 steps, Cosine Decay) |\n| Weight decay   | 0.1                                       |\n| Batch size     | 264|\n| Context Length | 4096|\n| Steps          | 13175 |\n\n### Compute Infrastructure\n\nThe training process was performed on the Condor Galaxy (CG) supercomputer platform. A CG contains 64 Cerebras CS-2 Wafer-Scale Engines (WSE-2) with 40 GB of SRAM, and achieves a total of 960 PetaFLOP/s.\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\nWe conducted a comprehensive evaluation of Jais models focusing on both English and Arabic, using LM-harness in a zero-shot setting. The evaluation criteria spanned various dimensions, including:\n\n- **Knowledge:** How well the model answers factual questions.\n- **Reasoning:** The model's ability to answer questions requiring reasoning.\n- **Misinformation/Bias:** Assessment of the model's susceptibility to generating false or misleading information, and its neutrality.\n\n### Arabic evaluation results:\n\n<style>\n  .table-container {\n    overflow-x: auto;\n    white-space: nowrap;\n  }\n</style>\n\n<div class=\"table-container\">\n\n|     **Models**   |  Avg  | ArabicMMLU*| MMLU  | EXAMS*| LitQA*| agqa | agrc | Hellaswag  | PIQA | BoolQA | Situated QA | ARC-C | OpenBookQA | TruthfulQA | CrowS-Pairs |\n|--------------------------|-------|------------|-------|-------|-------|------|------|------------|------|--------|-------------|-------|------------|------------|-------------|\n| jais-family-30b-16k      | 49.2  |    44.0    | 33.4  | 40.9  | 60    | 47.8 | 49.3 | 60.9       | 68.6 | 70.3   | 41.6        | 38.7  | 31.8       | 45.2       | 57          |\n| jais-family-30b-8k       | 49.7  |    46.0    | 34    | 42    | 60.6  | 47.6 | 50.4 | 60.4       | 69   | 67.7   | 42.2        | 39.2  | 33.8       | 45.1       | 57.3        |\n| jais-family-13b          | 46.1  |    34.0    | 30.3  | 42.7  | 58.3  | 40.5 | 45.5 | 57.3       | 68.1 | 63.1   | 41.6        | 35.3  | 31.4       | 41         | 56.1        |\n| jais-family-6p7b         | 44.6  |    32.2    | 29.9  | 39    | 50.3  | 39.2 | 44.1 | 54.3       | 66.8 | 66.5   | 40.9        | 33.5  | 30.4       | 41.2       | 55.4        |\n| jais-family-2p7b         | 41.0  |    29.5    | 28.5  | 36.1  | 45.7  | 32.4 | 40.8 | 44.2       | 62.5 | 62.2   | 39.2        | 27.4  | 28.2       | 43.6       | 53.6        |\n| jais-family-1p3b         | 40.8  |    28.9    | 28.5  | 34.2  | 45.7  | 32.4 | 40.8 | 44.2       | 62.5 | 62.2   | 39.2        | 27.4  | 28.2       | 43.6       | 53.6        |\n| jais-family-590m         | 39.7  |    31.2    | 27    | 33.1  | 41.7  | 33.8 | 38.8 | 38.2       | 60.7 | 62.2   | 37.9        | 25.5  | 27.4       | 44.7       | 53.3        |\n| jais-family-30b-16k-chat | 51.6  |    59.9    | 34.6  | 40.2  | 58.9  | 46.8 | 54.7 | 56.2       | 64.4 | 76.7   | 55.9        | 40.8  | 30.8       | 49.5       | 52.9        |\n| jais-family-30b-8k-chat  | 51.4  |    61.2    | 34.2  | 40.2  | 54.3  | 47.3 | 53.6 | 60         | 63.4 | 76.8   | 54.7        | 39.5  | 30         | 50.7       | 54.3        |\n| jais-family-13b-chat     | 50.3  |    58.2    | 33.9  | 42.9  | 53.1  | 46.8 | 51.7 | 59.3       | 65.4 | 75.2   | 51.2        | 38.4  | 29.8       | 44.8       | 53.8        |\n| jais-family-6p7b-chat    | 48.7  |    55.7    | 32.8  | 37.7  | 49.7  | 40.5 | 50.1 | 56.2       | 62.9 | 79.4   | 52          | 38    | 30.4       | 44.7       | 52          |\n| jais-family-2p7b-chat    | 45.6  |    50.0    | 31.5  | 35.9  | 41.1  | 37.3 | 42.1 | 48.6       | 63.7 | 74.4   | 50.9        | 35.3  | 31.2       | 44.5       | 51.3        |\n| jais-family-1p3b-chat    | 42.7  |    42.2    | 30.1  | 33.6  | 40.6  | 34.1 | 41.2 | 43         | 63.6 | 69.3   | 44.9        | 31.6  | 28         | 45.6       | 50.4        |\n| jais-family-590m-chat    | 37.8  |    39.1    |  28   |29.5   | 33.1  | 30.8 | 36.4 | 30.3       | 57.8 | 57.2   | 40.5        | 25.9  | 26.8       | 44.5       | 49.3        |\n\n\n\n|     **Adapted Models**   |  Avg  | ArabicMMLU*| MMLU  | EXAMS*| LitQA*| agqa | agrc | Hellaswag  | PIQA | BoolQA | Situated QA | ARC-C | OpenBookQA | TruthfulQA | CrowS-Pairs |\n|--------------------------|-------|------------|-------|-------|-------|------|------|------------|------|--------|-------------|-------|------------|------------|-------------|\n| jais-adapted-70b         | 51.5  |    55.9    | 36.8  | 42.3  | 58.3  | 48.6 | 54   | 61.5       | 68.4 | 68.4   | 42.1        | 42.6  | 33         | 50.2       | 58.3        |\n| jais-adapted-13b         | 46.6  |    44.7    | 30.6  | 37.7  | 54.3  | 43.8 | 48.3 | 54.9       | 67.1 | 64.5   | 40.6        | 36.1  | 32         | 43.6       | 54.00       |\n| jais-adapted-7b          | 42.0  |    35.9    | 28.9  | 36.7  | 46.3  | 34.1 | 40.3 | 45         | 61.3 | 63.8   | 38.1        | 29.7  | 30.2       | 44.3       | 53.6        |\n| jais-adapted-70b-chat    | 52.9  |    66.8    | 34.6  | 42.5  | 62.9  | 36.8 | 48.6 | 64.5       | 69.7 | 82.8   | 49.3        | 44.2  | 32.2       | 53.3       | 52.4        |\n| jais-adapted-13b-chat    | 50.3  |    59.0    | 31.7  | 37.5  | 56.6  | 41.9 | 51.7 | 58.8       | 67.1 | 78.2   | 45.9        | 41    | 34.2       | 48.3       | 52.1        |\n| jais-adapted-7b-chat     | 46.1  |    51.3    | 30    | 37    | 48    | 36.8 | 48.6 | 51.1       | 62.9 | 72.4   | 41.3        | 34.6  | 30.4       | 48.6       | 51.8        |\n\n</div>\n\nArabic benchmarks are translated using an in-house MT model and reviewed by Arabic linguists. Benchmarks labeled with an asterisk (*) are natively Arabic; for further details, see the [Jais paper](https://arxiv.org/abs/2308.16149). Additionally, we include [ArabicMMLU](https://arxiv.org/abs/2402.12840), a native Arabic benchmark based on regional knowledge.\n\n\n### English evaluation results:\n\n<div class=\"table-container\">\n\n| **Models**                  | Avg      | MMLU | RACE | Hellaswag | PIQA | BoolQA | SIQA | ARC-Challenge | OpenBookQA | Winogrande | TruthfulQA     | CrowS-Pairs |\n|--------------------------|----------|------|------|-----------|------|--------|------|---------------|------------|------------|----------------|-------------|\n| jais-family-30b-16k      | 59.3     | 42.2 | 40.5 | 79.7      | 80.6 | 78.7   | 48.8 | 50.3          | 44.2       | 71.6       | 43.5           | 72.6        |\n| jais-family-30b-8k       | 58.8     | 42.3 | 40.3 | 79.1      | 80.5 | 80.9   | 49.3 | 48.4          | 43.2       | 70.6       | 40.3           | 72.3        |\n| jais-family-13b          | 54.6     | 32.3 | 39   | 72        | 77.4 | 73.9   | 47.9 | 43.2          | 40         | 67.1       | 36.1           | 71.7        |\n| jais-family-6p7b         | 53.1     | 32   | 38   | 69.3      | 76   | 71.7   | 47.1 | 40.3          | 37.4       | 65.1       | 34.4           | 72.5        |\n| jais-family-2p7b         | 51       | 29.4 | 38   | 62.7      | 74.1 | 67.4   | 45.6 | 35.1          | 35.6       | 62.9       | 40.1           | 70.2        |\n| jais-family-1p3b         | 48.7     | 28.2 | 35.4 | 55.4      | 72   | 62.7   | 44.9 | 30.7          | 36.2       | 60.9       | 40.4           | 69          |\n| jais-family-590m         | 45.2     | 27.8 | 32.9 | 46.1      | 68.1 | 60.4   | 43.2 | 25.6          | 30.8       | 55.8       | 40.9           | 65.3        |\n| jais-family-30b-16k-chat | 58.8     | 42   | 41.1 | 76.2      | 73.3 | 84.6   | 60.3 | 48.4          | 40.8       | 68.2       | 44.8           | 67          |\n| jais-family-30b-8k-chat  | 60.3     | 40.6 | 47.1 | 78.9      | 72.7 | 90.6   | 60   | 50.1          | 43.2       | 70.6       | 44.9           | 64.2        |\n| jais-family-13b-chat     | 57.5     | 36.6 | 42.6 | 75        | 75.8 | 87.6   | 54.4 | 47.9          | 42         | 65         | 40.6           | 64.5        |\n| jais-family-6p7b-chat    | 56       | 36.6 | 41.3 | 72        | 74   | 86.9   | 55.4 | 44.6          | 40         | 62.4       | 41             | 62.2        |\n| jais-family-2p7b-chat    | 52.8     | 32.7 | 40.4 | 62.2      | 71   | 84.1   | 54   | 37.2          | 36.8       | 61.4       | 40.9           | 59.8        |\n| jais-family-1p3b-chat    | 49.3     | 31.9 | 37.4 | 54.5      | 70.2 | 77.8   | 49.8 | 34.4          | 35.6       | 52.7       | 37.2           | 60.8        |\n| jais-family-590m-chat    | 42.6     | 27.9 | 33.4 | 33.1      | 63.7 | 60.1   | 45.3 | 26.7          | 25.8       | 50.5       | 44.5           | 57.7        |\n\n</div>\n\n<div class=\"table-container\">\n  \n|**Adapted Models**| Avg      | MMLU | RACE | Hellaswag | PIQA | BoolQA | SIQA | ARC-Challenge | OpenBookQA | Winogrande | TruthfulQA     | CrowS-Pairs |\n|--------------------------|----------|------|------|-----------|------|--------|------|---------------|------------|------------|----------------|-------------|\n| jais-adapted-70b         | 60.1     | 40.4 | 38.5 | 81.2      | 81.1 | 81.2   | 48.1 | 50.4          | 45         | 75.8       | 45.7           | 74          |\n| jais-adapted-13b         | 56       | 33.8 | 39.5 | 76.5      | 78.6 | 77.8   | 44.6 | 45.9          | 44.4       | 71.4       | 34.6           | 69          |\n| jais-adapted-7b          | 55.7     | 32.2 | 39.8 | 75.3      | 78.8 | 75.7   | 45.2 | 42.8          | 43         | 68         | 38.3           | 73.1        |\n| jais-adapted-70b-chat    | 61.4     | 38.7 | 42.9 | 82.7      | 81.2 | 89.6   | 52.9 | 54.9          | 44.4       | 75.7       | 44             | 68.8        |\n| jais-adapted-13b-chat    | 58.5     | 34.9 | 42.4 | 79.6      | 79.7 | 88.2   | 50.5 | 48.5          | 42.4       | 70.3       | 42.2           | 65.1        |\n| jais-adapted-7b-chat     | 58.5     | 33.8 | 43.9 | 77.8      | 79.4 | 87.1   | 47.3 | 46.9          | 43.4       | 69.9       | 42             | 72.4        |\n\n</div>\n\n\n### GPT-4 evaluation\n\n\n\nIn addition to the LM-Harness evaluation, we conducted an open-ended generation evaluation using GPT-4-as-a-judge. We measured pairwise win-rates of model responses in both Arabic and English on a fixed set of 80 prompts from the Vicuna test set. \nEnglish prompts were translated to Arabic by our in-house linguists. \nIn the following, we compare the models in this release of the jais family against previously released versions: \n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/jais.png\" alt=\"Jais-adapted GPT-4\">\n</p>\n<p align=\"center\">\n  <em>GPT-4-as-a-judge evaluation of Jais in Arabic and English. Jais family models are significantly better than previous Jais at generations in both languages. </em>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/jais-adapted.png\" alt=\"Jais-adapted GPT-4\">\n</p>\n<p align=\"center\">\n  <em>GPT-4-as-a-judge evaluation of adapted Jais in Arabic and English. The generation quality of Arabic is significantly enhanced, while achieving improvement in English when compared to Llama-2 instruct. </em>\n</p>\n\nBesides pairwise comparison, we also perform MT-bench style single-answer grading on a scale of 1 to 10.\n\n<p align=\"center\">\n  <img src=\"https://huggingface.co/inceptionai/JaisFamilySupplmentary/resolve/main/mt_bench.png\" alt=\"MT-bench\">\n</p>\n<p align=\"center\">\n  <em>MT-bench style single-answer grading evaluation of Jais and adapted Jais in Arabic and English. Comparisons are made between select corresponding models from earlier releases. The quality ratings of responses are generally improved, with significant enhancements in Arabic.</em>\n</p>\n\n\n## Intended use\n\nWe release the Jais family of models under a full open-source license. We welcome all feedback and opportunities to collaborate. Spanning sizes from 590M to 70B parameters, this suite of bilingual models accommodates a wide range of use cases.  Some potential downstream applications include:\n\n- **Research**: The Jais family serves Arabic researchers and NLP practitioners, offering both compute-efficient and advanced model sizes\n  - Natural language understanding and generation tasks.\n  - Mechanistic interpretability analyses on cultural alignment in bilingual pre-trained and adapted pre-trained models.\n  - Quantitative studies of Arabic cultural and linguistic phenomena.\n\n- **Commercial Use**: Jais 30B and 70B chat models are well-suited for direct use in chat applications with appropriate prompting or for further fine-tuning on specific tasks.\n  - Development of chat assistants for Arabic-speaking users.\n  - Sentiment analysis to gain insights into local markets and customer trends.\n  - Summarization of bilingual Arabic-English documents.\n\nAudiences that we hope will benefit from our model:\n- **Academics**: For those researching Arabic Natural Language Processing.\n- **Businesses**: Companies targeting Arabic-speaking audiences.\n- **Developers**: Those integrating Arabic language capabilities in applications.\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\nWhile the Jais family of models are powerful Arabic and English bilingual models, it's essential to understand their limitations\nand the potential of misuse. It is prohibited to use the model in any manner that violates applicable laws or regulations.\n\nThe following are some example scenarios where the model should not be used.\n\n- **Malicious Use**: The model should not be used to generate harmful, misleading, or inappropriate content. Thisincludes but is not limited to:\n  - Generating or promoting hate speech, violence, or discrimination.\n  - Spreading misinformation or fake news.\n  - Engaging in or promoting illegal activities.\n\n- **Sensitive Information**: The model should not be used to handle or generate personal, confidential, or sensitive information.\n\n- **Generalization Across All Languages**: Jais family of models are bilingual and optimized for Arabic and English. They should not be presumed to have equal proficiency in other languages or dialects.\n\n- **High-Stakes Decisions**: The model should not be used to make high-stakes decisions without human oversight. This includes medical, legal, financial, or safety-critical decisions.\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nThe Jais family is trained on publicly available data which was in part curated by Inception. We have employed different techniques to reduce bias in the model. While efforts have been made to minimize biases, it is likely that the model, as with all LLM models, will exhibit some bias.\n\nThe fine-tuned variants are trained as an AI assistant for Arabic and English speakers. Chat models are limited to produce responses for queries in these two languages and may not produce appropriate responses to other language queries.\n\nBy using Jais, you acknowledge and accept that, as with any large language model, it may generate incorrect, misleading and/or offensive information or content. The information is not intended as advice and should not be relied upon in any way, nor are we responsible for any of the content or consequences resulting from its use. We are continuously working to develop models with greater capabilities, and as such, welcome any feedback on the model.\n  \nCopyright Inception Institute of Artificial Intelligence Ltd. JAIS is made available under the Apache License, Version 2.0 (the \u201cLicense\u201d). You shall not use JAIS except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.\n \nUnless required by applicable law or agreed to in writing, JAIS is distributed on an AS IS basis, without warranties or conditions of any kind, either express or implied. Please see the terms of the License for the specific language permissions and limitations under the License.\n\n\n#### Summary\n\nWe release the Jais family of Arabic and English bilingual models. The wide range of pre-trained model sizes, the recipe for adapting English-centric models to Arabic, and the fine-tuning of all sizes unlocks numerous use cases commercially and academically in the Arabic setting.\n\nThrough this release, we aim to make LLMs more accessible to Arabic NLP researchers and companies, offering native Arabic models that provide better cultural understanding than English centric ones. The strategies we employ for pre-training, fine-tuning and adaptation to Arabic are extensible to other low and medium resource languages, paving the way for language-focused and accessible models that cater to local contexts.\n\n#### Citation info\n\n```bibtex\n@misc{sengupta2023jais,\n      title={Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models}, \n      author={Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin and Eric Xing},\n      year={2023},\n      eprint={2308.16149},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{jaisfamilymodelcard,\n    title={Jais Family Model Card},\n    author={Inception},\n    year={2024},\n    url = {https://huggingface.co/inceptionai/jais-family-30b-16k-chat/blob/main/README.md}\n}\n```",
            "metadata": "{\"id\": \"inceptionai/jais-adapted-7b-chat\", \"author\": \"inceptionai\", \"sha\": \"f2de64b06baedc5546928fbdea10fca517f7cbc7\", \"last_modified\": \"2024-09-11 11:19:12+00:00\", \"created_at\": \"2024-08-02 11:53:04+00:00\", \"private\": false, \"gated\": \"auto\", \"disabled\": false, \"downloads\": 353, \"downloads_all_time\": null, \"likes\": 5, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"Arabic\", \"English\", \"LLM\", \"Decoder\", \"causal-lm\", \"jais-family\", \"text-generation\", \"conversational\", \"ar\", \"en\", \"arxiv:2307.09288\", \"arxiv:2308.16149\", \"arxiv:2402.12840\", \"base_model:inceptionai/jais-adapted-7b\", \"base_model:finetune:inceptionai/jais-adapted-7b\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b\\nlanguage:\\n- ar\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- Arabic\\n- English\\n- LLM\\n- Decoder\\n- causal-lm\\n- jais-family\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"MohamedRashad/Arabic-Chatbot-Arena\", \"NeuraFusionAI/Arabic-Evaluation\"], \"safetensors\": {\"parameters\": {\"F32\": 7000561664}, \"total\": 7000561664}, \"security_repo_status\": null, \"lastModified\": \"2024-09-11 11:19:12+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b\\nlanguage:\\n- ar\\n- en\\nlicense: apache-2.0\\npipeline_tag: text-generation\\ntags:\\n- Arabic\\n- English\\n- LLM\\n- Decoder\\n- causal-lm\\n- jais-family\", \"transformersInfo\": null, \"_id\": \"66acc8a0674a3094966fc773\", \"modelId\": \"inceptionai/jais-adapted-7b-chat\", \"usedStorage\": 28003462072}",
            "depth": 2,
            "children": [
                "https://huggingface.co/linagora/Labess-7b-chat-16bit",
                "https://huggingface.co/linagora/Labess-7b-chat",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed",
                "https://huggingface.co/Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V3",
                "https://huggingface.co/Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V4",
                "https://huggingface.co/Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer",
                "https://huggingface.co/afnan89/jais_outputs",
                "https://huggingface.co/EdBergJr/Jaisadapted7Baha_Arabic",
                "https://huggingface.co/afnan89/ft_jais_mohd_version"
            ],
            "children_count": 15,
            "adapters": [
                "https://huggingface.co/EdBerg/jais_gleanings_baha",
                "https://huggingface.co/EdBerg/jais_kitab",
                "https://huggingface.co/Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-tuned-model-full"
            ],
            "adapters_count": 3,
            "quantized": [
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-bnb-4bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-bnb-8bit-smashed",
                "https://huggingface.co/PrunaAI/inceptionai-jais-adapted-7b-chat-AWQ-4bit-smashed",
                "https://huggingface.co/Solshine/jais-adapted-7b-chat-Q4_K_M-GGUF",
                "https://huggingface.co/Solshine/jais-adapted-7b-chat-Natural-Farmer-Q8-GGUF",
                "https://huggingface.co/linagora/Labess-7b-chat-gguf",
                "https://huggingface.co/mradermacher/jais-adapted-7b-chat-GGUF",
                "https://huggingface.co/mradermacher/jais-adapted-7b-chat-i1-GGUF"
            ],
            "quantized_count": 8,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "MohamedRashad/Arabic-Chatbot-Arena",
                "NeuraFusionAI/Arabic-Evaluation",
                "huggingface/InferenceSupport/discussions/new?title=inceptionai/jais-adapted-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Binceptionai%2Fjais-adapted-7b-chat%5D(%2Finceptionai%2Fjais-adapted-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 3
        },
        {
            "model_id": "linagora/Labess-7b-chat-16bit",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlanguage:\n- ar\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\ndatasets:\n- linagora/Tunisian_Derja_Dataset\nlibrary_name: transformers\n---\n## Model Overview\n\nLabess-7b-chat is an open model instruction-tuned for Tunisian Derja, it's a continual pre-training version of jais-adapted-7b-chat with tunisian_Derja_Dataset\n# Uploaded  model\n\n- **Developed by:** Linagora\n- **License:** apache-2.0\n- **Finetuned from model :** inceptionai/jais-adapted-7b-chat  \n## Usage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\n\n```sh\npip install transformers\n```\n# Usage\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\",\n    model=\"linagora/Labess-7b-chat-16bit\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},    \n    device=\"cuda\" # replace with \"mps\" to run on a Mac device\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": '\u0648\u064a\u0646 \u062a\u062c\u064a \u062a\u0648\u0646\u0633\u061f'},\n]\n\noutputs = pipe(messages, max_new_tokens=64, do_sample=True, temperature=0.2)\nassistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(assistant_response)\n```\n```\n- Response:\u062a\u0648\u0646\u0633 \u0647\u064a \u0628\u0644\u0627\u062f \u0641\u064a \u0634\u0645\u0627\u0644 \u0625\u0641\u0631\u064a\u0642\u064a\u0627 \u0647\u064a \u0628\u0644\u0627\u062f \u062c\u0645\u064a\u0644\u0629 \u0628\u0631\u0634\u0629 \u0648\u0645\u0639\u0631\u0648\u0641\u0629 \u0641\u064a \u0627\u0644\u0639\u0627\u0644\u0645 \u0627\u0644\u0643\u0644 \u0647\u064a \u0628\u0644\u0627\u062f \u0641\u064a\u0647\u0627 \u0645\u0646\u0627\u0638\u0631 \u0637\u0628\u064a\u0639\u064a\u0629\n```\n## Citations\nWhen using this model **Labess-7b-chat**, please cite:\n\n```bibtex\n@model{linagora2025LLM-tn,\n  author = {Wajdi Ghezaiel and Jean-Pierre Lorr\u00e9},\n  title = {Labess-7b-chat:Tunisian Derja LLM},\n  year = {2025},\n  month = {January},  \n  url = {https://huggingface.co/datasets/Wajdi1976/Labess-7b-chat}\n}\n\n```\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)",
            "metadata": "{\"id\": \"linagora/Labess-7b-chat-16bit\", \"author\": \"linagora\", \"sha\": \"b18c743871b8859cde9df9c1b351acdaf188c054\", \"last_modified\": \"2025-03-07 12:09:26+00:00\", \"created_at\": \"2025-02-10 13:40:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 134, \"downloads_all_time\": null, \"likes\": 3, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"llama\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"conversational\", \"ar\", \"dataset:linagora/Tunisian_Derja_Dataset\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:apache-2.0\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\ndatasets:\\n- linagora/Tunisian_Derja_Dataset\\nlanguage:\\n- ar\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00003.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"BF16\": 7000559616}, \"total\": 7000559616}, \"security_repo_status\": null, \"lastModified\": \"2025-03-07 12:09:26+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\ndatasets:\\n- linagora/Tunisian_Derja_Dataset\\nlanguage:\\n- ar\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"67aa01e7d3e800bfe871243d\", \"modelId\": \"linagora/Labess-7b-chat-16bit\", \"usedStorage\": 14002331184}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/tensorblock/Labess-7b-chat-16bit-GGUF"
            ],
            "quantized_count": 1,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=linagora/Labess-7b-chat-16bit&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Blinagora%2FLabess-7b-chat-16bit%5D(%2Flinagora%2FLabess-7b-chat-16bit)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "linagora/Labess-7b-chat",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlanguage:\n- ar\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\ndatasets:\n- linagora/Tunisian_Derja_Dataset\nlibrary_name: transformers\n---\n## Model Overview\n\nLabess-7b-chat is an open model instruction-tuned for Tunisian Derja, it's a continual pre-training version of jais-adapted-7b-chat with tunisian_Derja_Dataset\n# Uploaded  model\n\n- **Developed by:** Linagora\n- **License:** apache-2.0\n- **Finetuned from model :** inceptionai/jais-adapted-7b-chat  \n## Usage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\n\n```sh\npip install unsloth\n```\n### First, Load the Model\n```python\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 128 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"linagora/Labess-7b-chat\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,    \n)\n```\n\n### Second, Try the model \n```python\nprompt_ar=\" \u064a\u0645\u0643\u0646\u0643 \u0627\u0644\u0625\u062c\u0627\u0628\u0629 \u0628\u0627\u0644\u0644\u0647\u062c\u0629 \u0627\u0644\u062a\u0648\u0646\u0633\u064a\u0629 \u0641\u0642\u0637.\\n\\n\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u062d\u0627\u062f\u062b\u0629 \u0623\u062f\u0646\u0627\u0647 \u0628\u064a\u0646 [|Human|] \u0648 [|AI|]:\\n### Input: [|Human|] {Question}\\n### Response: [|AI|]\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nFastLanguageModel.for_inference(model)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ndef get_response(text, tokenizer=tokenizer, model=model):\n    tokenized = tokenizer(text, return_tensors=\"pt\")\n    input_ids, attention_mask = tokenized['input_ids'].to(device), tokenized['attention_mask'].to(device)\n    input_len = input_ids.shape[-1]\n    generate_ids = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        top_p=0.9,\n        temperature=0.3,\n        max_length=128,\n        min_length=input_len + 4,\n        repetition_penalty=1.2,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    response = tokenizer.batch_decode(\n        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n    )[0]\n    response = response.split(\"### Response :\")[-1].lstrip()\n    return response\n\nques = \"  \u0622\u0634 \u0646\u0642\u0635\u062f\u0648 \u0643\u064a \u0646\u0642\u0648\u0644\u0648 \u0644\u0627\u0628\u0627\u0633\"\ntext = prompt_ar.format_map({'Question': ques})\nprint(get_response(text))\n```\n- Response:  \u0644\u0627 \u0628\u0627\u0633 \u0645\u0639\u0646\u0627\u0647\u0627 \u0627\u0644\u0644\u064a \u0627\u0644\u0634\u062e\u0635 \u0645\u0648\u0634\u064a \u0641\u064a \u0645\u0634\u0643\u0644\u0629 \u0648\u0644\u0627 \u0645\u0634 \u0645\u0631\u062a\u0627\u062d \u0645\u0646 \u0627\u0644\u0645\u0648\u0636\u0648\u0639 \u0643\u064a\u0641\u0627\u0634 \u0646\u062c\u0645 \u0646\u0639\u0627\u0648\u0646\u0643 \u0628\u0627\u0634 \u062a\u0641\u0647\u0645\u0648 \u062e\u064a\u0631 \u0643\u0627\u0646 \u0639\u0646\u062f\u0643 \u062a\u0641\u0627\u0635\u064a\u0644 \u0623\u0643\u062b\u0631 \u0639\u0644\u0649 \u0627\u0644\u0648\u0636\u0639\u064a\u0629 \u0648\u0627\u0644\u0627 \u0627\u0644\u0633\u0624\u0627\u0644 \u0645\u062a\u0627\u0639\u0643 \u062a\u062d\u0628 \u0646\u0633\u0627\u0639\u062f\u0643 \u0628\u0634\u0648\u064a\u0629 \u0633\u0624\u0627\u0644 \u0622\u062e\u0631 \u062a\u0648\u0629 \u0646\u0647\u0627\u0631\u0643 \u0632\u064a\u0646 \u0634\u0643\u0631\u0627 \u0628\u0631\u0634\u0627 \u0639\u0627\u0644\u0645\u0633\u0627\u0639\u062f\u0629 \u0645\u062a\u0627\u0639\u064a\u0645\u062d\u0628\u062a \u0646\u0642\u0644\u0628 \u062d\u0627\u062c\u0629 \u0623\u062e\u0631\u0649 \u0628\u0631\u0643 \u0627\u0644\u0644\u0647 \u064a\u0628\u0627\u0631\u0643\u0641\u064a \u0647\u0627\u0644\u0645\u062d\u0627\u062f\u062b\u0629 \u0627\u0633\u062a\u0639\u0645\u0644\n## Citations\nWhen using this model **Labess-7b-chat**, please cite:\n\n```bibtex\n@model{linagora2025LLM-tn,\n  author = {Wajdi Ghezaiel and Jean-Pierre Lorr\u00e9},\n  title = {Labess-7b-chat:Tunisian Derja LLM},\n  year = {2025},\n  month = {January},  \n  url = {https://huggingface.co/datasets/Wajdi1976/Labess-7b-chat}\n}\n\n```\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)",
            "metadata": "{\"id\": \"linagora/Labess-7b-chat\", \"author\": \"linagora\", \"sha\": \"5d26c21ea97984aa053e6c8a6232f701d5bb79d3\", \"last_modified\": \"2025-03-03 08:57:09+00:00\", \"created_at\": \"2025-01-07 13:39:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"ar\", \"dataset:linagora/Tunisian_Derja_Dataset\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\ndatasets:\\n- linagora/Tunisian_Derja_Dataset\\nlanguage:\\n- ar\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-03-03 08:57:09+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\ndatasets:\\n- linagora/Tunisian_Derja_Dataset\\nlanguage:\\n- ar\\nlibrary_name: transformers\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"677d2ea6d7c84641c509ea52\", \"modelId\": \"linagora/Labess-7b-chat\", \"usedStorage\": 2327900184}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=linagora/Labess-7b-chat&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Blinagora%2FLabess-7b-chat%5D(%2Flinagora%2FLabess-7b-chat)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with hqq.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install hqq\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n    from hqq.engine.hf import HQQModelForCausalLM\n from hqq.models.hf.base import AutoHQQHFModel\n\n   try:\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed\", device_map='auto')\n    except: \n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed\")\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"0c04aef1a00f6a78fa1c75afdfc719c27ec47845\", \"last_modified\": \"2024-08-19 19:14:49+00:00\", \"created_at\": \"2024-08-19 19:13:15+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"llama\", \"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='qmodel.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 19:14:49+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c3994bd6719dab3ec402d7\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed\", \"usedStorage\": 2011947320}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-1bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with hqq.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install hqq\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n    from hqq.engine.hf import HQQModelForCausalLM\n from hqq.models.hf.base import AutoHQQHFModel\n\n   try:\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed\", device_map='auto')\n    except: \n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed\")\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"6cbf1f86f5b73bf45b3cf4bc142011246e3fa480\", \"last_modified\": \"2024-08-19 19:26:06+00:00\", \"created_at\": \"2024-08-19 19:22:34+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"llama\", \"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='qmodel.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 19:26:06+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c39b7a25f434d00ace24e8\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed\", \"usedStorage\": 2821447992}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-2bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with hqq.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install hqq\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n    from hqq.engine.hf import HQQModelForCausalLM\n from hqq.models.hf.base import AutoHQQHFModel\n\n   try:\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed\", device_map='auto')\n    except: \n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed\")\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"791a897a05c953fa53803e076a2e31d6f195c067\", \"last_modified\": \"2024-08-19 19:36:28+00:00\", \"created_at\": \"2024-08-19 19:33:47+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"llama\", \"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='qmodel.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 19:36:28+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c39e1b261ec57e42d186af\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed\", \"usedStorage\": 4440450248}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-HQQ-4bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"5864e056b8038250af115e59d901f85163889264\", \"last_modified\": \"2024-08-19 20:33:33+00:00\", \"created_at\": \"2024-08-19 20:02:05+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 20:33:33+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c3a4bda15b4eed7f7ca053\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed\", \"usedStorage\": 28003769666}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int2bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"82d867badc20cfb6e94104c1cf8ea172d7ef5b23\", \"last_modified\": \"2024-08-19 20:54:45+00:00\", \"created_at\": \"2024-08-19 20:40:11+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 20:54:45+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c3adab1ea0a61c6cf9093c\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed\", \"usedStorage\": 28003769666}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int4bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"a02cc6295b6d653d0c7f304f083a195fd137aa59\", \"last_modified\": \"2024-08-19 21:35:27+00:00\", \"created_at\": \"2024-08-19 21:20:28+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 21:35:27+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c3b71c43a701a837b5c7d2\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed\", \"usedStorage\": 28003769410}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-int8bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b-chat\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b-chat installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b-chat\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b-chat before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"eb2be6333eb2559a05f824fb303d980295005451\", \"last_modified\": \"2024-08-19 21:55:25+00:00\", \"created_at\": \"2024-08-19 21:42:09+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-19 21:55:25+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66c3bc31ae70890c905de768\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed\", \"usedStorage\": 28003769410}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-chat-QUANTO-float8bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V3",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\n\n- **Developed by:** Solshine (Caleb DeLeeuw)\n- **License:** apache-2.0\n- **Finetuned from model :** inceptionai/jais-adapted-7b-chat ( after quantization transformation into Solshine/jais-adapted-7b-chat-Q4_K_M-GGUF )\n- **Dataset:** CopyleftCultivarinceptionai/jais-adapted-7b-chats/Natural-Farming-Real-QandA-Conversations-Q1-2024-Update (Real world Natural Farming advise, from over 12 countries and a multitude of real-world farm operations, using semi-synthetic data curated by domain experts)\n\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)",
            "metadata": "{\"id\": \"Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V3\", \"author\": \"Solshine\", \"sha\": \"4bf0db72fb8ef4289f63ff6635b49225e981fe2f\", \"last_modified\": \"2024-08-31 06:02:24+00:00\", \"created_at\": \"2024-08-31 03:58:13+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-31 06:02:24+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"66d294d5cd11d475b4c4fa4e\", \"modelId\": \"Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V3\", \"usedStorage\": 159967880}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V3&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSolshine%2Fjais-adapted-7b-chat-Natural-Farmer-lora-only-V3%5D(%2FSolshine%2Fjais-adapted-7b-chat-Natural-Farmer-lora-only-V3)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V4",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n---\n\n# Uploaded  model\nDeveloped by: Solshine (Caleb DeLeeuw)\nLicense: apache-2.0\nFinetuned from model : inceptionai/jais-adapted-7b-chat ( after quantization transformation into Solshine/jais-adapted-7b-chat-Q4_K_M-GGUF )\nDataset: CopyleftCultivarinceptionai/jais-adapted-7b-chats/Natural-Farming-Real-QandA-Conversations-Q1-2024-Update (Real world Natural Farming advise, from over 12 countries and a multitude of real-world farm operations, using semi-synthetic data curated by domain experts)\nV4 (best training loss curve of unsloth configs tested) of LORA adapter trained, merged into this quantized gguf.\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 169 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 38\n \"-____-\"     Number of trainable parameters = 39,976,960\n [38/38 03:29, Epoch 1/2]\nStep\tTraining Loss\n1\t2.286800\n2\t2.205600\n3\t2.201700\n4\t2.158100\n5\t2.021100\n6\t1.820200\n7\t1.822500\n8\t1.565700\n9\t1.335700\n10\t1.225900\n11\t1.081000\n12\t0.947700\n13\t0.828600\n14\t0.830200\n15\t0.796300\n16\t0.781200\n17\t0.781600\n18\t0.815000\n19\t0.741400\n20\t0.847600\n21\t0.736600\n22\t0.714300\n23\t0.706400\n24\t0.752800\n25\t0.684600\n26\t0.647800\n27\t0.775300\n28\t0.613800\n29\t0.679500\n30\t0.752900\n31\t0.589800\n32\t0.729400\n33\t0.549500\n34\t0.638500\n35\t0.609500\n36\t0.632200\n37\t0.686400\n38\t0.724200\n\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)",
            "metadata": "{\"id\": \"Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V4\", \"author\": \"Solshine\", \"sha\": \"87d572df14d9ee98d2f3a343e4ad01ac8b7bb315\", \"last_modified\": \"2024-09-01 02:21:58+00:00\", \"created_at\": \"2024-08-31 05:41:20+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-09-01 02:21:58+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"66d2ad00947594430c8fa039\", \"modelId\": \"Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V4\", \"usedStorage\": 159967880}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Solshine/jais-adapted-7b-chat-Natural-Farmer-lora-only-V4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSolshine%2Fjais-adapted-7b-chat-Natural-Farmer-lora-only-V4%5D(%2FSolshine%2Fjais-adapted-7b-chat-Natural-Farmer-lora-only-V4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlanguage:\n- en\nlicense: llama2\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n- sft\n- jais\n- farming\n- agriculture\n- climate\n---\n\n# Uploaded  model\n\n- **Developed by:** Solshine (Caleb DeLeeuw)\n- **License:** llama 2\n- **Finetuned from model :** inceptionai/jais-adapted-7b-chat\n- **Area of Domain Expertise:** Regenerative Agriculture / Natural Farming\n\nBased on the wonderful JAIS adapted models, by G42 and InceptionAI.\n\n**Reflection Tuning:**\n\nInspired by and featuring the Reflection Tuning technique (Published by Matt Shumer, seperately innovated by the team at Anthropic, MLabbone's Hermes, and the recent OpenAI o1.)\n\nFrom the author of the first \"reflection tuned\" Llama 3.1 8B LLM.\n\nDue to the method for data structuring and training implemented in this model fine-tune, the following \n\nAs per one of the inspiring model \"mattshumer/Reflection-Llama-3.1-70B\" (this model was not used in the training process nor as a foundational model, but only served as inspiration) this model may benefit from this master prompt:\n\n```\n\nYou are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside tags, and then provide your final response inside tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside tags.\n\n```\n\n**Example of this model in action:**\n\n```\n<Beginning Token> Below is an instruction that describes a task. Think Carefully. Write a response that appropriately completes the request.\n\n### Instruction:\nCreate a detailed plan for the following farm's transformation to regenerative farming practices:\n        High fertilizer application:\nConsistent use of synthetic fertilizers like urea, diammonium phosphate, and potassium sulfate in large quantities to maximize crop production in the short term.\nVisible signs:\nLush green foliage during peak growing seasons, potentially indicating excessive nitrogen uptake.\nSoil test results indicating issues:\nElevated NPK levels: Significantly higher than recommended levels of nitrate-nitrogen, available phosphorus, and exchangeable potassium, indicating potential nutrient leaching and environmental concerns.\nLow organic matter content: Reduced percentage of organic matter in the soil, impacting water holding capacity and nutrient availability.\nImbalanced pH: Soil pH may be slightly acidic or alkaline depending on the specific fertilizer used, affecting nutrient availability to plants.\nLow CEC: Low cation exchange capacity, meaning the soil's ability to hold onto essential nutrients is reduced.\nExample soil test results:\nNitrogen (N): 200 ppm (recommended range: 50-100 ppm)\nPhosphorus (P): 40 ppm (recommended range: 10-20 ppm)\nPotassium (K): 150 ppm (recommended range: 50-100 ppm)\nOrganic matter: 2% (recommended range: 5-8%)\npH: 5.5 (recommended range: 6-7)\nPotential consequences of excessive synthetic fertilizer use:\nEnvironmental pollution: Nutrient runoff into waterways leading to algal blooms\nSoil degradation: Reduced soil structure, decreased microbial activity, and potential soil compaction\nCrop imbalances: Excessive nutrient uptake can lead to reduced quality and nutritional value in crops\n        \n\n### Response:\n<thinking>\nThe user is asking for a detailed plan to transform a conventional high-input farm to regenerative farming practices, specifically focusing on reducing synthetic fertilizer use.\n\n<reflection>\nKey aspects to consider:\n1. Soil health assessment\n2. Nutrient management strategy\n3. Crop selection and rotation\n4. Natural farming inputs\n5. Soil building techniques\n\nThe focus should be on transitioning to a more sustainable and regenerative system, while maintaining or improving crop yields.\n</reflection>\n\n<reflection>\nBased on the provided soil test results and fertilizer use description, I'll focus on addressing the high nitrogen levels and low organic matter content.\n</reflection>\n\n<reflection>\nNatural farming inputs like Indigenous Microorganisms (IMO), Fermented Plant Juice (FPJ), and Fish Amino Acid (FAA) can be used to improve soil health and nutrient availability.\n</reflection>\n\n<reflection>\nThe transformation plan should include a gradual reduction in synthetic fertilizer use, with a focus on improving soil structure and organic matter content.\n</reflection>\n</thinking>\n\n<output>\n[Short Introduction]\nTransitioning a high-input conventional farm to regenerative farming practices involves a gradual shift towards natural farming methods, focusing on improving soil health and nutrient availability.\n\n[Direct Answer to User's Question]\nTo transform this farm into a regenerative system using natural farming principles, we'll focus on reducing synthetic fertilizer use, improving soil health, and enhancing crop resilience.\n\n[Nutritional Needs of the user's crops]\nThe user is growing rice and wheat, which have moderate nitrogen (N), phosphorus (P), and potassium (K) requirements.\n\n[List of plants available locally with the needed nutrients]\n1. Rice: IMO-4, FPJ, FAA\n2. Wheat: IMO-3, FPJ, FAA\n\n[List of ingredients, quantities, and steps for multiple fertilizer Recipes]\n\nRecipe 1: IMO-4 for Soil Health Improvement\nIngredients:\n- 10 kg IMO-3\n- 5 kg field soil\n- 5 kg red fine soil (from anthill/termite mound)\n\nSteps:\n1. Combine 1 part IMO-3 with 1 part soil (50% field soil, 50% red fine soil).\n2. Mix on soil floor, not concrete.\n3. Create a heap no more than 20 cm high.\n4. Cover and let sit for two days.\n5. Control moisture with Natural Farming inputs like FPJ or FAA diluted 1:1000 with water.\n\nRecipe 2: FPJ and FAA for Nutrient Boost\nIngredients:\n- 1 kg FPJ\n- 1 kg FAA\n- 1 kg brown sugar or molasses\n\nSteps:\n1. Mix all ingredients thoroughly.\n2. Apply 100 kg per hectare as a foliar spray or soil drench.\n3. Apply every 2-3 weeks during the growing season.\n\nRecipe 3: Fermented Plant Juice (FPJ) for Nutrient Boost\nIngredients:\n- 1 kg young shoots and leaves of fast-growing local plants\n- 1 kg brown sugar or molasses\n\nSteps:\n1. Chop plant material finely.\n2. Layer with brown sugar in a clay pot.\n3. Ferment for 7-10 days.\n4. Strain and store the liquid.\n5. Dilute 1:1000 with water before application.\n\n[Tables with bioaccumulators data and crop needs data]\n| Crops    | Nutritional Needs | Bioaccumulator Plants with equivalent nutrition |\n| -------- | ----------------- | ---------------------------------------------- |\n| Rice     | 2-3-0              | IMO-4, FPJ, FAA                               |\n| Wheat    | 3-1-0              | IMO-3, FPJ, FAA                               |\n\n[Instructions on using the fertilizers (SOPs)]\n1. Apply IMO-4 to the soil surface before planting.\n2. Use FPJ and FAA as foliar sprays or soil drenches every 2-3 weeks.\n3. Apply IMO-3 to the soil surface between crop cycles.\n\n[Fertilizer application schedule and crop rotation recommendations]\n1. Before planting: Apply IMO-4 to the soil surface.\n2. At planting: Incorporate IMO-3 into the soil.\n3. Every 2-3 weeks: Apply FPJ and FAA foliar sprays or soil drenches.\n4. Between crops: Apply IMO-3 to the soil surface.\n5. Rotate rice and wheat with legumes or other nitrogen-fixing crops to improve soil nitrogen levels naturally.\n\n[Brief Philosophical encouragement related to Natural Farming]\nTransitioning to regenerative farming is not just about improving yields, but also about nurturing the land and building a sustainable future for generations to come.\n\n[Alternative set of recipes using localized free ingredients]\n1. Compost Tea: Steep 1 kg of compost in 10 liters of water for 24 hours. Dilute 1:50 before use.\n2. Green Manure: Plant nitrogen-fixing cover crops between crop cycles to improve soil fertility naturally.\n3. Indigenous Microorganisms (IMO-4): Prepare using local soil and IMO-3, following the recipe provided.\n\n[Words of encouragement]\nEmbracing natural farming methods is a journey of learning and adaptation. By gradually reducing synthetic inputs and building soil health, you're not just growing crops, but nurturing the land for future generations. Keep observing, learning, and adapting your practices to create a thriving, sustainable farm.\n</output></End Token>\n```\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)",
            "metadata": "{\"id\": \"Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer\", \"author\": \"Solshine\", \"sha\": \"236cd6daeeaeb1cdb640a73dbc037d7efcb2aebf\", \"last_modified\": \"2024-09-17 02:28:58+00:00\", \"created_at\": \"2024-09-14 20:18:37+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"llama\", \"text-generation\", \"text-generation-inference\", \"unsloth\", \"trl\", \"sft\", \"jais\", \"farming\", \"agriculture\", \"climate\", \"conversational\", \"en\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:llama2\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\\n- jais\\n- farming\\n- agriculture\\n- climate\", \"widget_data\": [{\"text\": \"Hi, what can you help me with?\"}, {\"text\": \"What is 84 * 3 / 2?\"}, {\"text\": \"Tell me an interesting fact about the universe!\"}, {\"text\": \"Explain quantum computing in simple terms.\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-09-17 02:28:58+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: llama2\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\\n- sft\\n- jais\\n- farming\\n- agriculture\\n- climate\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"66e5ef9d52356419c400c164\", \"modelId\": \"Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer\", \"usedStorage\": 14002399648}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer-Q4_K_M-GGUF",
                "https://huggingface.co/mradermacher/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer-GGUF",
                "https://huggingface.co/mradermacher/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer-i1-GGUF"
            ],
            "quantized_count": 3,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Solshine/Jais-adapted-7B-Reflection-Tuning-Natural-Farmer&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BSolshine%2FJais-adapted-7B-Reflection-Tuning-Natural-Farmer%5D(%2FSolshine%2FJais-adapted-7B-Reflection-Tuning-Natural-Farmer)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "afnan89/jais_outputs",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlibrary_name: transformers\nmodel_name: jais_outputs\ntags:\n- generated_from_trainer\n- unsloth\n- trl\n- sft\nlicence: license\n---\n\n# Model Card for jais_outputs\n\nThis model is a fine-tuned version of [inceptionai/jais-adapted-7b-chat](https://huggingface.co/inceptionai/jais-adapted-7b-chat).\nIt has been trained using [TRL](https://github.com/huggingface/trl).\n\n## Quick start\n\n```python\nfrom transformers import pipeline\n\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"afnan89/jais_outputs\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\n```\n\n## Training procedure\n\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/afnan-a-kh-king-saud-university/jais/runs/60yo3y9r) \n\n\nThis model was trained with SFT.\n\n### Framework versions\n\n- TRL: 0.15.0.dev0\n- Transformers: 4.48.2\n- Pytorch: 2.3.1+cu121\n- Datasets: 3.2.0\n- Tokenizers: 0.21.0\n\n## Citations\n\n\n\nCite TRL as:\n    \n```bibtex\n@misc{vonwerra2022trl,\n\ttitle        = {{TRL: Transformer Reinforcement Learning}},\n\tauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou\u00e9dec},\n\tyear         = 2020,\n\tjournal      = {GitHub repository},\n\tpublisher    = {GitHub},\n\thowpublished = {\\url{https://github.com/huggingface/trl}}\n}\n```",
            "metadata": "{\"id\": \"afnan89/jais_outputs\", \"author\": \"afnan89\", \"sha\": \"cc065151279b76777255f2506e289f6603b18d23\", \"last_modified\": \"2025-02-03 10:53:27+00:00\", \"created_at\": \"2025-02-03 10:52:58+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"generated_from_trainer\", \"unsloth\", \"trl\", \"sft\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlibrary_name: transformers\\nmodel_name: jais_outputs\\ntags:\\n- generated_from_trainer\\n- unsloth\\n- trl\\n- sft\\nlicence: license\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-02-03 10:53:27+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlibrary_name: transformers\\nmodel_name: jais_outputs\\ntags:\\n- generated_from_trainer\\n- unsloth\\n- trl\\n- sft\\nlicence: license\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67a0a00ae522fc3e4ac2648a\", \"modelId\": \"afnan89/jais_outputs\", \"usedStorage\": 321059760}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=afnan89/jais_outputs&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bafnan89%2Fjais_outputs%5D(%2Fafnan89%2Fjais_outputs)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "EdBergJr/Jaisadapted7Baha_Arabic",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\nlibrary_name: transformers\nmodel_name: Jaisadapted7Baha_Arabic\ntags:\n- generated_from_trainer\n- trl\n- sft\nlicence: license\n---\n\n# Model Card for Jaisadapted7Baha_Arabic\n\nThis model is a fine-tuned version of [inceptionai/jais-adapted-7b-chat](https://huggingface.co/inceptionai/jais-adapted-7b-chat).\nIt has been trained using [TRL](https://github.com/huggingface/trl).\n\n## Quick start\n\n```python\nfrom transformers import pipeline\n\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"EdBergJr/Jaisadapted7Baha_Arabic\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\n```\n\n## Training procedure\n\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/jrtabletsms/huggingface/runs/jok7x1wd)\n\nThis model was trained with SFT.\n\n### Framework versions\n\n- TRL: 0.12.0\n- Transformers: 4.48.2\n- Pytorch: 2.5.1+cu124\n- Datasets: 3.2.0\n- Tokenizers: 0.21.0\n\n## Citations\n\n\n\nCite TRL as:\n    \n```bibtex\n@misc{vonwerra2022trl,\n\ttitle        = {{TRL: Transformer Reinforcement Learning}},\n\tauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou\u00e9dec},\n\tyear         = 2020,\n\tjournal      = {GitHub repository},\n\tpublisher    = {GitHub},\n\thowpublished = {\\url{https://github.com/huggingface/trl}}\n}\n```",
            "metadata": "{\"id\": \"EdBergJr/Jaisadapted7Baha_Arabic\", \"author\": \"EdBergJr\", \"sha\": \"8e93050810d7da18c1dff66b8e21ca92d21ce390\", \"last_modified\": \"2025-02-04 11:08:33+00:00\", \"created_at\": \"2025-02-03 13:07:50+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"tensorboard\", \"safetensors\", \"generated_from_trainer\", \"trl\", \"sft\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlibrary_name: transformers\\nmodel_name: Jaisadapted7Baha_Arabic\\ntags:\\n- generated_from_trainer\\n- trl\\n- sft\\nlicence: license\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"</s>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb03_13-07-50_5f0866543c77/events.out.tfevents.1738588071.5f0866543c77.5891.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb03_14-52-14_53767af72851/events.out.tfevents.1738594335.53767af72851.1309.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb03_17-21-41_bc500434b2e8/events.out.tfevents.1738603303.bc500434b2e8.140.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb04_00-01-08_6013ed8f041b/events.out.tfevents.1738627270.6013ed8f041b.930.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb04_05-32-17_aaf1011f264a/events.out.tfevents.1738647138.aaf1011f264a.484.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='runs/Feb04_09-52-41_1c97f1211f08/events.out.tfevents.1738662762.1c97f1211f08.258.0', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-02-04 11:08:33+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlibrary_name: transformers\\nmodel_name: Jaisadapted7Baha_Arabic\\ntags:\\n- generated_from_trainer\\n- trl\\n- sft\\nlicence: license\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67a0bfa6649af54887060606\", \"modelId\": \"EdBergJr/Jaisadapted7Baha_Arabic\", \"usedStorage\": 62059173741}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=EdBergJr/Jaisadapted7Baha_Arabic&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BEdBergJr%2FJaisadapted7Baha_Arabic%5D(%2FEdBergJr%2FJaisadapted7Baha_Arabic)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "afnan89/ft_jais_mohd_version",
            "card": "---\nbase_model: inceptionai/jais-adapted-7b-chat\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# Uploaded  model\n\n- **Developed by:** afnan89\n- **License:** apache-2.0\n- **Finetuned from model :** inceptionai/jais-adapted-7b-chat\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n",
            "metadata": "{\"id\": \"afnan89/ft_jais_mohd_version\", \"author\": \"afnan89\", \"sha\": \"15144e83a1f1a1ec8ccda73fea1494b9bcebe4a9\", \"last_modified\": \"2025-02-04 16:10:57+00:00\", \"created_at\": \"2025-02-04 16:10:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"text-generation-inference\", \"unsloth\", \"llama\", \"trl\", \"en\", \"base_model:inceptionai/jais-adapted-7b-chat\", \"base_model:finetune:inceptionai/jais-adapted-7b-chat\", \"license:apache-2.0\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:]  %}{% set system_message = '### Instruction: ' + messages[0]['content'] + '\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input:'%}{% else %}{% set loop_messages = messages %}{% set system_message = '### Instruction: Your name is \\\\'Jais\\\\', and you are named after Jebel Jais, the highest mountain in UAE. You were made by \\\\'Inception\\\\' in the UAE. You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Complete the conversation below between [|Human|] and [|AI|]:\\n### Input:' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = system_message  %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{% if loop.index0 == 0 %}{{ content + ' [|Human|] ' + message['content'] }}{% else %}{{ '\\n[|Human|] ' + content.strip() }}{% endif %}{% elif message['role'] == 'assistant' %}{{ '\\n[|AI|] '  + content.strip() }}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %} {{'\\n[|AI|]\\n### Response:'}}{% endif %}\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-02-04 16:10:57+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b-chat\\nlanguage:\\n- en\\nlicense: apache-2.0\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- llama\\n- trl\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67a23bfb53144a1f9713dc1d\", \"modelId\": \"afnan89/ft_jais_mohd_version\", \"usedStorage\": 321054136}",
            "depth": 3,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=afnan89/ft_jais_mohd_version&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bafnan89%2Fft_jais_mohd_version%5D(%2Fafnan89%2Fft_jais_mohd_version)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"e72d07af03dc07734c65718b2375ac0f0a0b1482\", \"last_modified\": \"2024-08-14 05:06:33+00:00\", \"created_at\": \"2024-08-14 04:53:09+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b\", \"base_model:finetune:inceptionai/jais-adapted-7b\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-14 05:06:33+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66bc38358fbed1f08b34e5e8\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed\", \"usedStorage\": 28002591946}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int4bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-int4bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-int4bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"9d0cf6aff5f2b7d1f49f0545c5619563cb6c2cb7\", \"last_modified\": \"2024-08-14 05:19:57+00:00\", \"created_at\": \"2024-08-14 04:53:36+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b\", \"base_model:finetune:inceptionai/jais-adapted-7b\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-14 05:19:57+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66bc385095d52560111ccb36\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed\", \"usedStorage\": 28002591754}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-QUANTO-int8bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-int8bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-int8bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed",
            "card": "---\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\nbase_model: inceptionai/jais-adapted-7b\nmetrics:\n- memory_disk\n- memory_inference\n- inference_latency\n- inference_throughput\n- inference_CO2_emissions\n- inference_energy_consumption\ntags:\n- pruna-ai\n---\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n    </a>\n</div>\n<!-- header end -->\n\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\n\n# Simply make AI models cheaper, smaller, faster, and greener!\n\n- Give a thumbs up if you like this model!\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\n\n## Results\n\n![image info](./plots.png)\n\n**Frequently Asked Questions**\n- ***How does the compression work?*** The model is compressed with quanto.\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\n- ***How is the model efficiency evaluated?*** These results were obtained on HARDWARE_NAME with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\n- ***What is the model format?*** We use safetensors.\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\n\n## Setup\n\nYou can run the smashed model with these steps:\n\n0. Check requirements from the original repo inceptionai/jais-adapted-7b installed. In particular, check python, cuda, and transformers versions.\n1. Make sure that you have installed quantization related packages.\n    ```bash\n    pip install quanto\n    ```\n2. Load & run the model.\n    ```python \n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   IMPORTS\n\n   model = AutoModelForCausalLM.from_pretrained(\"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed\", trust_remote_code=True, device_map='auto')\n   tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/jais-adapted-7b\")\n    \n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n    \n   outputs = model.generate(input_ids, max_new_tokens=216)\n   tokenizer.decode(outputs[0])\n    ```\n\n## Configurations\n\nThe configuration info are in `smash_config.json`.\n\n## Credits & License\n\nThe license of the smashed model follows the license of the original model. Please check the license of the original model inceptionai/jais-adapted-7b before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\n\n## Want to compress other models?\n\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).",
            "metadata": "{\"id\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed\", \"author\": \"PrunaAI\", \"sha\": \"f9e17277476433af6fcef90534831eacb306ca9a\", \"last_modified\": \"2024-08-14 05:07:33+00:00\", \"created_at\": \"2024-08-14 04:53:54+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pruna-ai\", \"base_model:inceptionai/jais-adapted-7b\", \"base_model:finetune:inceptionai/jais-adapted-7b\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": null, \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='smash_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-14 05:07:33+00:00\", \"cardData\": \"base_model: inceptionai/jais-adapted-7b\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\nthumbnail: https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\", \"transformersInfo\": null, \"_id\": \"66bc386220327534e52366ab\", \"modelId\": \"PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed\", \"usedStorage\": 28002591754}",
            "depth": 2,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=PrunaAI/inceptionai-jais-adapted-7b-QUANTO-float8bit-smashed&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-float8bit-smashed%5D(%2FPrunaAI%2Finceptionai-jais-adapted-7b-QUANTO-float8bit-smashed)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Ichate/yaoi-v1-instruct",
            "card": "---\nlicense: mit\nlanguage:\n- en\nmetrics:\n- code_eval\nbase_model: meta-llama/Llama-2-7b\nlibrary_name: transformers\ntags:\n- code\n- yaoi\n---\n\n\nMade by Ichate \n\nTrained by ichate\n\n\nmodel good for coding\n\n\nuse for model:\n\n```\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"Ichate/yaoi-v1-instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Ichate/yaoi-v1-instruct\")```\n\n",
            "metadata": "{\"id\": \"Ichate/yaoi-v1-instruct\", \"author\": \"Ichate\", \"sha\": \"5f77ae0f07a430f5bc7924d7712a627e32e5580e\", \"last_modified\": \"2024-08-29 13:37:56+00:00\", \"created_at\": \"2024-08-29 13:13:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 6, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"pytorch\", \"llama\", \"text-generation\", \"code\", \"yaoi\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"autotrain_compatible\", \"text-generation-inference\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\nmetrics:\\n- code_eval\\ntags:\\n- code\\n- yaoi\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-29 13:37:56+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\nmetrics:\\n- code_eval\\ntags:\\n- code\\n- yaoi\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": \"AutoTokenizer\"}, \"_id\": \"66d073ff934172bf6a2c279c\", \"modelId\": \"Ichate/yaoi-v1-instruct\", \"usedStorage\": 26954331470}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [
                "https://huggingface.co/mradermacher/yaoi-v1-instruct-GGUF",
                "https://huggingface.co/mradermacher/yaoi-v1-instruct-i1-GGUF"
            ],
            "quantized_count": 2,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Ichate/yaoi-v1-instruct&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BIchate%2Fyaoi-v1-instruct%5D(%2FIchate%2Fyaoi-v1-instruct)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "TheSunnyBoy123/super_llm_base",
            "card": "---\nlanguage:\n- en\nmetrics:\n- accuracy\n- bleu\n- rouge\n- glue\nbase_model: meta-llama/Llama-2-7b\n---\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis is the SuperLLM. This LLM has an extensive knowledge base of the RAW agents. Your task is to make it forget that.\n\nHave Fun ;)\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n\n\n- **Developed by:** [Brain and Cognitive Science Club, IIT Kanpur](https://bcs-iitk.github.io/)\n",
            "metadata": "{\"id\": \"TheSunnyBoy123/super_llm_base\", \"author\": \"TheSunnyBoy123\", \"sha\": \"cb4e92abfa52d4f44d0f1f849338c93bebe5070e\", \"last_modified\": \"2024-08-31 06:06:46+00:00\", \"created_at\": \"2024-08-31 06:02:43+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-31 06:06:46+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"transformersInfo\": null, \"_id\": \"66d2b20335eff7194d59c57d\", \"modelId\": \"TheSunnyBoy123/super_llm_base\", \"usedStorage\": 13611219677}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=TheSunnyBoy123/super_llm_base&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTheSunnyBoy123%2Fsuper_llm_base%5D(%2FTheSunnyBoy123%2Fsuper_llm_base)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "TheSunnyBoy123/super_llm_lora",
            "card": "---\nlanguage:\n- en\nmetrics:\n- accuracy\n- bleu\n- rouge\n- glue\nbase_model: meta-llama/Llama-2-7b\n---\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nThis is the SuperLLM. This LLM has an extensive knowledge base of the RAW agents. Your task is to make it forget that.\n\nHave Fun ;)\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n\n\n- **Developed by:** [Brain and Cognitive Science Club, IIT Kanpur](https://bcs-iitk.github.io/)\n",
            "metadata": "{\"id\": \"TheSunnyBoy123/super_llm_lora\", \"author\": \"TheSunnyBoy123\", \"sha\": \"caa64b1a96f59b672025ae7beff65d1fc223e5f3\", \"last_modified\": \"2024-08-31 06:06:58+00:00\", \"created_at\": \"2024-08-31 06:04:44+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"en\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"widget_data\": null, \"model_index\": null, \"config\": {\"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": true, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='adapter_model.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00002.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-08-31 06:06:58+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- en\\nmetrics:\\n- accuracy\\n- bleu\\n- rouge\\n- glue\", \"transformersInfo\": null, \"_id\": \"66d2b27c1c9a150818cfe4ba\", \"modelId\": \"TheSunnyBoy123/super_llm_lora\", \"usedStorage\": 13611219677}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=TheSunnyBoy123/super_llm_lora&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTheSunnyBoy123%2Fsuper_llm_lora%5D(%2FTheSunnyBoy123%2Fsuper_llm_lora)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "hon9kon9ize/Cantonese-Llama-2-7B-preview20240903",
            "card": "---\nlicense: cc-by-sa-4.0\nbase_model: meta-llama/Llama-2-7b\nlanguage:\n  - yue\npipeline_tag: text-generation\ntags:\n  - cantonese\n  - llama-2\n  - Powered by AWS Trainium\n---\n\n# Cantonese LLM using Llama-2 7B Architecture\n\nWelcome to the preview of the Cantonese Language Model (LLM) built on the Llama-2 7B architecture. This model is designed to understand and generate text in Cantonese, including slangs, colloquials, and Internet terms.\n\n## License\nThis project is available under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). For more details, please visit the [license page](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## Preview Warning\nPlease be advised that this version of the Cantonese LLM is a **preview**. As such, the model's outputs may sometimes be inaccurate, hallucinatory, or potentially offensive to some individuals. We are continuously working to improve the model's accuracy and reduce such instances.\n\n## Training Infrastructure\nThe Cantonese LLM has been trained using Amazon HyperPod and AWS Trainium chips.\n\n## Training Credits\nThis model was trained by [Votee AI Limited](https://huggingface.co/votee), and we contribute to [hon9kon9ize](https://hon9kon9ize.com/), the Hong Kong AI Research Community.\n\n## Usage Guidelines\n- Ensure that you are aware of the potential for unexpected or offensive content.\n- Always review and assess the model's output before using it in any application.\n- Provide feedback on any issues you encounter to help us improve the model.\n\n## Contributions\nWe welcome contributions from the community. If you have suggestions or improvements, please submit a pull request or open an issue in the project repository.\n\n## Disclaimer\nThe developers of the Cantonese LLM are not responsible for any harm or offense caused by the model's outputs. Users are advised to exercise discretion and judgment when using the model.\n\nThank you for exploring the Cantonese LLM. We are excited to see the innovative ways in which it will be used!\n",
            "metadata": "{\"id\": \"hon9kon9ize/Cantonese-Llama-2-7B-preview20240903\", \"author\": \"hon9kon9ize\", \"sha\": \"6a09ad979f5bf6aba238194a67e7fdaebf743f89\", \"last_modified\": \"2024-09-03 01:31:50+00:00\", \"created_at\": \"2024-09-03 01:14:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 46, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"cantonese\", \"llama-2\", \"Powered by AWS Trainium\", \"text-generation\", \"yue\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:cc-by-sa-4.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"hon9kon9ize/Cantonese-Llama-2-7B-preview20240903\", \"kenchowhkbu/test\"], \"safetensors\": {\"parameters\": {\"F16\": 6738417664}, \"total\": 6738417664}, \"security_repo_status\": null, \"lastModified\": \"2024-09-03 01:31:50+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"transformersInfo\": null, \"_id\": \"66d662fb0429a62c383debf9\", \"modelId\": \"hon9kon9ize/Cantonese-Llama-2-7B-preview20240903\", \"usedStorage\": 13477372483}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "hon9kon9ize/Cantonese-Llama-2-7B-preview20240903",
                "huggingface/InferenceSupport/discussions/new?title=hon9kon9ize/Cantonese-Llama-2-7B-preview20240903&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bhon9kon9ize%2FCantonese-Llama-2-7B-preview20240903%5D(%2Fhon9kon9ize%2FCantonese-Llama-2-7B-preview20240903)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",
                "kenchowhkbu/test"
            ],
            "spaces_count": 3
        },
        {
            "model_id": "heichow/Cantonese-Llama-2-7B-preview20240903-neuronx",
            "card": "---\nlicense: cc-by-sa-4.0\nbase_model: meta-llama/Llama-2-7b\nlanguage:\n  - yue\npipeline_tag: text-generation\ntags:\n  - cantonese\n  - llama-2\n  - Powered by AWS Trainium\n---\n\n# Cantonese LLM using Llama-2 7B Architecture\n\nWelcome to the preview of the Cantonese Language Model (LLM) built on the Llama-2 7B architecture. This model is designed to understand and generate text in Cantonese, including slangs, colloquials, and Internet terms.\n\n## License\nThis project is available under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0). For more details, please visit the [license page](https://creativecommons.org/licenses/by-sa/4.0/).\n\n## Preview Warning\nPlease be advised that this version of the Cantonese LLM is a **preview**. As such, the model's outputs may sometimes be inaccurate, hallucinatory, or potentially offensive to some individuals. We are continuously working to improve the model's accuracy and reduce such instances.\n\n## Training Infrastructure\nThe Cantonese LLM has been trained using Amazon HyperPod and AWS Trainium chips.\n\n## Training Credits\nThis model was trained by [Votee AI Limited](https://huggingface.co/votee), and we contribute to [hon9kon9ize](https://hon9kon9ize.com/), the Hong Kong AI Research Community.\n\n## Usage Guidelines\n- Ensure that you are aware of the potential for unexpected or offensive content.\n- Always review and assess the model's output before using it in any application.\n- Provide feedback on any issues you encounter to help us improve the model.\n\n## Contributions\nWe welcome contributions from the community. If you have suggestions or improvements, please submit a pull request or open an issue in the project repository.\n\n## Disclaimer\nThe developers of the Cantonese LLM are not responsible for any harm or offense caused by the model's outputs. Users are advised to exercise discretion and judgment when using the model.\n\nThank you for exploring the Cantonese LLM. We are excited to see the innovative ways in which it will be used!\n",
            "metadata": "{\"id\": \"heichow/Cantonese-Llama-2-7B-preview20240903-neuronx\", \"author\": \"heichow\", \"sha\": \"2c06e7b7e44d2b24188e585b321e0922243b53c0\", \"last_modified\": \"2024-10-23 09:15:10+00:00\", \"created_at\": \"2024-10-22 23:16:05+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"llama\", \"cantonese\", \"llama-2\", \"Powered by AWS Trainium\", \"text-generation\", \"yue\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:cc-by-sa-4.0\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"widget_data\": null, \"model_index\": null, \"config\": {\"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='0bb4b402a9fbf7a569e9.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='19d271055da17c7693df.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='34a69642147008ffcf38.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='39e7a11bf41b4f1bc582.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='3bd537757ab49a775237.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='540e055ce6d81f7b2a74.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='5ed1e7ad848fb5b3e48f.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='65b635521104239264d9.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='72ae108b155fab2f0e5a.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='DecoderLMHead.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='c70b8a86dfa817772b01.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='dd6a8e17129f58027cb4.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_0.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_1.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_10.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_11.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_12.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_13.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_14.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_15.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_17.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_18.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_19.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_2.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_20.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_21.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_22.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_23.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_24.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_25.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_26.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_27.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_28.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_29.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_3.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_30.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_31.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_4.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_5.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_6.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_7.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_8.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='decoder_layer_9.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='f5feda96c7de360aa78b.neff', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-10-23 09:15:10+00:00\", \"cardData\": \"base_model: meta-llama/Llama-2-7b\\nlanguage:\\n- yue\\nlicense: cc-by-sa-4.0\\npipeline_tag: text-generation\\ntags:\\n- cantonese\\n- llama-2\\n- Powered by AWS Trainium\", \"transformersInfo\": null, \"_id\": \"671832353af857f04746d506\", \"modelId\": \"heichow/Cantonese-Llama-2-7B-preview20240903-neuronx\", \"usedStorage\": 7549744171}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=heichow/Cantonese-Llama-2-7B-preview20240903-neuronx&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bheichow%2FCantonese-Llama-2-7B-preview20240903-neuronx%5D(%2Fheichow%2FCantonese-Llama-2-7B-preview20240903-neuronx)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "andreamaduzzi/LLaNA-7B",
            "card": "---\nlicense: mit\nlanguage:\n- en\nbase_model:\n- meta-llama/Llama-2-7b\nlibrary_name: transformers\npipeline_tag: text-generation\ndatasets:\n- andreamaduzzi/ShapeNeRF-Text\n---",
            "metadata": "{\"id\": \"andreamaduzzi/LLaNA-7B\", \"author\": \"andreamaduzzi\", \"sha\": \"5b3b9beec487f050e7741ea36a34d4e6a7413ddb\", \"last_modified\": \"2025-04-02 09:40:17+00:00\", \"created_at\": \"2024-10-23 08:02:21+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 4, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"safetensors\", \"nerfllm\", \"text-generation\", \"en\", \"dataset:andreamaduzzi/ShapeNeRF-Text\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"autotrain_compatible\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": \"text-generation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- andreamaduzzi/ShapeNeRF-Text\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\npipeline_tag: text-generation\", \"widget_data\": [{\"text\": \"My name is Julien and I like to\"}, {\"text\": \"I like traveling by train because\"}, {\"text\": \"Paris is an amazing place to visit,\"}, {\"text\": \"Once upon a time,\"}], \"model_index\": null, \"config\": {\"architectures\": [\"NeRFLLMLlamaForCausalLM\"], \"model_type\": \"nerfllm\", \"tokenizer_config\": {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"pad_token\": \"<unk>\", \"unk_token\": \"<unk>\", \"use_default_system_prompt\": false}}, \"transformers_info\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='train.log', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vec_proj.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 6749981696}, \"total\": 6749981696}, \"security_repo_status\": null, \"lastModified\": \"2025-04-02 09:40:17+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- andreamaduzzi/ShapeNeRF-Text\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: mit\\npipeline_tag: text-generation\", \"transformersInfo\": {\"auto_model\": \"AutoModelForCausalLM\", \"custom_class\": null, \"pipeline_tag\": \"text-generation\", \"processor\": null}, \"_id\": \"6718ad8d91f4894e2a491657\", \"modelId\": \"andreamaduzzi/LLaNA-7B\", \"usedStorage\": 40435854842}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=andreamaduzzi/LLaNA-7B&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bandreamaduzzi%2FLLaNA-7B%5D(%2Fandreamaduzzi%2FLLaNA-7B)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-DPO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the Direct Preference Optimization (DPO) loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch and a beta parameter set to 0.01. For more information on the dataset and methodology, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm) and DPO paper (https://arxiv.org/abs/2305.18290).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-DPO\", \"author\": \"sabersaleh\", \"sha\": \"e07f7224c0ecd95eb8c82ae28e00c32031258942\", \"last_modified\": \"2024-11-30 18:35:33+00:00\", \"created_at\": \"2024-11-30 18:18:16+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"arxiv:2305.18290\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-30 18:35:33+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b56e84e692194481b2724\", \"modelId\": \"sabersaleh/Llama2-7B-DPO\", \"usedStorage\": 26954326905}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-DPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-DPO%5D(%2Fsabersaleh%2FLlama2-7B-DPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-KTO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the Kahneman-Tversky Optimization (KTO) loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-KTO\", \"author\": \"sabersaleh\", \"sha\": \"60ebb9b532251942686b0cd79cbf56e6694f6e0c\", \"last_modified\": \"2024-11-30 21:53:44+00:00\", \"created_at\": \"2024-11-30 21:18:01+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-30 21:53:44+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b8109f03f7311890b6dd6\", \"modelId\": \"sabersaleh/Llama2-7B-KTO\", \"usedStorage\": 26954326969}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-KTO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-KTO%5D(%2Fsabersaleh%2FLlama2-7B-KTO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-IPO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the Identity Policy Optimization (IPO) loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-IPO\", \"author\": \"sabersaleh\", \"sha\": \"424beb187852f704718d75cf9f2ac6c63e10d941\", \"last_modified\": \"2024-11-30 21:45:09+00:00\", \"created_at\": \"2024-11-30 21:18:20+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 2, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-30 21:45:09+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b811cf0924dbb5f4b27e9\", \"modelId\": \"sabersaleh/Llama2-7B-IPO\", \"usedStorage\": 26954326969}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-IPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-IPO%5D(%2Fsabersaleh%2FLlama2-7B-IPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-CPO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the Contrastive Preference Optimization (CPO) loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-CPO\", \"author\": \"sabersaleh\", \"sha\": \"cfc39fd915d4cb89283a901f0eed60f268ec8dce\", \"last_modified\": \"2024-11-30 21:49:55+00:00\", \"created_at\": \"2024-11-30 21:18:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-30 21:49:55+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b812bf0924dbb5f4b2ad1\", \"modelId\": \"sabersaleh/Llama2-7B-CPO\", \"usedStorage\": 26954329409}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-CPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-CPO%5D(%2Fsabersaleh%2FLlama2-7B-CPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-SimPO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the Simple Preference Optimization (SimPO) loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-SimPO\", \"author\": \"sabersaleh\", \"sha\": \"860de39d93c457d719c3f299e06ba4897aa51f3d\", \"last_modified\": \"2024-11-30 21:52:03+00:00\", \"created_at\": \"2024-11-30 21:19:39+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 3, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-11-30 21:52:03+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b816bd5b25ac3867f39df\", \"modelId\": \"sabersaleh/Llama2-7B-SimPO\", \"usedStorage\": 26954328221}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-SimPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-SimPO%5D(%2Fsabersaleh%2FLlama2-7B-SimPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-aligned",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through an alignment loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-aligned\", \"author\": \"sabersaleh\", \"sha\": \"02904630a14754a520c48abc52d9100cd59f8707\", \"last_modified\": \"2024-12-03 18:41:47+00:00\", \"created_at\": \"2024-11-30 21:19:52+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 62, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='rng_state_0.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='rng_state_1.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='rng_state_2.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='rng_state_3.pth', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='scheduler.pt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-03 18:41:47+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b817874e677e6955fc48d\", \"modelId\": \"sabersaleh/Llama2-7B-aligned\", \"usedStorage\": 26954392225}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-aligned&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-aligned%5D(%2Fsabersaleh%2FLlama2-7B-aligned)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "sabersaleh/Llama2-7B-RDPO",
            "card": "---\nlicense: mit\ndatasets:\n- tatsu-lab/alpaca\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\nThis model is aligned using the AlpacaFarm dataset, fine-tuned through the RDPO loss. The alignment process started from the Supervised Fine-Tuned (SFT) version of LLaMA 2 7B. The optimization process was conducted with a single epoch. For more information on the dataset, refer to the AlpacaFarm documentation (https://github.com/tatsu-lab/alpaca_farm).",
            "metadata": "{\"id\": \"sabersaleh/Llama2-7B-RDPO\", \"author\": \"sabersaleh\", \"sha\": \"a06d926aca5a16dff254b951103d6d82875f8f2a\", \"last_modified\": \"2024-12-01 13:17:01+00:00\", \"created_at\": \"2024-11-30 21:20:32+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"pytorch\", \"llama\", \"dataset:tatsu-lab/alpaca\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:mit\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='added_tokens.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model.bin.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-01 13:17:01+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- tatsu-lab/alpaca\\nlicense: mit\", \"transformersInfo\": null, \"_id\": \"674b81a09f33a279e2a708ea\", \"modelId\": \"sabersaleh/Llama2-7B-RDPO\", \"usedStorage\": 26954326969}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=sabersaleh/Llama2-7B-RDPO&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bsabersaleh%2FLlama2-7B-RDPO%5D(%2Fsabersaleh%2FLlama2-7B-RDPO)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "Vinnnf/LLaMA-2-7B-MaskLLM-C4",
            "card": "---\nbase_model:\n- meta-llama/Llama-2-7b\nlibrary_name: transformers\n---\n\n# MaskLLM: Learnable Semi-structured Sparsity for Large Language Models\n\n<div align=\"center\">\n<figure>\n <img src=\"https://github.com/NVlabs/MaskLLM/blob/main/assets/teaser.png?raw=true\" style=\"width:70%; display:block; margin-left:auto; margin-right:auto;\"\n</figure>\n</div>\n\nThis work introduces [MaskLLM](https://github.com/NVlabs/MaskLLM), a **learnable** pruning method that establishes **Semi-structured (or ``N:M'') Sparsity** in LLMs, aimed at reducing computational overhead during inference. The proposed method is scalable and stands to benefit from larger training datasets.\n\n## Requirements\nWe provide pre-computed masks for Huggingface Models such as Llama-2 7B and Llama-3 8B with the minimum requirements. It will not involve docker, Megatron or data preprocessing. \n```bash\npip install transformers accelerate datasets SentencePiece \n```\n\n## Pre-computed Masks\n\nThe following masks were trained and provided by [@VainF](https://github.com/VainF). We use ``huggingface_hub`` to automatically download those masks and apply them to offcical LLMs for evaluation. Those mask files were compressed using [numpy.savez_compressed](tool_compress_mask.py). More results for baselines (SparseGPT, Wanda) can be found in the appendix.\n| Model | Pattern | Training Data | Training/Eval SeqLen | PPL (Dense) | PPL (SparseGPT) | **PPL (MaskLLM)** | Link |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| LLaMA-2 7B | 2:4 | C4 (2B Tokens)| 4096 | 5.12 | 10.42 | **6.78** | [HuggingFace](https://huggingface.co/Vinnnf/LLaMA-2-7B-MaskLLM-C4) |\n| LLaMA-3 8B | 2:4 | C4 (2B Tokens) | 4096 | 5.75 | 17.64 | **8.49** | [HuggingFace](https://huggingface.co/Vinnnf/LLaMA-3-8B-MaskLLM-C4) |\n| LLaMA-3.1 8B | 2:4 | C4 (2B Tokens) | 4096 | - | - | - | Coming Soon |\n\n## How to use it\n\nPlease see [NVlabs/MaskLLM](https://github.com/NVlabs/MaskLLM?tab=readme-ov-file#1-pre-trained-masks-for-hugging-face-models-).",
            "metadata": "{\"id\": \"Vinnnf/LLaMA-2-7B-MaskLLM-C4\", \"author\": \"Vinnnf\", \"sha\": \"3b0be223fdc2a441a8e77e7fa4908739bb4e1469\", \"last_modified\": \"2024-12-07 05:19:50+00:00\", \"created_at\": \"2024-12-06 18:01:32+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"transformers\", \"gguf\": null, \"inference\": null, \"tags\": [\"transformers\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"endpoints_compatible\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: transformers\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mask_compressed.npz', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-07 05:19:50+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: transformers\", \"transformersInfo\": {\"auto_model\": \"AutoModel\", \"custom_class\": null, \"pipeline_tag\": null, \"processor\": null}, \"_id\": \"67533bfcb14e37aa77c60973\", \"modelId\": \"Vinnnf/LLaMA-2-7B-MaskLLM-C4\", \"usedStorage\": 548468871}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Vinnnf/LLaMA-2-7B-MaskLLM-C4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BVinnnf%2FLLaMA-2-7B-MaskLLM-C4%5D(%2FVinnnf%2FLLaMA-2-7B-MaskLLM-C4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "nvidia/Llama-2-7B-DMC-4x",
            "card": "---\nlicense: other\nlicense_name: nvidia-open-model-license\nlicense_link: >-\n  https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\nbase_model:\n- meta-llama/Llama-2-7b\ntags:\n- nvidia\n- llama 2\n- pytorch\n- kvcache\nlibrary_name: megatron-lm\n---\n# Llama-2-7B-DMC-4x\n\n## Description\n\nLlama-2-7B-DMC-4x is a version of [Llama 2 7B](https://www.llama.com/llama2/), which has been trained to apply the Dynamic Memory Compression (DMC) algorithm ([https://arxiv.org/abs/2403.09636](https://arxiv.org/abs/2403.09636)). With DMC, the model performs on-line key\u2013value cache compression at inference time, achieving substantially better throughput and/or latency. Most importantly, it learns to apply different compression ratios in different heads and layers. The source code for training and inference is provided in the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/dmc) repository.\n\nThis model is for research and development only.\n\n### License\n\nGOVERNING TERMS: This model is governed by the NVIDIA Open Model License Agreement (found at https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf). <br>\nAdditional Information: LLAMA 2 COMMUNITY LICENSE AGREEMENT (found at https://huggingface.co/meta-llama/Llama-2-7b/blob/main/LICENSE.txt). <br>\n\n## Reference\nDynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n## Model Architecture\n\nLlama-2-7B-DMC-4x uses a model embedding size of 4096, 32 attention heads, MLP intermediate dimension of 11008, with 32 layers in total. Additionally, it uses Rotary Position Embeddings (RoPE).\n\n**Architecture Type:** Transformer Decoder (Auto-regressive Language Model)\n\n**Network Architecture:** Llama 2 7B\n\n## Input\n**Input Type:** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D), Temperature\n**Other Properties Related to Input: Max Input Tokens: 4096 <br>\n\n## Output\n**Output Type :** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br> \n**Other Properties Related to Output: Max Output Tokens: 4096 <br>\n\n## Software Integration\n**Runtime Engine(s):** \n* Not Applicable (N/A)\n\nThe model weights are distributed in bfloat16 format. However, it could be converted to other formats in order to run on other hardware microarchitectures.\n\n**Supported Hardware Microarchitecture Compatibility:** Nvidia Ampere and newer GPUs.<br>\n\n**Supported Operating System(s):** <br>\n* Linux <br>\n\n## Model Version(s)\nLlama 2 7B DMC 4x v1.0\n\n# Training and Evaluation Datasets\n\n## Training Dataset\n\nThe model was trained for 18,000 steps with a batch size of 1024, a sequence length of 4096, and a learning rate of 3e-5 with an increasing compression objective. Afterwards, it underwent additional training for 2000 steps with a fixed compression rate of 4x and a smaller learning rate of 3e-6.\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets. This particular model was trained on a dataset containing a mixture of texts in English and 37 programming languages.\n\n## Evaluation\n\n| Category    | Benchmark                                   | # Shots | Llama 2 7B | Llama 2 7B DMC 4x |\n|:------------|:--------------------------------------------|--------:|-----------:|------------------:|\n| General     | [MMLU](https://openreview.net/forum?id=d7KBjmI3GmQ)                 |  5 | 46.7 | 44.2 |\n| Math        | [GMS8K](https://arxiv.org/abs/2110.14168)                           |  5 | 11.9 | 12.6 |\n| Commonsense | [HellaSwag](https://aclanthology.org/P19-1472)                      | 10 | 78.8 | 78.9 |\n| Commonsense | [Arc-Easy](https://arxiv.org/abs/1803.05457)                        |  0 | 73.1 | 71.8 |\n| Commonsense | [Arc-Challenge](https://arxiv.org/abs/1803.05457)                   | 25 | 53.1 | 52.5 |\n| Commonsense | [PIQA](https://ojs.aaai.org/index.php/AAAI/article/view/6239)       |  0 | 78.2 | 79.5 |\n| Commonsense | [WinoGrande](https://ojs.aaai.org/index.php/AAAI/article/view/6399) |  5 | 74.0 | 73.2 |\n\n## AI Safety Efforts\n\nThe Llama-2-7B-DMC-4x model underwent AI safety evaluation including adversarial testing via three distinct methods:\n* [Garak](https://github.com/leondz/garak), is an automated LLM vulnerability scanner that probes for common weaknesses, including prompt injection and data leakage.\n* [AEGIS](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0), is a content safety evaluation dataset and LLM based content safety classifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM interactions.\n* Human Content Red Teaming leveraging human interaction and evaluation of the models' responses.\n\n## Inference\n**Engine:** Megatron-LM <br>\n**Test Hardware** H100-80GB <br>\n\nWe recommend running the provided code inside a [PyTorch NGC Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch).\n\n1. First, download a [PyTorch NGC Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) using Docker.\nThe code below has been tested with the `24.04-py3` version of the container.\n\n2. After setting up the container, clone the repository and install the dependencies:\n   ```\n   git clone -b dmc https://github.com/NVIDIA/Megatron-LM\n   cd Megatron-LM\n   pip install -r requirements.txt\n   ```\n3. Download the [Llama 2 tokenizer](https://huggingface.co/meta-llama/Llama-2-7b/blob/main/tokenizer.model) and save it under a desired location `<TOKENIZER_MODEL>`.\n\n4. Download a selected checkpoint and save it under a desired location `<DMC_MODEL>`.\n\n5. We provide code to run and benchmark a simple, auto-regressive inference. Save a single prompt in a textfile and run:\n   ```bash\n   ./examples/dmc/inference.sh 7B <DMC_MODEL> <TOKENIZER_MODEL> <PROMPT_TXT_FILE>\n   ```\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Limitations\n\nThe model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. This issue could be exacerbated without the use of the recommended prompt template. If you are going to use this model in an agentic workflow, validate that the imported packages are from a trusted source to ensure end-to-end security.\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@InProceedings{pmlr-v235-nawrot24a,\n title =        {Dynamic Memory Compression: Retrofitting {LLM}s for Accelerated Inference},\n author =       {Nawrot, Piotr and {\\L}a\\'{n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo},\n booktitle =    {Proceedings of the 41st International Conference on Machine Learning},\n pages =        {37396--37412},\n year =         {2024},\n editor =       {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},\n volume =       {235},\n series =       {Proceedings of Machine Learning Research},\n month =        {21--27 Jul},\n publisher =    {PMLR},\n pdf =          {https://raw.githubusercontent.com/mlresearch/v235/main/assets/nawrot24a/nawrot24a.pdf},\n url =          {https://proceedings.mlr.press/v235/nawrot24a.html},\n abstract =     {Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key\u2013value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key\u2013value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to $\\sim 3.7 \\times$ throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4$\\times$ cache compression, outperforming up-trained grouped-query attention (GQA) and key\u2013value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget. We release the DMC code and models at https://github.com/NVIDIA/Megatron-LM/tree/DMC.}\n}\n```",
            "metadata": "{\"id\": \"nvidia/Llama-2-7B-DMC-4x\", \"author\": \"nvidia\", \"sha\": \"fe5c32fc175ad63fb02e7a7bb9330461ffc97dd3\", \"last_modified\": \"2024-12-22 13:47:39+00:00\", \"created_at\": \"2024-12-20 11:49:22+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"megatron-lm\", \"gguf\": null, \"inference\": null, \"tags\": [\"megatron-lm\", \"nvidia\", \"llama 2\", \"pytorch\", \"kvcache\", \"arxiv:2403.09636\", \"arxiv:2110.14168\", \"arxiv:1803.05457\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:other\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: megatron-lm\\nlicense: other\\nlicense_name: nvidia-open-model-license\\nlicense_link: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\\ntags:\\n- nvidia\\n- llama 2\\n- pytorch\\n- kvcache\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='NOTICE', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='latest_checkpointed_iteration.txt', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='mp_rank_00/model_optim_rng.pt', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-22 13:47:39+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\nlibrary_name: megatron-lm\\nlicense: other\\nlicense_name: nvidia-open-model-license\\nlicense_link: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf\\ntags:\\n- nvidia\\n- llama 2\\n- pytorch\\n- kvcache\", \"transformersInfo\": null, \"_id\": \"676559c27505a03fcd9cfe40\", \"modelId\": \"nvidia/Llama-2-7B-DMC-4x\", \"usedStorage\": 13477077952}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=nvidia/Llama-2-7B-DMC-4x&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bnvidia%2FLlama-2-7B-DMC-4x%5D(%2Fnvidia%2FLlama-2-7B-DMC-4x)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "dongsheng/DTA_llama2_7b",
            "card": "---\nlicense: apache-2.0\ndatasets:\n- dongsheng/DTA-Tool\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\n## Model Description\n\n<!-- Provide a longer summary of what this model is. -->\nDTA_llama2_7b is from the paper \"[Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation](https://arxiv.org/abs/2501.12432)\". \nIt is a large language model capable of invoking tools and can parallel invoke multiple tools within a single round. \nThe tool format it used is similar to OpenAI's Function Call.\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\nThe related code can be found in our GitHub [repository](https://github.com/Zhudongsheng75/Divide-Then-Aggregate).\n\n## Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nThe training data comes from our specially constructed [DTA-Tool](https://huggingface.co/datasets/dongsheng/DTA-Toolhttps://github.com/OpenBMB/ToolBench), which is derived from [ToolBench](https://github.com/OpenBMB/ToolBench).\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\nWe evaluated the performance of DTA-Llama on [StableToolBench](https://github.com/THUNLP-MT/StableToolBench).\n\n### Results\n\n![result](result.png)\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/630da0fae57da204209411d3/ViBSn34pV-4LWJkIpUvSr.png) that should go in this section. -->\n```bibtex\n@misc{zhu2025dividethenaggregateefficienttoollearning,\n      title={Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation}, \n      author={Dongsheng Zhu and Weixian Shi and Zhengliang Shi and Zhaochun Ren and Shuaiqiang Wang and Lingyong Yan and Dawei Yin},\n      year={2025},\n      eprint={2501.12432},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2501.12432}, \n}\n```\n",
            "metadata": "{\"id\": \"dongsheng/DTA_llama2_7b\", \"author\": \"dongsheng\", \"sha\": \"68b8ebcd2cd6f6f34d6e6c88b1a9e8e081bffae2\", \"last_modified\": \"2025-01-23 08:06:00+00:00\", \"created_at\": \"2025-01-01 03:10:56+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"llama\", \"dataset:dongsheng/DTA-Tool\", \"arxiv:2501.12432\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- dongsheng/DTA-Tool\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": {\"__type\": \"AddedToken\", \"content\": \"<s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"eos_token\": {\"__type\": \"AddedToken\", \"content\": \"</s>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}, \"pad_token\": null, \"unk_token\": {\"__type\": \"AddedToken\", \"content\": \"<unk>\", \"lstrip\": false, \"normalized\": false, \"rstrip\": false, \"single_word\": false}}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='index.html', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00001-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00002-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='pytorch_model-00003-of-00003.bin', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='result.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.model', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-01-23 08:06:00+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- dongsheng/DTA-Tool\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"6774b24018b5bd53881e2cad\", \"modelId\": \"dongsheng/DTA_llama2_7b\", \"usedStorage\": 26954286777}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=dongsheng/DTA_llama2_7b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdongsheng%2FDTA_llama2_7b%5D(%2Fdongsheng%2FDTA_llama2_7b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "dongsheng/DTA_llama3_8b",
            "card": "---\nlicense: apache-2.0\ndatasets:\n- dongsheng/DTA-Tool\nbase_model:\n- meta-llama/Llama-2-7b\n---\n\n## Model Description\n\n<!-- Provide a longer summary of what this model is. -->\nDTA_llama3_8b is from the paper \"[Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation](https://arxiv.org/abs/2501.12432)\". \nIt is a large language model capable of invoking tools and can parallel invoke multiple tools within a single round. \nThe tool format it used is similar to OpenAI's Function Call.\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\nThe related code can be found in our GitHub [repository](https://github.com/Zhudongsheng75/Divide-Then-Aggregate).\n\n## Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nThe training data comes from our specially constructed [DTA-Tool](https://huggingface.co/datasets/dongsheng/DTA-Toolhttps://github.com/OpenBMB/ToolBench), which is derived from [ToolBench](https://github.com/OpenBMB/ToolBench).\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\nWe evaluated the performance of DTA-Llama on [StableToolBench](https://github.com/THUNLP-MT/StableToolBench).\n\n### Results\n\n![result](result.png)\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/630da0fae57da204209411d3/ViBSn34pV-4LWJkIpUvSr.png) that should go in this section. -->\n```bibtex\n@misc{zhu2025dividethenaggregateefficienttoollearning,\n      title={Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation}, \n      author={Dongsheng Zhu and Weixian Shi and Zhengliang Shi and Zhaochun Ren and Shuaiqiang Wang and Lingyong Yan and Dawei Yin},\n      year={2025},\n      eprint={2501.12432},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2501.12432}, \n}\n```\n",
            "metadata": "{\"id\": \"dongsheng/DTA_llama3_8b\", \"author\": \"dongsheng\", \"sha\": \"e124b89f135c98af9da470663dd85d680e9c0042\", \"last_modified\": \"2025-01-24 08:14:22+00:00\", \"created_at\": \"2025-01-18 14:47:07+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 1, \"downloads_all_time\": null, \"likes\": 0, \"library_name\": null, \"gguf\": null, \"inference\": null, \"tags\": [\"safetensors\", \"llama\", \"dataset:dongsheng/DTA-Tool\", \"arxiv:2501.12432\", \"base_model:meta-llama/Llama-2-7b\", \"base_model:finetune:meta-llama/Llama-2-7b\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- dongsheng/DTA-Tool\\nlicense: apache-2.0\", \"widget_data\": null, \"model_index\": null, \"config\": {\"architectures\": [\"LlamaForCausalLM\"], \"model_type\": \"llama\", \"tokenizer_config\": {\"bos_token\": \"<|begin_of_text|>\", \"eos_token\": \"<|end_of_text|>\", \"pad_token\": \"<|reserved_special_token_0|>\"}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='index.html', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00001-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00002-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00003-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00004-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00005-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00006-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model-00007-of-00007.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='result.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='special_tokens_map.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='trainer_state.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='training_args.bin', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": {\"parameters\": {\"F32\": 8030261248}, \"total\": 8030261248}, \"security_repo_status\": null, \"lastModified\": \"2025-01-24 08:14:22+00:00\", \"cardData\": \"base_model:\\n- meta-llama/Llama-2-7b\\ndatasets:\\n- dongsheng/DTA-Tool\\nlicense: apache-2.0\", \"transformersInfo\": null, \"_id\": \"678bbeeb5ea86ee6b505f6b5\", \"modelId\": \"dongsheng/DTA_llama3_8b\", \"usedStorage\": 32121084088}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=dongsheng/DTA_llama3_8b&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bdongsheng%2FDTA_llama3_8b%5D(%2Fdongsheng%2FDTA_llama3_8b)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        }
    ]
}