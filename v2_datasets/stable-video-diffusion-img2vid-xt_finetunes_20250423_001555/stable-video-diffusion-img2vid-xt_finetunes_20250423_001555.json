{
    "models": [
        {
            "model_id": "stabilityai/stable-video-diffusion-img2vid-xt",
            "card": "---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 25 frames at resolution 576x1024 given a context frame of the same size, finetuned from [SVD Image-to-Video [14 frames]](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid).\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/svd_xt_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n- **Finetuned from model:** SVD Image-to-Video [14 frames]\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this [license](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/LICENSE.md). Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nFor commercial use, please refer to https://stability.ai/license.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for both non-commercial and commercial usage.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n# Appendix: \n\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters.\nNo explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation.\nNo other third party was involved in the development of this model; the model was fully developed in-house at Stability AI.\nTraining the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh.\nThe released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos.\nWith the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards.\nThe information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. \nThe released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.  \nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI\u2019s future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. \nFor usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.\n",
            "metadata": "{\"id\": \"stabilityai/stable-video-diffusion-img2vid-xt\", \"author\": \"stabilityai\", \"sha\": \"9e43909513c6714f1bc78bcb44d96e733cd242aa\", \"last_modified\": \"2024-07-10 11:43:18+00:00\", \"created_at\": \"2023-11-20 23:45:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 209896, \"downloads_all_time\": null, \"likes\": 3013, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"image-to-video\", \"license:other\", \"diffusers:StableVideoDiffusionPipeline\", \"region:us\"], \"pipeline_tag\": \"image-to-video\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"license: other\\nlicense_name: stable-video-diffusion-community\\nlicense_link: LICENSE.md\\npipeline_tag: image-to-video\", \"widget_data\": null, \"model_index\": null, \"config\": {\"diffusers\": {\"_class_name\": \"StableVideoDiffusionPipeline\"}}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='comparison.png', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='feature_extractor/preprocessor_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='image_encoder/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='image_encoder/model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='image_encoder/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='model_index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='output_tile.gif', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='scheduler/scheduler_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd_xt.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd_xt_image_decoder.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet/diffusion_pytorch_model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vae/diffusion_pytorch_model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"multimodalart/stable-video-diffusion\", \"wangfuyun/AnimateLCM-SVD\", \"wwen1997/Framer\", \"fffiloni/SVFR-demo\", \"PAIR/StreamingT2V\", \"tencent/DepthCrafter\", \"fffiloni/AniDoc\", \"fffiloni/svd_keyframe_interpolation\", \"fffiloni/MimicMotion\", \"mrcuddle/SDXT-Image-To-Video\", \"TencentARC/GeometryCrafter\", \"heheyas/V3D\", \"jhshao/ChronoDepth\", \"Doubiiu/TrajectoryCrafter\", \"VIDraft/Portrait-Animation\", \"vilarin/Diffutoon-ExVideo\", \"asahi417/stable-video-diffusion-upscale\", \"hehao13/CameraCtrl-svd\", \"Kvikontent/AI-Movie-Factory\", \"emmadrex/stable-video-diffusion\", \"pablovela5620/mini-nvs-solver\", \"modelscope/DiffSynth-Painter\", \"Yanrui95/NormalCrafter\", \"AiGuaratuba/Stable-Video-Diffusion-Img2Vid-10seconds\", \"HikariDawn/This-and-That\", \"guardiancc/dance-monkey\", \"yslan/ObjCtrl-2.5D\", \"nightfury/SD-Img2Vid-10sec\", \"nroggendorff/svd\", \"CrazyEric/AnimateLCM-SVD\", \"kevinwang676/Diffutoon\", \"xinxiaoxin/MimicMotion\", \"123LETSPLAY/imagetovideo-try1\", \"DDUF/dduf-my-diffusers-repo\", \"Taf2023/stable-video-diffusion\", \"quantumcontrol/stable-video-diffusion\", \"sejamenath2023/stable-video-diffusionv2\", \"awacke1/StableDiffusionVideoTo3D\", \"jbilcke-hf/ai-tube-model-als-1\", \"SamarthPersonal/LumiereIQ\", \"Dragunflie-420/MimicMotion\", \"salomonsky/video\", \"thecosmicdoctor/unboxai_publicVideo\", \"svjack/AniDoc\", \"waloneai/SDXT-Image-To-Video\", \"jack1969/SVFR-demo\", \"humvee1249/stable-video-diffusion\", \"raymerjacque/Image-To-Video\", \"vloikas/stable-video-diffusion\", \"Nymbo/stable-video-diffusion\", \"TheVilfer/stable-video-diffusion\", \"piaoyu2011/stable-video-diffusion\", \"zparadox/stable-video-diffusion\", \"AlekseyCalvin/SVDxt_to_Neurealize_History\", \"tiagojardins/stable-video-diffusion\", \"waledrashed/stable-video-diffusion\", \"MgMetaverso/stable-video-diffusion\", \"sejamenath2023/stable-video-diffusionv3\", \"besarismaili/sd-video\", \"pallab7685/stable-video-diffusion\", \"dclxviclan/Stabblediffusiontest\", \"Bernaj/stable-video-diffusion\", \"fantos/vidiani\", \"Ziaistan/AnimateLCM-SVD\", \"Moldwebs/sd-video\", \"Tonic/stable-video-diffusion-xt-1\", \"Moldwebs/stable-video-diffusion\", \"panyanyany/stable-video-diffusion\", \"dadstt/stable-video-diffusion\", \"ajajjajajaja/StreamingT2V\", \"conzikool/Video-Diffusion\", \"Divergent007/stable-video-diffusion\", \"dereckd/stable-video-diffusion4\", \"womprat/stable-video-diffusion\", \"sy555/stable-video-diffusion\", \"jjqsdq/stable-video-diffusion\", \"adilraufkahara/video-model\", \"Me5/StreamingT2V\", \"weijiang2024/Suanfamama_AIGC_alg5\", \"eslemkurt/mySpace\", \"dsdad/MimicMotion\", \"fantaxy/MimicMotion\", \"asahi417/stable-video-diffusion\", \"Couchpotato35/stability\", \"aman81/Moore-AnimateAnyone\", \"charbelmalo/SV3d\", \"Nymbo/MimicMotion\", \"Clairmor/stable-video-diffusion\", \"Uhhy/stable-video-diffusion\", \"abi445/MimicMotion\", \"szili2011/SpriteSheet-FromOneFrame\", \"atikur-rabbi/stable-video-diffusion\", \"supersolar/DepthCrafter\", \"sushann222/helloName\", \"Sollity/Imagine\", \"Sollity/stable-video-diffusion\", \"svjack/Framer\", \"cocktailpeanut/Framer\", \"cakemus/testsson\", \"svjack/AnimateLCM-SVD-Genshin-Impact-Demo\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-07-10 11:43:18+00:00\", \"cardData\": \"license: other\\nlicense_name: stable-video-diffusion-community\\nlicense_link: LICENSE.md\\npipeline_tag: image-to-video\", \"transformersInfo\": null, \"_id\": \"655befb36d02c2b1a9284bf0\", \"modelId\": \"stabilityai/stable-video-diffusion-img2vid-xt\", \"usedStorage\": 51729327024}",
            "depth": 0,
            "children": [
                "https://huggingface.co/Yuyang-z/genxd",
                "https://huggingface.co/NullVoider/V1",
                "https://huggingface.co/TencentARC/GeometryCrafter",
                "https://huggingface.co/jhshao/ChronoDepth-v1"
            ],
            "children_count": 4,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "Doubiiu/TrajectoryCrafter",
                "Kvikontent/AI-Movie-Factory",
                "TencentARC/GeometryCrafter",
                "VIDraft/Portrait-Animation",
                "Yanrui95/NormalCrafter",
                "fffiloni/MimicMotion",
                "fffiloni/svd_keyframe_interpolation",
                "huggingface/InferenceSupport/discussions/5",
                "jhshao/ChronoDepth",
                "modelscope/DiffSynth-Painter",
                "multimodalart/stable-video-diffusion",
                "tencent/DepthCrafter",
                "wwen1997/Framer"
            ],
            "spaces_count": 13
        },
        {
            "model_id": "Yuyang-z/genxd",
            "card": "---\ntags:\n- Image-to-3D\n- Image-to-4D\n- GenXD\nlanguage:\n- en\n- zh\nbase_model:\n- stabilityai/stable-video-diffusion-img2vid-xt\npipeline_tag: image-to-3d\nlicense: apache-2.0\ndatasets:\n- Yuyang-z/CamVid-30K\n---\n\n# GenXD Model Card\n\n<p align=\"center\" style=\"border-radius: 10px\">\n  <img src=\"https://gen-x-d.github.io/materials/figures/icon.jpg\" width=\"35%\" alt=\"logo\"/>\n</p>\n\n<div style=\"display:flex;justify-content: center\">\n  <a href=\"https://huggingface.co/Yuyang-z/genxd\"><img src=\"https://img.shields.io/static/v1?label=Weights&message=Huggingface&color=yellow\"></a> &ensp;\n  <a href=\"https://github.com/HeliosZhao/GenXD\"><img src=\"https://img.shields.io/static/v1?label=Code&message=Github&color=blue&logo=github\"></a> &ensp;\n  <a href=\"https://gen-x-d.github.io/\"><img src=\"https://img.shields.io/static/v1?label=GenXD&message=Project&color=purple\"></a> &ensp;\n  <a href=\"https://arxiv.org/abs/2411.02319\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=Arxiv&color=red&logo=arxiv\"></a> &ensp;\n  <a href=\"https://huggingface.co/datasets/Yuyang-z/CamVid-30K\"><img src=\"https://img.shields.io/static/v1?label=CamVid-30K&message=HuggingFace&color=yellow\"></a> &ensp;\n</div>\n\n\n\n## Model Details\n\n<p align=\"center\" border-raduis=\"10px\">\n  <img src=\"https://gen-x-d.github.io/materials/figures/diffusion-model.png\" width=\"80%\" alt=\"teaser_page1\"/>\n</p>\n\n### Model Description\nGenXD leverages mask latent conditioned diffusion model to generate 3D and 4D samples with both camera and image conditions. In addition, multiview-temporal modules together with alpha-fusing are proposed to effectively disentangle and fuse multiview and temporal information.\n\n- **Developed by:** NUS, Microsoft\n- **Model type:** image-to-3D diffusion model, image-to-video diffusion model, image-to-4D diffusion model\n- **License:** Apache-2.0\n\n### Model Sources\n\n- **Project Page:** https://gen-x-d.github.io\n- **Repository:** https://github.com/HeliosZhao/GenXD\n- **Paper:** https://arxiv.org/abs/2411.02319\n- **Data:** https://huggingface.co/datasets/Yuyang-z/CamVid-30K\n\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n\n- Probing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism.\n- The model does not achieve perfect 3D consistency.\n\n### Bias\nWhile the capabilities of generation model is impressive, it can also reinforce or exacerbate social biases.\n",
            "metadata": "{\"id\": \"Yuyang-z/genxd\", \"author\": \"Yuyang-z\", \"sha\": \"8fa7a9870658b8b1f39a10463775ac98105f1a83\", \"last_modified\": \"2025-03-30 09:11:28+00:00\", \"created_at\": \"2025-02-26 13:48:01+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 0, \"downloads_all_time\": null, \"likes\": 18, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"Image-to-3D\", \"Image-to-4D\", \"GenXD\", \"image-to-3d\", \"en\", \"zh\", \"dataset:Yuyang-z/CamVid-30K\", \"arxiv:2411.02319\", \"base_model:stabilityai/stable-video-diffusion-img2vid-xt\", \"base_model:finetune:stabilityai/stable-video-diffusion-img2vid-xt\", \"license:apache-2.0\", \"region:us\"], \"pipeline_tag\": \"image-to-3d\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-video-diffusion-img2vid-xt\\ndatasets:\\n- Yuyang-z/CamVid-30K\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\npipeline_tag: image-to-3d\\ntags:\\n- Image-to-3D\\n- Image-to-4D\\n- GenXD\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='scheduler/scheduler_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-03-30 09:11:28+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-video-diffusion-img2vid-xt\\ndatasets:\\n- Yuyang-z/CamVid-30K\\nlanguage:\\n- en\\n- zh\\nlicense: apache-2.0\\npipeline_tag: image-to-3d\\ntags:\\n- Image-to-3D\\n- Image-to-4D\\n- GenXD\", \"transformersInfo\": null, \"_id\": \"67bf1b91fb9f0fdbb4b71365\", \"modelId\": \"Yuyang-z/genxd\", \"usedStorage\": 8119754692}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=Yuyang-z/genxd&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BYuyang-z%2Fgenxd%5D(%2FYuyang-z%2Fgenxd)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "NullVoider/V1",
            "card": "---\nlanguage:\n- en\nbase_model:\n- tencent/HunyuanVideo\n- stabilityai/stable-video-diffusion-img2vid-xt\ntag: \n- text-to-video\n- image-to-video\n- video-to-video\n---\n\n# V1: Human-Centric Video Foundation Model\n\n<p align=\"center\">\n\u00b7<a href=\"https://github.com/immortalshadow007/V1\" target=\"_blank\">\ud83c\udf10 Github</a> \u00b7\n</p>\n\n---\nThis repo contains Diffusers-format model weights for V1 Text-to-Video, Image-to-Video models, and Video-to-Video. You can find the inference code on our github repository [V1](https://github.com/immortalshadow007/V1).\n\n## Introduction\nV1 is an open-source human-centric video foundation model. By fine-tuning <a href=\"https://huggingface.co/tencent/HunyuanVideo\">HunyuanVideo</a> on O(10M) high-quality film and television clips, V1 offers three key advantages:\n\n## \ud83d\udd11 Key Features\n\n### 1. Advanced Model Capabilities\n\n1. **Open-Source Leadership**: The Text-to-Video model achieves state-of-the-art (SOTA) performance among open-source models, comparable to proprietary models like Kling and Hailuo.\n2. **Advanced Facial Animation**: Captures 33 distinct facial expressions with over 400 natural movement combinations, accurately reflecting human emotions.\n3. **Cinematic Lighting and Aesthetics**: Trained on high-quality Hollywood-level film and television data, each generated frame exhibits cinematic quality in composition, actor positioning, and camera angles.\n\n### 2. Self-Developed Data Cleaning and Annotation Pipeline\n\nOur model is built on a self-developed data cleaning and annotation pipeline, creating a vast dataset of high-quality film, television, and documentary content.\n\n- **Expression Classification**: Categorizes human facial expressions into 33 distinct types.\n- **Character Spatial Awareness**: Utilizes 3D human reconstruction technology to understand spatial relationships between multiple people in a video, enabling film-level character positioning.\n- **Action Recognition**: Constructs over 400 action semantic units to achieve a precise understanding of human actions.\n- **Scene Understanding**: Conducts cross-modal correlation analysis of clothing, scenes, and plots.\n\n### 3. Multi-Stage Image-to-Video Pretraining\n\nOur multi-stage pretraining pipeline, inspired by the <a href=\"https://huggingface.co/tencent/HunyuanVideo\">HunyuanVideo</a> design, consists of the following stages:\n\n- **Stage 1: Model Domain Transfer Pretraining**: We use a large dataset (O(10M) of film and television content) to adapt the text-to-video model to the human-centric video domain.\n- **Stage 2: Image-to-Video Model Pretraining**: We convert the text-to-video model from Stage 1 into an image-to-video model by adjusting the conv-in parameters. This new model is then pretrained on the same dataset used in Stage 1.\n- **Stage 3: High-Quality Fine-Tuning**: We fine-tune the image-to-video model on a high-quality subset of the original dataset, ensuring superior performance and quality.\n\n## \ud83d\udce6 Model Introduction\n| Model Name      | Resolution | Video Length | FPS |\n|-----------------|------------|--------------|-----|\n| V1-Hunyuan-I2V  | 544px960p  | 97           | 24  |\n| V1-Hunyuan-T2V  | 544px960p  | 97           | 24  |\n| V1-SVD-V2V      | 544px960p  | 97           | 24  |\n\n## Usage\n\n**Note**: The V1 model is a hybrid of two models (`tencent/HunyuanVideo` and `stabilityai/stable-video-diffusion-img2vid-xt`) and cannot be loaded directly using `DiffusionPipeline.from_pretrained(\"NullVoider/V1\")`. Instead, you need to clone the model repository locally and use the inference code provided in the associated GitHub repository.\n\n### Usage Guide\n\n#### 1. Clone the Model Repository Locally\nThe model weights are hosted on Hugging Face. Clone the repository to your local machine using `git`:\n\n```bash\ngit clone https://huggingface.co/NullVoider/V1",
            "metadata": "{\"id\": \"NullVoider/V1\", \"author\": \"NullVoider\", \"sha\": \"91bfbef233a8aa8dba30a2ac446a02cd79268d17\", \"last_modified\": \"2025-03-28 15:01:02+00:00\", \"created_at\": \"2025-03-19 10:40:35+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 74, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"en\", \"base_model:stabilityai/stable-video-diffusion-img2vid-xt\", \"base_model:finetune:stabilityai/stable-video-diffusion-img2vid-xt\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- tencent/HunyuanVideo\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlanguage:\\n- en\\ntag:\\n- text-to-video\\n- image-to-video\\n- video-to-video\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00001-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00002-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00003-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00004-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00005-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model-00006-of-00006.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model.safetensors.index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/feature_extractor/preprocessor_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/image_encoder/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/image_encoder/model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/image_encoder/model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/scheduler/scheduler_config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/svd.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/svd_image_decoder.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/svd_model_index.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/unet/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/unet/diffusion_pytorch_model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/unet/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/vae/diffusion_pytorch_model.fp16.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='svd/vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-03-28 15:01:02+00:00\", \"cardData\": \"base_model:\\n- tencent/HunyuanVideo\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlanguage:\\n- en\\ntag:\\n- text-to-video\\n- image-to-video\\n- video-to-video\", \"transformersInfo\": null, \"_id\": \"67da9f23c6f8119e09829903\", \"modelId\": \"NullVoider/V1\", \"usedStorage\": 59007130014}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=NullVoider/V1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BNullVoider%2FV1%5D(%2FNullVoider%2FV1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A"
            ],
            "spaces_count": 1
        },
        {
            "model_id": "TencentARC/GeometryCrafter",
            "card": "---\nbase_model:\n- tencent/DepthCrafter\n- stabilityai/stable-video-diffusion-img2vid-xt\nlanguage:\n- en\nlibrary_name: geometry-crafter\nlicense: other\npipeline_tag: video-to-3d\ntags:\n- point-cloud\n---\n\n## ___***GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors***___\n<div align=\"center\">\n\n_**[Tian-Xing Xu<sup>1</sup>](https://scholar.google.com/citations?user=zHp0rMIAAAAJ&hl=zh-CN), \n[Xiangjun Gao<sup>3</sup>](https://scholar.google.com/citations?user=qgdesEcAAAAJ&hl=en), \n[Wenbo Hu<sup>2 &dagger;</sup>](https://wbhu.github.io), \n[Xiaoyu Li<sup>2</sup>](https://xiaoyu258.github.io), \n[Song-Hai Zhang<sup>1 &dagger;</sup>](https://scholar.google.com/citations?user=AWtV-EQAAAAJ&hl=en), \n[Ying Shan<sup>2</sup>](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en)**_\n<br>\n<sup>1</sup>Tsinghua University\n<sup>2</sup>ARC Lab, Tencent PCG\n<sup>3</sup>HKUST\n\n![Version](https://img.shields.io/badge/version-1.0.0-blue) &nbsp;\n <a href='https://arxiv.org/abs/2504.01016'><img src='https://img.shields.io/badge/arXiv-2504.01016-b31b1b.svg'></a> &nbsp;\n <a href='https://geometrycrafter.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a> &nbsp;\n <a href='https://huggingface.co/spaces/TencentARC/GeometryCrafter'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a> &nbsp;\n\n</div>\n\n## \ud83d\udd06 Notice\n\n**GeometryCrafter is still under active development!**\n\nWe recommend that everyone use English to communicate on issues, as this helps developers from around the world discuss, share experiences, and answer questions together. For further implementation details, please contact `xutx21@mails.tsinghua.edu.cn`. For business licensing and other related inquiries, don't hesitate to contact `wbhu@tencent.com`.\n\nIf you find GeometryCrafter useful, **please help \u2b50 this repo**, which is important to Open-Source projects. Thanks!\n\n## \ud83d\udcdd Introduction\n\nWe present GeometryCrafter, a novel approach that estimates temporally consistent, high-quality point maps from open-world videos, facilitating downstream applications such as 3D/4D reconstruction and depth-based video editing or generation. This model is described in detail in the paper [GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors](https://arxiv.org/abs/2504.01016).\n\nRelease Notes:\n- `[01/04/2025]` \ud83d\udd25\ud83d\udd25\ud83d\udd25**GeometryCrafter** is released now, have fun!\n\n## \ud83d\ude80 Quick Start\n\n### Installation\n1. Clone this repo:\n```bash\ngit clone --recursive https://github.com/TencentARC/GeometryCrafter\n```\n2. Install dependencies (please refer to [requirements.txt](requirements.txt)):\n```bash\npip install -r requirements.txt\n```\n\n### Inference\n\nRun inference code on our provided demo videos at 1.27FPS, which requires a GPU with ~40GB memory for 110 frames with 1024x576 resolution:\n\n```bash\npython run.py \\\n  --video_path examples/video1.mp4 \\\n  --save_folder workspace/examples_output \\\n  --height 576 --width 1024\n  # resize the input video to the target resolution for processing, which should be divided by 64 \n  # the output point maps will be restored to the original resolution before saving\n  # you can use --downsample_ratio to downsample the input video or reduce --decode_chunk_size to save the memory usage\n```\n\nRun inference code with our deterministic variant at 1.50 FPS\n\n```bash\npython run.py \\\n  --video_path examples/video1.mp4 \\\n  --save_folder workspace/examples_output \\\n  --height 576 --width 1024 \\\n  --model_type determ\n```\n\nRun low-resolution processing at 2.49 FPS, which requires a GPU with ~22GB memory:\n\n```bash\npython run.py \\\n  --video_path examples/video1.mp4 \\\n  --save_folder workspace/examples_output \\\n  --height 384 --width 640\n```\n\n### Visualization\n\nVisualize the predicted point maps with `Viser`\n\n```bash\npython visualize/vis_point_maps.py \\\n  --video_path examples/video1.mp4 \\\n  --data_path workspace/examples_output/video1.npz\n```\n\n## \ud83e\udd16 Gradio Demo\n\n- Online demo: [**GeometryCrafter**](https://huggingface.co/spaces/TencentARC/GeometryCrafter)\n- Local demo:\n  ```bash\n  gradio app.py\n  ```\n\n## \ud83d\udcca Dataset Evaluation\n\nPlease check the `evaluation` folder. \n- To create the dataset we use in the paper, you need to run `evaluation/preprocess/gen_{dataset_name}.py`.\n- You need to change `DATA_DIR` and `OUTPUT_DIR` first accordint to your working environment.\n- Then you will get the preprocessed datasets containing extracted RGB video and point map npz files. We also provide the catelog of these files.\n- Inference for all datasets scripts:\n  ```bash\n  bash evaluation/run_batch.sh\n  ```\n  (Remember to replace the `data_root_dir` and `save_root_dir` with your path.)\n- Evaluation for all datasets scripts (scale-invariant point map estimation):\n  ```bash\n  bash evaluation/eval.sh\n  ```\n   (Remember to replace the `pred_data_root_dir` and `gt_data_root_dir` with your path.)\n- Evaluation for all datasets scripts (affine-invariant depth estimation):\n  ```bash\n  bash evaluation/eval_depth.sh\n  ```\n   (Remember to replace the `pred_data_root_dir` and `gt_data_root_dir` with your path.)\n- We also provide the comparison results of MoGe and the deterministic variant of our method. You can evaluate these methods under the same protocol by uncomment the corresponding lines in `evaluation/run.sh` `evaluation/eval.sh` `evaluation/run_batch.sh` and `evaluation/eval_depth.sh`.\n\n## \ud83e\udd1d Contributing\n\n- Welcome to open issues and pull requests.\n- Welcome to optimize the inference speed and memory usage, e.g., through model quantization, distillation, or other acceleration techniques.\n\n## \ud83d\udcdc Citation\n\nIf you find this work helpful, please consider citing:\n\n```BibTeXw\n  TODO\n```",
            "metadata": "{\"id\": \"TencentARC/GeometryCrafter\", \"author\": \"TencentARC\", \"sha\": \"4150ea0736890a986110f0117d41baf424cf9565\", \"last_modified\": \"2025-04-08 10:48:42+00:00\", \"created_at\": \"2025-03-31 08:41:06+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 159, \"downloads_all_time\": null, \"likes\": 8, \"library_name\": \"geometry-crafter\", \"gguf\": null, \"inference\": null, \"tags\": [\"geometry-crafter\", \"diffusers\", \"safetensors\", \"point-cloud\", \"video-to-3d\", \"en\", \"arxiv:2504.01016\", \"base_model:stabilityai/stable-video-diffusion-img2vid-xt\", \"base_model:finetune:stabilityai/stable-video-diffusion-img2vid-xt\", \"license:other\", \"region:us\"], \"pipeline_tag\": null, \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- tencent/DepthCrafter\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlanguage:\\n- en\\nlibrary_name: geometry-crafter\\nlicense: other\\npipeline_tag: video-to-3d\\ntags:\\n- point-cloud\", \"widget_data\": null, \"model_index\": null, \"config\": null, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='NOTICE', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='point_map_vae/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='point_map_vae/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet_determ/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet_determ/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet_diff/config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='unet_diff/diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"TencentARC/GeometryCrafter\", \"slothfulxtx/demo\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2025-04-08 10:48:42+00:00\", \"cardData\": \"base_model:\\n- tencent/DepthCrafter\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlanguage:\\n- en\\nlibrary_name: geometry-crafter\\nlicense: other\\npipeline_tag: video-to-3d\\ntags:\\n- point-cloud\", \"transformersInfo\": null, \"_id\": \"67ea552265dae663aa4f0593\", \"modelId\": \"TencentARC/GeometryCrafter\", \"usedStorage\": 6491914200}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "TencentARC/GeometryCrafter",
                "huggingface/InferenceSupport/discussions/new?title=TencentARC/GeometryCrafter&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5BTencentARC%2FGeometryCrafter%5D(%2FTencentARC%2FGeometryCrafter)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",
                "slothfulxtx/demo"
            ],
            "spaces_count": 3
        },
        {
            "model_id": "jhshao/ChronoDepth-v1",
            "card": "---\nlicense: mit\nlibrary_name: diffusers\npipeline_tag: depth-estimation\ntags:\n- video depth estimation\nbase_model:\n- stabilityai/stable-video-diffusion-img2vid-xt\n---\n# ChronoDepth: Learning Temporally Consistent Video Depth from Video Diffusion Priors\n\nThis model represents the official checkpoint of the paper titled \"Learning Temporally Consistent Video Depth from Video Diffusion Priors\".\n\n[![Website](https://img.shields.io/website?url=https%3A%2F%2Fjhaoshao.github.io%2FChronoDepth%2F&up_message=ChronoDepth&up_color=blue&style=flat&logo=timescale&logoColor=%23FFDC0F)](https://xdimlab.github.io/ChronoDepth/) [![Paper](https://img.shields.io/badge/arXiv-PDF-b31b1b)](https://arxiv.org/abs/2406.01493)[![GitHub](https://img.shields.io/github/stars/jhaoshao/ChronoDepth?style=default&label=GitHub%20\u2605&logo=github)](https://github.com/jhaoshao/ChronoDepth)\n\n[Jiahao Shao*](https://jiahao-shao1.github.io/), Yuanbo Yang*, Hongyu Zhou, [Youmin Zhang](https://youmi-zym.github.io/),  [Yujun Shen](https://shenyujun.github.io/), [Vitor Guizilini](https://vitorguizilini.github.io/), [Yue Wang](https://yuewang.xyz/), [Matteo Poggi](https://mattpoggi.github.io/), [Yiyi Liao\u2020](https://yiyiliao.github.io/ )\n\n## \ud83c\udf93 Citation\n\nPlease cite our paper if you find this repository useful:\n\n```bibtex\n@misc{shao2024learningtemporallyconsistentvideo,\n      title={Learning Temporally Consistent Video Depth from Video Diffusion Priors}, \n      author={Jiahao Shao and Yuanbo Yang and Hongyu Zhou and Youmin Zhang and Yujun Shen and Vitor Guizilini and Yue Wang and Matteo Poggi and Yiyi Liao},\n      year={2024},\n      eprint={2406.01493},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2406.01493}, \n}\n```",
            "metadata": "{\"id\": \"jhshao/ChronoDepth-v1\", \"author\": \"jhshao\", \"sha\": \"8ed51301cb97ec7a61a94a1bff1f7a107f73e9e8\", \"last_modified\": \"2024-12-13 10:02:49+00:00\", \"created_at\": \"2024-12-02 18:00:55+00:00\", \"private\": false, \"gated\": false, \"disabled\": false, \"downloads\": 28, \"downloads_all_time\": null, \"likes\": 1, \"library_name\": \"diffusers\", \"gguf\": null, \"inference\": null, \"tags\": [\"diffusers\", \"safetensors\", \"video depth estimation\", \"depth-estimation\", \"arxiv:2406.01493\", \"base_model:stabilityai/stable-video-diffusion-img2vid-xt\", \"base_model:finetune:stabilityai/stable-video-diffusion-img2vid-xt\", \"license:mit\", \"region:us\"], \"pipeline_tag\": \"depth-estimation\", \"mask_token\": null, \"trending_score\": null, \"card_data\": \"base_model:\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlibrary_name: diffusers\\nlicense: mit\\npipeline_tag: depth-estimation\\ntags:\\n- video depth estimation\", \"widget_data\": null, \"model_index\": null, \"config\": {}, \"transformers_info\": null, \"siblings\": [\"RepoSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='README.md', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='config.json', size=None, blob_id=None, lfs=None)\", \"RepoSibling(rfilename='diffusion_pytorch_model.safetensors', size=None, blob_id=None, lfs=None)\"], \"spaces\": [\"jhshao/ChronoDepth\"], \"safetensors\": null, \"security_repo_status\": null, \"lastModified\": \"2024-12-13 10:02:49+00:00\", \"cardData\": \"base_model:\\n- stabilityai/stable-video-diffusion-img2vid-xt\\nlibrary_name: diffusers\\nlicense: mit\\npipeline_tag: depth-estimation\\ntags:\\n- video depth estimation\", \"transformersInfo\": null, \"_id\": \"674df5d7a8512f78604fed6a\", \"modelId\": \"jhshao/ChronoDepth-v1\", \"usedStorage\": 3049435868}",
            "depth": 1,
            "children": [],
            "children_count": 0,
            "adapters": [],
            "adapters_count": 0,
            "quantized": [],
            "quantized_count": 0,
            "merges": [],
            "merges_count": 0,
            "spaces": [
                "huggingface/InferenceSupport/discussions/new?title=jhshao/ChronoDepth-v1&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjhshao%2FChronoDepth-v1%5D(%2Fjhshao%2FChronoDepth-v1)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A",
                "jhshao/ChronoDepth"
            ],
            "spaces_count": 2
        }
    ]
}